{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yGv4BRk8g3I7",
   "metadata": {
    "id": "yGv4BRk8g3I7"
   },
   "source": [
    "# Colab Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2FzOxFFzz4tc",
   "metadata": {
    "id": "2FzOxFFzz4tc"
   },
   "source": [
    "## Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liIojGmNz3Yx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liIojGmNz3Yx",
    "outputId": "6e7f5519-3dd1-4323-8b3b-b5ea73876285"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JXd4r-8D0EIv",
   "metadata": {
    "id": "JXd4r-8D0EIv"
   },
   "source": [
    "## Change Root Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "egHrtl0kyY1l",
   "metadata": {
    "id": "egHrtl0kyY1l"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = '/content/drive/MyDrive/Storage/Github/hyuckjinkim/data-scientist-competitions/Dacon/23_퍼즐이미지맞추기/'\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {
    "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jHVvRwH8j-yC",
   "metadata": {
    "id": "jHVvRwH8j-yC"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f393b96-3a73-4b9b-8c2a-655d79502b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RV-k4QWShufb",
   "metadata": {
    "id": "RV-k4QWShufb"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {
    "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khj/anaconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "N9xY92L7jBuJ",
   "metadata": {
    "id": "N9xY92L7jBuJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def mkdir(paths,verbose=True)->None:\n",
    "    if isinstance(paths,str):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            if verbose:\n",
    "                print('directory created: {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "n5swyqDeg1FP",
   "metadata": {
    "id": "n5swyqDeg1FP"
   },
   "outputs": [],
   "source": [
    "def flush():\n",
    "    while True:\n",
    "        k = gc.collect()\n",
    "        if k==0:\n",
    "            break\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tKSjMuwehTC2",
   "metadata": {
    "id": "tKSjMuwehTC2"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''\n",
    "    Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {
    "id": "fc7df3f2-62d0-4499-a46e-47d01699def0"
   },
   "source": [
    "## Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {
    "id": "c3367399-9798-4e38-967b-fd2320b9a2b2"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    EPOCHS = 10\n",
    "    LR = 1e-3\n",
    "    BATCH_SIZE = 64\n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "JMM0ClxghRex",
   "metadata": {
    "id": "JMM0ClxghRex"
   },
   "outputs": [],
   "source": [
    "seed_everything(CFG.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {
    "id": "05a4172e-5791-446f-9616-35c09d8bf25a"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Ta8XECUhzO1",
   "metadata": {
    "id": "5Ta8XECUhzO1"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {
    "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df['img_path'] = train_df['img_path'].apply(lambda x: x.replace('./','./data/'))\n",
    "test_df ['img_path'] = test_df ['img_path'].apply(lambda x: x.replace('./','./data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WefmiG96dWsa",
   "metadata": {
    "id": "WefmiG96dWsa"
   },
   "outputs": [],
   "source": [
    "# train_df = train_df[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKh6gfmYh2UY",
   "metadata": {
    "id": "FKh6gfmYh2UY"
   },
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbcd2bc-6324-4cf3-8032-05080b019c5f",
   "metadata": {
    "id": "dbbcd2bc-6324-4cf3-8032-05080b019c5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_df, val_df = train_test_split(train_df,test_size=0.3,shuffle=True,random_state=CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca4c3bb-d300-4c2a-9147-94d90e6faab6",
   "metadata": {
    "id": "3ca4c3bb-d300-4c2a-9147-94d90e6faab6"
   },
   "outputs": [],
   "source": [
    "tr_labels  = tr_df .iloc[:,2:].values.reshape(-1, 4, 4)\n",
    "val_labels = val_df.iloc[:,2:].values.reshape(-1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd787b46-3bf0-4f36-9dec-69a03bf921e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd787b46-3bf0-4f36-9dec-69a03bf921e0",
    "outputId": "53c39f1b-9a64-4181-fd89-11c470ed468a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49000, 4, 4), (21000, 4, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tr_labels.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {
    "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {
    "id": "16fd60a5-24e2-4539-bfd0-1c374a641699"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transform=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            label = torch.tensor(self.label_list[index], dtype=torch.long) - 1\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "uBKcYt9FLyPX",
   "metadata": {
    "id": "uBKcYt9FLyPX"
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, img_paths, labels, transform=None, n_jobs=os.cpu_count()):\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "\n",
    "#         self.images = Parallel(n_jobs=n_jobs)(\n",
    "#             delayed(self._get_image)(img_path) for img_path in tqdm(img_paths)\n",
    "#         )\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             self.labels = []\n",
    "#             for label in labels:\n",
    "#                 label = torch.tensor(label, dtype=torch.long) - 1\n",
    "#                 self.labels.append(label)\n",
    "\n",
    "#     def _get_image(self,img_path):\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         if self.labels is not None:\n",
    "#             return self.images[index], self.labels[index]\n",
    "#         else:\n",
    "#             return self.images[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c01b4067-0669-44a9-bbbc-3065d8cb00c2",
   "metadata": {
    "id": "c01b4067-0669-44a9-bbbc-3065d8cb00c2"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {
    "id": "9d880481-1965-499d-9caa-fdfa8526f789"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(tr_df['img_path'].values, tr_labels, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_df['img_path'].values, val_labels, test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset = CustomDataset(test_df['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG.BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AOxZlKBEhhlw",
   "metadata": {
    "id": "AOxZlKBEhhlw"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJm3N7NchnwZ",
   "metadata": {
    "id": "pJm3N7NchnwZ"
   },
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fg0ySP-4kLYC",
   "metadata": {
    "id": "fg0ySP-4kLYC"
   },
   "outputs": [],
   "source": [
    "class PuzzleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PuzzleModel, self).__init__()\n",
    "        self.model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\")\n",
    "\n",
    "        self.final_pool = nn.MaxPool2d(2)\n",
    "        self.final_conv = nn.Conv2d(1, 16, kernel_size=28, stride=28)\n",
    "        self.final_bn = nn.BatchNorm2d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.final_pool(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_bn(x) # (B,16,4,4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {
    "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "#         self.bn = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# class DeconvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels):\n",
    "#         super(DeconvBlock, self).__init__()\n",
    "#         self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "#         self.conv_mid = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "#         self.bn_mid = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu_mid = nn.ReLU()\n",
    "#         self.conv_block = ConvBlock(mid_channels + mid_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.up(x)\n",
    "#         x = self.relu_mid(self.bn_mid(self.conv_mid(x)))\n",
    "#         x = torch.cat((x, skip), dim=1)\n",
    "#         return self.conv_block(x)\n",
    "\n",
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         # Contraction path\n",
    "#         self.conv1 = ConvBlock(3, 16)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv2 = ConvBlock(16, 32)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.conv3 = ConvBlock(32, 64)\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "#         self.conv4 = ConvBlock(64, 128)\n",
    "#         self.pool4 = nn.MaxPool2d(2)\n",
    "#         self.conv5 = ConvBlock(128, 256)\n",
    "\n",
    "#         # Expansion path\n",
    "#         self.up6 = DeconvBlock(256, 128, 128)\n",
    "#         self.up7 = DeconvBlock(128, 64, 64)\n",
    "#         self.up8 = DeconvBlock(64, 32, 32)\n",
    "#         self.up9 = DeconvBlock(32, 16, 16)\n",
    "\n",
    "#         self.final_pool = nn.MaxPool2d(2)\n",
    "#         self.final_conv = nn.Conv2d(16, 16, kernel_size=28, stride=28)\n",
    "#         self.final_bn = nn.BatchNorm2d(16)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Contraction path\n",
    "#         x1 = self.conv1(x)\n",
    "#         x = self.pool1(x1)\n",
    "#         x2 = self.conv2(x)\n",
    "#         x = self.pool2(x2)\n",
    "#         x3 = self.conv3(x)\n",
    "#         x = self.pool3(x3)\n",
    "#         x4 = self.conv4(x)\n",
    "#         x = self.pool4(x4)\n",
    "#         x5 = self.conv5(x)\n",
    "\n",
    "#         # Expansion path\n",
    "#         x = self.up6(x5, x4)\n",
    "#         x = self.up7(x, x3)\n",
    "#         x = self.up8(x, x2)\n",
    "#         x = self.up9(x, x1)\n",
    "\n",
    "#         x = self.final_pool(x)\n",
    "#         out = self.final_bn(self.final_conv(x)) # (B,16,4,4)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {
    "id": "122af0aa-a1fd-4595-9488-35761e3cb596"
   },
   "source": [
    "## Train Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vMxmdgQQb9xI",
   "metadata": {
    "id": "vMxmdgQQb9xI"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path:str=None, trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: None\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        # if self.verbose:\n",
    "        #     self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        if self.path is not None:\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44S7jMz2cAA5",
   "metadata": {
    "id": "44S7jMz2cAA5"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, train_loader, valid_loader, criterion, epochs,\n",
    "    optimizer, scheduler=None, early_stopping=None, device='cpu', metric_period=1,\n",
    "    verbose=True, save_dir = './checkpoints',\n",
    "):\n",
    "    global img, label, output\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    best_loss  = 999999999\n",
    "    best_epoch = 1\n",
    "    best_model = None\n",
    "    is_best    = np.nan\n",
    "    best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training step\n",
    "        train_loss_list = []\n",
    "        train_acc_list = []\n",
    "        model.train()\n",
    "        for img, label in tqdm(iter(train_loader),desc='Train Batch'):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)#.float()\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            loss.backward()  # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            predicted_label = torch.argmax(output, dim=1)\n",
    "            for prediction, actual in zip(predicted_label, label):\n",
    "                acc = ((prediction == actual).sum() / 16).item()\n",
    "                train_acc_list.append(acc)\n",
    "\n",
    "        train_loss = np.mean(train_loss_list)\n",
    "        train_acc  = np.mean(train_acc_list)\n",
    "\n",
    "        # valiation step\n",
    "        valid_loss_list = []\n",
    "        valid_acc_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for img, label in tqdm(iter(valid_loader),desc='Validation'):\n",
    "                img = img.to(device)\n",
    "                label = label.to(device)\n",
    "                output = model(img)#.float()\n",
    "\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "                valid_loss_list.append(loss.item())\n",
    "\n",
    "                predicted_label = torch.argmax(output, dim=1)\n",
    "                for prediction, actual in zip(predicted_label, label):\n",
    "                    acc = ((prediction == actual).sum() / 16).item()\n",
    "                    valid_acc_list.append(acc)\n",
    "\n",
    "            valid_loss = np.mean(valid_loss_list)\n",
    "            valid_acc  = np.mean(valid_acc_list)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        # update the best epoch & best loss\n",
    "        if (best_loss > valid_loss) | (epoch==1):\n",
    "            best_epoch = epoch\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            is_best = 1\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "            save_path = os.path.join(save_dir,'epoch({})-val_loss({:.3f}).pt'.format(epoch,valid_loss))\n",
    "            torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            is_best = 0\n",
    "\n",
    "        # 결과물 printing\n",
    "        if (verbose) & (epoch % metric_period == 0):\n",
    "            mark = '*' if is_best else ' '\n",
    "            epoch_str = str(epoch).zfill(len(str(epochs)))\n",
    "            progress = '{}[{}/{}] loss: {:.4f}, acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                .format(\n",
    "                    mark,\n",
    "                    epoch_str,\n",
    "                    epochs,\n",
    "                    train_loss,\n",
    "                    train_acc,\n",
    "                    valid_loss,\n",
    "                    valid_acc,\n",
    "                    best_epoch,\n",
    "                    epoch_e-epoch_s,\n",
    "                    epoch_e-start_time,\n",
    "                    (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                )\n",
    "            epoch_s = time.time()\n",
    "            print(progress)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        #print(model.state_dict()['linear.weight'])\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {
    "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749"
   },
   "outputs": [],
   "source": [
    "# def train(model, optimizer, train_loader, val_loader, device):\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#     best_val_acc = 0\n",
    "#     best_model = None\n",
    "#     for epoch in range(1, CFG['EPOCHS']+1):\n",
    "#         model.train()\n",
    "#         train_loss = []\n",
    "#         for imgs, labels in tqdm(iter(train_loader),desc='train batch'):\n",
    "#             imgs = imgs.float().to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             output = model(imgs)\n",
    "#             loss = criterion(output, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss.append(loss.item())\n",
    "\n",
    "#         _val_loss, _val_acc = validation(model, criterion, val_loader, device)\n",
    "#         _train_loss = np.mean(train_loss)\n",
    "#         print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val ACC : [{_val_acc:.5f}]')\n",
    "\n",
    "#         if best_val_acc < _val_acc:\n",
    "#             best_val_acc = _val_acc\n",
    "#             best_model = model\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "# def validation(model, criterion, val_loader, device):\n",
    "#     model.eval()\n",
    "#     val_loss = []\n",
    "#     val_acc = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, labels in tqdm(iter(val_loader),desc='validation batch'):\n",
    "#             imgs = imgs.float().to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             output = model(imgs)\n",
    "\n",
    "#             loss = criterion(output, labels)\n",
    "\n",
    "#             val_loss.append(loss.item())\n",
    "\n",
    "#             # 정확도 계산을 위한 예측 레이블 추출\n",
    "#             predicted_labels = torch.argmax(output, dim=1)\n",
    "\n",
    "#             # 샘플 별 정확도 계산\n",
    "#             for predicted_label, label in zip(predicted_labels, labels):\n",
    "#                 val_acc.append(((predicted_label == label).sum() / 16).item())\n",
    "\n",
    "#         _val_loss = np.mean(val_loss)\n",
    "#         _val_acc = np.mean(val_acc)\n",
    "\n",
    "#     return _val_loss, _val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {
    "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24"
   },
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52Ph1KCCDsaW",
   "metadata": {
    "id": "52Ph1KCCDsaW"
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# train_img_paths = glob.glob('./data/train/*')\n",
    "# test_img_paths  = glob.glob('./data/test/*')\n",
    "\n",
    "# len(train_img_paths), len(test_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Ls0G-E5YgCgl",
   "metadata": {
    "id": "Ls0G-E5YgCgl"
   },
   "outputs": [],
   "source": [
    "model = PuzzleModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = CFG.LR)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-7, verbose=True)\n",
    "scheduler = None\n",
    "\n",
    "# early_stopping = EarlyStopping(patience=10, verbose=True, path=None)\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "Z85IngqOjEeo",
   "metadata": {
    "id": "Z85IngqOjEeo"
   },
   "outputs": [],
   "source": [
    "mkdir('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml1ZGHbugWRO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5c5bbfdd4f3f45c39255614b8f823775",
      "926e7e57c66645e5bc157610580c5a25",
      "6f53ce82711f49ac85e6dbba672bd7af",
      "8ef8b0ae7fad4e04ae0926ea5cadae27",
      "9ecbd71ac1c54f87aa6b2258d74bd7ea",
      "9c43ba98bee44751b467857816bb5f90",
      "d8afcc3e6c044bc081ec876d97bf8711",
      "eb902ff9dd614b2b8134beefe7713abf",
      "a765b253ecba47b28a5cf6d390d5085b",
      "70af956b27364bec9c04ce4413f97398",
      "0c882cfd231b45d49d230e647472ca47"
     ]
    },
    "id": "ml1ZGHbugWRO",
    "outputId": "6d98e0ed-cfe1-4cba-acd3-b52c811394f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batch:  30%|███████▏                | 230/766 [1:03:51<2:27:16, 16.49s/it]"
     ]
    }
   ],
   "source": [
    "flush()\n",
    "\n",
    "infer_model = train(\n",
    "    model,\n",
    "    train_loader, val_loader,\n",
    "    criterion,\n",
    "    CFG.EPOCHS,\n",
    "    optimizer, scheduler, early_stopping,\n",
    "    CFG.DEVICE,\n",
    "    metric_period = 1,\n",
    "    verbose = True,\n",
    "    save_dir = 'checkpoints',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375e55-9c73-443d-a836-2ec9bc20faee",
   "metadata": {
    "id": "14375e55-9c73-443d-a836-2ec9bc20faee"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bzrf-4mgiIB5",
   "metadata": {
    "id": "Bzrf-4mgiIB5"
   },
   "source": [
    "## Define Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e3678-a3c7-4f32-8e4a-253b1321b4c3",
   "metadata": {
    "id": "200e3678-a3c7-4f32-8e4a-253b1321b4c3"
   },
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "\n",
    "            output = model(imgs)\n",
    "\n",
    "            # 정확도 계산을 위한 예측 레이블 추출\n",
    "            predicted_labels = torch.argmax(output, dim=1).view(-1, 16)\n",
    "            predicted_labels = predicted_labels.cpu().detach().numpy()\n",
    "\n",
    "            preds.extend(predicted_labels)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8f47f-56fe-41f5-9daf-a550d72e1f78",
   "metadata": {
    "id": "d2f8f47f-56fe-41f5-9daf-a550d72e1f78"
   },
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad9d51-f03c-4e90-be42-e4e1824b07da",
   "metadata": {
    "id": "ffad9d51-f03c-4e90-be42-e4e1824b07da"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfc7bc-3436-40ea-864b-d6b1a721011a",
   "metadata": {
    "id": "febfc7bc-3436-40ea-864b-d6b1a721011a"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511aff4-c7d4-48eb-ae9b-f63171304f36",
   "metadata": {
    "id": "7511aff4-c7d4-48eb-ae9b-f63171304f36"
   },
   "outputs": [],
   "source": [
    "submit.iloc[:, 1:] = preds\n",
    "submit.iloc[:, 1:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2838bc5-dfc9-4615-8677-28aa8445984c",
   "metadata": {
    "id": "a2838bc5-dfc9-4615-8677-28aa8445984c"
   },
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch_env_3.10.13",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c882cfd231b45d49d230e647472ca47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c5bbfdd4f3f45c39255614b8f823775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_926e7e57c66645e5bc157610580c5a25",
       "IPY_MODEL_6f53ce82711f49ac85e6dbba672bd7af",
       "IPY_MODEL_8ef8b0ae7fad4e04ae0926ea5cadae27"
      ],
      "layout": "IPY_MODEL_9ecbd71ac1c54f87aa6b2258d74bd7ea"
     }
    },
    "6f53ce82711f49ac85e6dbba672bd7af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb902ff9dd614b2b8134beefe7713abf",
      "max": 766,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a765b253ecba47b28a5cf6d390d5085b",
      "value": 285
     }
    },
    "70af956b27364bec9c04ce4413f97398": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ef8b0ae7fad4e04ae0926ea5cadae27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70af956b27364bec9c04ce4413f97398",
      "placeholder": "​",
      "style": "IPY_MODEL_0c882cfd231b45d49d230e647472ca47",
      "value": " 285/766 [2:47:56&lt;4:41:09, 35.07s/it]"
     }
    },
    "926e7e57c66645e5bc157610580c5a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c43ba98bee44751b467857816bb5f90",
      "placeholder": "​",
      "style": "IPY_MODEL_d8afcc3e6c044bc081ec876d97bf8711",
      "value": "Train Batch:  37%"
     }
    },
    "9c43ba98bee44751b467857816bb5f90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ecbd71ac1c54f87aa6b2258d74bd7ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a765b253ecba47b28a5cf6d390d5085b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8afcc3e6c044bc081ec876d97bf8711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb902ff9dd614b2b8134beefe7713abf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
