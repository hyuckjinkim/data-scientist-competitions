{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b2772b",
   "metadata": {
    "id": "pYzhJrEibIlq"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01305f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdkit\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a5d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99bd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413d551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 0\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 1024\n",
    "    LEARNING_RATE = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfd58f",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a13af",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d98712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9980ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Num_H_Acceptors</th>\n",
       "      <th>Num_H_Donors</th>\n",
       "      <th>Num_RotatableBonds</th>\n",
       "      <th>LogD</th>\n",
       "      <th>Molecular_PolarSurfaceArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n",
       "      <td>26.010</td>\n",
       "      <td>50.680</td>\n",
       "      <td>3.259</td>\n",
       "      <td>400.495</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.259</td>\n",
       "      <td>117.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n",
       "      <td>29.270</td>\n",
       "      <td>50.590</td>\n",
       "      <td>2.169</td>\n",
       "      <td>301.407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.172</td>\n",
       "      <td>73.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n",
       "      <td>5.586</td>\n",
       "      <td>80.892</td>\n",
       "      <td>1.593</td>\n",
       "      <td>297.358</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.585</td>\n",
       "      <td>62.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n",
       "      <td>5.710</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.771</td>\n",
       "      <td>494.652</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.475</td>\n",
       "      <td>92.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n",
       "      <td>93.270</td>\n",
       "      <td>99.990</td>\n",
       "      <td>2.335</td>\n",
       "      <td>268.310</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.337</td>\n",
       "      <td>42.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             SMILES     MLM  \\\n",
       "0  TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n",
       "1  TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n",
       "2  TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n",
       "3  TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n",
       "4  TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n",
       "\n",
       "      HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
       "0  50.680  3.259           400.495                5             2   \n",
       "1  50.590  2.169           301.407                2             1   \n",
       "2  80.892  1.593           297.358                5             0   \n",
       "3   2.000  4.771           494.652                6             0   \n",
       "4  99.990  2.335           268.310                3             0   \n",
       "\n",
       "   Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  \n",
       "0                   8  3.259                      117.37  \n",
       "1                   2  2.172                       73.47  \n",
       "2                   3  1.585                       62.45  \n",
       "3                   5  3.475                       92.60  \n",
       "4                   1  2.337                       42.43  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44f0f6",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a010c10",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d5805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6898b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = train_df.columns[train_df.dtypes!='object'].tolist()\n",
    "# for i,col in enumerate(num_features):\n",
    "\n",
    "#     fig = plt.figure(figsize=(15,7))\n",
    "#     fig.add_subplot(121)\n",
    "#     sns.histplot(train_df[col],bins=20)\n",
    "#     plt.grid()\n",
    "\n",
    "#     fig.add_subplot(122)\n",
    "#     sns.histplot(np.log(train_df[col]+1e-3),bins=20)\n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.suptitle('[{}/{}] {}'.format(i+1,len(num_features),col))\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # -> ['Molecular_Weight','Molecular_PolarSurfaceArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74fc8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "# for col in cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=train_df[col],y=train_df['HLM'])\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27f5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds']\n",
    "# for col in cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=train_df[col],y=train_df.MLM)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5854cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ba1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists = sorted(train_df['Num_H_Acceptors'].unique())\n",
    "# for v in lists:\n",
    "#     print('########',v)\n",
    "#     d = train_df[train_df['Num_H_Acceptors']==v]\n",
    "    \n",
    "#     cols = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "#     for col in cols:\n",
    "#         print(col)\n",
    "#         plt.figure(figsize=(15,7))\n",
    "#         sns.scatterplot(x=d[col],y=d['HLM'])\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8b6d5",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb7af0",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb4868",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab60100",
   "metadata": {},
   "source": [
    "## Set target range to [0,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7d464",
   "metadata": {},
   "source": [
    "- [Dacon](https://dacon.io/competitions/official/236127/talkboard/409051?page=1&dtype=recent)에 따르면 100이 넘는 값도 나올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9571e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = ['MLM','HLM']\n",
    "# for t in targets:\n",
    "#     train_df[t] = [0 if x<0 else\n",
    "#                    100 if x>100 else\n",
    "#                    x for x in train_df[t]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec0f46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8080b",
   "metadata": {},
   "source": [
    "## Make molecule features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d48a94",
   "metadata": {
    "id": "oXOFfJVW22DL"
   },
   "outputs": [],
   "source": [
    "# Molecule to MorganFingerprint\n",
    "def mol2fp(mol):\n",
    "    fp = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n",
    "    ar = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892f4490",
   "metadata": {
    "id": "7KbqRv6I19Rg"
   },
   "outputs": [],
   "source": [
    "# (1) SMILES를 통해 Molecule(분자구조) 생성\n",
    "PandasTools.AddMoleculeColumnToFrame(train_df,'SMILES','Molecule')\n",
    "PandasTools.AddMoleculeColumnToFrame(test_df ,'SMILES','Molecule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f6dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d06e76",
   "metadata": {
    "id": "1MTlg0wx22DM"
   },
   "outputs": [],
   "source": [
    "# (2) Morgan Fingerprint column 추가\n",
    "train_df[\"FPs\"] = train_df.Molecule.apply(mol2fp)\n",
    "test_df [\"FPs\"] = test_df .Molecule.apply(mol2fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41944920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498\n"
     ]
    }
   ],
   "source": [
    "# (3) Morgan Fingerprint 중, variance가 0.05보다 작은 컬럼들을 지우기\n",
    "feature_select = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "# 일부사용\n",
    "tr_fps_selected = feature_select.fit_transform(np.stack(train_df['FPs']))\n",
    "te_fps_selected = feature_select.transform(np.stack(test_df['FPs']))\n",
    "print(len(tr_fps_selected))\n",
    "\n",
    "# # 전체사용\n",
    "# tr_fps_selected = np.stack(train_df['FPs'])\n",
    "# te_fps_selected = np.stack(test_df ['FPs'])\n",
    "\n",
    "fps_names = ['fps'+str(i+1) for i in range(tr_fps_selected.shape[1])]\n",
    "\n",
    "train_df = pd.concat([\n",
    "    train_df.drop('FPs',axis=1),\n",
    "    pd.DataFrame(tr_fps_selected,columns=fps_names),\n",
    "],axis=1)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df.drop('FPs',axis=1),\n",
    "    pd.DataFrame(te_fps_selected,columns=fps_names),\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81aa07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fps_raw_sum'] = np.sum(train_df[fps_names].values,axis=1)\n",
    "# test_df ['fps_raw_sum'] = np.sum(test_df [fps_names].values,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4153e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps_dummy_names = []\n",
    "# for col in tqdm(fps_names):\n",
    "#     train_df[f'{col}_dmy'] = np.where(train_df[col]==0,0,1)\n",
    "#     test_df [f'{col}_dmy'] = np.where(test_df [col]==0,0,1)\n",
    "#     fps_dummy_names.append(f'{col}_dmy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82bbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fps_dummy_sum'] = np.sum(train_df[fps_names].values,axis=1)\n",
    "# test_df ['fps_dummy_sum'] = np.sum(test_df [fps_names].values,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47dac22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 column만 추출\n",
    "features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors',\n",
    "            'Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "fps_features = [col for col in train_df.columns if col.find('fps')==0]\n",
    "smiles_feature = 'SMILES'\n",
    "targets  = ['MLM','HLM']\n",
    "\n",
    "train_df = train_df[features+fps_features+[smiles_feature]+targets]\n",
    "test_df  = test_df[features+fps_features+[smiles_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed203e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3498, 261)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3ebca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02336469",
   "metadata": {},
   "source": [
    "## Imputaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f4d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f16c003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_check(data):\n",
    "    d = data.copy()\n",
    "    null_info = d.isnull().sum()\n",
    "    null_info = null_info[null_info!=0]\n",
    "    display(null_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c71eab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlogP    2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlogP    1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('> train')\n",
    "null_check(train_df)\n",
    "\n",
    "print('> test')\n",
    "null_check(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc1c95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_features = ['AlogP']\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "train_df[null_features] = imputer.fit_transform(train_df[null_features])\n",
    "test_df [null_features] = imputer.transform(test_df[null_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91e284b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('> train')\n",
    "null_check(train_df)\n",
    "\n",
    "print('> test')\n",
    "null_check(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a17fa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1504d",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c236a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf998ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, va_df = train_test_split(train_df,test_size=0.1,shuffle=True,random_state=CFG.SEED)\n",
    "te_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22440925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148, 350, 483)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_df), len(va_df), len(te_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fb7d8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57040d0a",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f150c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaling_features = features#+fps_features\n",
    "scalers = {}\n",
    "for f in scaling_features:\n",
    "    scaler = MinMaxScaler()\n",
    "    tr_df[f] = scaler.fit_transform(np.array(tr_df[f]).reshape(-1,1))\n",
    "    va_df[f] = scaler.transform(np.array(va_df[f]).reshape(-1,1))\n",
    "    te_df[f] = scaler.transform(np.array(te_df[f]).reshape(-1,1))\n",
    "    scalers[f] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97071d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276563d",
   "metadata": {},
   "source": [
    "## Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65586c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import trange\n",
    "\n",
    "def get_abs_corr(x,y):\n",
    "    return np.abs(np.corrcoef(x,y))[0,1]\n",
    "\n",
    "class InteractionTerm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,num_features,corr_cutoff=0.7):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        \n",
    "        d = data.copy()\n",
    "        self.interaction_list = []\n",
    "        for i in trange(len(num_features),desc='fitting...'):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    col_i = num_features[i]\n",
    "                    col_j = num_features[j]\n",
    "                    \n",
    "                    # 상관계수가 cutoff보다 큰 경우에는 interaction을 생성하지 않음\n",
    "                    if (get_abs_corr(d[col_i]*d[col_j],d[col_i])>=corr_cutoff) | (get_abs_corr(d[col_i]*d[col_j],d[col_j])>=corr_cutoff):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.interaction_list.append(f'{col_i}*{col_j}')\n",
    "    \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        print('> the number of interaction term:',len(self.interaction_list))\n",
    "        for interaction in self.interaction_list:\n",
    "            col_i,col_j = interaction.split('*')\n",
    "            d[interaction] = d[col_i]*d[col_j]\n",
    "        return d\n",
    "    \n",
    "    def fit_transform(self,data,num_features,corr_cutoff=0.7):\n",
    "        self.fit(data,num_features,corr_cutoff)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0a6a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = features + fps_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6617d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_maker = InteractionTerm()\n",
    "# interaction_maker.fit(\n",
    "#     data=tr_df,\n",
    "#     num_features=num_features,\n",
    "#     corr_cutoff=0.15,\n",
    "# )\n",
    "# tr_df = interaction_maker.transform(tr_df)\n",
    "# va_df = interaction_maker.transform(va_df)\n",
    "# te_df = interaction_maker.transform(te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c32d4351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148, 261)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663563da",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503959b",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6c5ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in targets:\n",
    "#     tr_df[t] = np.log(tr_df[t]+1e-3)\n",
    "#     va_df[t] = np.log(va_df[t]+1e-3)\n",
    "\n",
    "# def inverse_transform(x):\n",
    "#     return torch.exp(x)-1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c26f7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "# def boxcox_transform(x):\n",
    "#     _lambda = 0.25\n",
    "#     return boxcox(x,_lambda)\n",
    "\n",
    "# def inverse_boxcox_transform(x):\n",
    "#     _lambda = 0.25\n",
    "#     return inv_boxcox(x,_lambda)\n",
    "\n",
    "# tr_df[targets] = tr_df[targets].apply(boxcox_transform)\n",
    "# va_df[targets] = va_df[targets].apply(boxcox_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81def623",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d5f72",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a506a",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "275f2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, smiles, transforms=None, is_test=False):\n",
    "        self.data = data.copy()\n",
    "        self.targets = targets\n",
    "        self.smiles = smiles\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.smiles_features = []\n",
    "        for s in tqdm(data[smiles].values):\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            img = Draw.MolToImage(m)#, size=(224,224))\n",
    "            img = np.array(img)\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(image=img)['image']\n",
    "            self.smiles_features.append(img)\n",
    "            \n",
    "        if not self.is_test:\n",
    "            self.target_features = self.data[self.targets].values\n",
    "            self.num_features = self.data.drop(columns=targets+[smiles],axis=1).values\n",
    "        else:\n",
    "            self.num_features = self.data.drop(columns=[smiles],axis=1).values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            return (\n",
    "                torch.Tensor(self.num_features[index]),\n",
    "                torch.Tensor(self.smiles_features[index]),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.Tensor(self.num_features[index]),\n",
    "                torch.Tensor(self.smiles_features[index]),\n",
    "                torch.Tensor(self.target_features[index]),\n",
    "            )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77eecd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e030fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  이미지 변환\n",
    "transform = A.Compose([\n",
    "    A.Resize(CFG.IMG_SIZE,CFG.IMG_SIZE),\n",
    "    A.ToGray(p=1),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b77890c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3148/3148 [00:29<00:00, 106.00it/s]\n",
      "100%|██████████| 350/350 [00:03<00:00, 104.25it/s]\n",
      "100%|██████████| 483/483 [00:04<00:00, 106.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(tr_df, ['MLM','HLM'], 'SMILES', transform, False)\n",
    "val_dataset   = CustomDataset(va_df, ['MLM','HLM'], 'SMILES', transform, False)\n",
    "test_dataset  = CustomDataset(te_df, ['MLM','HLM'], 'SMILES', transform, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a5e0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset  , batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b13c1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.smiles_features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbbce830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [feat for feat,img,target in train_loader][0]\n",
    "# [img for feat,img in test_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "355c1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _img = [img for feat,img,target in train_loader][0][0].numpy()\n",
    "# plt.imshow(_img.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08a535",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93a27d",
   "metadata": {
    "id": "8CHzFfvrbnOM"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63780ea0",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f617f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d623b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, feature_input_size, output_size, hidden_size, dropout_rate):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.image_output_size = 1000\n",
    "        self.feature_output_size = 1000\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # # efficientnet\n",
    "        # self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        # self.backbone.classifier = nn.Sequential(\n",
    "        #     nn.Dropout(p=0.2,inplace=True),\n",
    "        #     nn.Linear(self.backbone.classifier[-1].in_features,self.image_output_size),\n",
    "        # )\n",
    "        \n",
    "        # resnet\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features,self.image_output_size)\n",
    "        \n",
    "        self.image_layer = nn.Sequential(\n",
    "            self.backbone,\n",
    "            #nn.BatchNorm1d(self.image_output_size),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Dropout(self.dropout_rate),\n",
    "        )\n",
    "        \n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(feature_input_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,self.feature_output_size),\n",
    "        )\n",
    "        \n",
    "        fc_input_size = self.image_output_size+self.feature_output_size\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(fc_input_size,fc_input_size//2),\n",
    "            nn.BatchNorm1d(fc_input_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//2,fc_input_size//4),\n",
    "            nn.BatchNorm1d(fc_input_size//4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//4,fc_input_size//8),\n",
    "            nn.BatchNorm1d(fc_input_size//8),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//8,fc_input_size//16),\n",
    "        )\n",
    "        self.fc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_input_size,fc_input_size//2),\n",
    "            nn.BatchNorm1d(fc_input_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//2,fc_input_size//4),\n",
    "            nn.BatchNorm1d(fc_input_size//4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//4,fc_input_size//8),\n",
    "            nn.BatchNorm1d(fc_input_size//8),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//8,fc_input_size//16),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size//8,fc_input_size//16),\n",
    "            nn.BatchNorm1d(fc_input_size//16),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//16,fc_input_size//32),\n",
    "            nn.BatchNorm1d(fc_input_size//32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//32,output_size),\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, feature):\n",
    "        image_output = self.image_layer(image)\n",
    "        feature_output = self.feature_layer(feature)\n",
    "        combined = torch.cat((image_output,feature_output),dim=1)\n",
    "        output1 = self.fc_1(combined)\n",
    "        output2 = self.fc_2(combined)\n",
    "        output = torch.cat((output1,output2),dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339bb9c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9320485",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bc7bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiRMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss1 = torch.sqrt(torch.mean((output[:,0]-target[:,0])**2))\n",
    "        loss2 = torch.sqrt(torch.mean((output[:,1]-target[:,1])**2))\n",
    "        loss = 0.5*loss1+0.5*loss2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6e15f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        if self.path!='':\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        \n",
    "def train(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, epochs,\n",
    "    early_stopping, device='cpu', scheduler=None, metric_period=1, \n",
    "    verbose=True, save_model_path = './mc/best_model.pt',\n",
    "    use_best_model=True,\n",
    "    inverse_transform=None,\n",
    "):  \n",
    "    seed_everything(CFG.SEED)\n",
    "    model.to(device)\n",
    "\n",
    "    best_loss  = 999999999\n",
    "    best_epoch = 1\n",
    "    best_model = None\n",
    "    is_best    = np.nan\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        gc.collect()\n",
    "        \n",
    "        #model.train()\n",
    "        train_loss = []\n",
    "        for feat,img,target in train_loader:\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img,feat)#.float()\n",
    "            \n",
    "            if inverse_transform is not None:\n",
    "                output = inverse_transform(output)\n",
    "                target = inverse_transform(target)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()  # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            valid_loss = validation(model, valid_loader, criterion, device, inverse_transform)\n",
    "        else:\n",
    "            valid_loss = loss\n",
    "            \n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        # update the best epoch & best loss\n",
    "        if (best_loss > valid_loss) | (epoch==1):\n",
    "            best_epoch = epoch\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            is_best = 1\n",
    "            torch.save(best_model.state_dict(), save_model_path)\n",
    "        else:\n",
    "            is_best = 0\n",
    "            if not use_best_model:\n",
    "                torch.save(best_model.state_dict(), save_model_path)\n",
    "            \n",
    "        # 결과물 printing\n",
    "        if (verbose) & (epoch % metric_period == 0):\n",
    "            mark = '*' if is_best else ' '\n",
    "            epoch_str = str(epoch).zfill(len(str(epochs)))\n",
    "            if valid_loader is not None:\n",
    "                progress = '{}[{}/{}] loss: {:.5f}, val_loss: {:.5f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                    .format(\n",
    "                        mark,\n",
    "                        epoch_str,\n",
    "                        epochs,\n",
    "                        np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        best_epoch,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                    )\n",
    "            else:\n",
    "                progress = '{}[{}/{}] loss: {:.5f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                    .format(\n",
    "                        mark,\n",
    "                        epoch_str,\n",
    "                        epochs,\n",
    "                        np.mean(train_loss),\n",
    "                        best_epoch,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                    )\n",
    "            epoch_s = time.time()\n",
    "            print(progress)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation(model, valid_loader, criterion, device, inverse_transform):\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for feat,img,target in valid_loader:\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(img,feat)#.float()\n",
    "            \n",
    "            if inverse_transform is not None:\n",
    "                output = inverse_transform(output)\n",
    "                target = inverse_transform(target)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)\n",
    "\n",
    "def predict(best_model,loader,device,inverse_transform):\n",
    "    best_model.to(device)\n",
    "\n",
    "    true_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for feat,img,target in iter(loader):\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = best_model(img,feat)\n",
    "            \n",
    "            if inverse_transform is not None:\n",
    "                output = inverse_transform(output)\n",
    "                target = inverse_transform(target)\n",
    "\n",
    "            true_list.append(target)\n",
    "            pred_list.append(output)\n",
    "\n",
    "    trues = torch.cat(true_list,dim=0)\n",
    "    preds = torch.cat(pred_list,dim=0)\n",
    "\n",
    "    trues = trues.cpu().numpy()\n",
    "    preds = preds.cpu().numpy()\n",
    "\n",
    "    return trues, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46b79538",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_input_size = [feat for feat,smiles,target in train_dataset][0].shape[0]\n",
    "output_size = 2\n",
    "hidden_size = 1024*4\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7a890ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "623ae5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskModel(feature_input_size,output_size,hidden_size,dropout_rate)\n",
    "criterion = MultiRMSELoss()\n",
    "# optimizer= torch.optim.Adam(model.parameters(), lr=CFG.LEARNING_RATE)#, weight_decay=5e-4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=CFG.LEARNING_RATE, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-7, verbose=False)\n",
    "# early_stopping = EarlyStopping(patience=10,verbose=False,path='')\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "574b44bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[0001/1024] loss: 56.17857, val_loss: 52.69664, best_epoch: 1, elapsed: 10.84s, total: 10.84s, remaining: 11093.69s\n",
      "*[0002/1024] loss: 50.34136, val_loss: 46.15892, best_epoch: 2, elapsed: 10.87s, total: 30.18s, remaining: 11104.26s\n",
      "*[0003/1024] loss: 41.81252, val_loss: 38.76560, best_epoch: 3, elapsed: 10.86s, total: 50.72s, remaining: 11086.99s\n",
      "*[0004/1024] loss: 34.19629, val_loss: 33.33059, best_epoch: 4, elapsed: 10.87s, total: 71.48s, remaining: 11086.94s\n",
      "*[0005/1024] loss: 29.51281, val_loss: 32.83685, best_epoch: 5, elapsed: 10.89s, total: 85.90s, remaining: 11100.91s\n",
      " [0006/1024] loss: 28.06670, val_loss: 33.41343, best_epoch: 5, elapsed: 10.93s, total: 97.81s, remaining: 11123.98s\n",
      " [0007/1024] loss: 26.54598, val_loss: 34.02107, best_epoch: 5, elapsed: 10.90s, total: 109.52s, remaining: 11090.09s\n",
      " [0008/1024] loss: 26.23562, val_loss: 34.36341, best_epoch: 5, elapsed: 10.91s, total: 121.26s, remaining: 11088.15s\n",
      " [0009/1024] loss: 24.68973, val_loss: 33.29876, best_epoch: 5, elapsed: 10.94s, total: 133.00s, remaining: 11100.35s\n",
      " [0010/1024] loss: 23.48197, val_loss: 33.75716, best_epoch: 5, elapsed: 10.96s, total: 144.74s, remaining: 11112.81s\n",
      " [0011/1024] loss: 22.51190, val_loss: 34.42862, best_epoch: 5, elapsed: 10.95s, total: 156.51s, remaining: 11095.11s\n",
      " [0012/1024] loss: 21.78128, val_loss: 34.95487, best_epoch: 5, elapsed: 10.96s, total: 168.29s, remaining: 11091.45s\n",
      " [0013/1024] loss: 20.97192, val_loss: 35.38601, best_epoch: 5, elapsed: 10.97s, total: 180.25s, remaining: 11090.85s\n",
      " [0014/1024] loss: 20.09157, val_loss: 35.66298, best_epoch: 5, elapsed: 11.00s, total: 192.06s, remaining: 11110.25s\n",
      " [0015/1024] loss: 19.61623, val_loss: 35.99858, best_epoch: 5, elapsed: 11.04s, total: 203.91s, remaining: 11134.98s\n",
      " [0016/1024] loss: 19.14450, val_loss: 36.09379, best_epoch: 5, elapsed: 11.06s, total: 215.81s, remaining: 11152.68s\n",
      " [0017/1024] loss: 18.59469, val_loss: 36.18048, best_epoch: 5, elapsed: 11.07s, total: 227.69s, remaining: 11148.72s\n",
      " [0018/1024] loss: 18.38553, val_loss: 36.31317, best_epoch: 5, elapsed: 11.07s, total: 239.57s, remaining: 11132.97s\n",
      " [0019/1024] loss: 18.16906, val_loss: 36.38262, best_epoch: 5, elapsed: 11.07s, total: 251.45s, remaining: 11124.33s\n",
      " [0020/1024] loss: 17.63437, val_loss: 36.48734, best_epoch: 5, elapsed: 11.06s, total: 263.49s, remaining: 11107.04s\n",
      " [0021/1024] loss: 17.33428, val_loss: 36.32361, best_epoch: 5, elapsed: 11.11s, total: 275.39s, remaining: 11138.59s\n",
      " [0022/1024] loss: 16.89992, val_loss: 36.37662, best_epoch: 5, elapsed: 11.05s, total: 287.31s, remaining: 11074.60s\n",
      " [0023/1024] loss: 16.51557, val_loss: 36.44159, best_epoch: 5, elapsed: 11.06s, total: 299.16s, remaining: 11066.43s\n",
      " [0024/1024] loss: 16.14801, val_loss: 36.53485, best_epoch: 5, elapsed: 11.06s, total: 311.03s, remaining: 11060.19s\n",
      " [0025/1024] loss: 15.93711, val_loss: 36.59080, best_epoch: 5, elapsed: 11.06s, total: 322.93s, remaining: 11051.46s\n",
      " [0026/1024] loss: 15.79635, val_loss: 36.65152, best_epoch: 5, elapsed: 11.09s, total: 334.82s, remaining: 11067.27s\n",
      " [0027/1024] loss: 15.64175, val_loss: 36.69692, best_epoch: 5, elapsed: 11.12s, total: 346.74s, remaining: 11089.59s\n",
      " [0028/1024] loss: 15.56798, val_loss: 36.72828, best_epoch: 5, elapsed: 11.13s, total: 358.90s, remaining: 11085.59s\n",
      " [0029/1024] loss: 15.50153, val_loss: 36.75880, best_epoch: 5, elapsed: 11.12s, total: 370.88s, remaining: 11067.51s\n",
      " [0030/1024] loss: 15.42485, val_loss: 36.77691, best_epoch: 5, elapsed: 11.11s, total: 382.81s, remaining: 11045.68s\n",
      " [0031/1024] loss: 15.39190, val_loss: 36.79418, best_epoch: 5, elapsed: 11.12s, total: 394.74s, remaining: 11043.97s\n",
      " [0032/1024] loss: 15.35988, val_loss: 36.81028, best_epoch: 5, elapsed: 11.10s, total: 406.64s, remaining: 11013.81s\n",
      " [0033/1024] loss: 15.32261, val_loss: 36.81873, best_epoch: 5, elapsed: 11.11s, total: 418.56s, remaining: 11007.09s\n",
      " [0034/1024] loss: 15.30703, val_loss: 36.82718, best_epoch: 5, elapsed: 11.09s, total: 430.46s, remaining: 10976.40s\n",
      " [0035/1024] loss: 15.29163, val_loss: 36.83532, best_epoch: 5, elapsed: 11.09s, total: 442.36s, remaining: 10969.43s\n",
      " [0036/1024] loss: 15.27352, val_loss: 36.83950, best_epoch: 5, elapsed: 11.11s, total: 454.27s, remaining: 10977.58s\n",
      " [0037/1024] loss: 15.26597, val_loss: 36.84366, best_epoch: 5, elapsed: 11.11s, total: 466.18s, remaining: 10962.66s\n",
      " [0038/1024] loss: 15.25849, val_loss: 36.84772, best_epoch: 5, elapsed: 11.10s, total: 478.08s, remaining: 10940.45s\n",
      " [0039/1024] loss: 15.24962, val_loss: 36.84977, best_epoch: 5, elapsed: 11.13s, total: 490.02s, remaining: 10964.06s\n",
      " [0040/1024] loss: 15.24593, val_loss: 36.85180, best_epoch: 5, elapsed: 11.13s, total: 501.97s, remaining: 10955.67s\n",
      " [0041/1024] loss: 15.24226, val_loss: 36.85382, best_epoch: 5, elapsed: 11.16s, total: 514.14s, remaining: 10973.81s\n",
      " [0042/1024] loss: 15.23789, val_loss: 36.85483, best_epoch: 5, elapsed: 11.15s, total: 526.12s, remaining: 10951.02s\n",
      " [0043/1024] loss: 15.23608, val_loss: 36.85584, best_epoch: 5, elapsed: 11.12s, total: 538.04s, remaining: 10909.18s\n",
      " [0044/1024] loss: 15.23427, val_loss: 36.85684, best_epoch: 5, elapsed: 11.10s, total: 549.95s, remaining: 10881.45s\n",
      " [0045/1024] loss: 15.23211, val_loss: 36.85733, best_epoch: 5, elapsed: 11.10s, total: 561.86s, remaining: 10871.71s\n",
      " [0046/1024] loss: 15.23121, val_loss: 36.85783, best_epoch: 5, elapsed: 11.09s, total: 573.72s, remaining: 10845.63s\n",
      " [0047/1024] loss: 15.23031, val_loss: 36.85833, best_epoch: 5, elapsed: 11.12s, total: 585.62s, remaining: 10861.53s\n",
      " [0048/1024] loss: 15.22924, val_loss: 36.85858, best_epoch: 5, elapsed: 11.16s, total: 597.68s, remaining: 10895.98s\n",
      " [0049/1024] loss: 15.22880, val_loss: 36.85883, best_epoch: 5, elapsed: 11.14s, total: 609.62s, remaining: 10862.08s\n",
      " [0050/1024] loss: 15.22835, val_loss: 36.85908, best_epoch: 5, elapsed: 11.17s, total: 621.60s, remaining: 10875.59s\n",
      " [0051/1024] loss: 15.22783, val_loss: 36.85921, best_epoch: 5, elapsed: 11.24s, total: 633.73s, remaining: 10933.79s\n",
      " [0052/1024] loss: 15.22759, val_loss: 36.85934, best_epoch: 5, elapsed: 11.18s, total: 645.72s, remaining: 10864.67s\n",
      " [0053/1024] loss: 15.22735, val_loss: 36.85946, best_epoch: 5, elapsed: 11.16s, total: 657.70s, remaining: 10837.33s\n",
      " [0054/1024] loss: 15.22711, val_loss: 36.85959, best_epoch: 5, elapsed: 11.20s, total: 669.71s, remaining: 10859.78s\n",
      " [0055/1024] loss: 15.22687, val_loss: 36.85972, best_epoch: 5, elapsed: 11.12s, total: 681.82s, remaining: 10778.20s\n",
      " [0056/1024] loss: 15.22664, val_loss: 36.85984, best_epoch: 5, elapsed: 11.13s, total: 693.75s, remaining: 10774.42s\n",
      " [0057/1024] loss: 15.22640, val_loss: 36.85997, best_epoch: 5, elapsed: 11.11s, total: 705.64s, remaining: 10746.04s\n",
      " [0058/1024] loss: 15.22616, val_loss: 36.86009, best_epoch: 5, elapsed: 11.11s, total: 717.59s, remaining: 10734.91s\n",
      " [0059/1024] loss: 15.22593, val_loss: 36.86022, best_epoch: 5, elapsed: 11.11s, total: 729.51s, remaining: 10723.18s\n",
      " [0060/1024] loss: 15.22569, val_loss: 36.86035, best_epoch: 5, elapsed: 11.12s, total: 741.44s, remaining: 10722.55s\n",
      " [0061/1024] loss: 15.22546, val_loss: 36.86047, best_epoch: 5, elapsed: 11.11s, total: 753.37s, remaining: 10699.83s\n",
      " [0062/1024] loss: 15.22522, val_loss: 36.86060, best_epoch: 5, elapsed: 11.11s, total: 765.30s, remaining: 10692.24s\n",
      " [0063/1024] loss: 15.22498, val_loss: 36.86072, best_epoch: 5, elapsed: 12.54s, total: 778.65s, remaining: 12046.68s\n",
      " [0064/1024] loss: 15.22475, val_loss: 36.86085, best_epoch: 5, elapsed: 13.13s, total: 792.58s, remaining: 12601.21s\n",
      " [0065/1024] loss: 15.22451, val_loss: 36.86097, best_epoch: 5, elapsed: 13.14s, total: 806.54s, remaining: 12602.03s\n",
      " [0066/1024] loss: 15.22428, val_loss: 36.86110, best_epoch: 5, elapsed: 13.29s, total: 820.70s, remaining: 12730.85s\n",
      " [0067/1024] loss: 15.22404, val_loss: 36.86122, best_epoch: 5, elapsed: 13.54s, total: 835.03s, remaining: 12954.06s\n",
      " [0068/1024] loss: 15.22381, val_loss: 36.86135, best_epoch: 5, elapsed: 13.29s, total: 849.18s, remaining: 12705.20s\n",
      " [0069/1024] loss: 15.22357, val_loss: 36.86147, best_epoch: 5, elapsed: 13.40s, total: 863.63s, remaining: 12793.11s\n",
      " [0070/1024] loss: 15.22334, val_loss: 36.86160, best_epoch: 5, elapsed: 13.22s, total: 877.66s, remaining: 12614.85s\n",
      " [0071/1024] loss: 15.22310, val_loss: 36.86172, best_epoch: 5, elapsed: 13.39s, total: 891.84s, remaining: 12763.67s\n",
      " [0072/1024] loss: 15.22287, val_loss: 36.86185, best_epoch: 5, elapsed: 13.25s, total: 905.90s, remaining: 12613.27s\n",
      " [0073/1024] loss: 15.22264, val_loss: 36.86197, best_epoch: 5, elapsed: 13.44s, total: 920.09s, remaining: 12780.31s\n",
      " [0074/1024] loss: 15.22240, val_loss: 36.86210, best_epoch: 5, elapsed: 13.18s, total: 934.10s, remaining: 12525.14s\n",
      " [0075/1024] loss: 15.22217, val_loss: 36.86222, best_epoch: 5, elapsed: 13.41s, total: 948.35s, remaining: 12722.48s\n",
      " [0076/1024] loss: 15.22193, val_loss: 36.86235, best_epoch: 5, elapsed: 13.22s, total: 962.44s, remaining: 12537.09s\n",
      " [0077/1024] loss: 15.22170, val_loss: 36.86247, best_epoch: 5, elapsed: 13.44s, total: 976.67s, remaining: 12728.05s\n",
      " [0078/1024] loss: 15.22147, val_loss: 36.86259, best_epoch: 5, elapsed: 13.24s, total: 990.77s, remaining: 12521.29s\n",
      " [0079/1024] loss: 15.22123, val_loss: 36.86272, best_epoch: 5, elapsed: 13.29s, total: 1004.85s, remaining: 12554.89s\n",
      " [0080/1024] loss: 15.22100, val_loss: 36.86284, best_epoch: 5, elapsed: 12.93s, total: 1018.67s, remaining: 12201.94s\n",
      " [0081/1024] loss: 15.22076, val_loss: 36.86297, best_epoch: 5, elapsed: 13.00s, total: 1032.22s, remaining: 12260.86s\n",
      " [0082/1024] loss: 15.22053, val_loss: 36.86309, best_epoch: 5, elapsed: 12.87s, total: 1045.91s, remaining: 12123.52s\n",
      " [0083/1024] loss: 15.22030, val_loss: 36.86322, best_epoch: 5, elapsed: 12.93s, total: 1059.65s, remaining: 12162.89s\n",
      " [0084/1024] loss: 15.22006, val_loss: 36.86334, best_epoch: 5, elapsed: 12.95s, total: 1073.42s, remaining: 12175.80s\n",
      " [0085/1024] loss: 15.21983, val_loss: 36.86346, best_epoch: 5, elapsed: 12.78s, total: 1087.02s, remaining: 11999.70s\n",
      " [0086/1024] loss: 15.21960, val_loss: 36.86359, best_epoch: 5, elapsed: 12.87s, total: 1100.69s, remaining: 12068.99s\n",
      " [0087/1024] loss: 15.21936, val_loss: 36.86371, best_epoch: 5, elapsed: 12.74s, total: 1114.24s, remaining: 11934.35s\n",
      " [0088/1024] loss: 15.21913, val_loss: 36.86384, best_epoch: 5, elapsed: 12.94s, total: 1128.01s, remaining: 12111.93s\n",
      " [0089/1024] loss: 15.21889, val_loss: 36.86396, best_epoch: 5, elapsed: 12.74s, total: 1141.52s, remaining: 11913.57s\n",
      " [0090/1024] loss: 15.21866, val_loss: 36.86409, best_epoch: 5, elapsed: 12.96s, total: 1155.32s, remaining: 12104.52s\n",
      " [0091/1024] loss: 15.21843, val_loss: 36.86421, best_epoch: 5, elapsed: 12.74s, total: 1168.88s, remaining: 11887.01s\n",
      " [0092/1024] loss: 15.21819, val_loss: 36.86434, best_epoch: 5, elapsed: 13.05s, total: 1182.74s, remaining: 12164.97s\n",
      " [0093/1024] loss: 15.21796, val_loss: 36.86446, best_epoch: 5, elapsed: 12.77s, total: 1196.39s, remaining: 11887.80s\n",
      " [0094/1024] loss: 15.21773, val_loss: 36.86458, best_epoch: 5, elapsed: 12.98s, total: 1210.13s, remaining: 12069.82s\n",
      " [0095/1024] loss: 15.21749, val_loss: 36.86471, best_epoch: 5, elapsed: 12.87s, total: 1223.88s, remaining: 11954.84s\n",
      " [0096/1024] loss: 15.21726, val_loss: 36.86483, best_epoch: 5, elapsed: 13.01s, total: 1237.70s, remaining: 12071.67s\n",
      " [0097/1024] loss: 15.21703, val_loss: 36.86496, best_epoch: 5, elapsed: 11.83s, total: 1250.40s, remaining: 10966.35s\n",
      " [0098/1024] loss: 15.21679, val_loss: 36.86508, best_epoch: 5, elapsed: 10.90s, total: 1262.11s, remaining: 10096.25s\n",
      " [0099/1024] loss: 15.21656, val_loss: 36.86521, best_epoch: 5, elapsed: 10.94s, total: 1273.88s, remaining: 10122.48s\n",
      " [0100/1024] loss: 15.21633, val_loss: 36.86533, best_epoch: 5, elapsed: 10.98s, total: 1285.72s, remaining: 10143.37s\n",
      " [0101/1024] loss: 15.21609, val_loss: 36.86545, best_epoch: 5, elapsed: 11.00s, total: 1297.57s, remaining: 10155.54s\n",
      " [0102/1024] loss: 15.21586, val_loss: 36.86558, best_epoch: 5, elapsed: 11.04s, total: 1309.45s, remaining: 10175.90s\n",
      " [0103/1024] loss: 15.21563, val_loss: 36.86570, best_epoch: 5, elapsed: 11.06s, total: 1321.36s, remaining: 10190.00s\n",
      " [0104/1024] loss: 15.21539, val_loss: 36.86583, best_epoch: 5, elapsed: 11.10s, total: 1333.31s, remaining: 10207.95s\n",
      " [0105/1024] loss: 15.21516, val_loss: 36.86595, best_epoch: 5, elapsed: 11.13s, total: 1345.26s, remaining: 10224.91s\n",
      " [0106/1024] loss: 15.21493, val_loss: 36.86607, best_epoch: 5, elapsed: 11.18s, total: 1357.26s, remaining: 10266.48s\n",
      " [0107/1024] loss: 15.21470, val_loss: 36.86620, best_epoch: 5, elapsed: 11.16s, total: 1369.27s, remaining: 10235.99s\n",
      " [0108/1024] loss: 15.21446, val_loss: 36.86632, best_epoch: 5, elapsed: 11.18s, total: 1381.27s, remaining: 10238.36s\n",
      " [0109/1024] loss: 15.21423, val_loss: 36.86644, best_epoch: 5, elapsed: 11.19s, total: 1393.27s, remaining: 10242.39s\n",
      " [0110/1024] loss: 15.21400, val_loss: 36.86657, best_epoch: 5, elapsed: 11.20s, total: 1405.30s, remaining: 10234.04s\n",
      " [0111/1024] loss: 15.21376, val_loss: 36.86669, best_epoch: 5, elapsed: 11.18s, total: 1417.28s, remaining: 10208.66s\n",
      " [0112/1024] loss: 15.21353, val_loss: 36.86681, best_epoch: 5, elapsed: 11.17s, total: 1429.26s, remaining: 10189.49s\n",
      " [0113/1024] loss: 15.21330, val_loss: 36.86694, best_epoch: 5, elapsed: 11.18s, total: 1441.29s, remaining: 10182.61s\n",
      " [0114/1024] loss: 15.21306, val_loss: 36.86706, best_epoch: 5, elapsed: 11.33s, total: 1453.41s, remaining: 10313.58s\n",
      " [0115/1024] loss: 15.21283, val_loss: 36.86718, best_epoch: 5, elapsed: 11.17s, total: 1465.45s, remaining: 10155.12s\n",
      " [0116/1024] loss: 15.21260, val_loss: 36.86731, best_epoch: 5, elapsed: 11.16s, total: 1477.43s, remaining: 10135.75s\n",
      " [0117/1024] loss: 15.21237, val_loss: 36.86743, best_epoch: 5, elapsed: 11.17s, total: 1489.40s, remaining: 10135.13s\n",
      " [0118/1024] loss: 15.21213, val_loss: 36.86755, best_epoch: 5, elapsed: 11.13s, total: 1501.35s, remaining: 10085.31s\n",
      " [0119/1024] loss: 15.21190, val_loss: 36.86768, best_epoch: 5, elapsed: 10.96s, total: 1513.15s, remaining: 9923.12s\n",
      " [0120/1024] loss: 15.21167, val_loss: 36.86780, best_epoch: 5, elapsed: 10.86s, total: 1524.82s, remaining: 9814.87s\n",
      " [0121/1024] loss: 15.21143, val_loss: 36.86792, best_epoch: 5, elapsed: 10.86s, total: 1536.47s, remaining: 9805.89s\n",
      " [0122/1024] loss: 15.21120, val_loss: 36.86805, best_epoch: 5, elapsed: 10.77s, total: 1548.05s, remaining: 9715.56s\n",
      " [0123/1024] loss: 15.21097, val_loss: 36.86817, best_epoch: 5, elapsed: 10.76s, total: 1559.61s, remaining: 9692.69s\n",
      " [0124/1024] loss: 15.21074, val_loss: 36.86830, best_epoch: 5, elapsed: 10.74s, total: 1571.16s, remaining: 9667.14s\n",
      " [0125/1024] loss: 15.21050, val_loss: 36.86842, best_epoch: 5, elapsed: 10.74s, total: 1582.71s, remaining: 9651.01s\n",
      " [0126/1024] loss: 15.21027, val_loss: 36.86854, best_epoch: 5, elapsed: 10.73s, total: 1594.29s, remaining: 9638.81s\n",
      " [0127/1024] loss: 15.21004, val_loss: 36.86866, best_epoch: 5, elapsed: 10.74s, total: 1605.84s, remaining: 9631.14s\n",
      " [0128/1024] loss: 15.20981, val_loss: 36.86879, best_epoch: 5, elapsed: 10.73s, total: 1617.39s, remaining: 9612.07s\n",
      " [0129/1024] loss: 15.20957, val_loss: 36.86891, best_epoch: 5, elapsed: 10.72s, total: 1628.99s, remaining: 9591.68s\n",
      " [0130/1024] loss: 15.20934, val_loss: 36.86903, best_epoch: 5, elapsed: 10.71s, total: 1640.56s, remaining: 9578.05s\n",
      " [0131/1024] loss: 15.20911, val_loss: 36.86916, best_epoch: 5, elapsed: 10.70s, total: 1652.12s, remaining: 9555.96s\n",
      " [0132/1024] loss: 15.20888, val_loss: 36.86928, best_epoch: 5, elapsed: 10.71s, total: 1663.64s, remaining: 9557.46s\n",
      " [0133/1024] loss: 15.20864, val_loss: 36.86940, best_epoch: 5, elapsed: 10.70s, total: 1675.14s, remaining: 9531.04s\n",
      " [0134/1024] loss: 15.20841, val_loss: 36.86953, best_epoch: 5, elapsed: 10.71s, total: 1686.66s, remaining: 9530.43s\n",
      " [0135/1024] loss: 15.20818, val_loss: 36.86965, best_epoch: 5, elapsed: 10.71s, total: 1698.17s, remaining: 9519.28s\n",
      " [0136/1024] loss: 15.20795, val_loss: 36.86977, best_epoch: 5, elapsed: 10.70s, total: 1709.69s, remaining: 9505.24s\n",
      " [0137/1024] loss: 15.20771, val_loss: 36.86990, best_epoch: 5, elapsed: 10.76s, total: 1721.26s, remaining: 9544.44s\n",
      " [0138/1024] loss: 15.20748, val_loss: 36.87002, best_epoch: 5, elapsed: 10.71s, total: 1732.78s, remaining: 9492.90s\n",
      " [0139/1024] loss: 15.20725, val_loss: 36.87014, best_epoch: 5, elapsed: 10.73s, total: 1744.28s, remaining: 9498.49s\n",
      " [0140/1024] loss: 15.20702, val_loss: 36.87027, best_epoch: 5, elapsed: 10.73s, total: 1756.76s, remaining: 9488.86s\n",
      " [0141/1024] loss: 15.20678, val_loss: 36.87039, best_epoch: 5, elapsed: 10.74s, total: 1769.50s, remaining: 9486.75s\n",
      " [0142/1024] loss: 15.20655, val_loss: 36.87051, best_epoch: 5, elapsed: 10.74s, total: 1781.63s, remaining: 9476.86s\n",
      " [0143/1024] loss: 15.20632, val_loss: 36.87063, best_epoch: 5, elapsed: 10.74s, total: 1793.30s, remaining: 9465.88s\n",
      " [0144/1024] loss: 15.20609, val_loss: 36.87076, best_epoch: 5, elapsed: 10.76s, total: 1804.97s, remaining: 9472.14s\n",
      " [0145/1024] loss: 15.20585, val_loss: 36.87088, best_epoch: 5, elapsed: 10.78s, total: 1816.69s, remaining: 9478.85s\n",
      " [0146/1024] loss: 15.20562, val_loss: 36.87100, best_epoch: 5, elapsed: 10.78s, total: 1831.10s, remaining: 9463.82s\n",
      " [0147/1024] loss: 15.20539, val_loss: 36.87113, best_epoch: 5, elapsed: 10.80s, total: 1842.70s, remaining: 9474.41s\n",
      " [0148/1024] loss: 15.20516, val_loss: 36.87125, best_epoch: 5, elapsed: 10.82s, total: 1854.32s, remaining: 9475.91s\n",
      " [0149/1024] loss: 15.20492, val_loss: 36.87137, best_epoch: 5, elapsed: 10.84s, total: 1865.94s, remaining: 9484.53s\n",
      " [0150/1024] loss: 15.20469, val_loss: 36.87149, best_epoch: 5, elapsed: 10.99s, total: 1877.71s, remaining: 9605.47s\n",
      " [0151/1024] loss: 15.20446, val_loss: 36.87162, best_epoch: 5, elapsed: 10.89s, total: 1889.41s, remaining: 9506.12s\n",
      " [0152/1024] loss: 15.20423, val_loss: 36.87174, best_epoch: 5, elapsed: 10.98s, total: 1901.15s, remaining: 9571.96s\n",
      " [0153/1024] loss: 15.20399, val_loss: 36.87186, best_epoch: 5, elapsed: 10.97s, total: 1912.95s, remaining: 9556.46s\n",
      " [0154/1024] loss: 15.20376, val_loss: 36.87198, best_epoch: 5, elapsed: 11.01s, total: 1924.78s, remaining: 9579.72s\n",
      " [0155/1024] loss: 15.20353, val_loss: 36.87211, best_epoch: 5, elapsed: 11.04s, total: 1936.59s, remaining: 9590.11s\n",
      " [0156/1024] loss: 15.20330, val_loss: 36.87223, best_epoch: 5, elapsed: 11.06s, total: 1948.43s, remaining: 9603.26s\n",
      " [0157/1024] loss: 15.20307, val_loss: 36.87235, best_epoch: 5, elapsed: 11.10s, total: 1960.29s, remaining: 9620.53s\n",
      " [0158/1024] loss: 15.20283, val_loss: 36.87247, best_epoch: 5, elapsed: 11.87s, total: 1972.98s, remaining: 10277.70s\n",
      " [0159/1024] loss: 15.20260, val_loss: 36.87260, best_epoch: 5, elapsed: 13.54s, total: 1987.34s, remaining: 11715.87s\n",
      " [0160/1024] loss: 15.20237, val_loss: 36.87272, best_epoch: 5, elapsed: 13.42s, total: 2001.53s, remaining: 11597.95s\n",
      " [0161/1024] loss: 15.20214, val_loss: 36.87284, best_epoch: 5, elapsed: 13.64s, total: 2015.96s, remaining: 11768.02s\n",
      " [0162/1024] loss: 15.20190, val_loss: 36.87296, best_epoch: 5, elapsed: 13.62s, total: 2030.35s, remaining: 11742.18s\n",
      " [0163/1024] loss: 15.20167, val_loss: 36.87308, best_epoch: 5, elapsed: 13.31s, total: 2044.42s, remaining: 11462.47s\n",
      " [0164/1024] loss: 15.20144, val_loss: 36.87321, best_epoch: 5, elapsed: 13.18s, total: 2058.35s, remaining: 11339.00s\n",
      " [0165/1024] loss: 15.20121, val_loss: 36.87333, best_epoch: 5, elapsed: 12.92s, total: 2072.04s, remaining: 11101.46s\n",
      " [0166/1024] loss: 15.20098, val_loss: 36.87345, best_epoch: 5, elapsed: 13.12s, total: 2085.90s, remaining: 11260.55s\n",
      " [0167/1024] loss: 15.20074, val_loss: 36.87357, best_epoch: 5, elapsed: 10.82s, total: 2097.53s, remaining: 9269.46s\n",
      " [0168/1024] loss: 15.20051, val_loss: 36.87369, best_epoch: 5, elapsed: 10.73s, total: 2109.02s, remaining: 9185.43s\n",
      " [0169/1024] loss: 15.20028, val_loss: 36.87381, best_epoch: 5, elapsed: 12.41s, total: 2122.24s, remaining: 10606.59s\n",
      " [0170/1024] loss: 15.20005, val_loss: 36.87394, best_epoch: 5, elapsed: 12.96s, total: 2135.98s, remaining: 11071.58s\n",
      " [0171/1024] loss: 15.19982, val_loss: 36.87406, best_epoch: 5, elapsed: 12.76s, total: 2149.50s, remaining: 10880.43s\n",
      " [0172/1024] loss: 15.19958, val_loss: 36.87418, best_epoch: 5, elapsed: 12.97s, total: 2163.19s, remaining: 11046.33s\n",
      " [0173/1024] loss: 15.19935, val_loss: 36.87430, best_epoch: 5, elapsed: 12.84s, total: 2176.84s, remaining: 10929.70s\n",
      " [0174/1024] loss: 15.19912, val_loss: 36.87442, best_epoch: 5, elapsed: 13.04s, total: 2190.60s, remaining: 11081.09s\n",
      " [0175/1024] loss: 15.19889, val_loss: 36.87454, best_epoch: 5, elapsed: 13.04s, total: 2204.45s, remaining: 11066.85s\n",
      " [0176/1024] loss: 15.19866, val_loss: 36.87466, best_epoch: 5, elapsed: 12.85s, total: 2218.07s, remaining: 10892.73s\n",
      " [0177/1024] loss: 15.19842, val_loss: 36.87479, best_epoch: 5, elapsed: 13.00s, total: 2231.80s, remaining: 11011.28s\n",
      " [0178/1024] loss: 15.19819, val_loss: 36.87491, best_epoch: 5, elapsed: 12.78s, total: 2245.35s, remaining: 10809.80s\n",
      " [0179/1024] loss: 15.19796, val_loss: 36.87503, best_epoch: 5, elapsed: 13.08s, total: 2259.18s, remaining: 11049.69s\n",
      " [0180/1024] loss: 15.19773, val_loss: 36.87515, best_epoch: 5, elapsed: 12.88s, total: 2272.89s, remaining: 10873.77s\n",
      " [0181/1024] loss: 15.19750, val_loss: 36.87527, best_epoch: 5, elapsed: 13.01s, total: 2286.64s, remaining: 10968.09s\n",
      " [0182/1024] loss: 15.19727, val_loss: 36.87539, best_epoch: 5, elapsed: 13.07s, total: 2300.48s, remaining: 11005.50s\n",
      " [0183/1024] loss: 15.19703, val_loss: 36.87551, best_epoch: 5, elapsed: 13.05s, total: 2314.13s, remaining: 10975.81s\n",
      " [0184/1024] loss: 15.19680, val_loss: 36.87563, best_epoch: 5, elapsed: 13.10s, total: 2327.97s, remaining: 11002.16s\n",
      " [0185/1024] loss: 15.19657, val_loss: 36.87575, best_epoch: 5, elapsed: 12.95s, total: 2341.71s, remaining: 10862.04s\n",
      " [0186/1024] loss: 15.19634, val_loss: 36.87587, best_epoch: 5, elapsed: 13.24s, total: 2355.70s, remaining: 11098.48s\n",
      " [0187/1024] loss: 15.19611, val_loss: 36.87600, best_epoch: 5, elapsed: 13.02s, total: 2369.49s, remaining: 10894.09s\n",
      " [0188/1024] loss: 15.19588, val_loss: 36.87612, best_epoch: 5, elapsed: 13.39s, total: 2383.63s, remaining: 11196.42s\n",
      " [0189/1024] loss: 15.19564, val_loss: 36.87624, best_epoch: 5, elapsed: 13.27s, total: 2397.68s, remaining: 11077.54s\n",
      " [0190/1024] loss: 15.19541, val_loss: 36.87636, best_epoch: 5, elapsed: 13.54s, total: 2412.00s, remaining: 11295.28s\n",
      " [0191/1024] loss: 15.19518, val_loss: 36.87648, best_epoch: 5, elapsed: 13.39s, total: 2426.21s, remaining: 11153.25s\n",
      " [0192/1024] loss: 15.19495, val_loss: 36.87660, best_epoch: 5, elapsed: 13.59s, total: 2440.44s, remaining: 11308.55s\n",
      " [0193/1024] loss: 15.19472, val_loss: 36.87672, best_epoch: 5, elapsed: 13.54s, total: 2454.78s, remaining: 11253.58s\n",
      " [0194/1024] loss: 15.19449, val_loss: 36.87684, best_epoch: 5, elapsed: 13.39s, total: 2468.99s, remaining: 11114.10s\n",
      " [0195/1024] loss: 15.19425, val_loss: 36.87696, best_epoch: 5, elapsed: 13.76s, total: 2483.49s, remaining: 11403.03s\n",
      " [0196/1024] loss: 15.19402, val_loss: 36.87708, best_epoch: 5, elapsed: 13.57s, total: 2497.88s, remaining: 11236.39s\n",
      " [0197/1024] loss: 15.19379, val_loss: 36.87720, best_epoch: 5, elapsed: 13.73s, total: 2512.39s, remaining: 11357.86s\n",
      " [0198/1024] loss: 15.19356, val_loss: 36.87732, best_epoch: 5, elapsed: 13.51s, total: 2526.68s, remaining: 11162.57s\n",
      " [0199/1024] loss: 15.19333, val_loss: 36.87744, best_epoch: 5, elapsed: 13.69s, total: 2541.10s, remaining: 11295.57s\n",
      " [0200/1024] loss: 15.19310, val_loss: 36.87756, best_epoch: 5, elapsed: 13.48s, total: 2555.36s, remaining: 11103.61s\n",
      " [0201/1024] loss: 15.19287, val_loss: 36.87769, best_epoch: 5, elapsed: 13.65s, total: 2569.76s, remaining: 11234.06s\n",
      " [0202/1024] loss: 15.19263, val_loss: 36.87781, best_epoch: 5, elapsed: 13.65s, total: 2584.22s, remaining: 11219.42s\n",
      " [0203/1024] loss: 15.19240, val_loss: 36.87793, best_epoch: 5, elapsed: 13.52s, total: 2598.51s, remaining: 11099.50s\n",
      " [0204/1024] loss: 15.19217, val_loss: 36.87805, best_epoch: 5, elapsed: 13.66s, total: 2612.91s, remaining: 11199.23s\n",
      " [0205/1024] loss: 15.19194, val_loss: 36.87817, best_epoch: 5, elapsed: 13.47s, total: 2627.15s, remaining: 11031.72s\n",
      " [0206/1024] loss: 15.19171, val_loss: 36.87829, best_epoch: 5, elapsed: 13.62s, total: 2641.53s, remaining: 11143.79s\n",
      " [0207/1024] loss: 15.19148, val_loss: 36.87841, best_epoch: 5, elapsed: 13.44s, total: 2655.74s, remaining: 10984.12s\n",
      " [0208/1024] loss: 15.19125, val_loss: 36.87853, best_epoch: 5, elapsed: 13.68s, total: 2670.16s, remaining: 11164.14s\n",
      " [0209/1024] loss: 15.19101, val_loss: 36.87865, best_epoch: 5, elapsed: 13.49s, total: 2684.42s, remaining: 10990.55s\n",
      " [0210/1024] loss: 15.19078, val_loss: 36.87877, best_epoch: 5, elapsed: 13.65s, total: 2698.85s, remaining: 11112.19s\n",
      " [0211/1024] loss: 15.19055, val_loss: 36.87889, best_epoch: 5, elapsed: 13.63s, total: 2713.24s, remaining: 11077.86s\n",
      " [0212/1024] loss: 15.19032, val_loss: 36.87901, best_epoch: 5, elapsed: 13.66s, total: 2727.68s, remaining: 11088.73s\n",
      " [0213/1024] loss: 15.19009, val_loss: 36.87913, best_epoch: 5, elapsed: 13.62s, total: 2742.15s, remaining: 11044.07s\n",
      " [0214/1024] loss: 15.18986, val_loss: 36.87925, best_epoch: 5, elapsed: 13.47s, total: 2756.51s, remaining: 10910.27s\n",
      " [0215/1024] loss: 15.18963, val_loss: 36.87937, best_epoch: 5, elapsed: 13.75s, total: 2771.00s, remaining: 11120.25s\n",
      " [0216/1024] loss: 15.18940, val_loss: 36.87949, best_epoch: 5, elapsed: 13.52s, total: 2785.32s, remaining: 10921.80s\n",
      " [0217/1024] loss: 15.18916, val_loss: 36.87961, best_epoch: 5, elapsed: 13.63s, total: 2799.70s, remaining: 10998.94s\n",
      " [0218/1024] loss: 15.18893, val_loss: 36.87973, best_epoch: 5, elapsed: 13.37s, total: 2813.86s, remaining: 10774.72s\n",
      " [0219/1024] loss: 15.18870, val_loss: 36.87985, best_epoch: 5, elapsed: 13.54s, total: 2828.18s, remaining: 10897.25s\n",
      " [0220/1024] loss: 15.18847, val_loss: 36.87997, best_epoch: 5, elapsed: 13.22s, total: 2843.91s, remaining: 10631.43s\n",
      " [0221/1024] loss: 15.18824, val_loss: 36.88009, best_epoch: 5, elapsed: 13.52s, total: 2858.20s, remaining: 10857.22s\n",
      " [0222/1024] loss: 15.18801, val_loss: 36.88021, best_epoch: 5, elapsed: 13.44s, total: 2872.41s, remaining: 10781.94s\n",
      " [0223/1024] loss: 15.18778, val_loss: 36.88033, best_epoch: 5, elapsed: 13.58s, total: 2886.53s, remaining: 10880.50s\n",
      " [0224/1024] loss: 15.18755, val_loss: 36.88045, best_epoch: 5, elapsed: 13.61s, total: 2900.92s, remaining: 10889.62s\n",
      " [0225/1024] loss: 15.18732, val_loss: 36.88057, best_epoch: 5, elapsed: 13.55s, total: 2915.27s, remaining: 10822.62s\n",
      " [0226/1024] loss: 15.18708, val_loss: 36.88069, best_epoch: 5, elapsed: 13.60s, total: 2929.62s, remaining: 10852.24s\n",
      " [0227/1024] loss: 15.18685, val_loss: 36.88081, best_epoch: 5, elapsed: 13.41s, total: 2943.80s, remaining: 10688.13s\n",
      " [0228/1024] loss: 15.18662, val_loss: 36.88093, best_epoch: 5, elapsed: 13.57s, total: 2958.12s, remaining: 10799.97s\n",
      " [0229/1024] loss: 15.18639, val_loss: 36.88105, best_epoch: 5, elapsed: 13.38s, total: 2972.29s, remaining: 10633.35s\n",
      " [0230/1024] loss: 15.18616, val_loss: 36.88117, best_epoch: 5, elapsed: 13.70s, total: 2986.76s, remaining: 10874.63s\n",
      " [0231/1024] loss: 15.18593, val_loss: 36.88129, best_epoch: 5, elapsed: 13.49s, total: 3001.05s, remaining: 10694.92s\n",
      " [0232/1024] loss: 15.18570, val_loss: 36.88141, best_epoch: 5, elapsed: 13.62s, total: 3015.48s, remaining: 10788.97s\n",
      " [0233/1024] loss: 15.18547, val_loss: 36.88153, best_epoch: 5, elapsed: 13.51s, total: 3029.81s, remaining: 10682.54s\n",
      " [0234/1024] loss: 15.18523, val_loss: 36.88165, best_epoch: 5, elapsed: 13.52s, total: 3043.87s, remaining: 10684.35s\n",
      " [0235/1024] loss: 15.18500, val_loss: 36.88177, best_epoch: 5, elapsed: 13.42s, total: 3061.26s, remaining: 10591.18s\n",
      " [0236/1024] loss: 15.18477, val_loss: 36.88189, best_epoch: 5, elapsed: 13.58s, total: 3075.73s, remaining: 10699.50s\n",
      " [0237/1024] loss: 15.18454, val_loss: 36.88201, best_epoch: 5, elapsed: 13.53s, total: 3090.11s, remaining: 10645.18s\n",
      " [0238/1024] loss: 15.18431, val_loss: 36.88213, best_epoch: 5, elapsed: 13.66s, total: 3104.54s, remaining: 10736.00s\n",
      " [0239/1024] loss: 15.18408, val_loss: 36.88225, best_epoch: 5, elapsed: 13.55s, total: 3118.89s, remaining: 10636.15s\n",
      " [0240/1024] loss: 15.18385, val_loss: 36.88237, best_epoch: 5, elapsed: 13.59s, total: 3133.04s, remaining: 10657.53s\n",
      " [0241/1024] loss: 15.18362, val_loss: 36.88249, best_epoch: 5, elapsed: 13.58s, total: 3147.44s, remaining: 10636.23s\n",
      " [0242/1024] loss: 15.18339, val_loss: 36.88261, best_epoch: 5, elapsed: 13.40s, total: 3161.66s, remaining: 10477.93s\n",
      " [0243/1024] loss: 15.18316, val_loss: 36.88273, best_epoch: 5, elapsed: 13.55s, total: 3176.01s, remaining: 10585.48s\n",
      " [0244/1024] loss: 15.18292, val_loss: 36.88285, best_epoch: 5, elapsed: 13.32s, total: 3190.11s, remaining: 10389.44s\n",
      " [0245/1024] loss: 15.18269, val_loss: 36.88297, best_epoch: 5, elapsed: 13.60s, total: 3204.45s, remaining: 10590.63s\n",
      " [0246/1024] loss: 15.18246, val_loss: 36.88309, best_epoch: 5, elapsed: 13.41s, total: 3218.69s, remaining: 10435.66s\n",
      " [0247/1024] loss: 15.18223, val_loss: 36.88321, best_epoch: 5, elapsed: 13.55s, total: 3233.01s, remaining: 10526.11s\n",
      " [0248/1024] loss: 15.18200, val_loss: 36.88333, best_epoch: 5, elapsed: 13.46s, total: 3247.25s, remaining: 10446.68s\n",
      " [0249/1024] loss: 15.18177, val_loss: 36.88345, best_epoch: 5, elapsed: 13.61s, total: 3261.66s, remaining: 10543.99s\n",
      " [0250/1024] loss: 15.18154, val_loss: 36.88357, best_epoch: 5, elapsed: 13.50s, total: 3275.95s, remaining: 10451.80s\n",
      " [0251/1024] loss: 15.18131, val_loss: 36.88369, best_epoch: 5, elapsed: 13.50s, total: 3290.23s, remaining: 10435.04s\n",
      " [0252/1024] loss: 15.18108, val_loss: 36.88381, best_epoch: 5, elapsed: 13.60s, total: 3304.60s, remaining: 10501.74s\n",
      " [0253/1024] loss: 15.18085, val_loss: 36.88393, best_epoch: 5, elapsed: 13.38s, total: 3318.77s, remaining: 10312.92s\n",
      " [0254/1024] loss: 15.18062, val_loss: 36.88405, best_epoch: 5, elapsed: 13.56s, total: 3333.08s, remaining: 10438.89s\n",
      " [0255/1024] loss: 15.18039, val_loss: 36.88417, best_epoch: 5, elapsed: 13.36s, total: 3347.29s, remaining: 10273.43s\n",
      " [0256/1024] loss: 15.18015, val_loss: 36.88429, best_epoch: 5, elapsed: 13.61s, total: 3361.70s, remaining: 10448.72s\n",
      " [0257/1024] loss: 15.17992, val_loss: 36.88441, best_epoch: 5, elapsed: 13.34s, total: 3375.84s, remaining: 10233.96s\n",
      " [0258/1024] loss: 15.17969, val_loss: 36.88453, best_epoch: 5, elapsed: 13.59s, total: 3390.17s, remaining: 10412.71s\n",
      " [0259/1024] loss: 15.17946, val_loss: 36.88465, best_epoch: 5, elapsed: 13.58s, total: 3404.55s, remaining: 10392.06s\n",
      " [0260/1024] loss: 15.17923, val_loss: 36.88477, best_epoch: 5, elapsed: 13.72s, total: 3418.92s, remaining: 10485.34s\n",
      " [0261/1024] loss: 15.17900, val_loss: 36.88489, best_epoch: 5, elapsed: 13.74s, total: 3433.41s, remaining: 10487.31s\n",
      " [0262/1024] loss: 15.17877, val_loss: 36.88501, best_epoch: 5, elapsed: 13.40s, total: 3447.59s, remaining: 10212.92s\n",
      " [0263/1024] loss: 15.17854, val_loss: 36.88513, best_epoch: 5, elapsed: 13.69s, total: 3462.02s, remaining: 10417.71s\n",
      " [0264/1024] loss: 15.17831, val_loss: 36.88525, best_epoch: 5, elapsed: 11.26s, total: 3474.12s, remaining: 8555.41s\n",
      " [0265/1024] loss: 15.17808, val_loss: 36.88537, best_epoch: 5, elapsed: 11.10s, total: 3486.01s, remaining: 8428.03s\n",
      " [0266/1024] loss: 15.17785, val_loss: 36.88549, best_epoch: 5, elapsed: 11.10s, total: 3497.89s, remaining: 8414.11s\n",
      " [0267/1024] loss: 15.17762, val_loss: 36.88561, best_epoch: 5, elapsed: 11.21s, total: 3509.87s, remaining: 8488.45s\n",
      " [0268/1024] loss: 15.17739, val_loss: 36.88573, best_epoch: 5, elapsed: 11.16s, total: 3521.81s, remaining: 8436.04s\n",
      " [0269/1024] loss: 15.17715, val_loss: 36.88585, best_epoch: 5, elapsed: 11.15s, total: 3533.72s, remaining: 8416.33s\n",
      " [0270/1024] loss: 15.17692, val_loss: 36.88597, best_epoch: 5, elapsed: 11.15s, total: 3545.63s, remaining: 8407.21s\n",
      " [0271/1024] loss: 15.17669, val_loss: 36.88609, best_epoch: 5, elapsed: 11.16s, total: 3557.69s, remaining: 8407.08s\n",
      " [0272/1024] loss: 15.17646, val_loss: 36.88620, best_epoch: 5, elapsed: 11.17s, total: 3569.63s, remaining: 8399.47s\n",
      " [0273/1024] loss: 15.17623, val_loss: 36.88632, best_epoch: 5, elapsed: 11.27s, total: 3581.68s, remaining: 8461.52s\n",
      " [0274/1024] loss: 15.17600, val_loss: 36.88644, best_epoch: 5, elapsed: 12.00s, total: 3594.49s, remaining: 8999.53s\n",
      " [0275/1024] loss: 15.17577, val_loss: 36.88656, best_epoch: 5, elapsed: 13.61s, total: 3608.92s, remaining: 10197.62s\n",
      " [0276/1024] loss: 15.17554, val_loss: 36.88668, best_epoch: 5, elapsed: 13.32s, total: 3623.04s, remaining: 9963.28s\n",
      " [0277/1024] loss: 15.17531, val_loss: 36.88680, best_epoch: 5, elapsed: 13.51s, total: 3637.30s, remaining: 10092.30s\n",
      " [0278/1024] loss: 15.17508, val_loss: 36.88692, best_epoch: 5, elapsed: 13.46s, total: 3651.57s, remaining: 10044.21s\n",
      " [0279/1024] loss: 15.17485, val_loss: 36.88704, best_epoch: 5, elapsed: 13.46s, total: 3665.59s, remaining: 10029.34s\n",
      " [0280/1024] loss: 15.17462, val_loss: 36.88716, best_epoch: 5, elapsed: 13.58s, total: 3679.92s, remaining: 10105.17s\n",
      " [0281/1024] loss: 15.17439, val_loss: 36.88728, best_epoch: 5, elapsed: 13.33s, total: 3694.06s, remaining: 9907.75s\n",
      " [0282/1024] loss: 15.17416, val_loss: 36.88740, best_epoch: 5, elapsed: 13.63s, total: 3710.37s, remaining: 10116.03s\n",
      " [0283/1024] loss: 15.17393, val_loss: 36.88752, best_epoch: 5, elapsed: 13.51s, total: 3724.67s, remaining: 10009.52s\n",
      " [0284/1024] loss: 15.17370, val_loss: 36.88764, best_epoch: 5, elapsed: 13.58s, total: 3739.02s, remaining: 10045.56s\n",
      " [0285/1024] loss: 15.17347, val_loss: 36.88776, best_epoch: 5, elapsed: 13.45s, total: 3753.26s, remaining: 9940.18s\n",
      " [0286/1024] loss: 15.17324, val_loss: 36.88788, best_epoch: 5, elapsed: 13.57s, total: 3767.56s, remaining: 10011.13s\n",
      " [0287/1024] loss: 15.17301, val_loss: 36.88800, best_epoch: 5, elapsed: 13.37s, total: 3781.70s, remaining: 9854.38s\n",
      " [0288/1024] loss: 15.17278, val_loss: 36.88812, best_epoch: 5, elapsed: 13.50s, total: 3795.98s, remaining: 9937.76s\n",
      " [0289/1024] loss: 15.17254, val_loss: 36.88824, best_epoch: 5, elapsed: 13.30s, total: 3810.08s, remaining: 9775.38s\n",
      " [0290/1024] loss: 15.17231, val_loss: 36.88836, best_epoch: 5, elapsed: 13.62s, total: 3824.46s, remaining: 9997.08s\n",
      " [0291/1024] loss: 15.17208, val_loss: 36.88848, best_epoch: 5, elapsed: 13.43s, total: 3838.74s, remaining: 9844.54s\n",
      " [0292/1024] loss: 15.17185, val_loss: 36.88860, best_epoch: 5, elapsed: 13.54s, total: 3852.82s, remaining: 9913.54s\n",
      " [0293/1024] loss: 15.17162, val_loss: 36.88872, best_epoch: 5, elapsed: 13.64s, total: 3867.23s, remaining: 9971.91s\n",
      " [0294/1024] loss: 15.17139, val_loss: 36.88884, best_epoch: 5, elapsed: 13.43s, total: 3881.44s, remaining: 9802.76s\n",
      " [0295/1024] loss: 15.17116, val_loss: 36.88895, best_epoch: 5, elapsed: 13.67s, total: 3895.85s, remaining: 9963.84s\n",
      " [0296/1024] loss: 15.17093, val_loss: 36.88907, best_epoch: 5, elapsed: 13.39s, total: 3910.01s, remaining: 9750.61s\n",
      " [0297/1024] loss: 15.17070, val_loss: 36.88919, best_epoch: 5, elapsed: 13.65s, total: 3924.41s, remaining: 9921.74s\n",
      " [0298/1024] loss: 15.17047, val_loss: 36.88931, best_epoch: 5, elapsed: 13.50s, total: 3938.72s, remaining: 9797.56s\n",
      " [0299/1024] loss: 15.17024, val_loss: 36.88943, best_epoch: 5, elapsed: 13.62s, total: 3953.12s, remaining: 9870.91s\n",
      " [0300/1024] loss: 15.17001, val_loss: 36.88955, best_epoch: 5, elapsed: 13.39s, total: 3967.29s, remaining: 9691.76s\n",
      " [0301/1024] loss: 15.16978, val_loss: 36.88967, best_epoch: 5, elapsed: 13.66s, total: 3981.70s, remaining: 9872.93s\n",
      " [0302/1024] loss: 15.16955, val_loss: 36.88979, best_epoch: 5, elapsed: 13.56s, total: 3996.10s, remaining: 9786.74s\n",
      " [0303/1024] loss: 15.16932, val_loss: 36.88991, best_epoch: 5, elapsed: 13.73s, total: 4010.43s, remaining: 9897.33s\n",
      " [0304/1024] loss: 15.16909, val_loss: 36.89003, best_epoch: 5, elapsed: 13.54s, total: 4024.78s, remaining: 9752.12s\n",
      " [0305/1024] loss: 15.16886, val_loss: 36.89015, best_epoch: 5, elapsed: 13.40s, total: 4038.96s, remaining: 9632.53s\n",
      " [0306/1024] loss: 15.16863, val_loss: 36.89026, best_epoch: 5, elapsed: 13.63s, total: 4053.37s, remaining: 9788.69s\n",
      " [0307/1024] loss: 15.16840, val_loss: 36.89038, best_epoch: 5, elapsed: 13.39s, total: 4067.61s, remaining: 9602.39s\n",
      " [0308/1024] loss: 15.16817, val_loss: 36.89050, best_epoch: 5, elapsed: 13.67s, total: 4082.02s, remaining: 9786.03s\n",
      " [0309/1024] loss: 15.16794, val_loss: 36.89062, best_epoch: 5, elapsed: 13.48s, total: 4096.31s, remaining: 9637.20s\n",
      " [0310/1024] loss: 15.16771, val_loss: 36.89074, best_epoch: 5, elapsed: 13.60s, total: 4110.66s, remaining: 9711.00s\n",
      " [0311/1024] loss: 15.16748, val_loss: 36.89086, best_epoch: 5, elapsed: 13.46s, total: 4124.92s, remaining: 9599.23s\n",
      " [0312/1024] loss: 15.16725, val_loss: 36.89098, best_epoch: 5, elapsed: 13.64s, total: 4139.33s, remaining: 9711.85s\n",
      " [0313/1024] loss: 15.16702, val_loss: 36.89110, best_epoch: 5, elapsed: 13.51s, total: 4153.65s, remaining: 9602.82s\n",
      " [0314/1024] loss: 15.16679, val_loss: 36.89122, best_epoch: 5, elapsed: 13.61s, total: 4167.84s, remaining: 9665.89s\n",
      " [0315/1024] loss: 15.16656, val_loss: 36.89134, best_epoch: 5, elapsed: 13.61s, total: 4182.24s, remaining: 9652.86s\n",
      " [0316/1024] loss: 15.16633, val_loss: 36.89146, best_epoch: 5, elapsed: 13.42s, total: 4196.43s, remaining: 9503.18s\n",
      " [0317/1024] loss: 15.16610, val_loss: 36.89158, best_epoch: 5, elapsed: 13.65s, total: 4210.86s, remaining: 9652.15s\n",
      " [0318/1024] loss: 15.16587, val_loss: 36.89169, best_epoch: 5, elapsed: 13.42s, total: 4225.06s, remaining: 9477.37s\n",
      " [0319/1024] loss: 15.16564, val_loss: 36.89181, best_epoch: 5, elapsed: 13.65s, total: 4239.45s, remaining: 9622.98s\n",
      " [0320/1024] loss: 15.16541, val_loss: 36.89193, best_epoch: 5, elapsed: 13.45s, total: 4253.68s, remaining: 9467.44s\n",
      " [0321/1024] loss: 15.16518, val_loss: 36.89205, best_epoch: 5, elapsed: 13.56s, total: 4268.00s, remaining: 9534.12s\n",
      " [0322/1024] loss: 15.16495, val_loss: 36.89217, best_epoch: 5, elapsed: 13.50s, total: 4282.30s, remaining: 9480.34s\n",
      " [0323/1024] loss: 15.16472, val_loss: 36.89229, best_epoch: 5, elapsed: 13.67s, total: 4296.73s, remaining: 9584.92s\n",
      " [0324/1024] loss: 15.16449, val_loss: 36.89241, best_epoch: 5, elapsed: 13.50s, total: 4311.03s, remaining: 9453.48s\n",
      " [0325/1024] loss: 15.16426, val_loss: 36.89253, best_epoch: 5, elapsed: 13.56s, total: 4325.26s, remaining: 9479.43s\n",
      " [0326/1024] loss: 15.16403, val_loss: 36.89265, best_epoch: 5, elapsed: 13.60s, total: 4339.62s, remaining: 9491.76s\n",
      " [0327/1024] loss: 15.16380, val_loss: 36.89277, best_epoch: 5, elapsed: 13.41s, total: 4353.81s, remaining: 9345.33s\n",
      " [0328/1024] loss: 15.16357, val_loss: 36.89289, best_epoch: 5, elapsed: 13.61s, total: 4368.16s, remaining: 9470.94s\n",
      " [0329/1024] loss: 15.16334, val_loss: 36.89300, best_epoch: 5, elapsed: 13.45s, total: 4382.40s, remaining: 9345.76s\n",
      " [0330/1024] loss: 15.16311, val_loss: 36.89312, best_epoch: 5, elapsed: 13.67s, total: 4396.84s, remaining: 9486.19s\n",
      " [0331/1024] loss: 15.16288, val_loss: 36.89324, best_epoch: 5, elapsed: 13.43s, total: 4411.05s, remaining: 9309.61s\n",
      " [0332/1024] loss: 15.16265, val_loss: 36.89336, best_epoch: 5, elapsed: 13.61s, total: 4425.42s, remaining: 9419.86s\n",
      " [0333/1024] loss: 15.16242, val_loss: 36.89348, best_epoch: 5, elapsed: 13.49s, total: 4439.70s, remaining: 9319.20s\n",
      " [0334/1024] loss: 15.16219, val_loss: 36.89360, best_epoch: 5, elapsed: 13.58s, total: 4453.79s, remaining: 9369.57s\n",
      " [0335/1024] loss: 15.16196, val_loss: 36.89372, best_epoch: 5, elapsed: 13.67s, total: 4468.37s, remaining: 9416.75s\n",
      " [0336/1024] loss: 15.16173, val_loss: 36.89384, best_epoch: 5, elapsed: 13.43s, total: 4482.61s, remaining: 9238.18s\n",
      " [0337/1024] loss: 15.16150, val_loss: 36.89396, best_epoch: 5, elapsed: 13.51s, total: 4496.93s, remaining: 9284.35s\n",
      " [0338/1024] loss: 15.16127, val_loss: 36.89408, best_epoch: 5, elapsed: 13.30s, total: 4511.01s, remaining: 9126.06s\n",
      " [0339/1024] loss: 15.16104, val_loss: 36.89420, best_epoch: 5, elapsed: 13.46s, total: 4525.24s, remaining: 9219.11s\n",
      " [0340/1024] loss: 15.16081, val_loss: 36.89432, best_epoch: 5, elapsed: 13.45s, total: 4539.49s, remaining: 9200.94s\n",
      " [0341/1024] loss: 15.16058, val_loss: 36.89443, best_epoch: 5, elapsed: 13.61s, total: 4553.83s, remaining: 9295.44s\n",
      " [0342/1024] loss: 15.16035, val_loss: 36.89455, best_epoch: 5, elapsed: 13.42s, total: 4568.07s, remaining: 9154.32s\n",
      " [0343/1024] loss: 15.16012, val_loss: 36.89467, best_epoch: 5, elapsed: 13.57s, total: 4582.39s, remaining: 9238.31s\n",
      " [0344/1024] loss: 15.15989, val_loss: 36.89479, best_epoch: 5, elapsed: 13.51s, total: 4596.71s, remaining: 9188.48s\n",
      " [0345/1024] loss: 15.15967, val_loss: 36.89491, best_epoch: 5, elapsed: 13.62s, total: 4610.96s, remaining: 9249.99s\n",
      " [0346/1024] loss: 15.15944, val_loss: 36.89503, best_epoch: 5, elapsed: 13.39s, total: 4625.13s, remaining: 9078.39s\n",
      " [0347/1024] loss: 15.15921, val_loss: 36.89515, best_epoch: 5, elapsed: 13.07s, total: 4638.98s, remaining: 8847.23s\n",
      " [0348/1024] loss: 15.15898, val_loss: 36.89527, best_epoch: 5, elapsed: 13.17s, total: 4652.90s, remaining: 8903.89s\n",
      " [0349/1024] loss: 15.15875, val_loss: 36.89538, best_epoch: 5, elapsed: 12.91s, total: 4666.58s, remaining: 8711.33s\n",
      " [0350/1024] loss: 15.15852, val_loss: 36.89550, best_epoch: 5, elapsed: 13.00s, total: 4680.38s, remaining: 8761.28s\n",
      " [0351/1024] loss: 15.15829, val_loss: 36.89562, best_epoch: 5, elapsed: 12.76s, total: 4693.92s, remaining: 8588.00s\n",
      " [0352/1024] loss: 15.15806, val_loss: 36.89574, best_epoch: 5, elapsed: 13.03s, total: 4707.71s, remaining: 8754.97s\n",
      " [0353/1024] loss: 15.15783, val_loss: 36.89586, best_epoch: 5, elapsed: 12.90s, total: 4721.38s, remaining: 8656.34s\n",
      " [0354/1024] loss: 15.15760, val_loss: 36.89598, best_epoch: 5, elapsed: 12.99s, total: 4734.98s, remaining: 8701.92s\n",
      " [0355/1024] loss: 15.15737, val_loss: 36.89610, best_epoch: 5, elapsed: 13.00s, total: 4748.72s, remaining: 8693.97s\n",
      " [0356/1024] loss: 15.15714, val_loss: 36.89622, best_epoch: 5, elapsed: 12.81s, total: 4762.30s, remaining: 8556.53s\n",
      " [0357/1024] loss: 15.15691, val_loss: 36.89634, best_epoch: 5, elapsed: 13.11s, total: 4776.21s, remaining: 8741.43s\n",
      " [0358/1024] loss: 15.15668, val_loss: 36.89645, best_epoch: 5, elapsed: 12.85s, total: 4789.92s, remaining: 8560.41s\n",
      " [0359/1024] loss: 15.15645, val_loss: 36.89657, best_epoch: 5, elapsed: 12.99s, total: 4803.66s, remaining: 8636.89s\n",
      " [0360/1024] loss: 15.15622, val_loss: 36.89669, best_epoch: 5, elapsed: 12.96s, total: 4817.41s, remaining: 8603.93s\n",
      " [0361/1024] loss: 15.15599, val_loss: 36.89681, best_epoch: 5, elapsed: 13.20s, total: 4831.30s, remaining: 8751.33s\n",
      " [0362/1024] loss: 15.15576, val_loss: 36.89693, best_epoch: 5, elapsed: 13.14s, total: 4845.21s, remaining: 8700.72s\n",
      " [0363/1024] loss: 15.15553, val_loss: 36.89705, best_epoch: 5, elapsed: 13.12s, total: 4858.94s, remaining: 8671.68s\n",
      " [0364/1024] loss: 15.15530, val_loss: 36.89717, best_epoch: 5, elapsed: 13.17s, total: 4872.85s, remaining: 8693.07s\n",
      " [0365/1024] loss: 15.15507, val_loss: 36.89729, best_epoch: 5, elapsed: 13.05s, total: 4886.74s, remaining: 8602.86s\n",
      " [0366/1024] loss: 15.15484, val_loss: 36.89740, best_epoch: 5, elapsed: 13.31s, total: 4900.80s, remaining: 8758.16s\n",
      " [0367/1024] loss: 15.15461, val_loss: 36.89752, best_epoch: 5, elapsed: 13.18s, total: 4914.83s, remaining: 8659.31s\n",
      " [0368/1024] loss: 15.15438, val_loss: 36.89764, best_epoch: 5, elapsed: 13.45s, total: 4929.06s, remaining: 8820.91s\n",
      " [0369/1024] loss: 15.15415, val_loss: 36.89776, best_epoch: 5, elapsed: 13.38s, total: 4943.22s, remaining: 8761.54s\n",
      " [0370/1024] loss: 15.15392, val_loss: 36.89788, best_epoch: 5, elapsed: 13.55s, total: 4957.53s, remaining: 8863.70s\n",
      " [0371/1024] loss: 15.15369, val_loss: 36.89800, best_epoch: 5, elapsed: 13.44s, total: 4971.75s, remaining: 8774.76s\n",
      " [0372/1024] loss: 15.15347, val_loss: 36.89812, best_epoch: 5, elapsed: 13.58s, total: 4985.92s, remaining: 8851.98s\n",
      " [0373/1024] loss: 15.15324, val_loss: 36.89824, best_epoch: 5, elapsed: 13.62s, total: 5000.30s, remaining: 8868.27s\n",
      " [0374/1024] loss: 15.15301, val_loss: 36.89835, best_epoch: 5, elapsed: 13.58s, total: 5014.67s, remaining: 8827.71s\n",
      " [0375/1024] loss: 15.15278, val_loss: 36.89847, best_epoch: 5, elapsed: 13.70s, total: 5029.11s, remaining: 8892.25s\n",
      " [0376/1024] loss: 15.15255, val_loss: 36.89859, best_epoch: 5, elapsed: 13.48s, total: 5043.42s, remaining: 8733.04s\n",
      " [0377/1024] loss: 15.15232, val_loss: 36.89871, best_epoch: 5, elapsed: 13.66s, total: 5057.83s, remaining: 8839.51s\n",
      " [0378/1024] loss: 15.15209, val_loss: 36.89883, best_epoch: 5, elapsed: 13.44s, total: 5072.06s, remaining: 8679.26s\n",
      " [0379/1024] loss: 15.15186, val_loss: 36.89894, best_epoch: 5, elapsed: 13.57s, total: 5086.42s, remaining: 8750.34s\n",
      " [0380/1024] loss: 15.15163, val_loss: 36.89907, best_epoch: 5, elapsed: 13.35s, total: 5100.56s, remaining: 8596.20s\n",
      " [0381/1024] loss: 15.15140, val_loss: 36.89918, best_epoch: 5, elapsed: 13.57s, total: 5114.90s, remaining: 8727.14s\n",
      " [0382/1024] loss: 15.15117, val_loss: 36.89930, best_epoch: 5, elapsed: 13.53s, total: 5129.25s, remaining: 8683.71s\n",
      " [0383/1024] loss: 15.15094, val_loss: 36.89942, best_epoch: 5, elapsed: 13.53s, total: 5143.47s, remaining: 8670.73s\n",
      " [0384/1024] loss: 15.15071, val_loss: 36.89954, best_epoch: 5, elapsed: 13.68s, total: 5157.90s, remaining: 8757.09s\n",
      " [0385/1024] loss: 15.15048, val_loss: 36.89966, best_epoch: 5, elapsed: 13.49s, total: 5172.23s, remaining: 8622.74s\n",
      " [0386/1024] loss: 15.15025, val_loss: 36.89978, best_epoch: 5, elapsed: 13.63s, total: 5186.67s, remaining: 8694.53s\n",
      " [0387/1024] loss: 15.15002, val_loss: 36.89990, best_epoch: 5, elapsed: 13.39s, total: 5200.83s, remaining: 8532.46s\n",
      " [0388/1024] loss: 15.14979, val_loss: 36.90001, best_epoch: 5, elapsed: 13.61s, total: 5215.25s, remaining: 8657.65s\n",
      " [0389/1024] loss: 15.14957, val_loss: 36.90013, best_epoch: 5, elapsed: 13.38s, total: 5229.41s, remaining: 8495.16s\n",
      " [0390/1024] loss: 15.14934, val_loss: 36.90025, best_epoch: 5, elapsed: 13.57s, total: 5243.76s, remaining: 8603.93s\n",
      " [0391/1024] loss: 15.14911, val_loss: 36.90037, best_epoch: 5, elapsed: 13.49s, total: 5258.03s, remaining: 8536.52s\n",
      " [0392/1024] loss: 15.14888, val_loss: 36.90048, best_epoch: 5, elapsed: 13.67s, total: 5272.28s, remaining: 8637.42s\n",
      " [0393/1024] loss: 15.14865, val_loss: 36.90060, best_epoch: 5, elapsed: 13.63s, total: 5286.71s, remaining: 8600.63s\n",
      " [0394/1024] loss: 15.14842, val_loss: 36.90072, best_epoch: 5, elapsed: 13.51s, total: 5301.01s, remaining: 8513.47s\n",
      " [0395/1024] loss: 15.14819, val_loss: 36.90084, best_epoch: 5, elapsed: 13.58s, total: 5315.38s, remaining: 8542.31s\n",
      " [0396/1024] loss: 15.14796, val_loss: 36.90096, best_epoch: 5, elapsed: 13.40s, total: 5329.66s, remaining: 8417.74s\n",
      " [0397/1024] loss: 15.14773, val_loss: 36.90108, best_epoch: 5, elapsed: 13.68s, total: 5344.12s, remaining: 8574.55s\n",
      " [0398/1024] loss: 15.14750, val_loss: 36.90119, best_epoch: 5, elapsed: 13.38s, total: 5358.29s, remaining: 8377.20s\n",
      " [0399/1024] loss: 15.14727, val_loss: 36.90131, best_epoch: 5, elapsed: 13.60s, total: 5372.70s, remaining: 8499.49s\n",
      " [0400/1024] loss: 15.14704, val_loss: 36.90143, best_epoch: 5, elapsed: 13.31s, total: 5386.79s, remaining: 8307.22s\n",
      " [0401/1024] loss: 15.14681, val_loss: 36.90155, best_epoch: 5, elapsed: 13.70s, total: 5401.22s, remaining: 8532.25s\n",
      " [0402/1024] loss: 15.14659, val_loss: 36.90166, best_epoch: 5, elapsed: 13.31s, total: 5415.34s, remaining: 8276.53s\n",
      " [0403/1024] loss: 15.14636, val_loss: 36.90178, best_epoch: 5, elapsed: 13.48s, total: 5429.56s, remaining: 8371.47s\n",
      " [0404/1024] loss: 15.14613, val_loss: 36.90190, best_epoch: 5, elapsed: 13.51s, total: 5443.90s, remaining: 8373.48s\n",
      " [0405/1024] loss: 15.14590, val_loss: 36.90202, best_epoch: 5, elapsed: 13.50s, total: 5458.16s, remaining: 8357.00s\n",
      " [0406/1024] loss: 15.14567, val_loss: 36.90214, best_epoch: 5, elapsed: 13.54s, total: 5472.44s, remaining: 8367.10s\n",
      " [0407/1024] loss: 15.14544, val_loss: 36.90226, best_epoch: 5, elapsed: 13.35s, total: 5486.61s, remaining: 8238.07s\n",
      " [0408/1024] loss: 15.14521, val_loss: 36.90237, best_epoch: 5, elapsed: 13.53s, total: 5500.91s, remaining: 8337.04s\n",
      " [0409/1024] loss: 15.14498, val_loss: 36.90249, best_epoch: 5, elapsed: 13.38s, total: 5515.12s, remaining: 8227.47s\n",
      " [0410/1024] loss: 15.14475, val_loss: 36.90261, best_epoch: 5, elapsed: 13.56s, total: 5529.44s, remaining: 8324.71s\n",
      " [0411/1024] loss: 15.14452, val_loss: 36.90273, best_epoch: 5, elapsed: 13.29s, total: 5543.53s, remaining: 8147.17s\n",
      " [0412/1024] loss: 15.14429, val_loss: 36.90284, best_epoch: 5, elapsed: 13.52s, total: 5557.79s, remaining: 8276.25s\n",
      " [0413/1024] loss: 15.14407, val_loss: 36.90296, best_epoch: 5, elapsed: 13.51s, total: 5572.11s, remaining: 8254.68s\n",
      " [0414/1024] loss: 15.14384, val_loss: 36.90308, best_epoch: 5, elapsed: 13.58s, total: 5586.26s, remaining: 8285.27s\n",
      " [0415/1024] loss: 15.14361, val_loss: 36.90320, best_epoch: 5, elapsed: 13.52s, total: 5600.53s, remaining: 8231.84s\n",
      " [0416/1024] loss: 15.14338, val_loss: 36.90331, best_epoch: 5, elapsed: 13.42s, total: 5614.73s, remaining: 8161.46s\n",
      " [0417/1024] loss: 15.14315, val_loss: 36.90343, best_epoch: 5, elapsed: 13.56s, total: 5629.04s, remaining: 8233.04s\n",
      " [0418/1024] loss: 15.14292, val_loss: 36.90355, best_epoch: 5, elapsed: 13.32s, total: 5643.14s, remaining: 8073.87s\n",
      " [0419/1024] loss: 15.14269, val_loss: 36.90367, best_epoch: 5, elapsed: 13.59s, total: 5657.51s, remaining: 8219.67s\n",
      " [0420/1024] loss: 15.14246, val_loss: 36.90378, best_epoch: 5, elapsed: 13.40s, total: 5671.68s, remaining: 8091.05s\n",
      " [0421/1024] loss: 15.14223, val_loss: 36.90390, best_epoch: 5, elapsed: 13.51s, total: 5685.98s, remaining: 8147.49s\n",
      " [0422/1024] loss: 15.14201, val_loss: 36.90402, best_epoch: 5, elapsed: 13.35s, total: 5700.10s, remaining: 8034.30s\n",
      " [0423/1024] loss: 15.14178, val_loss: 36.90414, best_epoch: 5, elapsed: 13.57s, total: 5714.44s, remaining: 8152.83s\n",
      " [0424/1024] loss: 15.14155, val_loss: 36.90425, best_epoch: 5, elapsed: 13.53s, total: 5728.76s, remaining: 8115.36s\n",
      " [0425/1024] loss: 15.14132, val_loss: 36.90437, best_epoch: 5, elapsed: 13.45s, total: 5743.05s, remaining: 8053.89s\n",
      " [0426/1024] loss: 15.14109, val_loss: 36.90449, best_epoch: 5, elapsed: 13.52s, total: 5757.34s, remaining: 8082.48s\n",
      " [0427/1024] loss: 15.14086, val_loss: 36.90461, best_epoch: 5, elapsed: 13.33s, total: 5771.46s, remaining: 7957.20s\n",
      " [0428/1024] loss: 15.14063, val_loss: 36.90472, best_epoch: 5, elapsed: 13.55s, total: 5785.77s, remaining: 8073.57s\n",
      " [0429/1024] loss: 15.14040, val_loss: 36.90484, best_epoch: 5, elapsed: 13.45s, total: 5799.99s, remaining: 8000.64s\n",
      " [0430/1024] loss: 15.14018, val_loss: 36.90496, best_epoch: 5, elapsed: 13.65s, total: 5814.43s, remaining: 8108.13s\n",
      " [0431/1024] loss: 15.13995, val_loss: 36.90507, best_epoch: 5, elapsed: 13.42s, total: 5828.64s, remaining: 7958.32s\n",
      " [0432/1024] loss: 15.13972, val_loss: 36.90519, best_epoch: 5, elapsed: 13.54s, total: 5842.85s, remaining: 8015.20s\n",
      " [0433/1024] loss: 15.13949, val_loss: 36.90531, best_epoch: 5, elapsed: 13.49s, total: 5857.12s, remaining: 7974.57s\n",
      " [0434/1024] loss: 15.13926, val_loss: 36.90543, best_epoch: 5, elapsed: 13.46s, total: 5871.36s, remaining: 7943.02s\n",
      " [0435/1024] loss: 15.13903, val_loss: 36.90554, best_epoch: 5, elapsed: 13.52s, total: 5885.70s, remaining: 7963.35s\n",
      " [0436/1024] loss: 15.13880, val_loss: 36.90566, best_epoch: 5, elapsed: 13.33s, total: 5899.85s, remaining: 7836.72s\n",
      " [0437/1024] loss: 15.13857, val_loss: 36.90578, best_epoch: 5, elapsed: 13.54s, total: 5914.21s, remaining: 7950.79s\n",
      " [0438/1024] loss: 15.13835, val_loss: 36.90589, best_epoch: 5, elapsed: 13.50s, total: 5928.54s, remaining: 7908.59s\n",
      " [0439/1024] loss: 15.13812, val_loss: 36.90601, best_epoch: 5, elapsed: 13.59s, total: 5942.87s, remaining: 7952.45s\n",
      " [0440/1024] loss: 15.13789, val_loss: 36.90613, best_epoch: 5, elapsed: 13.31s, total: 5956.98s, remaining: 7771.17s\n",
      " [0441/1024] loss: 15.13766, val_loss: 36.90624, best_epoch: 5, elapsed: 13.63s, total: 5971.40s, remaining: 7947.13s\n",
      " [0442/1024] loss: 15.13743, val_loss: 36.90636, best_epoch: 5, elapsed: 13.43s, total: 5985.63s, remaining: 7819.11s\n",
      " [0443/1024] loss: 15.13720, val_loss: 36.90648, best_epoch: 5, elapsed: 13.52s, total: 5999.90s, remaining: 7856.43s\n",
      " [0444/1024] loss: 15.13698, val_loss: 36.90660, best_epoch: 5, elapsed: 13.41s, total: 6014.12s, remaining: 7776.75s\n",
      " [0445/1024] loss: 15.13675, val_loss: 36.90671, best_epoch: 5, elapsed: 13.48s, total: 6028.12s, remaining: 7807.23s\n",
      " [0446/1024] loss: 15.13652, val_loss: 36.90683, best_epoch: 5, elapsed: 13.57s, total: 6042.49s, remaining: 7845.84s\n",
      " [0447/1024] loss: 15.13629, val_loss: 36.90695, best_epoch: 5, elapsed: 13.44s, total: 6056.71s, remaining: 7754.69s\n",
      " [0448/1024] loss: 15.13606, val_loss: 36.90707, best_epoch: 5, elapsed: 13.54s, total: 6070.99s, remaining: 7800.15s\n",
      " [0449/1024] loss: 15.13583, val_loss: 36.90718, best_epoch: 5, elapsed: 13.32s, total: 6085.08s, remaining: 7657.62s\n",
      " [0450/1024] loss: 15.13561, val_loss: 36.90730, best_epoch: 5, elapsed: 13.51s, total: 6099.34s, remaining: 7753.19s\n",
      " [0451/1024] loss: 15.13538, val_loss: 36.90742, best_epoch: 5, elapsed: 13.39s, total: 6113.53s, remaining: 7671.44s\n",
      " [0452/1024] loss: 15.13515, val_loss: 36.90753, best_epoch: 5, elapsed: 13.64s, total: 6127.97s, remaining: 7799.29s\n",
      " [0453/1024] loss: 15.13492, val_loss: 36.90765, best_epoch: 5, elapsed: 13.31s, total: 6142.03s, remaining: 7599.39s\n",
      " [0454/1024] loss: 15.13469, val_loss: 36.90777, best_epoch: 5, elapsed: 13.53s, total: 6156.33s, remaining: 7709.65s\n",
      " [0455/1024] loss: 15.13446, val_loss: 36.90788, best_epoch: 5, elapsed: 13.60s, total: 6170.74s, remaining: 7740.98s\n",
      " [0456/1024] loss: 15.13424, val_loss: 36.90800, best_epoch: 5, elapsed: 13.53s, total: 6184.99s, remaining: 7686.54s\n",
      " [0457/1024] loss: 15.13401, val_loss: 36.90812, best_epoch: 5, elapsed: 13.54s, total: 6199.31s, remaining: 7677.52s\n",
      " [0458/1024] loss: 15.13378, val_loss: 36.90823, best_epoch: 5, elapsed: 13.32s, total: 6213.42s, remaining: 7537.13s\n",
      " [0459/1024] loss: 15.13355, val_loss: 36.90835, best_epoch: 5, elapsed: 13.50s, total: 6227.71s, remaining: 7628.21s\n",
      " [0460/1024] loss: 15.13332, val_loss: 36.90847, best_epoch: 5, elapsed: 13.36s, total: 6241.83s, remaining: 7533.21s\n",
      " [0461/1024] loss: 15.13309, val_loss: 36.90858, best_epoch: 5, elapsed: 13.52s, total: 6256.09s, remaining: 7611.95s\n",
      " [0462/1024] loss: 15.13287, val_loss: 36.90870, best_epoch: 5, elapsed: 13.30s, total: 6270.22s, remaining: 7474.96s\n",
      " [0463/1024] loss: 15.13264, val_loss: 36.90882, best_epoch: 5, elapsed: 13.56s, total: 6284.52s, remaining: 7607.20s\n",
      " [0464/1024] loss: 15.13241, val_loss: 36.90894, best_epoch: 5, elapsed: 13.38s, total: 6298.72s, remaining: 7491.27s\n",
      " [0465/1024] loss: 15.13218, val_loss: 36.90905, best_epoch: 5, elapsed: 13.49s, total: 6312.94s, remaining: 7541.05s\n",
      " [0466/1024] loss: 15.13195, val_loss: 36.90917, best_epoch: 5, elapsed: 13.46s, total: 6327.21s, remaining: 7513.08s\n",
      " [0467/1024] loss: 15.13173, val_loss: 36.90929, best_epoch: 5, elapsed: 13.41s, total: 6341.39s, remaining: 7469.24s\n",
      " [0468/1024] loss: 15.13150, val_loss: 36.90940, best_epoch: 5, elapsed: 13.52s, total: 6355.68s, remaining: 7519.64s\n",
      " [0469/1024] loss: 15.13127, val_loss: 36.90952, best_epoch: 5, elapsed: 13.35s, total: 6369.86s, remaining: 7408.17s\n",
      " [0470/1024] loss: 15.13104, val_loss: 36.90963, best_epoch: 5, elapsed: 13.48s, total: 6384.13s, remaining: 7465.96s\n",
      " [0471/1024] loss: 15.13081, val_loss: 36.90975, best_epoch: 5, elapsed: 13.25s, total: 6398.15s, remaining: 7328.49s\n",
      " [0472/1024] loss: 15.13058, val_loss: 36.90987, best_epoch: 5, elapsed: 13.50s, total: 6412.38s, remaining: 7454.52s\n",
      " [0473/1024] loss: 15.13036, val_loss: 36.90999, best_epoch: 5, elapsed: 13.32s, total: 6426.47s, remaining: 7338.25s\n",
      " [0474/1024] loss: 15.13013, val_loss: 36.91010, best_epoch: 5, elapsed: 13.44s, total: 6440.66s, remaining: 7389.64s\n",
      " [0475/1024] loss: 15.12990, val_loss: 36.91022, best_epoch: 5, elapsed: 13.36s, total: 6454.81s, remaining: 7336.28s\n",
      " [0476/1024] loss: 15.12967, val_loss: 36.91034, best_epoch: 5, elapsed: 13.46s, total: 6468.78s, remaining: 7374.15s\n",
      " [0477/1024] loss: 15.12944, val_loss: 36.91045, best_epoch: 5, elapsed: 13.47s, total: 6483.00s, remaining: 7366.26s\n",
      " [0478/1024] loss: 15.12922, val_loss: 36.91057, best_epoch: 5, elapsed: 13.30s, total: 6497.14s, remaining: 7261.10s\n",
      " [0479/1024] loss: 15.12899, val_loss: 36.91069, best_epoch: 5, elapsed: 13.43s, total: 6511.37s, remaining: 7317.44s\n",
      " [0480/1024] loss: 15.12876, val_loss: 36.91080, best_epoch: 5, elapsed: 13.25s, total: 6525.40s, remaining: 7208.07s\n",
      " [0481/1024] loss: 15.12853, val_loss: 36.91092, best_epoch: 5, elapsed: 13.45s, total: 6539.69s, remaining: 7305.33s\n",
      " [0482/1024] loss: 15.12830, val_loss: 36.91104, best_epoch: 5, elapsed: 13.22s, total: 6553.68s, remaining: 7165.46s\n",
      " [0483/1024] loss: 15.12808, val_loss: 36.91115, best_epoch: 5, elapsed: 13.45s, total: 6567.90s, remaining: 7278.36s\n",
      " [0484/1024] loss: 15.12785, val_loss: 36.91127, best_epoch: 5, elapsed: 13.39s, total: 6582.08s, remaining: 7232.75s\n",
      " [0485/1024] loss: 15.12762, val_loss: 36.91139, best_epoch: 5, elapsed: 13.44s, total: 6596.03s, remaining: 7246.35s\n",
      " [0486/1024] loss: 15.12739, val_loss: 36.91150, best_epoch: 5, elapsed: 13.44s, total: 6610.25s, remaining: 7232.75s\n",
      " [0487/1024] loss: 15.12716, val_loss: 36.91162, best_epoch: 5, elapsed: 13.29s, total: 6624.31s, remaining: 7137.90s\n",
      " [0488/1024] loss: 15.12694, val_loss: 36.91174, best_epoch: 5, elapsed: 13.42s, total: 6638.46s, remaining: 7191.71s\n",
      " [0489/1024] loss: 15.12671, val_loss: 36.91185, best_epoch: 5, elapsed: 13.34s, total: 6652.56s, remaining: 7137.07s\n",
      " [0490/1024] loss: 15.12648, val_loss: 36.91197, best_epoch: 5, elapsed: 13.46s, total: 6666.77s, remaining: 7189.16s\n",
      " [0491/1024] loss: 15.12625, val_loss: 36.91209, best_epoch: 5, elapsed: 13.30s, total: 6680.84s, remaining: 7088.76s\n",
      " [0492/1024] loss: 15.12603, val_loss: 36.91220, best_epoch: 5, elapsed: 13.50s, total: 6695.07s, remaining: 7180.34s\n",
      " [0493/1024] loss: 15.12580, val_loss: 36.91232, best_epoch: 5, elapsed: 13.25s, total: 6709.11s, remaining: 7034.69s\n",
      " [0494/1024] loss: 15.12557, val_loss: 36.91243, best_epoch: 5, elapsed: 13.48s, total: 6723.34s, remaining: 7143.91s\n",
      " [0495/1024] loss: 15.12534, val_loss: 36.91255, best_epoch: 5, elapsed: 13.50s, total: 6737.65s, remaining: 7144.05s\n",
      " [0496/1024] loss: 15.12512, val_loss: 36.91267, best_epoch: 5, elapsed: 13.51s, total: 6751.79s, remaining: 7133.73s\n",
      " [0497/1024] loss: 15.12489, val_loss: 36.91278, best_epoch: 5, elapsed: 13.47s, total: 6766.04s, remaining: 7098.13s\n",
      " [0498/1024] loss: 15.12466, val_loss: 36.91290, best_epoch: 5, elapsed: 13.23s, total: 6780.04s, remaining: 6959.13s\n",
      " [0499/1024] loss: 15.12443, val_loss: 36.91302, best_epoch: 5, elapsed: 13.53s, total: 6794.34s, remaining: 7103.39s\n",
      " [0500/1024] loss: 15.12420, val_loss: 36.91313, best_epoch: 5, elapsed: 13.26s, total: 6808.48s, remaining: 6949.89s\n",
      " [0501/1024] loss: 15.12398, val_loss: 36.91325, best_epoch: 5, elapsed: 13.43s, total: 6822.63s, remaining: 7022.81s\n",
      " [0502/1024] loss: 15.12375, val_loss: 36.91336, best_epoch: 5, elapsed: 13.24s, total: 6836.69s, remaining: 6908.89s\n",
      " [0503/1024] loss: 15.12352, val_loss: 36.91348, best_epoch: 5, elapsed: 13.50s, total: 6850.93s, remaining: 7035.25s\n",
      " [0504/1024] loss: 15.12329, val_loss: 36.91359, best_epoch: 5, elapsed: 13.34s, total: 6865.04s, remaining: 6939.22s\n",
      " [0505/1024] loss: 15.12307, val_loss: 36.91371, best_epoch: 5, elapsed: 13.47s, total: 6879.25s, remaining: 6991.63s\n",
      " [0506/1024] loss: 15.12284, val_loss: 36.91382, best_epoch: 5, elapsed: 13.43s, total: 6893.46s, remaining: 6957.68s\n",
      " [0507/1024] loss: 15.12261, val_loss: 36.91394, best_epoch: 5, elapsed: 13.42s, total: 6907.56s, remaining: 6936.76s\n",
      " [0508/1024] loss: 15.12238, val_loss: 36.91406, best_epoch: 5, elapsed: 13.42s, total: 6921.71s, remaining: 6922.30s\n",
      " [0509/1024] loss: 15.12216, val_loss: 36.91417, best_epoch: 5, elapsed: 13.22s, total: 6935.69s, remaining: 6807.90s\n",
      " [0510/1024] loss: 15.12193, val_loss: 36.91429, best_epoch: 5, elapsed: 13.48s, total: 6950.00s, remaining: 6926.97s\n",
      " [0511/1024] loss: 15.12170, val_loss: 36.91440, best_epoch: 5, elapsed: 13.37s, total: 6964.14s, remaining: 6857.24s\n",
      " [0512/1024] loss: 15.12147, val_loss: 36.91452, best_epoch: 5, elapsed: 13.51s, total: 6978.38s, remaining: 6916.54s\n",
      " [0513/1024] loss: 15.12125, val_loss: 36.91463, best_epoch: 5, elapsed: 13.25s, total: 6992.44s, remaining: 6768.20s\n",
      " [0514/1024] loss: 15.12102, val_loss: 36.91475, best_epoch: 5, elapsed: 13.48s, total: 7006.67s, remaining: 6874.78s\n",
      " [0515/1024] loss: 15.12079, val_loss: 36.91486, best_epoch: 5, elapsed: 13.28s, total: 7020.73s, remaining: 6760.66s\n",
      " [0516/1024] loss: 15.12056, val_loss: 36.91498, best_epoch: 5, elapsed: 13.51s, total: 7035.06s, remaining: 6863.31s\n",
      " [0517/1024] loss: 15.12033, val_loss: 36.91510, best_epoch: 5, elapsed: 13.46s, total: 7049.36s, remaining: 6822.39s\n",
      " [0518/1024] loss: 15.12011, val_loss: 36.91521, best_epoch: 5, elapsed: 12.05s, total: 7061.97s, remaining: 6096.83s\n",
      " [0519/1024] loss: 15.11988, val_loss: 36.91533, best_epoch: 5, elapsed: 11.05s, total: 7073.81s, remaining: 5578.18s\n",
      " [0520/1024] loss: 15.11965, val_loss: 36.91544, best_epoch: 5, elapsed: 11.04s, total: 7085.63s, remaining: 5565.38s\n",
      " [0521/1024] loss: 15.11943, val_loss: 36.91556, best_epoch: 5, elapsed: 11.07s, total: 7097.48s, remaining: 5568.84s\n",
      " [0522/1024] loss: 15.11920, val_loss: 36.91567, best_epoch: 5, elapsed: 11.04s, total: 7109.28s, remaining: 5543.48s\n",
      " [0523/1024] loss: 15.11897, val_loss: 36.91579, best_epoch: 5, elapsed: 11.08s, total: 7121.12s, remaining: 5553.22s\n",
      " [0524/1024] loss: 15.11874, val_loss: 36.91590, best_epoch: 5, elapsed: 11.04s, total: 7132.92s, remaining: 5518.70s\n",
      " [0525/1024] loss: 15.11852, val_loss: 36.91602, best_epoch: 5, elapsed: 11.05s, total: 7144.74s, remaining: 5512.05s\n",
      " [0526/1024] loss: 15.11829, val_loss: 36.91613, best_epoch: 5, elapsed: 11.05s, total: 7156.57s, remaining: 5501.98s\n",
      " [0527/1024] loss: 15.11806, val_loss: 36.91625, best_epoch: 5, elapsed: 11.04s, total: 7168.38s, remaining: 5489.09s\n",
      " [0528/1024] loss: 15.11783, val_loss: 36.91636, best_epoch: 5, elapsed: 11.05s, total: 7180.20s, remaining: 5480.53s\n",
      " [0529/1024] loss: 15.11761, val_loss: 36.91648, best_epoch: 5, elapsed: 11.04s, total: 7192.03s, remaining: 5466.83s\n",
      " [0530/1024] loss: 15.11738, val_loss: 36.91660, best_epoch: 5, elapsed: 11.06s, total: 7203.87s, remaining: 5462.41s\n",
      " [0531/1024] loss: 15.11715, val_loss: 36.91671, best_epoch: 5, elapsed: 11.04s, total: 7215.67s, remaining: 5442.86s\n",
      " [0532/1024] loss: 15.11692, val_loss: 36.91683, best_epoch: 5, elapsed: 11.05s, total: 7227.50s, remaining: 5434.62s\n",
      " [0533/1024] loss: 15.11670, val_loss: 36.91694, best_epoch: 5, elapsed: 11.04s, total: 7239.31s, remaining: 5422.53s\n",
      " [0534/1024] loss: 15.11647, val_loss: 36.91706, best_epoch: 5, elapsed: 11.05s, total: 7251.11s, remaining: 5414.15s\n",
      " [0535/1024] loss: 15.11624, val_loss: 36.91717, best_epoch: 5, elapsed: 11.04s, total: 7262.91s, remaining: 5398.10s\n",
      " [0536/1024] loss: 15.11601, val_loss: 36.91729, best_epoch: 5, elapsed: 11.05s, total: 7274.74s, remaining: 5392.43s\n",
      " [0537/1024] loss: 15.11579, val_loss: 36.91740, best_epoch: 5, elapsed: 11.04s, total: 7286.54s, remaining: 5378.13s\n",
      " [0538/1024] loss: 15.11556, val_loss: 36.91752, best_epoch: 5, elapsed: 11.08s, total: 7298.38s, remaining: 5382.83s\n",
      " [0539/1024] loss: 15.11533, val_loss: 36.91764, best_epoch: 5, elapsed: 11.04s, total: 7310.25s, remaining: 5356.52s\n",
      " [0540/1024] loss: 15.11511, val_loss: 36.91775, best_epoch: 5, elapsed: 11.05s, total: 7322.06s, remaining: 5347.58s\n",
      " [0541/1024] loss: 15.11488, val_loss: 36.91787, best_epoch: 5, elapsed: 11.05s, total: 7333.91s, remaining: 5338.06s\n",
      " [0542/1024] loss: 15.11465, val_loss: 36.91799, best_epoch: 5, elapsed: 11.05s, total: 7345.74s, remaining: 5324.25s\n",
      " [0543/1024] loss: 15.11442, val_loss: 36.91810, best_epoch: 5, elapsed: 11.04s, total: 7357.56s, remaining: 5307.96s\n",
      " [0544/1024] loss: 15.11420, val_loss: 36.91822, best_epoch: 5, elapsed: 11.05s, total: 7369.38s, remaining: 5301.70s\n",
      " [0545/1024] loss: 15.11397, val_loss: 36.91833, best_epoch: 5, elapsed: 11.05s, total: 7381.20s, remaining: 5291.44s\n",
      " [0546/1024] loss: 15.11374, val_loss: 36.91845, best_epoch: 5, elapsed: 11.05s, total: 7393.02s, remaining: 5282.01s\n",
      " [0547/1024] loss: 15.11352, val_loss: 36.91856, best_epoch: 5, elapsed: 11.04s, total: 7404.84s, remaining: 5266.49s\n",
      " [0548/1024] loss: 15.11329, val_loss: 36.91868, best_epoch: 5, elapsed: 11.04s, total: 7416.67s, remaining: 5256.24s\n",
      " [0549/1024] loss: 15.11306, val_loss: 36.91880, best_epoch: 5, elapsed: 11.03s, total: 7428.47s, remaining: 5239.36s\n",
      " [0550/1024] loss: 15.11283, val_loss: 36.91891, best_epoch: 5, elapsed: 11.04s, total: 7440.27s, remaining: 5231.06s\n",
      " [0551/1024] loss: 15.11261, val_loss: 36.91903, best_epoch: 5, elapsed: 11.05s, total: 7452.12s, remaining: 5224.42s\n",
      " [0552/1024] loss: 15.11238, val_loss: 36.91914, best_epoch: 5, elapsed: 11.03s, total: 7463.95s, remaining: 5207.22s\n",
      " [0553/1024] loss: 15.11215, val_loss: 36.91926, best_epoch: 5, elapsed: 11.05s, total: 7475.86s, remaining: 5204.60s\n",
      " [0554/1024] loss: 15.11192, val_loss: 36.91938, best_epoch: 5, elapsed: 11.03s, total: 7487.75s, remaining: 5183.80s\n",
      " [0555/1024] loss: 15.11170, val_loss: 36.91949, best_epoch: 5, elapsed: 11.03s, total: 7499.53s, remaining: 5175.25s\n",
      " [0556/1024] loss: 15.11147, val_loss: 36.91961, best_epoch: 5, elapsed: 11.03s, total: 7511.35s, remaining: 5161.75s\n",
      " [0557/1024] loss: 15.11124, val_loss: 36.91972, best_epoch: 5, elapsed: 11.03s, total: 7523.17s, remaining: 5152.89s\n",
      " [0558/1024] loss: 15.11102, val_loss: 36.91984, best_epoch: 5, elapsed: 11.05s, total: 7535.03s, remaining: 5151.33s\n",
      " [0559/1024] loss: 15.11079, val_loss: 36.91995, best_epoch: 5, elapsed: 11.03s, total: 7546.81s, remaining: 5127.13s\n",
      " [0560/1024] loss: 15.11056, val_loss: 36.92007, best_epoch: 5, elapsed: 11.03s, total: 7558.63s, remaining: 5118.72s\n",
      " [0561/1024] loss: 15.11034, val_loss: 36.92018, best_epoch: 5, elapsed: 11.04s, total: 7570.43s, remaining: 5110.51s\n",
      " [0562/1024] loss: 15.11011, val_loss: 36.92030, best_epoch: 5, elapsed: 11.04s, total: 7582.25s, remaining: 5099.19s\n",
      " [0563/1024] loss: 15.10988, val_loss: 36.92041, best_epoch: 5, elapsed: 11.04s, total: 7594.08s, remaining: 5089.81s\n",
      " [0564/1024] loss: 15.10965, val_loss: 36.92053, best_epoch: 5, elapsed: 11.05s, total: 7605.89s, remaining: 5082.51s\n",
      " [0565/1024] loss: 15.10943, val_loss: 36.92065, best_epoch: 5, elapsed: 11.04s, total: 7617.75s, remaining: 5068.76s\n",
      " [0566/1024] loss: 15.10920, val_loss: 36.92076, best_epoch: 5, elapsed: 11.05s, total: 7629.56s, remaining: 5060.50s\n",
      " [0567/1024] loss: 15.10897, val_loss: 36.92088, best_epoch: 5, elapsed: 10.99s, total: 7641.32s, remaining: 5023.80s\n",
      " [0568/1024] loss: 15.10875, val_loss: 36.92099, best_epoch: 5, elapsed: 10.87s, total: 7652.98s, remaining: 4955.85s\n",
      " [0569/1024] loss: 15.10852, val_loss: 36.92111, best_epoch: 5, elapsed: 10.80s, total: 7664.60s, remaining: 4913.70s\n",
      " [0570/1024] loss: 15.10829, val_loss: 36.92123, best_epoch: 5, elapsed: 10.72s, total: 7676.11s, remaining: 4866.13s\n",
      " [0571/1024] loss: 15.10807, val_loss: 36.92134, best_epoch: 5, elapsed: 10.73s, total: 7687.62s, remaining: 4858.67s\n",
      " [0572/1024] loss: 15.10784, val_loss: 36.92146, best_epoch: 5, elapsed: 10.71s, total: 7699.08s, remaining: 4839.54s\n",
      " [0573/1024] loss: 15.10761, val_loss: 36.92157, best_epoch: 5, elapsed: 10.71s, total: 7710.56s, remaining: 4831.81s\n",
      " [0574/1024] loss: 15.10739, val_loss: 36.92169, best_epoch: 5, elapsed: 10.68s, total: 7722.00s, remaining: 4805.58s\n",
      " [0575/1024] loss: 15.10716, val_loss: 36.92180, best_epoch: 5, elapsed: 10.69s, total: 7733.52s, remaining: 4800.71s\n",
      " [0576/1024] loss: 15.10693, val_loss: 36.92192, best_epoch: 5, elapsed: 10.68s, total: 7745.00s, remaining: 4786.16s\n",
      " [0577/1024] loss: 15.10670, val_loss: 36.92203, best_epoch: 5, elapsed: 10.69s, total: 7756.51s, remaining: 4778.59s\n",
      " [0578/1024] loss: 15.10648, val_loss: 36.92215, best_epoch: 5, elapsed: 10.69s, total: 7767.95s, remaining: 4768.12s\n",
      " [0579/1024] loss: 15.10625, val_loss: 36.92226, best_epoch: 5, elapsed: 10.70s, total: 7779.39s, remaining: 4759.90s\n",
      " [0580/1024] loss: 15.10602, val_loss: 36.92238, best_epoch: 5, elapsed: 10.70s, total: 7790.86s, remaining: 4752.19s\n",
      " [0581/1024] loss: 15.10580, val_loss: 36.92249, best_epoch: 5, elapsed: 10.71s, total: 7802.34s, remaining: 4745.10s\n",
      " [0582/1024] loss: 15.10557, val_loss: 36.92261, best_epoch: 5, elapsed: 10.72s, total: 7813.82s, remaining: 4738.02s\n",
      " [0583/1024] loss: 15.10534, val_loss: 36.92273, best_epoch: 5, elapsed: 10.73s, total: 7825.30s, remaining: 4733.75s\n",
      " [0584/1024] loss: 15.10512, val_loss: 36.92284, best_epoch: 5, elapsed: 10.79s, total: 7836.86s, remaining: 4747.31s\n",
      " [0585/1024] loss: 15.10489, val_loss: 36.92296, best_epoch: 5, elapsed: 10.75s, total: 7848.37s, remaining: 4720.73s\n",
      " [0586/1024] loss: 15.10466, val_loss: 36.92307, best_epoch: 5, elapsed: 10.77s, total: 7859.69s, remaining: 4718.05s\n",
      " [0587/1024] loss: 15.10444, val_loss: 36.92319, best_epoch: 5, elapsed: 10.78s, total: 7871.22s, remaining: 4710.41s\n",
      " [0588/1024] loss: 15.10421, val_loss: 36.92330, best_epoch: 5, elapsed: 10.80s, total: 7882.78s, remaining: 4707.32s\n",
      " [0589/1024] loss: 15.10398, val_loss: 36.92342, best_epoch: 5, elapsed: 10.82s, total: 7894.41s, remaining: 4708.61s\n",
      " [0590/1024] loss: 15.10376, val_loss: 36.92354, best_epoch: 5, elapsed: 10.87s, total: 7906.09s, remaining: 4718.11s\n",
      " [0591/1024] loss: 15.10353, val_loss: 36.92365, best_epoch: 5, elapsed: 10.88s, total: 7917.75s, remaining: 4712.50s\n",
      " [0592/1024] loss: 15.10330, val_loss: 36.92377, best_epoch: 5, elapsed: 10.93s, total: 7929.43s, remaining: 4721.20s\n",
      " [0593/1024] loss: 15.10308, val_loss: 36.92388, best_epoch: 5, elapsed: 10.96s, total: 7941.14s, remaining: 4722.78s\n",
      " [0594/1024] loss: 15.10285, val_loss: 36.92400, best_epoch: 5, elapsed: 10.98s, total: 7952.88s, remaining: 4720.23s\n",
      " [0595/1024] loss: 15.10262, val_loss: 36.92411, best_epoch: 5, elapsed: 11.01s, total: 7964.67s, remaining: 4724.35s\n",
      " [0596/1024] loss: 15.10240, val_loss: 36.92423, best_epoch: 5, elapsed: 11.04s, total: 7976.48s, remaining: 4725.01s\n",
      " [0597/1024] loss: 15.10217, val_loss: 36.92434, best_epoch: 5, elapsed: 11.06s, total: 7988.37s, remaining: 4724.64s\n",
      " [0598/1024] loss: 15.10194, val_loss: 36.92446, best_epoch: 5, elapsed: 11.07s, total: 8000.24s, remaining: 4717.27s\n",
      " [0599/1024] loss: 15.10172, val_loss: 36.92458, best_epoch: 5, elapsed: 11.11s, total: 8012.10s, remaining: 4721.06s\n",
      " [0600/1024] loss: 15.10149, val_loss: 36.92469, best_epoch: 5, elapsed: 11.15s, total: 8024.01s, remaining: 4727.36s\n",
      " [0601/1024] loss: 15.10126, val_loss: 36.92481, best_epoch: 5, elapsed: 11.13s, total: 8035.99s, remaining: 4708.92s\n",
      " [0602/1024] loss: 15.10104, val_loss: 36.92492, best_epoch: 5, elapsed: 11.14s, total: 8047.93s, remaining: 4699.26s\n",
      " [0603/1024] loss: 15.10081, val_loss: 36.92504, best_epoch: 5, elapsed: 11.12s, total: 8059.82s, remaining: 4682.31s\n",
      " [0604/1024] loss: 15.10058, val_loss: 36.92515, best_epoch: 5, elapsed: 11.09s, total: 8071.68s, remaining: 4657.89s\n",
      " [0605/1024] loss: 15.10036, val_loss: 36.92527, best_epoch: 5, elapsed: 11.07s, total: 8083.50s, remaining: 4636.78s\n",
      " [0606/1024] loss: 15.10013, val_loss: 36.92539, best_epoch: 5, elapsed: 11.05s, total: 8095.32s, remaining: 4617.86s\n",
      " [0607/1024] loss: 15.09990, val_loss: 36.92550, best_epoch: 5, elapsed: 11.06s, total: 8107.13s, remaining: 4610.24s\n",
      " [0608/1024] loss: 15.09968, val_loss: 36.92562, best_epoch: 5, elapsed: 11.04s, total: 8118.92s, remaining: 4593.53s\n",
      " [0609/1024] loss: 15.09945, val_loss: 36.92573, best_epoch: 5, elapsed: 11.05s, total: 8130.78s, remaining: 4587.25s\n",
      " [0610/1024] loss: 15.09922, val_loss: 36.92585, best_epoch: 5, elapsed: 11.07s, total: 8142.61s, remaining: 4582.35s\n",
      " [0611/1024] loss: 15.09900, val_loss: 36.92596, best_epoch: 5, elapsed: 11.11s, total: 8154.50s, remaining: 4589.63s\n",
      " [0612/1024] loss: 15.09877, val_loss: 36.92608, best_epoch: 5, elapsed: 11.10s, total: 8166.35s, remaining: 4575.22s\n",
      " [0613/1024] loss: 15.09854, val_loss: 36.92619, best_epoch: 5, elapsed: 11.12s, total: 8178.22s, remaining: 4568.57s\n",
      " [0614/1024] loss: 15.09832, val_loss: 36.92631, best_epoch: 5, elapsed: 11.10s, total: 8190.08s, remaining: 4550.73s\n",
      " [0615/1024] loss: 15.09809, val_loss: 36.92642, best_epoch: 5, elapsed: 11.12s, total: 8201.95s, remaining: 4547.46s\n",
      " [0616/1024] loss: 15.09786, val_loss: 36.92654, best_epoch: 5, elapsed: 11.08s, total: 8213.83s, remaining: 4522.30s\n",
      " [0617/1024] loss: 15.09764, val_loss: 36.92665, best_epoch: 5, elapsed: 11.09s, total: 8225.69s, remaining: 4513.15s\n",
      " [0618/1024] loss: 15.09741, val_loss: 36.92677, best_epoch: 5, elapsed: 11.09s, total: 8237.52s, remaining: 4502.08s\n",
      " [0619/1024] loss: 15.09718, val_loss: 36.92688, best_epoch: 5, elapsed: 11.08s, total: 8249.38s, remaining: 4485.67s\n",
      " [0620/1024] loss: 15.09696, val_loss: 36.92700, best_epoch: 5, elapsed: 11.08s, total: 8261.21s, remaining: 4475.81s\n",
      " [0621/1024] loss: 15.09673, val_loss: 36.92711, best_epoch: 5, elapsed: 11.08s, total: 8273.11s, remaining: 4465.90s\n",
      " [0622/1024] loss: 15.09651, val_loss: 36.92723, best_epoch: 5, elapsed: 11.07s, total: 8284.95s, remaining: 4450.59s\n",
      " [0623/1024] loss: 15.09628, val_loss: 36.92734, best_epoch: 5, elapsed: 11.07s, total: 8296.78s, remaining: 4438.14s\n",
      " [0624/1024] loss: 15.09605, val_loss: 36.92746, best_epoch: 5, elapsed: 11.06s, total: 8308.59s, remaining: 4424.52s\n",
      " [0625/1024] loss: 15.09583, val_loss: 36.92757, best_epoch: 5, elapsed: 11.07s, total: 8320.42s, remaining: 4416.23s\n",
      " [0626/1024] loss: 15.09560, val_loss: 36.92769, best_epoch: 5, elapsed: 11.07s, total: 8332.26s, remaining: 4406.34s\n",
      " [0627/1024] loss: 15.09537, val_loss: 36.92780, best_epoch: 5, elapsed: 11.06s, total: 8344.09s, remaining: 4392.38s\n",
      " [0628/1024] loss: 15.09515, val_loss: 36.92791, best_epoch: 5, elapsed: 11.08s, total: 8355.94s, remaining: 4388.70s\n",
      " [0629/1024] loss: 15.09492, val_loss: 36.92803, best_epoch: 5, elapsed: 11.08s, total: 8367.76s, remaining: 4375.57s\n",
      " [0630/1024] loss: 15.09469, val_loss: 36.92814, best_epoch: 5, elapsed: 11.09s, total: 8379.61s, remaining: 4370.00s\n",
      " [0631/1024] loss: 15.09447, val_loss: 36.92826, best_epoch: 5, elapsed: 11.07s, total: 8391.45s, remaining: 4352.13s\n",
      " [0632/1024] loss: 15.09424, val_loss: 36.92837, best_epoch: 5, elapsed: 11.06s, total: 8403.27s, remaining: 4335.32s\n",
      " [0633/1024] loss: 15.09402, val_loss: 36.92849, best_epoch: 5, elapsed: 11.07s, total: 8415.10s, remaining: 4328.18s\n",
      " [0634/1024] loss: 15.09379, val_loss: 36.92860, best_epoch: 5, elapsed: 11.06s, total: 8426.95s, remaining: 4314.47s\n",
      " [0635/1024] loss: 15.09356, val_loss: 36.92872, best_epoch: 5, elapsed: 11.07s, total: 8438.77s, remaining: 4306.30s\n",
      " [0636/1024] loss: 15.09334, val_loss: 36.92883, best_epoch: 5, elapsed: 11.06s, total: 8450.65s, remaining: 4290.78s\n",
      " [0637/1024] loss: 15.09311, val_loss: 36.92895, best_epoch: 5, elapsed: 11.06s, total: 8462.50s, remaining: 4282.02s\n",
      " [0638/1024] loss: 15.09288, val_loss: 36.92906, best_epoch: 5, elapsed: 11.07s, total: 8474.31s, remaining: 4271.50s\n",
      " [0639/1024] loss: 15.09266, val_loss: 36.92918, best_epoch: 5, elapsed: 11.07s, total: 8486.18s, remaining: 4262.86s\n",
      " [0640/1024] loss: 15.09243, val_loss: 36.92929, best_epoch: 5, elapsed: 11.07s, total: 8498.01s, remaining: 4251.54s\n",
      " [0641/1024] loss: 15.09221, val_loss: 36.92941, best_epoch: 5, elapsed: 11.07s, total: 8509.85s, remaining: 4241.49s\n",
      " [0642/1024] loss: 15.09198, val_loss: 36.92952, best_epoch: 5, elapsed: 11.06s, total: 8521.69s, remaining: 4225.25s\n",
      " [0643/1024] loss: 15.09175, val_loss: 36.92964, best_epoch: 5, elapsed: 11.08s, total: 8533.54s, remaining: 4219.76s\n",
      " [0644/1024] loss: 15.09153, val_loss: 36.92975, best_epoch: 5, elapsed: 11.06s, total: 8545.42s, remaining: 4203.34s\n",
      " [0645/1024] loss: 15.09130, val_loss: 36.92987, best_epoch: 5, elapsed: 11.10s, total: 8557.31s, remaining: 4206.06s\n",
      " [0646/1024] loss: 15.09108, val_loss: 36.92998, best_epoch: 5, elapsed: 11.08s, total: 8569.15s, remaining: 4188.68s\n",
      " [0647/1024] loss: 15.09085, val_loss: 36.93010, best_epoch: 5, elapsed: 11.07s, total: 8580.99s, remaining: 4173.37s\n",
      " [0648/1024] loss: 15.09062, val_loss: 36.93021, best_epoch: 5, elapsed: 11.06s, total: 8592.87s, remaining: 4160.16s\n",
      " [0649/1024] loss: 15.09040, val_loss: 36.93033, best_epoch: 5, elapsed: 11.07s, total: 8604.72s, remaining: 4151.64s\n",
      " [0650/1024] loss: 15.09017, val_loss: 36.93044, best_epoch: 5, elapsed: 11.07s, total: 8616.54s, remaining: 4140.36s\n",
      " [0651/1024] loss: 15.08994, val_loss: 36.93056, best_epoch: 5, elapsed: 11.07s, total: 8628.37s, remaining: 4127.71s\n",
      " [0652/1024] loss: 15.08972, val_loss: 36.93067, best_epoch: 5, elapsed: 11.08s, total: 8640.21s, remaining: 4122.37s\n",
      " [0653/1024] loss: 15.08949, val_loss: 36.93079, best_epoch: 5, elapsed: 11.07s, total: 8652.05s, remaining: 4108.21s\n",
      " [0654/1024] loss: 15.08927, val_loss: 36.93090, best_epoch: 5, elapsed: 11.07s, total: 8663.94s, remaining: 4095.60s\n",
      " [0655/1024] loss: 15.08904, val_loss: 36.93102, best_epoch: 5, elapsed: 11.07s, total: 8675.81s, remaining: 4085.85s\n",
      " [0656/1024] loss: 15.08881, val_loss: 36.93113, best_epoch: 5, elapsed: 11.06s, total: 8687.63s, remaining: 4071.54s\n",
      " [0657/1024] loss: 15.08859, val_loss: 36.93124, best_epoch: 5, elapsed: 11.07s, total: 8699.47s, remaining: 4063.20s\n",
      " [0658/1024] loss: 15.08836, val_loss: 36.93136, best_epoch: 5, elapsed: 11.07s, total: 8711.30s, remaining: 4051.68s\n",
      " [0659/1024] loss: 15.08814, val_loss: 36.93147, best_epoch: 5, elapsed: 11.08s, total: 8723.19s, remaining: 4043.23s\n",
      " [0660/1024] loss: 15.08791, val_loss: 36.93159, best_epoch: 5, elapsed: 11.11s, total: 8735.07s, remaining: 4045.80s\n",
      " [0661/1024] loss: 15.08768, val_loss: 36.93170, best_epoch: 5, elapsed: 11.07s, total: 8746.91s, remaining: 4018.26s\n",
      " [0662/1024] loss: 15.08746, val_loss: 36.93182, best_epoch: 5, elapsed: 11.06s, total: 8758.72s, remaining: 4002.89s\n",
      " [0663/1024] loss: 15.08723, val_loss: 36.93193, best_epoch: 5, elapsed: 10.92s, total: 8770.42s, remaining: 3942.54s\n",
      " [0664/1024] loss: 15.08701, val_loss: 36.93205, best_epoch: 5, elapsed: 10.81s, total: 8781.99s, remaining: 3890.30s\n",
      " [0665/1024] loss: 15.08678, val_loss: 36.93216, best_epoch: 5, elapsed: 10.75s, total: 8793.50s, remaining: 3858.99s\n",
      " [0666/1024] loss: 15.08655, val_loss: 36.93228, best_epoch: 5, elapsed: 10.73s, total: 8805.00s, remaining: 3841.03s\n",
      " [0667/1024] loss: 15.08633, val_loss: 36.93239, best_epoch: 5, elapsed: 10.73s, total: 8816.51s, remaining: 3829.00s\n",
      " [0668/1024] loss: 15.08610, val_loss: 36.93251, best_epoch: 5, elapsed: 10.73s, total: 8828.03s, remaining: 3820.01s\n",
      " [0669/1024] loss: 15.08588, val_loss: 36.93262, best_epoch: 5, elapsed: 10.71s, total: 8839.49s, remaining: 3803.77s\n",
      " [0670/1024] loss: 15.08565, val_loss: 36.93273, best_epoch: 5, elapsed: 10.70s, total: 8850.96s, remaining: 3789.05s\n",
      " [0671/1024] loss: 15.08543, val_loss: 36.93285, best_epoch: 5, elapsed: 10.70s, total: 8862.42s, remaining: 3777.29s\n",
      " [0672/1024] loss: 15.08520, val_loss: 36.93296, best_epoch: 5, elapsed: 10.70s, total: 8873.93s, remaining: 3768.16s\n",
      " [0673/1024] loss: 15.08497, val_loss: 36.93308, best_epoch: 5, elapsed: 10.71s, total: 8885.42s, remaining: 3760.69s\n",
      " [0674/1024] loss: 15.08475, val_loss: 36.93319, best_epoch: 5, elapsed: 10.71s, total: 8896.97s, remaining: 3747.36s\n",
      " [0675/1024] loss: 15.08452, val_loss: 36.93331, best_epoch: 5, elapsed: 10.72s, total: 8908.45s, remaining: 3741.22s\n",
      " [0676/1024] loss: 15.08430, val_loss: 36.93342, best_epoch: 5, elapsed: 10.75s, total: 8920.03s, remaining: 3740.82s\n",
      " [0677/1024] loss: 15.08407, val_loss: 36.93353, best_epoch: 5, elapsed: 10.75s, total: 8931.57s, remaining: 3729.01s\n",
      " [0678/1024] loss: 15.08384, val_loss: 36.93365, best_epoch: 5, elapsed: 10.74s, total: 8943.09s, remaining: 3716.84s\n",
      " [0679/1024] loss: 15.08362, val_loss: 36.93376, best_epoch: 5, elapsed: 10.75s, total: 8954.62s, remaining: 3710.06s\n",
      " [0680/1024] loss: 15.08339, val_loss: 36.93388, best_epoch: 5, elapsed: 10.76s, total: 8966.03s, remaining: 3702.14s\n",
      " [0681/1024] loss: 15.08317, val_loss: 36.93399, best_epoch: 5, elapsed: 10.77s, total: 8977.63s, remaining: 3695.66s\n",
      " [0682/1024] loss: 15.08294, val_loss: 36.93411, best_epoch: 5, elapsed: 10.79s, total: 8989.16s, remaining: 3691.19s\n",
      " [0683/1024] loss: 15.08272, val_loss: 36.93422, best_epoch: 5, elapsed: 10.80s, total: 9000.73s, remaining: 3682.64s\n",
      " [0684/1024] loss: 15.08249, val_loss: 36.93433, best_epoch: 5, elapsed: 10.82s, total: 9012.31s, remaining: 3680.23s\n",
      " [0685/1024] loss: 15.08226, val_loss: 36.93445, best_epoch: 5, elapsed: 10.85s, total: 9023.93s, remaining: 3677.14s\n",
      " [0686/1024] loss: 15.08204, val_loss: 36.93456, best_epoch: 5, elapsed: 10.88s, total: 9035.58s, remaining: 3678.76s\n",
      " [0687/1024] loss: 15.08181, val_loss: 36.93468, best_epoch: 5, elapsed: 10.91s, total: 9047.36s, remaining: 3677.03s\n",
      " [0688/1024] loss: 15.08159, val_loss: 36.93479, best_epoch: 5, elapsed: 10.96s, total: 9059.15s, remaining: 3682.72s\n",
      " [0689/1024] loss: 15.08136, val_loss: 36.93491, best_epoch: 5, elapsed: 10.98s, total: 9070.93s, remaining: 3677.77s\n",
      " [0690/1024] loss: 15.08114, val_loss: 36.93502, best_epoch: 5, elapsed: 11.00s, total: 9082.70s, remaining: 3674.91s\n",
      " [0691/1024] loss: 15.08091, val_loss: 36.93513, best_epoch: 5, elapsed: 11.05s, total: 9094.53s, remaining: 3679.47s\n",
      " [0692/1024] loss: 15.08068, val_loss: 36.93525, best_epoch: 5, elapsed: 11.06s, total: 9106.37s, remaining: 3672.30s\n",
      " [0693/1024] loss: 15.08046, val_loss: 36.93536, best_epoch: 5, elapsed: 11.08s, total: 9118.22s, remaining: 3667.47s\n",
      " [0694/1024] loss: 15.08023, val_loss: 36.93548, best_epoch: 5, elapsed: 11.10s, total: 9130.07s, remaining: 3661.56s\n",
      " [0695/1024] loss: 15.08001, val_loss: 36.93559, best_epoch: 5, elapsed: 11.12s, total: 9141.98s, remaining: 3657.54s\n",
      " [0696/1024] loss: 15.07978, val_loss: 36.93571, best_epoch: 5, elapsed: 11.12s, total: 9153.87s, remaining: 3648.48s\n",
      " [0697/1024] loss: 15.07956, val_loss: 36.93582, best_epoch: 5, elapsed: 11.11s, total: 9165.74s, remaining: 3633.00s\n",
      " [0698/1024] loss: 15.07933, val_loss: 36.93593, best_epoch: 5, elapsed: 11.10s, total: 9177.60s, remaining: 3618.18s\n",
      " [0699/1024] loss: 15.07911, val_loss: 36.93605, best_epoch: 5, elapsed: 11.09s, total: 9189.44s, remaining: 3604.65s\n",
      " [0700/1024] loss: 15.07888, val_loss: 36.93616, best_epoch: 5, elapsed: 11.08s, total: 9201.32s, remaining: 3589.93s\n",
      " [0701/1024] loss: 15.07866, val_loss: 36.93628, best_epoch: 5, elapsed: 11.08s, total: 9213.23s, remaining: 3578.97s\n",
      " [0702/1024] loss: 15.07843, val_loss: 36.93639, best_epoch: 5, elapsed: 11.07s, total: 9225.07s, remaining: 3565.74s\n",
      " [0703/1024] loss: 15.07820, val_loss: 36.93651, best_epoch: 5, elapsed: 11.07s, total: 9236.88s, remaining: 3553.21s\n",
      " [0704/1024] loss: 15.07798, val_loss: 36.93662, best_epoch: 5, elapsed: 11.06s, total: 9248.69s, remaining: 3540.61s\n",
      " [0705/1024] loss: 15.07775, val_loss: 36.93674, best_epoch: 5, elapsed: 11.06s, total: 9260.52s, remaining: 3527.52s\n",
      " [0706/1024] loss: 15.07753, val_loss: 36.93685, best_epoch: 5, elapsed: 11.08s, total: 9272.41s, remaining: 3523.09s\n",
      " [0707/1024] loss: 15.07730, val_loss: 36.93697, best_epoch: 5, elapsed: 11.08s, total: 9284.32s, remaining: 3513.10s\n",
      " [0708/1024] loss: 15.07708, val_loss: 36.93708, best_epoch: 5, elapsed: 11.06s, total: 9296.20s, remaining: 3494.63s\n",
      " [0709/1024] loss: 15.07685, val_loss: 36.93719, best_epoch: 5, elapsed: 11.06s, total: 9308.03s, remaining: 3483.90s\n",
      " [0710/1024] loss: 15.07663, val_loss: 36.93731, best_epoch: 5, elapsed: 11.06s, total: 9319.88s, remaining: 3471.63s\n",
      " [0711/1024] loss: 15.07640, val_loss: 36.93742, best_epoch: 5, elapsed: 11.05s, total: 9331.73s, remaining: 3457.49s\n",
      " [0712/1024] loss: 15.07618, val_loss: 36.93754, best_epoch: 5, elapsed: 11.06s, total: 9343.57s, remaining: 3449.61s\n",
      " [0713/1024] loss: 15.07595, val_loss: 36.93765, best_epoch: 5, elapsed: 11.07s, total: 9355.44s, remaining: 3441.47s\n",
      " [0714/1024] loss: 15.07573, val_loss: 36.93777, best_epoch: 5, elapsed: 11.05s, total: 9367.25s, remaining: 3426.32s\n",
      " [0715/1024] loss: 15.07550, val_loss: 36.93788, best_epoch: 5, elapsed: 11.05s, total: 9379.06s, remaining: 3415.26s\n",
      " [0716/1024] loss: 15.07528, val_loss: 36.93800, best_epoch: 5, elapsed: 11.04s, total: 9390.91s, remaining: 3401.27s\n",
      " [0717/1024] loss: 15.07505, val_loss: 36.93811, best_epoch: 5, elapsed: 11.06s, total: 9402.74s, remaining: 3395.04s\n",
      " [0718/1024] loss: 15.07483, val_loss: 36.93822, best_epoch: 5, elapsed: 11.05s, total: 9414.58s, remaining: 3382.58s\n",
      " [0719/1024] loss: 15.07460, val_loss: 36.93834, best_epoch: 5, elapsed: 11.06s, total: 9426.41s, remaining: 3372.03s\n",
      " [0720/1024] loss: 15.07437, val_loss: 36.93845, best_epoch: 5, elapsed: 11.05s, total: 9438.26s, remaining: 3359.94s\n",
      " [0721/1024] loss: 15.07415, val_loss: 36.93857, best_epoch: 5, elapsed: 11.04s, total: 9450.13s, remaining: 3346.32s\n",
      " [0722/1024] loss: 15.07392, val_loss: 36.93868, best_epoch: 5, elapsed: 11.11s, total: 9462.00s, remaining: 3353.92s\n",
      " [0723/1024] loss: 15.07370, val_loss: 36.93879, best_epoch: 5, elapsed: 11.05s, total: 9473.82s, remaining: 3325.10s\n",
      " [0724/1024] loss: 15.07347, val_loss: 36.93891, best_epoch: 5, elapsed: 11.05s, total: 9485.61s, remaining: 3314.81s\n",
      " [0725/1024] loss: 15.07325, val_loss: 36.93902, best_epoch: 5, elapsed: 11.05s, total: 9497.44s, remaining: 3302.99s\n",
      " [0726/1024] loss: 15.07302, val_loss: 36.93914, best_epoch: 5, elapsed: 11.04s, total: 9509.24s, remaining: 3289.62s\n",
      " [0727/1024] loss: 15.07280, val_loss: 36.93925, best_epoch: 5, elapsed: 11.05s, total: 9521.04s, remaining: 3280.63s\n",
      " [0728/1024] loss: 15.07257, val_loss: 36.93937, best_epoch: 5, elapsed: 11.04s, total: 9532.90s, remaining: 3268.93s\n",
      " [0729/1024] loss: 15.07235, val_loss: 36.93948, best_epoch: 5, elapsed: 11.05s, total: 9544.69s, remaining: 3259.00s\n",
      " [0730/1024] loss: 15.07212, val_loss: 36.93959, best_epoch: 5, elapsed: 11.05s, total: 9556.51s, remaining: 3247.76s\n",
      " [0731/1024] loss: 15.07190, val_loss: 36.93971, best_epoch: 5, elapsed: 11.06s, total: 9568.35s, remaining: 3240.84s\n",
      " [0732/1024] loss: 15.07167, val_loss: 36.93982, best_epoch: 5, elapsed: 11.04s, total: 9580.13s, remaining: 3224.34s\n",
      " [0733/1024] loss: 15.07145, val_loss: 36.93994, best_epoch: 5, elapsed: 11.05s, total: 9591.93s, remaining: 3214.70s\n",
      " [0734/1024] loss: 15.07122, val_loss: 36.94005, best_epoch: 5, elapsed: 11.05s, total: 9603.78s, remaining: 3205.45s\n",
      " [0735/1024] loss: 15.07100, val_loss: 36.94016, best_epoch: 5, elapsed: 11.04s, total: 9615.58s, remaining: 3191.70s\n",
      " [0736/1024] loss: 15.07077, val_loss: 36.94027, best_epoch: 5, elapsed: 11.04s, total: 9627.39s, remaining: 3180.69s\n",
      " [0737/1024] loss: 15.07055, val_loss: 36.94039, best_epoch: 5, elapsed: 11.09s, total: 9639.23s, remaining: 3181.52s\n",
      " [0738/1024] loss: 15.07032, val_loss: 36.94050, best_epoch: 5, elapsed: 11.06s, total: 9651.05s, remaining: 3162.43s\n",
      " [0739/1024] loss: 15.07010, val_loss: 36.94062, best_epoch: 5, elapsed: 10.93s, total: 9662.77s, remaining: 3114.68s\n",
      " [0740/1024] loss: 15.06987, val_loss: 36.94073, best_epoch: 5, elapsed: 10.82s, total: 9674.34s, remaining: 3072.70s\n",
      " [0741/1024] loss: 15.06965, val_loss: 36.94085, best_epoch: 5, elapsed: 10.75s, total: 9685.87s, remaining: 3042.32s\n",
      " [0742/1024] loss: 15.06942, val_loss: 36.94096, best_epoch: 5, elapsed: 10.73s, total: 9697.37s, remaining: 3025.41s\n",
      " [0743/1024] loss: 15.06920, val_loss: 36.94108, best_epoch: 5, elapsed: 10.71s, total: 9708.83s, remaining: 3010.09s\n",
      " [0744/1024] loss: 15.06897, val_loss: 36.94119, best_epoch: 5, elapsed: 10.71s, total: 9720.29s, remaining: 2997.86s\n",
      " [0745/1024] loss: 15.06875, val_loss: 36.94130, best_epoch: 5, elapsed: 10.70s, total: 9731.75s, remaining: 2986.40s\n",
      " [0746/1024] loss: 15.06852, val_loss: 36.94142, best_epoch: 5, elapsed: 10.70s, total: 9743.21s, remaining: 2973.73s\n",
      " [0747/1024] loss: 15.06830, val_loss: 36.94153, best_epoch: 5, elapsed: 10.69s, total: 9754.66s, remaining: 2962.42s\n",
      " [0748/1024] loss: 15.06807, val_loss: 36.94164, best_epoch: 5, elapsed: 10.69s, total: 9766.11s, remaining: 2951.08s\n",
      " [0749/1024] loss: 15.06785, val_loss: 36.94176, best_epoch: 5, elapsed: 10.70s, total: 9777.55s, remaining: 2941.63s\n",
      " [0750/1024] loss: 15.06762, val_loss: 36.94187, best_epoch: 5, elapsed: 10.70s, total: 9788.99s, remaining: 2931.76s\n",
      " [0751/1024] loss: 15.06740, val_loss: 36.94199, best_epoch: 5, elapsed: 10.72s, total: 9800.49s, remaining: 2926.06s\n",
      " [0752/1024] loss: 15.06717, val_loss: 36.94210, best_epoch: 5, elapsed: 10.72s, total: 9811.97s, remaining: 2915.54s\n",
      " [0753/1024] loss: 15.06695, val_loss: 36.94221, best_epoch: 5, elapsed: 10.74s, total: 9823.52s, remaining: 2910.55s\n",
      " [0754/1024] loss: 15.06672, val_loss: 36.94233, best_epoch: 5, elapsed: 10.73s, total: 9835.02s, remaining: 2896.37s\n",
      " [0755/1024] loss: 15.06650, val_loss: 36.94244, best_epoch: 5, elapsed: 10.76s, total: 9846.55s, remaining: 2893.96s\n",
      " [0756/1024] loss: 15.06627, val_loss: 36.94255, best_epoch: 5, elapsed: 10.75s, total: 9858.06s, remaining: 2880.39s\n",
      " [0757/1024] loss: 15.06605, val_loss: 36.94267, best_epoch: 5, elapsed: 10.77s, total: 9869.58s, remaining: 2874.43s\n",
      " [0758/1024] loss: 15.06582, val_loss: 36.94278, best_epoch: 5, elapsed: 10.78s, total: 9881.11s, remaining: 2866.69s\n",
      " [0759/1024] loss: 15.06560, val_loss: 36.94289, best_epoch: 5, elapsed: 10.78s, total: 9892.67s, remaining: 2857.69s\n",
      " [0760/1024] loss: 15.06538, val_loss: 36.94301, best_epoch: 5, elapsed: 10.82s, total: 9904.25s, remaining: 2855.83s\n",
      " [0761/1024] loss: 15.06515, val_loss: 36.94312, best_epoch: 5, elapsed: 10.83s, total: 9915.84s, remaining: 2848.67s\n",
      " [0762/1024] loss: 15.06493, val_loss: 36.94324, best_epoch: 5, elapsed: 10.86s, total: 9927.46s, remaining: 2846.53s\n",
      " [0763/1024] loss: 15.06470, val_loss: 36.94335, best_epoch: 5, elapsed: 10.90s, total: 9939.11s, remaining: 2844.69s\n",
      " [0764/1024] loss: 15.06448, val_loss: 36.94346, best_epoch: 5, elapsed: 10.93s, total: 9950.82s, remaining: 2840.63s\n",
      " [0765/1024] loss: 15.06425, val_loss: 36.94358, best_epoch: 5, elapsed: 10.99s, total: 9962.63s, remaining: 2845.24s\n",
      " [0766/1024] loss: 15.06403, val_loss: 36.94369, best_epoch: 5, elapsed: 11.00s, total: 9974.40s, remaining: 2838.82s\n",
      " [0767/1024] loss: 15.06380, val_loss: 36.94380, best_epoch: 5, elapsed: 11.03s, total: 9986.27s, remaining: 2835.10s\n",
      " [0768/1024] loss: 15.06358, val_loss: 36.94392, best_epoch: 5, elapsed: 11.10s, total: 9998.15s, remaining: 2841.61s\n",
      " [0769/1024] loss: 15.06335, val_loss: 36.94403, best_epoch: 5, elapsed: 10.92s, total: 10009.82s, remaining: 2784.26s\n",
      " [0770/1024] loss: 15.06313, val_loss: 36.94414, best_epoch: 5, elapsed: 10.81s, total: 10021.40s, remaining: 2746.63s\n",
      " [0771/1024] loss: 15.06290, val_loss: 36.94426, best_epoch: 5, elapsed: 10.77s, total: 10032.92s, remaining: 2724.48s\n",
      " [0772/1024] loss: 15.06268, val_loss: 36.94437, best_epoch: 5, elapsed: 10.72s, total: 10044.45s, remaining: 2702.41s\n",
      " [0773/1024] loss: 15.06245, val_loss: 36.94448, best_epoch: 5, elapsed: 10.71s, total: 10055.95s, remaining: 2687.74s\n",
      " [0774/1024] loss: 15.06223, val_loss: 36.94460, best_epoch: 5, elapsed: 10.69s, total: 10067.41s, remaining: 2672.22s\n",
      " [0775/1024] loss: 15.06201, val_loss: 36.94471, best_epoch: 5, elapsed: 10.69s, total: 10078.84s, remaining: 2660.98s\n",
      " [0776/1024] loss: 15.06178, val_loss: 36.94482, best_epoch: 5, elapsed: 10.69s, total: 10090.28s, remaining: 2650.35s\n",
      " [0777/1024] loss: 15.06156, val_loss: 36.94494, best_epoch: 5, elapsed: 10.69s, total: 10101.77s, remaining: 2639.70s\n",
      " [0778/1024] loss: 15.06133, val_loss: 36.94505, best_epoch: 5, elapsed: 10.68s, total: 10113.21s, remaining: 2628.05s\n",
      " [0779/1024] loss: 15.06111, val_loss: 36.94517, best_epoch: 5, elapsed: 10.68s, total: 10124.65s, remaining: 2615.92s\n",
      " [0780/1024] loss: 15.06088, val_loss: 36.94528, best_epoch: 5, elapsed: 10.69s, total: 10136.10s, remaining: 2608.01s\n",
      " [0781/1024] loss: 15.06066, val_loss: 36.94539, best_epoch: 5, elapsed: 10.70s, total: 10147.55s, remaining: 2599.64s\n",
      " [0782/1024] loss: 15.06043, val_loss: 36.94551, best_epoch: 5, elapsed: 10.71s, total: 10158.99s, remaining: 2591.07s\n",
      " [0783/1024] loss: 15.06021, val_loss: 36.94562, best_epoch: 5, elapsed: 10.71s, total: 10170.46s, remaining: 2581.25s\n",
      " [0784/1024] loss: 15.05998, val_loss: 36.94574, best_epoch: 5, elapsed: 10.76s, total: 10182.01s, remaining: 2581.98s\n",
      " [0785/1024] loss: 15.05976, val_loss: 36.94585, best_epoch: 5, elapsed: 10.72s, total: 10193.52s, remaining: 2562.71s\n",
      " [0786/1024] loss: 15.05954, val_loss: 36.94597, best_epoch: 5, elapsed: 10.73s, total: 10205.00s, remaining: 2553.26s\n",
      " [0787/1024] loss: 15.05931, val_loss: 36.94608, best_epoch: 5, elapsed: 10.75s, total: 10216.49s, remaining: 2547.04s\n",
      " [0788/1024] loss: 15.05909, val_loss: 36.94620, best_epoch: 5, elapsed: 10.78s, total: 10228.03s, remaining: 2543.53s\n",
      " [0789/1024] loss: 15.05886, val_loss: 36.94631, best_epoch: 5, elapsed: 10.77s, total: 10239.57s, remaining: 2532.06s\n",
      " [0790/1024] loss: 15.05864, val_loss: 36.94642, best_epoch: 5, elapsed: 10.77s, total: 10251.09s, remaining: 2519.10s\n",
      " [0791/1024] loss: 15.05841, val_loss: 36.94654, best_epoch: 5, elapsed: 10.72s, total: 10262.60s, remaining: 2497.88s\n",
      " [0792/1024] loss: 15.05819, val_loss: 36.94666, best_epoch: 5, elapsed: 10.71s, total: 10274.11s, remaining: 2484.69s\n",
      " [0793/1024] loss: 15.05796, val_loss: 36.94677, best_epoch: 5, elapsed: 10.69s, total: 10285.60s, remaining: 2470.28s\n",
      " [0794/1024] loss: 15.05774, val_loss: 36.94688, best_epoch: 5, elapsed: 10.67s, total: 10297.02s, remaining: 2453.55s\n",
      " [0795/1024] loss: 15.05752, val_loss: 36.94700, best_epoch: 5, elapsed: 10.66s, total: 10308.45s, remaining: 2442.28s\n",
      " [0796/1024] loss: 15.05729, val_loss: 36.94711, best_epoch: 5, elapsed: 10.67s, total: 10319.88s, remaining: 2431.63s\n",
      " [0797/1024] loss: 15.05707, val_loss: 36.94723, best_epoch: 5, elapsed: 10.66s, total: 10331.32s, remaining: 2420.74s\n",
      " [0798/1024] loss: 15.05684, val_loss: 36.94734, best_epoch: 5, elapsed: 10.66s, total: 10342.75s, remaining: 2408.89s\n",
      " [0799/1024] loss: 15.05662, val_loss: 36.94746, best_epoch: 5, elapsed: 10.70s, total: 10354.21s, remaining: 2407.02s\n",
      " [0800/1024] loss: 15.05639, val_loss: 36.94757, best_epoch: 5, elapsed: 10.68s, total: 10365.66s, remaining: 2391.45s\n",
      " [0801/1024] loss: 15.05617, val_loss: 36.94768, best_epoch: 5, elapsed: 10.66s, total: 10377.07s, remaining: 2376.38s\n",
      " [0802/1024] loss: 15.05595, val_loss: 36.94780, best_epoch: 5, elapsed: 10.64s, total: 10388.46s, remaining: 2362.24s\n",
      " [0803/1024] loss: 15.05572, val_loss: 36.94791, best_epoch: 5, elapsed: 10.66s, total: 10399.88s, remaining: 2356.41s\n",
      " [0804/1024] loss: 15.05550, val_loss: 36.94803, best_epoch: 5, elapsed: 10.66s, total: 10411.29s, remaining: 2344.61s\n",
      " [0805/1024] loss: 15.05527, val_loss: 36.94814, best_epoch: 5, elapsed: 10.67s, total: 10422.76s, remaining: 2335.67s\n",
      " [0806/1024] loss: 15.05505, val_loss: 36.94826, best_epoch: 5, elapsed: 10.66s, total: 10434.21s, remaining: 2324.80s\n",
      " [0807/1024] loss: 15.05482, val_loss: 36.94837, best_epoch: 5, elapsed: 10.67s, total: 10445.63s, remaining: 2316.02s\n",
      " [0808/1024] loss: 15.05460, val_loss: 36.94849, best_epoch: 5, elapsed: 10.67s, total: 10457.06s, remaining: 2304.67s\n",
      " [0809/1024] loss: 15.05438, val_loss: 36.94860, best_epoch: 5, elapsed: 10.68s, total: 10468.49s, remaining: 2296.33s\n",
      " [0810/1024] loss: 15.05415, val_loss: 36.94871, best_epoch: 5, elapsed: 10.69s, total: 10479.94s, remaining: 2287.77s\n",
      " [0811/1024] loss: 15.05393, val_loss: 36.94883, best_epoch: 5, elapsed: 10.70s, total: 10491.39s, remaining: 2278.35s\n",
      " [0812/1024] loss: 15.05370, val_loss: 36.94894, best_epoch: 5, elapsed: 10.71s, total: 10502.86s, remaining: 2269.76s\n",
      " [0813/1024] loss: 15.05348, val_loss: 36.94906, best_epoch: 5, elapsed: 10.71s, total: 10514.36s, remaining: 2258.82s\n",
      " [0814/1024] loss: 15.05325, val_loss: 36.94917, best_epoch: 5, elapsed: 10.71s, total: 10525.83s, remaining: 2249.83s\n",
      " [0815/1024] loss: 15.05303, val_loss: 36.94929, best_epoch: 5, elapsed: 10.76s, total: 10537.34s, remaining: 2247.98s\n",
      " [0816/1024] loss: 15.05281, val_loss: 36.94940, best_epoch: 5, elapsed: 10.75s, total: 10548.85s, remaining: 2236.10s\n",
      " [0817/1024] loss: 15.05258, val_loss: 36.94952, best_epoch: 5, elapsed: 10.75s, total: 10560.37s, remaining: 2226.08s\n",
      " [0818/1024] loss: 15.05236, val_loss: 36.94963, best_epoch: 5, elapsed: 10.78s, total: 10571.96s, remaining: 2221.16s\n",
      " [0819/1024] loss: 15.05213, val_loss: 36.94974, best_epoch: 5, elapsed: 10.79s, total: 10583.53s, remaining: 2212.49s\n",
      " [0820/1024] loss: 15.05191, val_loss: 36.94986, best_epoch: 5, elapsed: 10.82s, total: 10595.11s, remaining: 2207.77s\n",
      " [0821/1024] loss: 15.05169, val_loss: 36.94997, best_epoch: 5, elapsed: 10.85s, total: 10606.74s, remaining: 2201.56s\n",
      " [0822/1024] loss: 15.05146, val_loss: 36.95009, best_epoch: 5, elapsed: 10.89s, total: 10618.39s, remaining: 2200.65s\n",
      " [0823/1024] loss: 15.05124, val_loss: 36.95020, best_epoch: 5, elapsed: 10.92s, total: 10630.08s, remaining: 2195.81s\n",
      " [0824/1024] loss: 15.05101, val_loss: 36.95032, best_epoch: 5, elapsed: 10.90s, total: 10641.76s, remaining: 2179.25s\n",
      " [0825/1024] loss: 15.05079, val_loss: 36.95043, best_epoch: 5, elapsed: 10.78s, total: 10653.30s, remaining: 2145.63s\n",
      " [0826/1024] loss: 15.05057, val_loss: 36.95055, best_epoch: 5, elapsed: 10.73s, total: 10664.86s, remaining: 2124.52s\n",
      " [0827/1024] loss: 15.05034, val_loss: 36.95066, best_epoch: 5, elapsed: 10.72s, total: 10676.35s, remaining: 2110.93s\n",
      " [0828/1024] loss: 15.05012, val_loss: 36.95078, best_epoch: 5, elapsed: 10.71s, total: 10687.85s, remaining: 2099.82s\n",
      " [0829/1024] loss: 15.04989, val_loss: 36.95089, best_epoch: 5, elapsed: 10.69s, total: 10699.32s, remaining: 2084.57s\n",
      " [0830/1024] loss: 15.04967, val_loss: 36.95100, best_epoch: 5, elapsed: 10.69s, total: 10710.81s, remaining: 2074.21s\n",
      " [0831/1024] loss: 15.04945, val_loss: 36.95112, best_epoch: 5, elapsed: 10.73s, total: 10722.32s, remaining: 2069.96s\n",
      " [0832/1024] loss: 15.04922, val_loss: 36.95123, best_epoch: 5, elapsed: 10.69s, total: 10733.78s, remaining: 2051.54s\n",
      " [0833/1024] loss: 15.04900, val_loss: 36.95135, best_epoch: 5, elapsed: 10.69s, total: 10745.30s, remaining: 2041.98s\n",
      " [0834/1024] loss: 15.04877, val_loss: 36.95146, best_epoch: 5, elapsed: 10.69s, total: 10756.76s, remaining: 2031.49s\n",
      " [0835/1024] loss: 15.04855, val_loss: 36.95158, best_epoch: 5, elapsed: 10.70s, total: 10768.23s, remaining: 2022.75s\n",
      " [0836/1024] loss: 15.04833, val_loss: 36.95169, best_epoch: 5, elapsed: 10.69s, total: 10779.69s, remaining: 2010.28s\n",
      " [0837/1024] loss: 15.04810, val_loss: 36.95181, best_epoch: 5, elapsed: 10.71s, total: 10791.17s, remaining: 2002.41s\n",
      " [0838/1024] loss: 15.04788, val_loss: 36.95192, best_epoch: 5, elapsed: 10.74s, total: 10802.68s, remaining: 1997.42s\n",
      " [0839/1024] loss: 15.04765, val_loss: 36.95204, best_epoch: 5, elapsed: 10.72s, total: 10814.15s, remaining: 1984.10s\n",
      " [0840/1024] loss: 15.04743, val_loss: 36.95215, best_epoch: 5, elapsed: 10.74s, total: 10825.65s, remaining: 1976.91s\n",
      " [0841/1024] loss: 15.04721, val_loss: 36.95227, best_epoch: 5, elapsed: 10.75s, total: 10837.20s, remaining: 1967.98s\n",
      " [0842/1024] loss: 15.04698, val_loss: 36.95238, best_epoch: 5, elapsed: 10.78s, total: 10848.77s, remaining: 1962.00s\n",
      " [0843/1024] loss: 15.04676, val_loss: 36.95250, best_epoch: 5, elapsed: 10.79s, total: 10860.35s, remaining: 1953.12s\n",
      " [0844/1024] loss: 15.04654, val_loss: 36.95261, best_epoch: 5, elapsed: 10.80s, total: 10871.91s, remaining: 1943.82s\n",
      " [0845/1024] loss: 15.04631, val_loss: 36.95273, best_epoch: 5, elapsed: 10.81s, total: 10883.48s, remaining: 1935.68s\n",
      " [0846/1024] loss: 15.04609, val_loss: 36.95284, best_epoch: 5, elapsed: 10.90s, total: 10895.15s, remaining: 1940.87s\n",
      " [0847/1024] loss: 15.04586, val_loss: 36.95296, best_epoch: 5, elapsed: 10.89s, total: 10906.85s, remaining: 1928.05s\n",
      " [0848/1024] loss: 15.04564, val_loss: 36.95307, best_epoch: 5, elapsed: 10.90s, total: 10918.53s, remaining: 1918.70s\n",
      " [0849/1024] loss: 15.04542, val_loss: 36.95319, best_epoch: 5, elapsed: 10.93s, total: 10930.23s, remaining: 1912.26s\n",
      " [0850/1024] loss: 15.04519, val_loss: 36.95331, best_epoch: 5, elapsed: 10.97s, total: 10941.96s, remaining: 1908.43s\n",
      " [0851/1024] loss: 15.04497, val_loss: 36.95342, best_epoch: 5, elapsed: 10.99s, total: 10953.70s, remaining: 1901.31s\n",
      " [0852/1024] loss: 15.04474, val_loss: 36.95354, best_epoch: 5, elapsed: 11.03s, total: 10965.48s, remaining: 1896.32s\n",
      " [0853/1024] loss: 15.04452, val_loss: 36.95365, best_epoch: 5, elapsed: 11.06s, total: 10977.30s, remaining: 1891.13s\n",
      " [0854/1024] loss: 15.04430, val_loss: 36.95377, best_epoch: 5, elapsed: 11.08s, total: 10989.15s, remaining: 1883.14s\n",
      " [0855/1024] loss: 15.04407, val_loss: 36.95388, best_epoch: 5, elapsed: 11.11s, total: 11001.06s, remaining: 1877.98s\n",
      " [0856/1024] loss: 15.04385, val_loss: 36.95400, best_epoch: 5, elapsed: 11.14s, total: 11012.96s, remaining: 1871.70s\n",
      " [0857/1024] loss: 15.04363, val_loss: 36.95411, best_epoch: 5, elapsed: 11.13s, total: 11024.84s, remaining: 1859.35s\n",
      " [0858/1024] loss: 15.04340, val_loss: 36.95423, best_epoch: 5, elapsed: 11.13s, total: 11036.75s, remaining: 1847.75s\n",
      " [0859/1024] loss: 15.04318, val_loss: 36.95434, best_epoch: 5, elapsed: 11.17s, total: 11048.69s, remaining: 1842.68s\n",
      " [0860/1024] loss: 15.04296, val_loss: 36.95446, best_epoch: 5, elapsed: 11.17s, total: 11060.63s, remaining: 1832.54s\n",
      " [0861/1024] loss: 15.04273, val_loss: 36.95457, best_epoch: 5, elapsed: 11.15s, total: 11072.53s, remaining: 1817.11s\n",
      " [0862/1024] loss: 15.04251, val_loss: 36.95469, best_epoch: 5, elapsed: 11.16s, total: 11084.45s, remaining: 1808.26s\n",
      " [0863/1024] loss: 15.04228, val_loss: 36.95480, best_epoch: 5, elapsed: 11.15s, total: 11096.41s, remaining: 1794.93s\n",
      " [0864/1024] loss: 15.04206, val_loss: 36.95492, best_epoch: 5, elapsed: 11.14s, total: 11108.29s, remaining: 1781.83s\n",
      " [0865/1024] loss: 15.04184, val_loss: 36.95503, best_epoch: 5, elapsed: 11.07s, total: 11120.12s, remaining: 1759.36s\n",
      " [0866/1024] loss: 15.04161, val_loss: 36.95515, best_epoch: 5, elapsed: 10.89s, total: 11131.76s, remaining: 1720.87s\n",
      " [0867/1024] loss: 15.04139, val_loss: 36.95527, best_epoch: 5, elapsed: 10.82s, total: 11143.42s, remaining: 1698.13s\n",
      " [0868/1024] loss: 15.04117, val_loss: 36.95538, best_epoch: 5, elapsed: 10.77s, total: 11155.01s, remaining: 1680.56s\n",
      " [0869/1024] loss: 15.04094, val_loss: 36.95550, best_epoch: 5, elapsed: 10.72s, total: 11166.56s, remaining: 1661.89s\n",
      " [0870/1024] loss: 15.04072, val_loss: 36.95561, best_epoch: 5, elapsed: 10.71s, total: 11178.02s, remaining: 1649.12s\n",
      " [0871/1024] loss: 15.04050, val_loss: 36.95573, best_epoch: 5, elapsed: 10.70s, total: 11189.48s, remaining: 1636.81s\n",
      " [0872/1024] loss: 15.04027, val_loss: 36.95584, best_epoch: 5, elapsed: 10.68s, total: 11200.95s, remaining: 1623.70s\n",
      " [0873/1024] loss: 15.04005, val_loss: 36.95596, best_epoch: 5, elapsed: 10.69s, total: 11212.42s, remaining: 1614.51s\n",
      " [0874/1024] loss: 15.03983, val_loss: 36.95607, best_epoch: 5, elapsed: 10.70s, total: 11223.88s, remaining: 1604.81s\n",
      " [0875/1024] loss: 15.03960, val_loss: 36.95619, best_epoch: 5, elapsed: 10.70s, total: 11235.34s, remaining: 1593.77s\n",
      " [0876/1024] loss: 15.03938, val_loss: 36.95630, best_epoch: 5, elapsed: 10.70s, total: 11246.81s, remaining: 1582.91s\n",
      " [0877/1024] loss: 15.03916, val_loss: 36.95642, best_epoch: 5, elapsed: 10.74s, total: 11258.33s, remaining: 1579.40s\n",
      " [0878/1024] loss: 15.03893, val_loss: 36.95653, best_epoch: 5, elapsed: 10.70s, total: 11269.84s, remaining: 1562.72s\n",
      " [0879/1024] loss: 15.03871, val_loss: 36.95665, best_epoch: 5, elapsed: 10.72s, total: 11281.38s, remaining: 1553.77s\n",
      " [0880/1024] loss: 15.03849, val_loss: 36.95676, best_epoch: 5, elapsed: 10.73s, total: 11292.89s, remaining: 1544.47s\n",
      " [0881/1024] loss: 15.03826, val_loss: 36.95688, best_epoch: 5, elapsed: 10.74s, total: 11304.38s, remaining: 1535.82s\n",
      " [0882/1024] loss: 15.03804, val_loss: 36.95699, best_epoch: 5, elapsed: 10.75s, total: 11315.90s, remaining: 1526.64s\n",
      " [0883/1024] loss: 15.03782, val_loss: 36.95711, best_epoch: 5, elapsed: 10.75s, total: 11327.41s, remaining: 1515.91s\n",
      " [0884/1024] loss: 15.03759, val_loss: 36.95722, best_epoch: 5, elapsed: 10.79s, total: 11338.98s, remaining: 1510.44s\n",
      " [0885/1024] loss: 15.03737, val_loss: 36.95733, best_epoch: 5, elapsed: 10.78s, total: 11350.53s, remaining: 1498.45s\n",
      " [0886/1024] loss: 15.03715, val_loss: 36.95745, best_epoch: 5, elapsed: 10.81s, total: 11362.12s, remaining: 1492.01s\n",
      " [0887/1024] loss: 15.03692, val_loss: 36.95757, best_epoch: 5, elapsed: 10.83s, total: 11373.72s, remaining: 1483.82s\n",
      " [0888/1024] loss: 15.03670, val_loss: 36.95768, best_epoch: 5, elapsed: 10.85s, total: 11385.34s, remaining: 1475.89s\n",
      " [0889/1024] loss: 15.03648, val_loss: 36.95780, best_epoch: 5, elapsed: 10.90s, total: 11397.02s, remaining: 1471.71s\n",
      " [0890/1024] loss: 15.03625, val_loss: 36.95791, best_epoch: 5, elapsed: 10.91s, total: 11408.68s, remaining: 1462.34s\n",
      " [0891/1024] loss: 15.03603, val_loss: 36.95803, best_epoch: 5, elapsed: 10.95s, total: 11420.43s, remaining: 1456.20s\n",
      " [0892/1024] loss: 15.03581, val_loss: 36.95814, best_epoch: 5, elapsed: 11.00s, total: 11432.24s, remaining: 1452.34s\n",
      " [0893/1024] loss: 15.03558, val_loss: 36.95826, best_epoch: 5, elapsed: 11.04s, total: 11444.07s, remaining: 1446.44s\n",
      " [0894/1024] loss: 15.03536, val_loss: 36.95837, best_epoch: 5, elapsed: 11.05s, total: 11455.88s, remaining: 1436.14s\n",
      " [0895/1024] loss: 15.03514, val_loss: 36.95849, best_epoch: 5, elapsed: 11.08s, total: 11467.71s, remaining: 1429.03s\n",
      " [0896/1024] loss: 15.03491, val_loss: 36.95860, best_epoch: 5, elapsed: 11.08s, total: 11479.58s, remaining: 1418.35s\n",
      " [0897/1024] loss: 15.03469, val_loss: 36.95872, best_epoch: 5, elapsed: 11.09s, total: 11491.43s, remaining: 1407.99s\n",
      " [0898/1024] loss: 15.03447, val_loss: 36.95883, best_epoch: 5, elapsed: 11.12s, total: 11503.31s, remaining: 1401.08s\n"
     ]
    }
   ],
   "source": [
    "best_model = train(\n",
    "    model, criterion, optimizer, train_loader, val_loader,\n",
    "    CFG.EPOCHS, early_stopping,\n",
    "    device=device, scheduler=scheduler,\n",
    "    metric_period=1, verbose=True,\n",
    "    save_model_path='./mc/best_model.pt',\n",
    "    use_best_model=False,\n",
    "    inverse_transform=inverse_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e076fa1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb4b84",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = MultiTaskModel(feature_input_size,output_size,hidden_size,dropout_rate)\n",
    "best_model.load_state_dict(torch.load('./mc/best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_true, tr_pred = predict(best_model,train_loader,device,inverse_transform)\n",
    "va_true, va_pred = predict(best_model,val_loader,device,inverse_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5dec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "(MultiRMSELoss()(torch.tensor(tr_true),torch.tensor(tr_pred)),\n",
    " MultiRMSELoss()(torch.tensor(va_true),torch.tensor(va_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78516950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_true[:10].round(1), tr_pred[:10].round(1)\n",
    "# va_true[:10].round(1), va_pred[:10].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(intercept,slope):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, linestyle='--', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.add_subplot(121)\n",
    "sns.scatterplot(x=tr_true[:,0],y=tr_pred[:,0])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "fig.add_subplot(122)\n",
    "sns.scatterplot(x=tr_true[:,1],y=tr_pred[:,1])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.suptitle('train',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.add_subplot(121)\n",
    "sns.scatterplot(x=va_true[:,0],y=va_pred[:,0])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "fig.add_subplot(122)\n",
    "sns.scatterplot(x=va_true[:,1],y=va_pred[:,1])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.suptitle('validation',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdaae30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "88a5da79f9030d36a713e3ceec9ed9a47a216907c035af9944c458137c4e5cb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
