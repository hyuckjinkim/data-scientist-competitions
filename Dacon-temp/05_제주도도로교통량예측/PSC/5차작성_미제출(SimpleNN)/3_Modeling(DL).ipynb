{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0ee973",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Setting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014e688",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13025345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97601dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytimekr\n",
    "# !pip install optuna\n",
    "# !pip install datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b36166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d MyPython/2_Dacon_JEJU/DAT/\n",
    "# unzip open.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47040a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fe = fm.FontEntry(fname='../NanumFont/NanumGothic.ttf',name='NanumGothic')\n",
    "fm.fontManager.ttflist.insert(0, fe)  # or append is fine\n",
    "mpl.rcParams['font.family'] = fe.name # = 'your custom ttf font name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "570779dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n",
    "os.environ['PYTHONWARNINGS']='ignore::FutureWarning'\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "from pytimekr import pytimekr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import datatable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1332185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution\n",
    "\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9064288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--',color=color)\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + directory)\n",
    "        \n",
    "def cnt(x):\n",
    "    vc = x.value_counts().sort_index()\n",
    "    res = pd.DataFrame({\n",
    "        'index' : vc.index,\n",
    "        'freq'  : vc.values,\n",
    "    })\n",
    "    res['rate'] = 100 * res['freq'] / res['freq'].sum()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82beddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# verbose=0로 만들어주는 함수\n",
    "# (참조) https://stackoverflow.com/questions/11130156/suppress-stdout-stderr-print-from-python-functions\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "    Python, i.e. will suppress all print, even if the print originates in a\n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7be2fb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "User Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a2c9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = \"/home/ec2-user/SageMaker/Temp/Dacon/5_제주도도로교통량예측/lib\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46b1fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from send_email import send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19aadc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3ccf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-12 18:12:50.236427\n"
     ]
    }
   ],
   "source": [
    "DAT_PATH = \"../DAT/\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e3e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_SETTING = '/device:GPU:0' #'/device:CPU:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1e01c",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4337d9",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling\n",
    "---\n",
    "- tf.data.dataset 참조\n",
    "    - [참조1](https://nodoudt.tistory.com/43)\n",
    "    - [참조2](https://ericabae.medium.com/tensorflow-2-0-csv-%ED%8C%8C%EC%9D%BC-%ED%98%95%EC%8B%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0-eddaa88d3112)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ec0b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c710ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.layers import Input, Dense, RepeatVector, LSTM, GRU, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7de4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for DL\n",
    "def huber_loss(true, pred):\n",
    "    threshold = 1\n",
    "    error = true - pred\n",
    "    is_small_error = tf.abs(error) <= threshold\n",
    "    small_error_loss = tf.square(error) / 2\n",
    "    big_error_loss = threshold * (tf.abs(error) - (0.5 * threshold))\n",
    "    return tf.where(is_small_error, small_error_loss, big_error_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b46cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path,batch_size,num_epochs,is_pack,shuffle,**kwargs):\n",
    "    \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        ignore_errors=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    def pack(features, label):\n",
    "        return tf.stack(list(features.values()), axis=-1), label\n",
    "    \n",
    "    if is_pack:\n",
    "        dataset = dataset.map(pack)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(500)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82bcafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "> start time : 2022-11-12 18:12:50.236427\n",
      ">   end time : 2022-11-12 18:13:00.942556\n",
      ">   run time : 0:00:10.706129\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "run_time = end_time-start_time\n",
    "\n",
    "print('='*45)\n",
    "print(f'> start time : {start_time}')\n",
    "print(f'>   end time : {end_time}')\n",
    "print(f'>   run time : {run_time:}')\n",
    "print('='*45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16c171",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eda07",
   "metadata": {},
   "source": [
    "### by all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_oh = pd.read_parquet('../OUT/X_train_oh.parquet.gz')\n",
    "# X_test_oh  = pd.read_parquet('../OUT/X_test_oh.parquet.gz')\n",
    "# y_train    = pd.read_csv('../OUT/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([\n",
    "#     X_train_oh.drop('segment',axis=1),\n",
    "#     y_train,\n",
    "# ],axis=1)\\\n",
    "#     .astype(float)\\\n",
    "#     .to_csv('../OUT/train_fn_oh_noseg.csv',index=False)\n",
    "# X_test_oh.drop('segment',axis=1).astype(float).to_csv('../OUT/test_fn_oh_noseg.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d964181",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133781ea",
   "metadata": {},
   "source": [
    "### by segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f5687",
   "metadata": {},
   "source": [
    "## by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "669695e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "segment = [path.replace('../OUT/segment/','') for path in glob.glob('../OUT/segment/*')]\n",
    "len(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccd3adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = pd.read_parquet(f'../OUT/X_train_oh.parquet.gz')['segment']\n",
    "\n",
    "total_size = len(segment)\n",
    "segment = segment.value_counts().sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "958b64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_check_df = pd.DataFrame(columns=['iter','total','segment','start_time','end_time','run_time'])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "epochs     = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "805d1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.device(DL_SETTING):\n",
    "\n",
    "    total=len(segment)\n",
    "    iter=0\n",
    "    pbar = tqdm(segment)\n",
    "    for seg in pbar:\n",
    "        iter+=1\n",
    "        \n",
    "        iter_size = dt.fread(f'../OUT/segment/{seg}/train_df.csv').shape[0]\n",
    "        each_epochs_size = int(np.ceil(iter_size / batch_size * num_epochs))\n",
    "        text = f'{seg} : ModelSize={each_epochs_size:,}, DataSize={iter_size/total_size*100:.2f}%({iter_size:,}/{total_size:,})'\n",
    "        pbar.set_description(text)\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        train_dataset = get_dataset(\n",
    "            file_path=f'../OUT/segment/{seg}/train_df.csv',\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            label_name='target',\n",
    "            is_pack=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64,activation='elu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=32,activation='elu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=1,activation=None,kernel_constraint='non_neg'))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01),loss='huber',metrics=['mae'])\n",
    "\n",
    "        # 'val_loss'\n",
    "        mc = ModelCheckpoint(f'../MDL/DL/model_{seg}.h5', save_best_only=True, monitor='loss', mode='min')\n",
    "        es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=[mc,es],\n",
    "            verbose=1,\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.datetime.now()\n",
    "        run_time = end_time-start_time\n",
    "        \n",
    "        tmp = pd.DataFrame({\n",
    "            'iter' : iter,\n",
    "            'total' : total,\n",
    "            'segment' : seg,\n",
    "            'start_time' : str(start_time),\n",
    "            'end_time' : str(end_time),\n",
    "            'run_time' : str(run_time),\n",
    "        },index=[0])\n",
    "        \n",
    "        bg_check_df = pd.concat([bg_check_df,tmp],axis=0)\n",
    "        bg_check_df.to_csv('../bg_check_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06ec2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\n",
    "    to_emails='hjkim@storelink.io', \n",
    "    subject='Jeju Prediction Finished',\n",
    "    email_body='Jeju Prediction Finished',\n",
    "    attach_file='/home/ec2-user/SageMaker/Temp/Dacon/5_제주도도로교통량예측/bg_check_df.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ec74e",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (참조) https://www.analyticsvidhya.com/blog/2020/10/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras/\n",
    "def CreateModel_DL(\n",
    "    trial,\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "):\n",
    "    \n",
    "    if str(type(trial))==\"<class 'optuna.trial._trial.Trial'>\":\n",
    "        setting_type = 'optuna'\n",
    "    elif str(type(trial))==\"<class 'dict'>\":\n",
    "        setting_type = 'dict'\n",
    "    else:\n",
    "        raise('unknown type of params')\n",
    "        \n",
    "    params = trial\n",
    "    \n",
    "    with tf.device(DL_SETTING):\n",
    "\n",
    "        seed_everything(0)\n",
    "\n",
    "        X_tr2, X_va2, X_te2, X_fc2 = X_tr.copy(), X_va.copy(), X_te.copy(), X_fc.copy()\n",
    "        y_tr2, y_va2, y_te2, y_fc2 = y_tr.copy(), y_va.copy(), y_te.copy(), y_fc.copy()\n",
    "        \n",
    "        tr_index_info, va_index_info, te_index_info, fc_index_info = index_info\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (1) parameter setting\n",
    "        #====================================================================================================#\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # (1-1) model hyper-parameter\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        if setting_type=='optuna':\n",
    "        \n",
    "            n_layer = trial.suggest_int(\"n_layer\", 1, 3)\n",
    "\n",
    "            n_node = [np.nan]*n_layer\n",
    "            n_node[0] = trial.suggest_int(\"n_node_1\", 256, 1024) #64, 256)\n",
    "            if n_layer>=2:\n",
    "                for i in range(n_layer-1):\n",
    "                    n_node[i+1] = trial.suggest_int(f\"n_node_{i+2}\", n_node[i]//2, n_node[i])\n",
    "\n",
    "            dropout_rate = [np.nan]*n_layer\n",
    "            for i in range(n_layer):\n",
    "                dropout_rate[i] = trial.suggest_float(f\"dropout_{i+1}\", 0.0, 0.3)\n",
    "\n",
    "            # https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/\n",
    "            activation = trial.suggest_categorical('activation',[\"None\",\"elu\",\"relu\"])\n",
    "\n",
    "            is_bidirectional = trial.suggest_categorical('is_bidirectional',[\"True\",\"False\"])\n",
    "\n",
    "            optimizer = trial.suggest_categorical('optimizer',[\"Adam\",\"SGD\",\"RMSprop\"])\n",
    "\n",
    "            algorithm = trial.suggest_categorical('algorithm',[\"LSTM\",\"GRU\"])\n",
    "\n",
    "            is_timedistributed = trial.suggest_categorical('is_timedistributed',[\"True\",\"False\"])\n",
    "        \n",
    "        elif setting_type=='dict':\n",
    "            \n",
    "            # n_layer = 2\n",
    "            # n_node = [128,64]\n",
    "            # dropout_rate = [0.2,0.2]\n",
    "            # activation = 'elu'\n",
    "            # is_bidirectional = 'True'\n",
    "            # algorithm = 'LSTM'\n",
    "            # is_timedistributed = 'True'\n",
    "            \n",
    "            n_layer = params['n_layer']\n",
    "            \n",
    "            n_node = [np.nan]*n_layer\n",
    "            for i in range(n_layer):\n",
    "                n_node[i] = params[f\"n_node_{i+1}\"]\n",
    "                    \n",
    "            dropout_rate = [np.nan]*n_layer\n",
    "            for i in range(n_layer):\n",
    "                dropout_rate[i] = params[f\"dropout_{i+1}\"]\n",
    "                \n",
    "            activation = params['activation']\n",
    "            \n",
    "            is_bidirectional = params['is_bidirectional']\n",
    "\n",
    "            optimizer = params['optimizer']\n",
    "\n",
    "            algorithm = params['algorithm']\n",
    "\n",
    "            is_timedistributed = params['is_timedistributed']\n",
    "        \n",
    "        else:\n",
    "            raise('unknown type of params')\n",
    "            \n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # (1-2) dataset 생성에 사용되는 parameter\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        if setting_type=='optuna':\n",
    "\n",
    "            scale_x = trial.suggest_categorical('scale_x',[\"True\",\"False\"])\n",
    "            scale_y = trial.suggest_categorical('scale_y',[\"True\",\"False\"])\n",
    "            is_day_feature = trial.suggest_categorical('is_day_feature',[\"True\",\"False\"])\n",
    "            is_feature_selection = trial.suggest_categorical('is_feature_selection',[\"True\",\"False\"])\n",
    "        \n",
    "        elif setting_type=='dict':\n",
    "            \n",
    "            scale_x = params['scale_x']\n",
    "            scale_y = params['scale_y']\n",
    "            is_day_feature = params['is_day_feature']\n",
    "            is_feature_selection = params['is_feature_selection']\n",
    "        \n",
    "        else:\n",
    "            raise('unknown type of params')\n",
    "        \n",
    "        dataset_params = {\n",
    "            'scale_x': scale_x,\n",
    "            'scale_y': scale_y,\n",
    "            'is_day_feature' : is_day_feature,\n",
    "            'is_feature_selection': is_feature_selection,\n",
    "        }\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # (1-3) get function from model hyper-parameter\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # activation (last dense)\n",
    "        if activation=='None':\n",
    "            activation_x = None\n",
    "        else:\n",
    "            activation_x = activation\n",
    "\n",
    "        # bidirectional\n",
    "        if eval(is_bidirectional):\n",
    "            bidirectional = Bidirectional\n",
    "        else:\n",
    "            bidirectional = identity\n",
    "\n",
    "        # lstm / gru\n",
    "        if algorithm=='LSTM':\n",
    "            dl_alg = LSTM\n",
    "        elif algorithm=='GRU':\n",
    "            dl_alg = GRU\n",
    "\n",
    "        # optimizer\n",
    "        if optimizer == 'Adam':\n",
    "            optimizer_x = tf.keras.optimizers.Adam()\n",
    "        elif optimizer == 'SGD':\n",
    "            optimizer_x = tf.keras.optimizers.SGD()\n",
    "        elif optimizer == 'RMSprop':\n",
    "            optimizer_x = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "        # timedistributed\n",
    "        if eval(is_timedistributed):\n",
    "            timedistributed = TimeDistributed\n",
    "        else:\n",
    "            timedistributed = identity\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # (1-4) model 생성에 사용되는 parameter\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        model_params = {\n",
    "            'n_layer' : n_layer,\n",
    "            'activation' : activation,\n",
    "            'is_bidirectional' : is_bidirectional,\n",
    "            'optimizer' : optimizer,\n",
    "            'algorithm' : algorithm,\n",
    "            'is_timedistributed' : is_timedistributed,\n",
    "        }\n",
    "        for i in range(n_layer):\n",
    "            model_params[f'n_node_{i+1}'] = n_node[i]\n",
    "            model_params[f'dropout_{i+1}'] = dropout_rate[i]\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (2) scaling\n",
    "        #====================================================================================================#\n",
    "        scale_x = eval(dataset_params['scale_x'])\n",
    "        scale_y = eval(dataset_params['scale_y'])\n",
    "        scaler = None\n",
    "        if (scale_x) | (scale_y):\n",
    "            X_tr2, X_va2, X_te2, X_fc2, y_tr2, y_va2, y_te2, y_fc2, scalers = scaling(\n",
    "                X_tr2,X_va2,X_te2,X_fc2,\n",
    "                y_tr2,y_va2,y_te2,y_fc2,\n",
    "                scale_x,scale_y,\n",
    "            )\n",
    "            if scale_y:\n",
    "                scaler = scalers[f'scaler_{TARGET}']\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (3) day feature\n",
    "        #====================================================================================================#\n",
    "        if eval(dataset_params['is_day_feature']):\n",
    "\n",
    "            # month\n",
    "            for i in range(1,12+1):\n",
    "                X_tr2[f'month_{i}'] = np.where(X_tr2.month==str(i),1,0)\n",
    "                X_va2[f'month_{i}'] = np.where(X_va2.month==str(i),1,0)\n",
    "                X_te2[f'month_{i}'] = np.where(X_te2.month==str(i),1,0)\n",
    "                X_fc2[f'month_{i}'] = np.where(X_fc2.month==str(i),1,0)\n",
    "\n",
    "            del X_tr2['month'], X_va2['month'], X_te2['month'], X_fc2['month']\n",
    "\n",
    "            # day\n",
    "            for i in range(1,31+1):\n",
    "                X_tr2[f'day_{i}'] = np.where(X_tr2.day==str(i),1,0)\n",
    "                X_va2[f'day_{i}'] = np.where(X_va2.day==str(i),1,0)\n",
    "                X_te2[f'day_{i}'] = np.where(X_te2.day==str(i),1,0)\n",
    "                X_fc2[f'day_{i}'] = np.where(X_fc2.day==str(i),1,0)\n",
    "\n",
    "            del X_tr2['day'], X_va2['day'], X_te2['day'], X_fc2['day']\n",
    "\n",
    "            # weekday\n",
    "            for i in range(7):\n",
    "                X_tr2[f'weekday_{i}'] = np.where(X_tr2.weekday==str(i),1,0)\n",
    "                X_va2[f'weekday_{i}'] = np.where(X_va2.weekday==str(i),1,0)\n",
    "                X_te2[f'weekday_{i}'] = np.where(X_te2.weekday==str(i),1,0)\n",
    "                X_fc2[f'weekday_{i}'] = np.where(X_fc2.weekday==str(i),1,0)\n",
    "\n",
    "            del X_tr2['weekday'], X_va2['weekday'], X_te2['weekday'], X_fc2['weekday']\n",
    "\n",
    "        else:\n",
    "            X_tr2 = X_tr2[setdiff(X_tr2.columns,['month','day','weekday','weekend','holiday'])]\n",
    "            X_va2 = X_va2[setdiff(X_va2.columns,['month','day','weekday','weekend','holiday'])]\n",
    "            X_te2 = X_te2[setdiff(X_te2.columns,['month','day','weekday','weekend','holiday'])]\n",
    "            X_fc2 = X_fc2[setdiff(X_fc2.columns,['month','day','weekday','weekend','holiday'])]\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (4) feature selection\n",
    "        #====================================================================================================#\n",
    "        is_feature_selection = eval(dataset_params['is_feature_selection'])\n",
    "        offset_info = None\n",
    "        if is_feature_selection:\n",
    "            X_tr2, X_va2, X_te2, X_fc2, offset_info = feature_selection(X_tr2, X_va2, X_te2, X_fc2, y_tr2)\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (5) make dataset\n",
    "        #====================================================================================================#\n",
    "        X_tr_list, y_tr_list = MakeDataset(tr_index_info, X_tr2, y_tr2, data_type='list')\n",
    "        X_va_list, y_va_list = MakeDataset(va_index_info, X_va2, y_va2, data_type='list')\n",
    "        X_te_list, y_te_list = MakeDataset(te_index_info, X_te2, y_te2, data_type='list')\n",
    "        X_fc_list, y_fc_list = MakeDataset(fc_index_info, X_fc2, y_fc2, data_type='list')\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (6) Input shape\n",
    "        #====================================================================================================#\n",
    "        n_past     = X_tr_list.shape[1]\n",
    "        n_future   = 1\n",
    "        n_features = X_tr_list.shape[2]\n",
    "\n",
    "        #====================================================================================================#\n",
    "        # (7) modelling\n",
    "        #====================================================================================================#\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # encoder input\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        encoder_state  = [np.nan]*n_layer\n",
    "        encoder_input  = [np.nan]*n_layer\n",
    "        encoder_output = [np.nan]*n_layer\n",
    "        for i_layer in range(n_layer):\n",
    "\n",
    "            # return_sequences\n",
    "            # : 마지막 layer에는 sequences를 return하면 안됨\n",
    "            if i_layer==(n_layer-1):\n",
    "                return_sequences = False\n",
    "            else:\n",
    "                return_sequences = True\n",
    "\n",
    "            # dl algorithm input\n",
    "            # : 처음에는 shape로, 이후에는 이전 encoder의 output을 사용\n",
    "            if i_layer==0:\n",
    "                encoder_input[i_layer] = Input(shape=(n_past, n_features))\n",
    "            else:\n",
    "                encoder_input[i_layer] = encoder_output[i_layer-1]\n",
    "\n",
    "            # hidden layer\n",
    "            dl_algorithm = bidirectional(dl_alg(\n",
    "                units = n_node[i_layer], \n",
    "                dropout = dropout_rate[i_layer], \n",
    "                activation = activation_x,\n",
    "                return_sequences = return_sequences, \n",
    "                return_state = True, \n",
    "            ))(encoder_input[i_layer])\n",
    "\n",
    "            # states : LSTM을 각 타임 스텝별로 분리해서 넣어주는 경우가 있을 수 있다. 그럴 때는 initial_state를 사용\n",
    "            # (참조) https://simpling.tistory.com/19\n",
    "            encoder_output[i_layer], encoder_state[i_layer] = dl_algorithm[0], dl_algorithm[1:]\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # decoder\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        decoder_output = [np.nan]*n_layer\n",
    "        for i_layer in range(n_layer):\n",
    "\n",
    "            # dl algorithm input\n",
    "            # : 처음에는 shape로, 이후에는 이전 encoder의 output을 사용\n",
    "            if i_layer==0:\n",
    "                decoder_input = RepeatVector(n_future)(encoder_output[-1])\n",
    "            else:\n",
    "                decoder_input = encoder_output[i_layer-1]\n",
    "\n",
    "            # hidden layer\n",
    "            decoder_output[i_layer] = bidirectional(dl_alg(\n",
    "                units = n_node[i_layer],\n",
    "                dropout = dropout_rate[i_layer], \n",
    "                activation = activation_x,\n",
    "                return_sequences = True,\n",
    "            ))(decoder_input,initial_state = encoder_state[i_layer])\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # TimeDistributed Dense\n",
    "        #----------------------------------------------------------------------------------------------------#\n",
    "        # output = timedistributed(Dense(1, activation = activation_x))(decoder_output[-1])\n",
    "        output = timedistributed(Dense(1, activation = None, kernel_constraint='non_neg'))(decoder_output[-1])\n",
    "        \n",
    "        #====================================================================================================#\n",
    "        # (8) Model Compile\n",
    "        #====================================================================================================#\n",
    "        # custom loss function by backend\n",
    "        from tensorflow.keras.backend import constant\n",
    "        from tensorflow.keras import backend as K\n",
    "\n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "        def tf_std_weighted_scoring(y_true,y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) + (K.std(y_true)/(K.std(y_pred)+constant(1e-2)))**constant(2)\n",
    "        \n",
    "        model = tf.keras.models.Model(\n",
    "            encoder_input[0],\n",
    "            output,\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=optimizer_x, \n",
    "            loss=tf.keras.losses.Huber(), \n",
    "            # loss='mean_squared_error',\n",
    "            # loss='mean_absolute_error',\n",
    "            # loss='mean_absolute_percentage_error',\n",
    "            # loss=tf_std_weighted_scoring,\n",
    "            metrics=['mape'],\n",
    "        )\n",
    "\n",
    "        X = (X_tr_list, X_va_list, X_te_list, X_fc_list)\n",
    "        y = (y_tr_list, y_va_list, y_te_list, y_fc_list)\n",
    "        params = (dataset_params, model_params)\n",
    "        info = (scaler, offset_info)\n",
    "\n",
    "        return model, X, y, params, info\n",
    "\n",
    "def FitModel_DL(\n",
    "    trial,\n",
    "    X_tr, X_va, X_te, X_fc,\n",
    "    y_tr, y_va, y_te, y_fc,\n",
    "    index_info,\n",
    "    batch_size, \n",
    "    epochs,\n",
    "    scoring,\n",
    "    patience = None,\n",
    "    verbose = 1,\n",
    "    mc_file = DL_MC_PATH + 'dl.h5',\n",
    "    seed = 0,\n",
    "    slient = True,\n",
    "    is_tqdm = True,\n",
    "):\n",
    "    \n",
    "    if str(type(trial))==\"<class 'optuna.trial._trial.Trial'>\":\n",
    "        setting_type = 'optuna'\n",
    "    elif str(type(trial))==\"<class 'dict'>\":\n",
    "        setting_type = 'dict'\n",
    "    else:\n",
    "        raise('unknown type of params')\n",
    "        \n",
    "    params = trial\n",
    "\n",
    "    #global model, tr_data, va_data, te_data, fc_data, info\n",
    "    with tf.device(DL_SETTING):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_tr2, X_va2, X_te2, X_fc2 = X_tr.copy(), X_va.copy(), X_te.copy(), X_fc.copy()\n",
    "        y_tr2, y_va2, y_te2, y_fc2 = y_tr.copy(), y_va.copy(), y_te.copy(), y_fc.copy()\n",
    "        \n",
    "        tr_index_info, va_index_info, te_index_info, fc_index_info = index_info\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------#\n",
    "        # (1) Clear clutter from previous session graphs.\n",
    "        #----------------------------------------------------------------------------------------------------------------------#\n",
    "        tf.keras.backend.clear_session()\n",
    "        # seed_everything(seed=seed)\n",
    "        # tf.random.set_seed(seed)\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------#\n",
    "        # (2) with GPU\n",
    "        #----------------------------------------------------------------------------------------------------------------------#\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        with strategy.scope():\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            # (3) model, dataset 생성\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            model, X, y, params, info = CreateModel_DL(\n",
    "                params,\n",
    "                X_tr2, X_va2, X_te2, X_fc2,\n",
    "                y_tr2, y_va2, y_te2, y_fc2,\n",
    "                index_info,\n",
    "            )\n",
    "\n",
    "            X_tr_list, X_va_list, X_te_list, X_fc_list = X\n",
    "            y_tr_list, y_va_list, y_te_list, y_fc_list = y\n",
    "            dataset_params, model_params = params\n",
    "            scaler, offset_info = info\n",
    "\n",
    "            # Wrap data in Dataset objects.\n",
    "            tr_data = tf.data.Dataset.from_tensor_slices((X_tr_list, y_tr_list))\n",
    "            va_data = tf.data.Dataset.from_tensor_slices((X_va_list, y_va_list))\n",
    "            te_data = tf.data.Dataset.from_tensor_slices((X_te_list, y_te_list))\n",
    "            fc_data = tf.data.Dataset.from_tensor_slices((X_fc_list, y_fc_list))\n",
    "\n",
    "            # The batch size must now be set on the Dataset objects.\n",
    "            tr_data = tr_data.batch(batch_size)\n",
    "            va_data = va_data.batch(batch_size)\n",
    "            te_data = te_data.batch(batch_size)\n",
    "            fc_data = fc_data.batch(batch_size)\n",
    "\n",
    "            # Disable AutoShard.\n",
    "            options = tf.data.Options()\n",
    "            options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "            tr_data = tr_data.with_options(options)\n",
    "            va_data = va_data.with_options(options)\n",
    "            te_data = te_data.with_options(options)\n",
    "            fc_data = fc_data.with_options(options)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            # (4) model callback 생성\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            # Fit the model on the training data.\n",
    "            # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
    "\n",
    "            # learning rate scheduler\n",
    "            # scheduler = ExponentialDecay(1e-3, 400*((len(X_train_list)*0.8)/batch_size), 1e-5)\n",
    "            # lr = LearningRateScheduler(scheduler, verbose=0)\n",
    "            \n",
    "            #lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "            #lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 0.05)\n",
    "            \n",
    "            # (참조) https://rinha7.github.io/keras-callbacks/\n",
    "            #lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 0.2)\n",
    "            #reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=5, min_lr=0.001, min_delta=0.0001)\n",
    "            mc = ModelCheckpoint(\n",
    "                mc_file,              # file명을 지정합니다\n",
    "                monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                verbose=0,            # 로그를 출력합니다\n",
    "                save_best_only=True,  # 가장 best 값만 저장합니다\n",
    "                mode='min'            # auto는 알아서 best를 찾습니다. min/max\n",
    "            )\n",
    "            if patience is None:\n",
    "                callbacks = [mc]#,lr,reduce_lr]\n",
    "            else:\n",
    "                es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
    "                callbacks = [mc,es]#,lr,reduce_lr]\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            # (5) model fitting\n",
    "            #----------------------------------------------------------------------------------------------------------------------#\n",
    "            # tr_data.batch(batch_size)에서 batch_size를 지정했기 때문에,\n",
    "            # model.fit에서 batch_size를 지정하면 에러가 발생\n",
    "            \n",
    "            # if verbose==0:\n",
    "            #     pbar = range(epochs)\n",
    "            # else:\n",
    "            #     pbar = tqdm(range(epochs))\n",
    "\n",
    "            # loss_list = []\n",
    "            # for i in pbar:\n",
    "            #     seed_everything(0)\n",
    "            #     model.fit(\n",
    "            #         tr_data,\n",
    "            #         #batch_size=batch_size,\n",
    "            #         callbacks=callbacks,\n",
    "            #         epochs=1,\n",
    "            #         validation_data=va_data,\n",
    "            #         verbose=verbose,\n",
    "            #     )\n",
    "            #     huber_loss_x = huber_loss(\n",
    "            #         true = y_va_list.flatten(),\n",
    "            #         pred = model.predict(va_data).flatten(),\n",
    "            #     )\n",
    "            #     loss_list.append(huber_loss_x)\n",
    "\n",
    "            #     if patience is not None:\n",
    "            #         es = Callback_EarlyStopping(loss_list,min_delta=None,patience=patience)\n",
    "            #         if es:\n",
    "            #             break\n",
    "\n",
    "            seed_everything(0)\n",
    "            model.fit(\n",
    "                tr_data,\n",
    "                #batch_size=batch_size,\n",
    "                callbacks=callbacks,\n",
    "                epochs=epochs,\n",
    "                validation_data=va_data,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            #---------------------------------------------------------------------------------------------------------#\n",
    "            # (6) scoring\n",
    "            #---------------------------------------------------------------------------------------------------------#\n",
    "            va_pred = model.predict(va_data)\n",
    "            va_true = y_va_list.copy()\n",
    "\n",
    "            if eval(dataset_params['scale_y']):\n",
    "                va_pred = scaler.inverse_transform(va_pred.reshape(-1,1))\n",
    "                # va_pred = log_scaler_inv(va_pred, offset_info[TARGET_FEATURE])\n",
    "\n",
    "                va_true = scaler.inverse_transform(va_true.reshape(-1,1))\n",
    "                # va_true = log_scaler_inv(va_true, offset_info[TARGET_FEATURE])\n",
    "\n",
    "            # reshape\n",
    "            va_pred = va_pred.flatten()#.reshape(int(len(va_pred.flatten())/5),5,1).flatten()\n",
    "            va_true = va_true.flatten()#.reshape(int(len(va_true.flatten())/5),5,1).flatten()\n",
    "\n",
    "            score = scoring(true=va_true,pred=va_pred)\n",
    "            pred_std = np.std(va_pred)\n",
    "            true_std = np.std(va_true)\n",
    "            data_range = max(va_true.flatten()) - min(va_true.flatten())\n",
    "\n",
    "            end_time = time.time()\n",
    "            run_time = end_time-start_time\n",
    "\n",
    "            # print information\n",
    "            global iter_dl\n",
    "            iter_dl+=1\n",
    "            \n",
    "            # save best model\n",
    "            dataset = (tr_data, va_data, te_data, fc_data)\n",
    "            info = [scaler, offset_info]\n",
    "            \n",
    "            # trial.set_user_attr(key=\"best_model\", value=model)\n",
    "            # trial.set_user_attr(key=\"best_model_data\", value=dataset)\n",
    "            # trial.set_user_attr(key=\"best_model_info\", value=info)\n",
    "\n",
    "            # > 총 16가지\n",
    "            # (1) scale_x : True, False\n",
    "            # (2) scale_y : True, False\n",
    "            # (3) is_day_feature : True, False\n",
    "            # (4) is_feature_selection : True, False\n",
    "            if not slient:\n",
    "                print(f'{color.BOLD}{color.CYAN}({iter}/{n_trials}){color.END}')\n",
    "                print(f'  > score={score:.3f}, pred_std={pred_std:.3f}, true_std={true_std:.3f}, range={data_range:.3f}, time={run_time:.2f}s')\n",
    "                print(f'  > dataset_params:')\n",
    "                for key,value in dataset_params.items():\n",
    "                    print(f'      {key} : {value}')\n",
    "                print(f'  > model_params :')\n",
    "                for key,value in model_params.items():\n",
    "                    print(f'      {key} : {value}')\n",
    "                print('')\n",
    "            else:\n",
    "                if is_tqdm:\n",
    "                    text = f'score={score:.3f}, pred_std={pred_std:.3f}, true_std={true_std:.3f}'\n",
    "                    pbar_dl.set_description(text)\n",
    "                    pbar_dl.update(1)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            if setting_type=='optuna':\n",
    "                return score\n",
    "\n",
    "            elif setting_type=='dict':\n",
    "                return model, dataset, info\n",
    "            \n",
    "            else:\n",
    "                raise('unknown type of params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df7.to_parquet('../OUT/train_fn.parquet.gz')\n",
    "test_df7 .to_parquet('../OUT/test_fn.parquet.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed577eae",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
