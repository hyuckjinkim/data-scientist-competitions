{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5a4a02-0048-408d-aded-882e118bae57",
   "metadata": {},
   "source": [
    "- _bf1,2,3 이런거 추가해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4041a5-44ba-4987-a6e7-3c3f40f10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 torch 다운로드\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e5953c-7a86-4c51-a887-423f7beb6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fp = fm.FontProperties(fname='/home/studio-lab-user/Dacon/tools/NanumFont/NanumGothic.ttf', size=10)\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "# # cuda (not Mac)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # mps (Mac)\n",
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47cef-a496-4dc7-9a8b-0f363f65cdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':128,#1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':0.05,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, input_paths, label_paths, test_input_paths, test_label_paths):\n",
    "        \n",
    "        self.input, self.label, self.test_input, self.test_label = None, None, None, None\n",
    "        \n",
    "        self.X_train, self.X_valid = None, None\n",
    "        self.y_train, self.y_valid = None, None\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, label_path in zip(input_paths, label_paths):\n",
    "            case_num = input_path.replace('./data/train_input/CASE_','').replace('.csv','')\n",
    "            \n",
    "            input_df = pd.read_csv(input_path)\n",
    "            label_df = pd.read_csv(label_path)\n",
    "\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_df['case_num'] = case_num\n",
    "            label_df['case_num'] = case_num\n",
    "            \n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(label_df)\n",
    "        \n",
    "        test_input_fn = []\n",
    "        test_label_fn = []\n",
    "        for test_input_path, test_label_path in zip(test_input_paths, test_label_paths):\n",
    "            case_num = test_input_path.replace('./data/test_input/TEST_','').replace('.csv','')\n",
    "            \n",
    "            test_input_df = pd.read_csv(test_input_path)\n",
    "            test_label_df = pd.read_csv(test_label_path)\n",
    "            \n",
    "            test_input_df['case_num'] = case_num\n",
    "            test_label_df['case_num'] = case_num\n",
    "            \n",
    "            test_input_fn.append(test_input_df)\n",
    "            test_label_fn.append(test_label_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0).sort_values(['case_num','DAT','obs_time'])\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_fn ,axis=0)\n",
    "        self.test_label  = pd.concat(test_label_fn ,axis=0)\n",
    "        \n",
    "        self.input     .obs_time = list(np.arange(0,24))*int(self.input     .shape[0]/24)\n",
    "        self.test_input.obs_time = list(np.arange(0,24))*int(self.test_input.shape[0]/24)\n",
    "        \n",
    "    def _data_return(self):\n",
    "        return self.input,self.label,self.test_input,self.test_label\n",
    "            \n",
    "    def _target_log(self):\n",
    "        self.label['predicted_weight_g'] = np.log(self.label['predicted_weight_g'])\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        self.input      = self.input     .drop(['obs_time'],axis=1)\n",
    "        self.test_input = self.test_input.drop(['obs_time'],axis=1)\n",
    "        \n",
    "        self.input      = self.input     .groupby(['case_num','DAT']).mean().reset_index()\n",
    "        self.test_input = self.test_input.groupby(['case_num','DAT']).mean().reset_index()\n",
    "        \n",
    "        self.input.DAT      = self.input     .DAT + 1\n",
    "        self.test_input.DAT = self.test_input.DAT + 1\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # https://dacon.io/competitions/official/236033/talkboard/407304?page=1&dtype=recent\n",
    "    def _scale_dataset(self,outlier):\n",
    "        \n",
    "        minmax_info = {\n",
    "            'none':[0,0],\n",
    "            # 'time':[0,28*24],\n",
    "            # '내부온도관측치':[4,40],\n",
    "            # '내부습도관측치':[0,100],\n",
    "            # 'co2관측치':[0,1200],\n",
    "            # 'ec관측치':[0,8],\n",
    "            # '시간당분무량':[0,3000],\n",
    "            # '일간누적분무량':[0,72000],\n",
    "            # '시간당백색광량':[0,120000],\n",
    "            # '일간누적백색광량':[0,2880000],\n",
    "            # '시간당적색광량':[0,120000],\n",
    "            # '일간누적적색광량':[0,2880000],\n",
    "            # '시간당청색광량':[0,120000],\n",
    "            # '일간누적청색광량':[0,2880000],\n",
    "            # '시간당총광량':[0,120000],\n",
    "            # '일간누적총광량':[0,2880000],\n",
    "        }\n",
    "            \n",
    "        scale_feature = [feature for feature,(min_info,max_info) in minmax_info.items() if feature in self.input.columns]\n",
    "        \n",
    "        # for train dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.input[col] = (self.input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.input[col][self.input[col]<0] = 0\n",
    "                self.input[col][self.input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.input[col][(self.input[col]<0) | (self.input[col]>1)] = np.nan\n",
    "            \n",
    "        # for test dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.test_input[col][self.test_input[col]<0] = 0\n",
    "                self.test_input[col][self.test_input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.test_input[col][(self.test_input[col]<0) | (self.test_input[col]>1)] = np.nan\n",
    "        \n",
    "        another_features = list(set(self.input.select_dtypes(exclude=[object]).columns)-set(scale_feature))\n",
    "        for col in another_features:\n",
    "            min_info,max_info = self.input[col].min(),self.input[col].max()\n",
    "            self.input[col]      = (self.input[col]     -min_info) / (max_info-min_info)\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "        \n",
    "    def _interaction_term(self):\n",
    "        # num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        # num_features = list(set(num_features)-set(['DAT','obs_time']))\n",
    "        num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    self.input     [f'{num_features[i]}*{num_features[j]}'] = self.input     [num_features[i]]*self.input     [num_features[j]]\n",
    "                    self.test_input[f'{num_features[i]}*{num_features[j]}'] = self.test_input[num_features[i]]*self.test_input[num_features[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46afb772-6e30-42b5-b09b-f0cdc584fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf407c58-fe03-456d-9bad-ec0ff6b18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# val_rate = 0.05\n",
    "\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# dataset._preprocess()\n",
    "# dataset._scale_dataset()\n",
    "# input_df, label_df = dataset._data_return()\n",
    "\n",
    "# for case_num in tqdm(sorted(input_df.case_num.unique())):\n",
    "\n",
    "#     input = input_df[input_df.case_num==case_num].drop('case_num',axis=1)\n",
    "#     label = label_df[label_df.case_num==case_num].drop('case_num',axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize=(20,15))\n",
    "#     nrow = 3\n",
    "#     ncol = 5\n",
    "\n",
    "#     iter = 0\n",
    "#     total = len(input.columns)-3\n",
    "#     for col in input.columns:\n",
    "#         if col not in ['time','DAT','obs_time']:\n",
    "#             iter+=1\n",
    "\n",
    "#             y1 = input[col]\n",
    "#             #y1 = (y1-y1.min())/(y1.max()-y1.min())\n",
    "\n",
    "#             y2 = label['predicted_weight_g']\n",
    "#             y2 = (y2-y2.min())/(y2.max()-y2.min())\n",
    "\n",
    "#             y3 = input.groupby('DAT')[col].mean().values\n",
    "\n",
    "#             corr, pvalue = pearsonr(y2,y3)\n",
    "\n",
    "#             fig.add_subplot(ncol,nrow,iter)\n",
    "#             sns.scatterplot(x=input.time  ,y=y1)\n",
    "#             sns.scatterplot(x=label.DAT*24,y=y2,color='red')\n",
    "#             sns.lineplot   (x=label.DAT*24,y=y3,color='blue',linestyle='--',alpha=0.7)\n",
    "#             plt.ylabel('')\n",
    "\n",
    "#             plt.title(f'{col}(corr={corr:.3f}(pvalue={pvalue:.3f}))',fontproperties=fp)\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'./fig/{case_num}.png',dpi=100)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a08-c07c-42bd-9b38-18eccfeba517",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff8530a6-2012-4619-903f-da297915ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=15, hidden_size=256, batch_first=True, bidirectional=False)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         hidden, _ = self.lstm(x)\n",
    "#         output = self.classifier(hidden[:,-1,:])\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee70e82-0375-4a8d-9a02-5100763d8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coding-yoon.tistory.com/131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_sizes, dropout_rates, num_classes, num_layers, bidirectional):\n",
    "#         super(BaseModel, self).__init__()\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_sizes = hidden_sizes\n",
    "#         self.dropout_rates = dropout_rates\n",
    "#         self.num_classes = num_classes\n",
    "#         self.num_layers = num_layers\n",
    "#         self.bidirectional = bidirectional\n",
    "        \n",
    "#         self.node_offset = 2 if self.bidirectional else 1\n",
    "\n",
    "#         self.lstm1 = nn.LSTM(\n",
    "#             input_size=self.input_size,\n",
    "#             hidden_size=self.hidden_sizes[0],\n",
    "#             # batch_first=True,\n",
    "#             bidirectional=self.bidirectional,\n",
    "#             dropout=self.dropout_rates[0],\n",
    "#             num_layers=self.num_layers,\n",
    "#         )\n",
    "        \n",
    "#         self.lstm2 = nn.LSTM(\n",
    "#             input_size=self.hidden_sizes[0]*self.node_offset, # bidirectional\n",
    "#             hidden_size=self.hidden_sizes[1],\n",
    "#             # batch_first=True,\n",
    "#             bidirectional=self.bidirectional,\n",
    "#             dropout=self.dropout_rates[1],\n",
    "#             num_layers=self.num_layers,\n",
    "#         )\n",
    "\n",
    "#         # self.fc = nn.Linear(self.hidden_sizes[1]*self.node_offset, self.num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         # self.elu  = nn.ELU()\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "#         self.module = nn.Sequential(\n",
    "#             nn.Linear(input_size, 128),\n",
    "#             #nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_sizes[0],\n",
    "#                             num_layers=self.num_layers, batch_first=True, dropout = self.dropout_rates[0])\n",
    "\n",
    "#         self.fc = nn.Linear(self.hidden_sizes[0], self.num_classes)\n",
    "        \n",
    "#     # def forward(self, x):\n",
    "#     #     hid, _ = self.lstm1(x)\n",
    "#     #     hid    = self.dropout(hid)\n",
    "#     #     hid    = self.relu(hid)\n",
    "#     #     hid, _ = self.lstm2(hid)\n",
    "#     #     hid    = self.dropout(hid)\n",
    "#     #     out    = self.relu(hid)\n",
    "#     #     # print(out.detach().numpy().shape) # (16, 24, 32)\n",
    "#     #     out    = self.fc(out[:,-1,:])\n",
    "#     #     # out    = self.fc(out)\n",
    "#     #     return out\n",
    "    \n",
    "#     # def forward(self, x):\n",
    "#     #     out = self.module(x)\n",
    "#     #     return out\n",
    "    \n",
    "#     # https://www.kaggle.com/code/omershect/learning-pytorch-lstm-deep-learning-with-m5-data\n",
    "#     def forward(self, x):\n",
    "#         h_0 = Variable(torch.zeros(\n",
    "#             self.num_layers, x.size(0), self.hidden_sizes[0]).to(device))\n",
    "        \n",
    "#         c_0 = Variable(torch.zeros(\n",
    "#             self.num_layers, x.size(0), self.hidden_sizes[0]).to(device))\n",
    "        \n",
    "#         # Propagate input through LSTM\n",
    "#         ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "#         h_out = h_out.view(-1, self.hidden_sizes[0])\n",
    "        \n",
    "#         out = self.fc(h_out)\n",
    "#         out = self.dropout(out)\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train, Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d419b-7332-4e64-a6e5-68aed8e5b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_fn(output, target):\n",
    "    return torch.sqrt(torch.mean((output-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26ee98e-b3ae-4d8e-a18e-8b166686c493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, scheduler, device, \n",
    "    early_stopping, epochs, metric_period=1, best_model_only=True, verbose=True,\n",
    "):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['ES_PATIENCE'], verbose = CFG['ES_VERBOSE'], path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            # print(output.shape,Y.shape) # torch.Size([4, 28, 1]) torch.Size([4, 24])\n",
    "            # print(output[:5],Y[:5])\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5],output.shape,Y.shape)\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % metric_period == 0:\n",
    "                epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "                progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "                    .format(\n",
    "                        epoch_str,\n",
    "                        epochs,np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)\n",
    "                    )\n",
    "                epoch_s = time.time()\n",
    "\n",
    "                if best_loss > valid_loss:\n",
    "                    mark = '*'\n",
    "                else:\n",
    "                    mark = ' '\n",
    "            \n",
    "                print(mark+progress)\n",
    "            \n",
    "        if best_model_only:\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model = model\n",
    "                \n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731d783-38cb-43e3-8f4f-1f9c6d187cdc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5372837a-12ee-431c-be3b-3102602e774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(input_list).shape\n",
    "# [x.size() for x in input_list]\n",
    "\n",
    "# [x.size() for x in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5023f6d2-1af1-4ccb-86b0-2b241cc9a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.size() for x in input_list]\n",
    "# [x.size() for x in label_list]\n",
    "# torch.Tensor(y_seq.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46086dc5-9016-4d90-9d11-e16031dc5007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [x[0] for x in train_loader]\n",
    "# X_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4fc984-65ef-4c99-ab47-39cdc5072fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# n_split = 2\n",
    "\n",
    "# # Preprocess Class\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# # (1) preprocessing + scaling + interaction term\n",
    "# dataset._preprocess()\n",
    "# # dataset._feature_exp()\n",
    "# dataset._scale_dataset()\n",
    "# # dataset._interaction_term()\n",
    "\n",
    "# # (2) Data Return for check\n",
    "# input_df, label_df, test_df = dataset._data_return()\n",
    "\n",
    "# # (3) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']\n",
    "\n",
    "# # (4) train/validaion split을 위한 index 생성\n",
    "# input_df['idx'] = np.repeat(np.arange(0,28*28),24)\n",
    "\n",
    "# # (5) delete features\n",
    "# input_df = input_df.drop(del_features,axis=1)\n",
    "\n",
    "# test_preds = []\n",
    "# kf = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "# for tr_idx,va_idx in tqdm(kf.split(label_df),total=n_split):\n",
    "    \n",
    "#     X_train = input_df[input_df.idx.isin(tr_idx)].drop(['idx','case_num'],axis=1)\n",
    "#     X_valid = input_df[input_df.idx.isin(va_idx)].drop(['idx','case_num'],axis=1)\n",
    "#     y_train = label_df.iloc[tr_idx]\n",
    "#     y_valid = label_df.iloc[va_idx]\n",
    "    \n",
    "#     # #temp\n",
    "#     # X_train = X_train.groupby(['case_num','DAT']).mean().reset_index().drop(['idx','case_num'],axis=1)\n",
    "#     # X_valid = X_valid.groupby(['case_num','DAT']).mean().reset_index().drop(['idx','case_num'],axis=1)\n",
    "    \n",
    "#     # X_train['DAT'] = (X_train['DAT']-0)/(28-0)\n",
    "#     # X_valid['DAT'] = (X_valid['DAT']-0)/(28-0)\n",
    "#     # X_train = X_train.drop('DAT',axis=1)\n",
    "#     # X_valid = X_valid.drop('DAT',axis=1)\n",
    "    \n",
    "#     train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False)\n",
    "#     # train_datatset = TensorDataset(torch.from_numpy(X_train.values),torch.from_numpy(y_train.values))\n",
    "#     train_loader  = DataLoader(train_dataset, batch_size = 1024, shuffle=False, num_workers=6) # CFG['BATCH_SIZE']\n",
    "#     # train_loader = DataLoader(train_dataset, num_workers=8)\n",
    "\n",
    "#     valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False)\n",
    "#     # valid_datatset = TensorDataset(torch.from_numpy(X_valid.values),torch.from_numpy(y_valid.values))\n",
    "#     valid_loader  = DataLoader(valid_dataset, batch_size = 1024, shuffle=False, num_workers=6) # CFG['BATCH_SIZE']\n",
    "#     # valid_loader = DataLoader(valid_dataset, num_workers=8)\n",
    "    \n",
    "#     # input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "#     model = BaseModel(\n",
    "#         input_size = X_train.shape[1],\n",
    "#         hidden_sizes=[64,32],\n",
    "#         dropout_rates=[0.2,0.2],\n",
    "#         num_classes=1,\n",
    "#         num_layers=1,\n",
    "#         bidirectional=False,\n",
    "#     )\n",
    "#     # model = GRUModel(input_dim=X_train.shape[1], hidden_dim=64, layer_dim=1, output_dim=1)\n",
    "    \n",
    "#     model.eval()\n",
    "#     optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])#, weight_decay=1e-5)\n",
    "#     # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=False)\n",
    "\n",
    "#     best_model = train(\n",
    "#         model,\n",
    "#         optimizer=optimizer,\n",
    "#         train_loader=train_loader,\n",
    "#         valid_loader=valid_loader,\n",
    "#         scheduler=scheduler,\n",
    "#         device=device,\n",
    "#         early_stopping=False,\n",
    "#         metric_period=1,\n",
    "#         epochs=2,\n",
    "#     )\n",
    "    \n",
    "#     test_df = test_df[X_train.columns]\n",
    "#     test_dataset = CustomTestDataset(input=test_df)\n",
    "#     test_loader  = DataLoader(test_dataset, batch_size = 1024, shuffle=False, num_workers=0)\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     test_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for X in iter(test_loader):\n",
    "#             X = X.float().to(device)\n",
    "\n",
    "#             model_pred = model(X)\n",
    "#             # model_pred = torch.exp(model_pred)\n",
    "#             model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "#             test_pred += model_pred\n",
    "            \n",
    "#     test_preds.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913d75a3-a641-4a4e-8e1e-bc329caa3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Preprocess Class\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# # (1) preprocessing + scaling + interaction term\n",
    "# dataset._preprocess()\n",
    "# # dataset._target_log()\n",
    "# dataset._scale_dataset()\n",
    "# # dataset._interaction_term()\n",
    "\n",
    "# # (2) Data Return for check\n",
    "# input_df, label_df, test_df = dataset._data_return()\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# # label_df = label_df['predicted_weight_g']\n",
    "\n",
    "# plot_df = input_df.copy()\n",
    "# features = input_df.drop(['DAT','case_num','time'],axis=1).columns\n",
    "\n",
    "# for col in ['predicted_weight_g']:\n",
    "#     label_df[col] = (label_df[col]-label_df[col].min()) / (label_df[col].max()-label_df[col].min())\n",
    "# for col in features:\n",
    "#     plot_df[col] = (plot_df[col] - plot_df[col].min()) / (plot_df[col].max()-plot_df[col].min())\n",
    "\n",
    "# for col in features:\n",
    "#     palette = sns.color_palette(\"pastel\",plot_df.case_num.nunique())\n",
    "#     plt.figure(figsize=(15,7))\n",
    "\n",
    "#     i=0\n",
    "#     for case_num in plot_df.case_num.unique():\n",
    "#         d1 = plot_df[plot_df.case_num==case_num]\n",
    "#         d2 = label_df[label_df.case_num==case_num]\n",
    "#         sns.lineplot(x=d1['time']*d2['DAT'].max()*24,y=d1[col],alpha=0.5,color=palette[i])\n",
    "#         sns.lineplot(x=d2['DAT']*24,y=d2['predicted_weight_g'],color=palette[i])\n",
    "#         i+=1\n",
    "\n",
    "#     plt.xlabel('')\n",
    "#     plt.ylabel('')\n",
    "#     plt.title(col,fontproperties=fp)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e95a60-344a-4d23-a7aa-913d23ea5c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agg_df = plot_df.groupby(['time']).mean().reset_index()\n",
    "\n",
    "# for col in features:\n",
    "\n",
    "#     x = plot_df['time']\n",
    "#     y1 = plot_df[col]\n",
    "#     y2 = (label_df-label_df.min()) / (label_df.max()-label_df.min())\n",
    "#     y3 = agg_df[col]\n",
    "\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=x,y=y1,alpha=0.5)\n",
    "#     sns.scatterplot(x=y2.index*24,y=y2,color='red')\n",
    "#     sns.lineplot(x=agg_df['time'],y=y3,color='black')\n",
    "#     plt.title(col,fontproperties=fp)\n",
    "#     plt.ylabel('')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d57fd0-ea32-4fb4-a626-1006236b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_label_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "all_test_label_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d65ad91-4f38-4423-99f9-c1e8089c2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess Class\n",
    "dataset = Preprocess(\n",
    "    input_paths = all_input_list,\n",
    "    label_paths = all_label_list,\n",
    "    test_input_paths = all_test_input_list,\n",
    "    test_label_paths = all_test_label_list,\n",
    ")\n",
    "\n",
    "# (1) preprocessing + scaling + interaction term\n",
    "dataset._preprocess()\n",
    "# dataset._target_log()\n",
    "dataset._scale_dataset(outlier='keep')\n",
    "dataset._interaction_term()\n",
    "\n",
    "# (2) Data Return for check\n",
    "input_df, label_df, test_input_df, test_label_df = dataset._data_return()\n",
    "\n",
    "# # (3) Delete Std zero features\n",
    "# std_zero_features = []\n",
    "# for case_num in input_df.case_num.unique():\n",
    "#     tmp = input_df[input_df.case_num==case_num]\n",
    "#     std_zero_feature = tmp.std().index[tmp.std()==0].tolist()\n",
    "#     std_zero_features += std_zero_feature\n",
    "    \n",
    "# std_zero_features = pd.unique(std_zero_features)\n",
    "\n",
    "# input_df = input_df.drop(std_zero_features,axis=1)\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e1319e6-fb96-4d3a-88a6-6523d1d18edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_num</th>\n",
       "      <th>DAT</th>\n",
       "      <th>내부온도관측치</th>\n",
       "      <th>내부습도관측치</th>\n",
       "      <th>co2관측치</th>\n",
       "      <th>ec관측치</th>\n",
       "      <th>시간당분무량</th>\n",
       "      <th>일간누적분무량</th>\n",
       "      <th>시간당백색광량</th>\n",
       "      <th>일간누적백색광량</th>\n",
       "      <th>시간당적색광량</th>\n",
       "      <th>일간누적적색광량</th>\n",
       "      <th>시간당청색광량</th>\n",
       "      <th>일간누적청색광량</th>\n",
       "      <th>시간당총광량</th>\n",
       "      <th>일간누적총광량</th>\n",
       "      <th>내부온도관측치*DAT</th>\n",
       "      <th>내부습도관측치*DAT</th>\n",
       "      <th>내부습도관측치*내부온도관측치</th>\n",
       "      <th>co2관측치*DAT</th>\n",
       "      <th>co2관측치*내부온도관측치</th>\n",
       "      <th>co2관측치*내부습도관측치</th>\n",
       "      <th>ec관측치*DAT</th>\n",
       "      <th>ec관측치*내부온도관측치</th>\n",
       "      <th>ec관측치*내부습도관측치</th>\n",
       "      <th>ec관측치*co2관측치</th>\n",
       "      <th>시간당분무량*DAT</th>\n",
       "      <th>시간당분무량*내부온도관측치</th>\n",
       "      <th>시간당분무량*내부습도관측치</th>\n",
       "      <th>시간당분무량*co2관측치</th>\n",
       "      <th>시간당분무량*ec관측치</th>\n",
       "      <th>일간누적분무량*DAT</th>\n",
       "      <th>일간누적분무량*내부온도관측치</th>\n",
       "      <th>일간누적분무량*내부습도관측치</th>\n",
       "      <th>일간누적분무량*co2관측치</th>\n",
       "      <th>일간누적분무량*ec관측치</th>\n",
       "      <th>일간누적분무량*시간당분무량</th>\n",
       "      <th>시간당백색광량*DAT</th>\n",
       "      <th>시간당백색광량*내부온도관측치</th>\n",
       "      <th>시간당백색광량*내부습도관측치</th>\n",
       "      <th>시간당백색광량*co2관측치</th>\n",
       "      <th>시간당백색광량*ec관측치</th>\n",
       "      <th>시간당백색광량*시간당분무량</th>\n",
       "      <th>시간당백색광량*일간누적분무량</th>\n",
       "      <th>일간누적백색광량*DAT</th>\n",
       "      <th>일간누적백색광량*내부온도관측치</th>\n",
       "      <th>일간누적백색광량*내부습도관측치</th>\n",
       "      <th>일간누적백색광량*co2관측치</th>\n",
       "      <th>일간누적백색광량*ec관측치</th>\n",
       "      <th>일간누적백색광량*시간당분무량</th>\n",
       "      <th>일간누적백색광량*일간누적분무량</th>\n",
       "      <th>일간누적백색광량*시간당백색광량</th>\n",
       "      <th>시간당적색광량*DAT</th>\n",
       "      <th>시간당적색광량*내부온도관측치</th>\n",
       "      <th>시간당적색광량*내부습도관측치</th>\n",
       "      <th>시간당적색광량*co2관측치</th>\n",
       "      <th>시간당적색광량*ec관측치</th>\n",
       "      <th>시간당적색광량*시간당분무량</th>\n",
       "      <th>시간당적색광량*일간누적분무량</th>\n",
       "      <th>시간당적색광량*시간당백색광량</th>\n",
       "      <th>시간당적색광량*일간누적백색광량</th>\n",
       "      <th>일간누적적색광량*DAT</th>\n",
       "      <th>일간누적적색광량*내부온도관측치</th>\n",
       "      <th>일간누적적색광량*내부습도관측치</th>\n",
       "      <th>일간누적적색광량*co2관측치</th>\n",
       "      <th>일간누적적색광량*ec관측치</th>\n",
       "      <th>일간누적적색광량*시간당분무량</th>\n",
       "      <th>일간누적적색광량*일간누적분무량</th>\n",
       "      <th>일간누적적색광량*시간당백색광량</th>\n",
       "      <th>일간누적적색광량*일간누적백색광량</th>\n",
       "      <th>일간누적적색광량*시간당적색광량</th>\n",
       "      <th>시간당청색광량*DAT</th>\n",
       "      <th>시간당청색광량*내부온도관측치</th>\n",
       "      <th>시간당청색광량*내부습도관측치</th>\n",
       "      <th>시간당청색광량*co2관측치</th>\n",
       "      <th>시간당청색광량*ec관측치</th>\n",
       "      <th>시간당청색광량*시간당분무량</th>\n",
       "      <th>시간당청색광량*일간누적분무량</th>\n",
       "      <th>시간당청색광량*시간당백색광량</th>\n",
       "      <th>시간당청색광량*일간누적백색광량</th>\n",
       "      <th>시간당청색광량*시간당적색광량</th>\n",
       "      <th>시간당청색광량*일간누적적색광량</th>\n",
       "      <th>일간누적청색광량*DAT</th>\n",
       "      <th>일간누적청색광량*내부온도관측치</th>\n",
       "      <th>일간누적청색광량*내부습도관측치</th>\n",
       "      <th>일간누적청색광량*co2관측치</th>\n",
       "      <th>일간누적청색광량*ec관측치</th>\n",
       "      <th>일간누적청색광량*시간당분무량</th>\n",
       "      <th>일간누적청색광량*일간누적분무량</th>\n",
       "      <th>일간누적청색광량*시간당백색광량</th>\n",
       "      <th>일간누적청색광량*일간누적백색광량</th>\n",
       "      <th>일간누적청색광량*시간당적색광량</th>\n",
       "      <th>일간누적청색광량*일간누적적색광량</th>\n",
       "      <th>일간누적청색광량*시간당청색광량</th>\n",
       "      <th>시간당총광량*DAT</th>\n",
       "      <th>시간당총광량*내부온도관측치</th>\n",
       "      <th>시간당총광량*내부습도관측치</th>\n",
       "      <th>시간당총광량*co2관측치</th>\n",
       "      <th>시간당총광량*ec관측치</th>\n",
       "      <th>시간당총광량*시간당분무량</th>\n",
       "      <th>시간당총광량*일간누적분무량</th>\n",
       "      <th>시간당총광량*시간당백색광량</th>\n",
       "      <th>시간당총광량*일간누적백색광량</th>\n",
       "      <th>시간당총광량*시간당적색광량</th>\n",
       "      <th>시간당총광량*일간누적적색광량</th>\n",
       "      <th>시간당총광량*시간당청색광량</th>\n",
       "      <th>시간당총광량*일간누적청색광량</th>\n",
       "      <th>일간누적총광량*DAT</th>\n",
       "      <th>일간누적총광량*내부온도관측치</th>\n",
       "      <th>일간누적총광량*내부습도관측치</th>\n",
       "      <th>일간누적총광량*co2관측치</th>\n",
       "      <th>일간누적총광량*ec관측치</th>\n",
       "      <th>일간누적총광량*시간당분무량</th>\n",
       "      <th>일간누적총광량*일간누적분무량</th>\n",
       "      <th>일간누적총광량*시간당백색광량</th>\n",
       "      <th>일간누적총광량*일간누적백색광량</th>\n",
       "      <th>일간누적총광량*시간당적색광량</th>\n",
       "      <th>일간누적총광량*일간누적적색광량</th>\n",
       "      <th>일간누적총광량*시간당청색광량</th>\n",
       "      <th>일간누적총광량*일간누적청색광량</th>\n",
       "      <th>일간누적총광량*시간당총광량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564266</td>\n",
       "      <td>0.823008</td>\n",
       "      <td>0.279704</td>\n",
       "      <td>0.279170</td>\n",
       "      <td>0.083194</td>\n",
       "      <td>0.082095</td>\n",
       "      <td>0.875564</td>\n",
       "      <td>0.961504</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.061177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481469</td>\n",
       "      <td>0.503443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157828</td>\n",
       "      <td>0.230199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157526</td>\n",
       "      <td>0.229759</td>\n",
       "      <td>0.078085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.068469</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>0.067565</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494051</td>\n",
       "      <td>0.720596</td>\n",
       "      <td>0.244899</td>\n",
       "      <td>0.244431</td>\n",
       "      <td>0.072842</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542544</td>\n",
       "      <td>0.791325</td>\n",
       "      <td>0.268937</td>\n",
       "      <td>0.268423</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.078935</td>\n",
       "      <td>0.841858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038540</td>\n",
       "      <td>0.056212</td>\n",
       "      <td>0.019104</td>\n",
       "      <td>0.019067</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.059801</td>\n",
       "      <td>0.065671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034520</td>\n",
       "      <td>0.050349</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.053564</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271676</td>\n",
       "      <td>0.396252</td>\n",
       "      <td>0.134669</td>\n",
       "      <td>0.134411</td>\n",
       "      <td>0.040055</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.421557</td>\n",
       "      <td>0.462934</td>\n",
       "      <td>0.032884</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284076</td>\n",
       "      <td>0.414338</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>0.140546</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.041330</td>\n",
       "      <td>0.440797</td>\n",
       "      <td>0.484063</td>\n",
       "      <td>0.034385</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.592162</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>0.262877</td>\n",
       "      <td>0.277330</td>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>0.963773</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.061192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482642</td>\n",
       "      <td>0.504578</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.030489</td>\n",
       "      <td>0.487468</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.164224</td>\n",
       "      <td>0.228298</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.049534</td>\n",
       "      <td>0.068860</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.023198</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.069630</td>\n",
       "      <td>0.022235</td>\n",
       "      <td>0.023458</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>0.519787</td>\n",
       "      <td>0.722588</td>\n",
       "      <td>0.230748</td>\n",
       "      <td>0.243434</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.570710</td>\n",
       "      <td>0.793379</td>\n",
       "      <td>0.253354</td>\n",
       "      <td>0.267283</td>\n",
       "      <td>0.080618</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.845980</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.040474</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.059996</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.036236</td>\n",
       "      <td>0.050374</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.053713</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.285802</td>\n",
       "      <td>0.397311</td>\n",
       "      <td>0.126876</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.040824</td>\n",
       "      <td>0.423653</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.298792</td>\n",
       "      <td>0.415369</td>\n",
       "      <td>0.132642</td>\n",
       "      <td>0.139934</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.486299</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.545077</td>\n",
       "      <td>0.816391</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.278995</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.085435</td>\n",
       "      <td>0.875723</td>\n",
       "      <td>0.961246</td>\n",
       "      <td>0.068185</td>\n",
       "      <td>0.061022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481510</td>\n",
       "      <td>0.503251</td>\n",
       "      <td>0.040376</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>0.444996</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.155881</td>\n",
       "      <td>0.233471</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.152074</td>\n",
       "      <td>0.227769</td>\n",
       "      <td>0.079787</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.047049</td>\n",
       "      <td>0.070467</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.046569</td>\n",
       "      <td>0.069748</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>0.023836</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.477337</td>\n",
       "      <td>0.714933</td>\n",
       "      <td>0.250438</td>\n",
       "      <td>0.244323</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.074817</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>0.523953</td>\n",
       "      <td>0.784753</td>\n",
       "      <td>0.274896</td>\n",
       "      <td>0.268183</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>0.841785</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.037166</td>\n",
       "      <td>0.055665</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.005885</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>0.065542</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.053439</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.262460</td>\n",
       "      <td>0.393101</td>\n",
       "      <td>0.137702</td>\n",
       "      <td>0.134339</td>\n",
       "      <td>0.041562</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.421670</td>\n",
       "      <td>0.462850</td>\n",
       "      <td>0.032832</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037278</td>\n",
       "      <td>0.274310</td>\n",
       "      <td>0.410849</td>\n",
       "      <td>0.143919</td>\n",
       "      <td>0.140404</td>\n",
       "      <td>0.043438</td>\n",
       "      <td>0.042995</td>\n",
       "      <td>0.440708</td>\n",
       "      <td>0.483748</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>0.030709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.516685</td>\n",
       "      <td>0.813247</td>\n",
       "      <td>0.620639</td>\n",
       "      <td>0.280708</td>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>0.963773</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.061192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482642</td>\n",
       "      <td>0.504578</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.420193</td>\n",
       "      <td>0.068960</td>\n",
       "      <td>0.320675</td>\n",
       "      <td>0.504732</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>0.228285</td>\n",
       "      <td>0.174219</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.043220</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.051916</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.043704</td>\n",
       "      <td>0.068788</td>\n",
       "      <td>0.052497</td>\n",
       "      <td>0.023744</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.097531</td>\n",
       "      <td>0.453535</td>\n",
       "      <td>0.713851</td>\n",
       "      <td>0.544783</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.107086</td>\n",
       "      <td>0.497968</td>\n",
       "      <td>0.783786</td>\n",
       "      <td>0.598155</td>\n",
       "      <td>0.270539</td>\n",
       "      <td>0.080618</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.845980</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.035315</td>\n",
       "      <td>0.055585</td>\n",
       "      <td>0.042421</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.059996</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.031617</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.037978</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.053713</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>0.249374</td>\n",
       "      <td>0.392507</td>\n",
       "      <td>0.299546</td>\n",
       "      <td>0.135482</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.040824</td>\n",
       "      <td>0.423653</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056064</td>\n",
       "      <td>0.260708</td>\n",
       "      <td>0.410346</td>\n",
       "      <td>0.313161</td>\n",
       "      <td>0.141639</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.486299</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.536601</td>\n",
       "      <td>0.823996</td>\n",
       "      <td>0.286433</td>\n",
       "      <td>0.281209</td>\n",
       "      <td>0.082588</td>\n",
       "      <td>0.078750</td>\n",
       "      <td>0.876083</td>\n",
       "      <td>0.961461</td>\n",
       "      <td>0.068218</td>\n",
       "      <td>0.061043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481710</td>\n",
       "      <td>0.503366</td>\n",
       "      <td>0.079496</td>\n",
       "      <td>0.122073</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.042434</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.236019</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.150897</td>\n",
       "      <td>0.231715</td>\n",
       "      <td>0.080547</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.068052</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>0.022145</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.129790</td>\n",
       "      <td>0.470107</td>\n",
       "      <td>0.721889</td>\n",
       "      <td>0.250939</td>\n",
       "      <td>0.246362</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.068992</td>\n",
       "      <td>0.142439</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>0.792240</td>\n",
       "      <td>0.275394</td>\n",
       "      <td>0.270371</td>\n",
       "      <td>0.079405</td>\n",
       "      <td>0.075715</td>\n",
       "      <td>0.842320</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.056211</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.059765</td>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.017485</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0.053479</td>\n",
       "      <td>0.058690</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071364</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>0.396927</td>\n",
       "      <td>0.137977</td>\n",
       "      <td>0.135461</td>\n",
       "      <td>0.039783</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.463145</td>\n",
       "      <td>0.032861</td>\n",
       "      <td>0.029405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074573</td>\n",
       "      <td>0.270107</td>\n",
       "      <td>0.414772</td>\n",
       "      <td>0.144181</td>\n",
       "      <td>0.141551</td>\n",
       "      <td>0.041572</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.440991</td>\n",
       "      <td>0.483967</td>\n",
       "      <td>0.034339</td>\n",
       "      <td>0.030727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_num       DAT   내부온도관측치   내부습도관측치    co2관측치     ec관측치    시간당분무량  \\\n",
       "0       01  0.000000  0.564266  0.823008  0.279704  0.279170  0.083194   \n",
       "1       01  0.037037  0.592162  0.823201  0.262877  0.277330  0.083649   \n",
       "2       01  0.074074  0.545077  0.816391  0.285979  0.278995  0.086316   \n",
       "3       01  0.111111  0.516685  0.813247  0.620639  0.280708  0.083649   \n",
       "4       01  0.148148  0.536601  0.823996  0.286433  0.281209  0.082588   \n",
       "\n",
       "    일간누적분무량   시간당백색광량  일간누적백색광량   시간당적색광량  일간누적적색광량  시간당청색광량  일간누적청색광량  \\\n",
       "0  0.082095  0.875564  0.961504  0.068300  0.061177      0.0       0.0   \n",
       "1  0.084585  0.877779  0.963773  0.068350  0.061192      0.0       0.0   \n",
       "2  0.085435  0.875723  0.961246  0.068185  0.061022      0.0       0.0   \n",
       "3  0.084585  0.877779  0.963773  0.068350  0.061192      0.0       0.0   \n",
       "4  0.078750  0.876083  0.961461  0.068218  0.061043      0.0       0.0   \n",
       "\n",
       "     시간당총광량   일간누적총광량  내부온도관측치*DAT  내부습도관측치*DAT  내부습도관측치*내부온도관측치  co2관측치*DAT  \\\n",
       "0  0.481469  0.503443     0.000000     0.000000         0.464396    0.000000   \n",
       "1  0.482642  0.504578     0.021932     0.030489         0.487468    0.009736   \n",
       "2  0.481510  0.503251     0.040376     0.060473         0.444996    0.021184   \n",
       "3  0.482642  0.504578     0.057409     0.090361         0.420193    0.068960   \n",
       "4  0.481710  0.503366     0.079496     0.122073         0.442157    0.042434   \n",
       "\n",
       "   co2관측치*내부온도관측치  co2관측치*내부습도관측치  ec관측치*DAT  ec관측치*내부온도관측치  ec관측치*내부습도관측치  \\\n",
       "0        0.157828        0.230199   0.000000       0.157526       0.229759   \n",
       "1        0.155666        0.216401   0.010271       0.164224       0.228298   \n",
       "2        0.155881        0.233471   0.020666       0.152074       0.227769   \n",
       "3        0.320675        0.504732   0.031190       0.145038       0.228285   \n",
       "4        0.153700        0.236019   0.041661       0.150897       0.231715   \n",
       "\n",
       "   ec관측치*co2관측치  시간당분무량*DAT  시간당분무량*내부온도관측치  시간당분무량*내부습도관측치  시간당분무량*co2관측치  \\\n",
       "0      0.078085    0.000000        0.046944        0.068469       0.023270   \n",
       "1      0.072904    0.003098        0.049534        0.068860       0.021989   \n",
       "2      0.079787    0.006394        0.047049        0.070467       0.024684   \n",
       "3      0.174219    0.009294        0.043220        0.068027       0.051916   \n",
       "4      0.080547    0.012235        0.044317        0.068052       0.023656   \n",
       "\n",
       "   시간당분무량*ec관측치  일간누적분무량*DAT  일간누적분무량*내부온도관측치  일간누적분무량*내부습도관측치  \\\n",
       "0      0.023225     0.000000         0.046323         0.067565   \n",
       "1      0.023198     0.003133         0.050088         0.069630   \n",
       "2      0.024082     0.006329         0.046569         0.069748   \n",
       "3      0.023481     0.009398         0.043704         0.068788   \n",
       "4      0.023224     0.011667         0.042257         0.064890   \n",
       "\n",
       "   일간누적분무량*co2관측치  일간누적분무량*ec관측치  일간누적분무량*시간당분무량  시간당백색광량*DAT  \\\n",
       "0        0.022962       0.022918        0.006830     0.000000   \n",
       "1        0.022235       0.023458        0.007075     0.032510   \n",
       "2        0.024433       0.023836        0.007374     0.064868   \n",
       "3        0.052497       0.023744        0.007075     0.097531   \n",
       "4        0.022557       0.022145        0.006504     0.129790   \n",
       "\n",
       "   시간당백색광량*내부온도관측치  시간당백색광량*내부습도관측치  시간당백색광량*co2관측치  시간당백색광량*ec관측치  \\\n",
       "0         0.494051         0.720596        0.244899       0.244431   \n",
       "1         0.519787         0.722588        0.230748       0.243434   \n",
       "2         0.477337         0.714933        0.250438       0.244323   \n",
       "3         0.453535         0.713851        0.544783       0.246400   \n",
       "4         0.470107         0.721889        0.250939       0.246362   \n",
       "\n",
       "   시간당백색광량*시간당분무량  시간당백색광량*일간누적분무량  일간누적백색광량*DAT  일간누적백색광량*내부온도관측치  \\\n",
       "0        0.072842         0.071879      0.000000          0.542544   \n",
       "1        0.073425         0.074247      0.035695          0.570710   \n",
       "2        0.075589         0.074817      0.071203          0.523953   \n",
       "3        0.073425         0.074247      0.107086          0.497968   \n",
       "4        0.072354         0.068992      0.142439          0.515921   \n",
       "\n",
       "   일간누적백색광량*내부습도관측치  일간누적백색광량*co2관측치  일간누적백색광량*ec관측치  일간누적백색광량*시간당분무량  \\\n",
       "0          0.791325         0.268937        0.268423         0.079991   \n",
       "1          0.793379         0.253354        0.267283         0.080618   \n",
       "2          0.784753         0.274896        0.268183         0.082971   \n",
       "3          0.783786         0.598155        0.270539         0.080618   \n",
       "4          0.792240         0.275394        0.270371         0.079405   \n",
       "\n",
       "   일간누적백색광량*일간누적분무량  일간누적백색광량*시간당백색광량  시간당적색광량*DAT  시간당적색광량*내부온도관측치  \\\n",
       "0          0.078935          0.841858     0.000000         0.038540   \n",
       "1          0.081521          0.845980     0.002531         0.040474   \n",
       "2          0.082124          0.841785     0.005051         0.037166   \n",
       "3          0.081521          0.845980     0.007594         0.035315   \n",
       "4          0.075715          0.842320     0.010106         0.036606   \n",
       "\n",
       "   시간당적색광량*내부습도관측치  시간당적색광량*co2관측치  시간당적색광량*ec관측치  시간당적색광량*시간당분무량  \\\n",
       "0         0.056212        0.019104       0.019067        0.005682   \n",
       "1         0.056266        0.017968       0.018955        0.005717   \n",
       "2         0.055665        0.019499       0.019023        0.005885   \n",
       "3         0.055585        0.042421       0.019186        0.005717   \n",
       "4         0.056211        0.019540       0.019183        0.005634   \n",
       "\n",
       "   시간당적색광량*일간누적분무량  시간당적색광량*시간당백색광량  시간당적색광량*일간누적백색광량  일간누적적색광량*DAT  \\\n",
       "0         0.005607         0.059801          0.065671      0.000000   \n",
       "1         0.005781         0.059996          0.065874      0.002266   \n",
       "2         0.005825         0.059711          0.065542      0.004520   \n",
       "3         0.005781         0.059996          0.065874      0.006799   \n",
       "4         0.005372         0.059765          0.065589      0.009043   \n",
       "\n",
       "   일간누적적색광량*내부온도관측치  일간누적적색광량*내부습도관측치  일간누적적색광량*co2관측치  일간누적적색광량*ec관측치  \\\n",
       "0          0.034520          0.050349         0.017111        0.017079   \n",
       "1          0.036236          0.050374         0.016086        0.016970   \n",
       "2          0.033262          0.049818         0.017451        0.017025   \n",
       "3          0.031617          0.049764         0.037978        0.017177   \n",
       "4          0.032756          0.050299         0.017485        0.017166   \n",
       "\n",
       "   일간누적적색광량*시간당분무량  일간누적적색광량*일간누적분무량  일간누적적색광량*시간당백색광량  일간누적적색광량*일간누적백색광량  \\\n",
       "0         0.005090          0.005022          0.053564           0.058822   \n",
       "1         0.005119          0.005176          0.053713           0.058976   \n",
       "2         0.005267          0.005213          0.053439           0.058657   \n",
       "3         0.005119          0.005176          0.053713           0.058976   \n",
       "4         0.005041          0.004807          0.053479           0.058690   \n",
       "\n",
       "   일간누적적색광량*시간당적색광량  시간당청색광량*DAT  시간당청색광량*내부온도관측치  시간당청색광량*내부습도관측치  \\\n",
       "0          0.004178          0.0              0.0              0.0   \n",
       "1          0.004182          0.0              0.0              0.0   \n",
       "2          0.004161          0.0              0.0              0.0   \n",
       "3          0.004182          0.0              0.0              0.0   \n",
       "4          0.004164          0.0              0.0              0.0   \n",
       "\n",
       "   시간당청색광량*co2관측치  시간당청색광량*ec관측치  시간당청색광량*시간당분무량  시간당청색광량*일간누적분무량  \\\n",
       "0             0.0            0.0             0.0              0.0   \n",
       "1             0.0            0.0             0.0              0.0   \n",
       "2             0.0            0.0             0.0              0.0   \n",
       "3             0.0            0.0             0.0              0.0   \n",
       "4             0.0            0.0             0.0              0.0   \n",
       "\n",
       "   시간당청색광량*시간당백색광량  시간당청색광량*일간누적백색광량  시간당청색광량*시간당적색광량  시간당청색광량*일간누적적색광량  \\\n",
       "0              0.0               0.0              0.0               0.0   \n",
       "1              0.0               0.0              0.0               0.0   \n",
       "2              0.0               0.0              0.0               0.0   \n",
       "3              0.0               0.0              0.0               0.0   \n",
       "4              0.0               0.0              0.0               0.0   \n",
       "\n",
       "   일간누적청색광량*DAT  일간누적청색광량*내부온도관측치  일간누적청색광량*내부습도관측치  일간누적청색광량*co2관측치  \\\n",
       "0           0.0               0.0               0.0              0.0   \n",
       "1           0.0               0.0               0.0              0.0   \n",
       "2           0.0               0.0               0.0              0.0   \n",
       "3           0.0               0.0               0.0              0.0   \n",
       "4           0.0               0.0               0.0              0.0   \n",
       "\n",
       "   일간누적청색광량*ec관측치  일간누적청색광량*시간당분무량  일간누적청색광량*일간누적분무량  일간누적청색광량*시간당백색광량  \\\n",
       "0             0.0              0.0               0.0               0.0   \n",
       "1             0.0              0.0               0.0               0.0   \n",
       "2             0.0              0.0               0.0               0.0   \n",
       "3             0.0              0.0               0.0               0.0   \n",
       "4             0.0              0.0               0.0               0.0   \n",
       "\n",
       "   일간누적청색광량*일간누적백색광량  일간누적청색광량*시간당적색광량  일간누적청색광량*일간누적적색광량  일간누적청색광량*시간당청색광량  \\\n",
       "0                0.0               0.0                0.0               0.0   \n",
       "1                0.0               0.0                0.0               0.0   \n",
       "2                0.0               0.0                0.0               0.0   \n",
       "3                0.0               0.0                0.0               0.0   \n",
       "4                0.0               0.0                0.0               0.0   \n",
       "\n",
       "   시간당총광량*DAT  시간당총광량*내부온도관측치  시간당총광량*내부습도관측치  시간당총광량*co2관측치  시간당총광량*ec관측치  \\\n",
       "0    0.000000        0.271676        0.396252       0.134669      0.134411   \n",
       "1    0.017876        0.285802        0.397311       0.126876      0.133851   \n",
       "2    0.035667        0.262460        0.393101       0.137702      0.134339   \n",
       "3    0.053627        0.249374        0.392507       0.299546      0.135482   \n",
       "4    0.071364        0.258486        0.396927       0.137977      0.135461   \n",
       "\n",
       "   시간당총광량*시간당분무량  시간당총광량*일간누적분무량  시간당총광량*시간당백색광량  시간당총광량*일간누적백색광량  \\\n",
       "0       0.040055        0.039526        0.421557         0.462934   \n",
       "1       0.040372        0.040824        0.423653         0.465158   \n",
       "2       0.041562        0.041138        0.421670         0.462850   \n",
       "3       0.040372        0.040824        0.423653         0.465158   \n",
       "4       0.039783        0.037935        0.422018         0.463145   \n",
       "\n",
       "   시간당총광량*시간당적색광량  시간당총광량*일간누적적색광량  시간당총광량*시간당청색광량  시간당총광량*일간누적청색광량  \\\n",
       "0        0.032884         0.029455             0.0              0.0   \n",
       "1        0.032989         0.029534             0.0              0.0   \n",
       "2        0.032832         0.029383             0.0              0.0   \n",
       "3        0.032989         0.029534             0.0              0.0   \n",
       "4        0.032861         0.029405             0.0              0.0   \n",
       "\n",
       "   일간누적총광량*DAT  일간누적총광량*내부온도관측치  일간누적총광량*내부습도관측치  일간누적총광량*co2관측치  \\\n",
       "0     0.000000         0.284076         0.414338        0.140815   \n",
       "1     0.018688         0.298792         0.415369        0.132642   \n",
       "2     0.037278         0.274310         0.410849        0.143919   \n",
       "3     0.056064         0.260708         0.410346        0.313161   \n",
       "4     0.074573         0.270107         0.414772        0.144181   \n",
       "\n",
       "   일간누적총광량*ec관측치  일간누적총광량*시간당분무량  일간누적총광량*일간누적분무량  일간누적총광량*시간당백색광량  \\\n",
       "0       0.140546        0.041884         0.041330         0.440797   \n",
       "1       0.139934        0.042207         0.042680         0.442908   \n",
       "2       0.140404        0.043438         0.042995         0.440708   \n",
       "3       0.141639        0.042207         0.042680         0.442908   \n",
       "4       0.141551        0.041572         0.039640         0.440991   \n",
       "\n",
       "   일간누적총광량*일간누적백색광량  일간누적총광량*시간당적색광량  일간누적총광량*일간누적적색광량  일간누적총광량*시간당청색광량  \\\n",
       "0          0.484063         0.034385          0.030799              0.0   \n",
       "1          0.486299         0.034488          0.030876              0.0   \n",
       "2          0.483748         0.034314          0.030709              0.0   \n",
       "3          0.486299         0.034488          0.030876              0.0   \n",
       "4          0.483967         0.034339          0.030727              0.0   \n",
       "\n",
       "   일간누적총광량*일간누적청색광량  일간누적총광량*시간당총광량  \n",
       "0               0.0        0.242392  \n",
       "1               0.0        0.243531  \n",
       "2               0.0        0.242320  \n",
       "3               0.0        0.243531  \n",
       "4               0.0        0.242476  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8979bc0a-1589-44b6-9cc6-6733be3e1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_df.isnull().sum()\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f93b40f-3e0c-4f7e-b84c-8c0958395769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_num', 'DAT', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in input_df.columns if col.find('*')<0])\n",
    "# print(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cc3e9c6-5ca0-43cf-99d4-4826cedf9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,input,label,infer_mode,seq_length):\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        input = input.sort_values(['case_num','DAT'])\n",
    "        label = label.sort_values(['case_num','DAT'])\n",
    "\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        for i in range(int(label.shape[0]/seq_length)):\n",
    "            i_df = input.iloc[i*seq_length:(i+1)*seq_length,:].drop('case_num',axis=1)\n",
    "            l_df = label.iloc[i*seq_length:(i+1)*seq_length]['predicted_weight_g']\n",
    "            self.input_list.append(torch.Tensor(i_df.values))\n",
    "            self.label_list.append(torch.Tensor(l_df.values))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data  = self.input_list[index]\n",
    "        label = self.label_list[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec6fe-a87d-4fe0-8f91-97136bd96e57",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19ff934e-3796-4a0b-9a87-8b1e605d0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        hidden  = [100]*4\n",
    "        dropout = [0.2]*4\n",
    "        num_layers = [1]*4\n",
    "        bidirectional = False\n",
    "        if bidirectional:\n",
    "            offset = 2\n",
    "        else:\n",
    "            offset = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden[0],\n",
    "            dropout=dropout[0],\n",
    "            num_layers=num_layers[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=offset*hidden[0],\n",
    "            hidden_size=hidden[1],\n",
    "            dropout=dropout[1],\n",
    "            num_layers=num_layers[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=offset*hidden[1],\n",
    "            hidden_size=hidden[2],\n",
    "            dropout=dropout[2],\n",
    "            num_layers=num_layers[2],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=offset*hidden[2],\n",
    "            hidden_size=hidden[3],\n",
    "            dropout=dropout[3],\n",
    "            num_layers=num_layers[3],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        \n",
    "        self.fc = nn.Linear(offset * hidden[3], 1)\n",
    "        self.fc = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st\n",
    "        x, _ = self.lstm1(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 2nd\n",
    "        x, _ = self.lstm2(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 3rd\n",
    "        x, _ = self.lstm3(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 4th\n",
    "        x, _ = self.lstm4(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # fully connected layer\n",
    "        x    = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a523935d-af75-45ce-976c-c78f41578e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        # print(x.shape,x_reshape.shape)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fcb61ab-2360-4805-8407-1005e3ca5b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update)\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                 kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update)\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x)\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd =SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "    \n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen):\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: \n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCINet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, num_decoder_layer = 1, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.num_decoder_layer = num_decoder_layer\n",
    "\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        self.div_projection = nn.ModuleList()\n",
    "        self.overlap_len = self.input_len//4\n",
    "        self.div_len = self.input_len//6\n",
    "\n",
    "        if self.num_decoder_layer > 1:\n",
    "            self.projection1 = nn.Linear(self.input_len, self.output_len)\n",
    "            for layer_idx in range(self.num_decoder_layer-1):\n",
    "                div_projection = nn.ModuleList()\n",
    "                for i in range(6):\n",
    "                    lens = min(i*self.div_len+self.overlap_len,self.input_len) - i*self.div_len\n",
    "                    div_projection.append(nn.Linear(lens, self.div_len))\n",
    "                self.div_projection.append(div_projection)\n",
    "\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x += pe[:, :, :-1]\n",
    "            else:\n",
    "                x += self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        if self.RIN:\n",
    "            print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            #mean\n",
    "            x = x - means\n",
    "            #var\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # affine\n",
    "            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x += res1\n",
    "        if self.num_decoder_layer == 1:\n",
    "            x = self.projection1(x)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    "            for div_projection in self.div_projection:\n",
    "                output = torch.zeros(x.shape,dtype=x.dtype).cuda()\n",
    "                for i, div_layer in enumerate(div_projection):\n",
    "                    div_x = x[:,:,i*self.div_len:min(i*self.div_len+self.overlap_len,self.input_len)]\n",
    "                    output[:,:,i*self.div_len:(i+1)*self.div_len] = div_layer(div_x)\n",
    "                x = output\n",
    "            x = self.projection1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            res2 = x\n",
    "            x = self.blocks2(x)\n",
    "            x += res2\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x, MidOutPut\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b693071-44da-46f4-a007-ccbfa2a9b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class SCINet_decompose(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet_decompose, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.decomp = series_decomp(25)\n",
    "        self.trend = nn.Linear(input_len,input_len)\n",
    "        self.trend_dec = nn.Linear(input_len,output_len)\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        x, trend = self.decomp(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # seq_means = x[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means = x[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # x = x - seq_means\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "            # print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means2 = trend.mean(1, keepdim=True).detach()\n",
    "            trend = trend - means2\n",
    "            stdev2 = torch.sqrt(torch.var(trend, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            trend /= stdev2\n",
    "            # seq_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # trend = trend - seq_means2 \n",
    "            trend = trend * self.affine_weight2 + self.affine_bias2\n",
    "        \n",
    "\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x = x + pe[:, :, :-1]\n",
    "            else:\n",
    "                x = x + self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        \n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x = self.projection1(x)\n",
    "\n",
    "        trend = trend.permute(0,2,1)\n",
    "        trend = self.trend(trend)  \n",
    "        trend = self.trend_dec(trend).permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                # x = x + pred_means\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            x = self.blocks2(x)\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend, MidOutPut\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e41047c2-5bff-4835-9c26-6f523fbbf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCINet_Model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(SCINet_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # 24,4,1,1,2,0.5,False,1,True,1\n",
    "        window_size = 1 # in (fixed)\n",
    "        horizon = 1      # out\n",
    "        hidden_size = 1\n",
    "        groups = 1\n",
    "        kernel = 1\n",
    "        dropout = 0.5\n",
    "        single_step_output_One = False\n",
    "        num_levels = 1\n",
    "        positionalEcoding = True\n",
    "        num_stacks = 1\n",
    "        self.scinet = SCINet(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        self.scinet_decompose = SCINet_decompose(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        \n",
    "        # hidden  = [64, 64, 64]\n",
    "        # dropout = [0.2, 0.5, 0.5]\n",
    "        # num_layers = [1,1,1]\n",
    "        # self.lstm1 = nn.LSTM(\n",
    "        #     input_size=input_size,\n",
    "        #     hidden_size=hidden[0],\n",
    "        #     dropout=dropout[0],\n",
    "        #     num_layers=num_layers[0],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm2 = nn.LSTM(\n",
    "        #     input_size=2*hidden[0],\n",
    "        #     hidden_size=hidden[1],\n",
    "        #     dropout=dropout[1],\n",
    "        #     num_layers=num_layers[1],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm3 = nn.LSTM(\n",
    "        #     input_size=2*hidden[1],\n",
    "        #     hidden_size=hidden[2],\n",
    "        #     dropout=dropout[2],\n",
    "        #     num_layers=num_layers[2],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.fc = nn.Linear(2*hidden[0], 1)\n",
    "        self.fc_1 = nn.Linear(input_size, 16)\n",
    "        self.fc_1 = TimeDistributed(self.fc_1)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        self.fc_2 = TimeDistributed(self.fc_2)\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.scinet(x)\n",
    "#         # x = self.bn(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm1(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm2(x)\n",
    "#         # x = self.selu(x)\n",
    "#         # x,_ = self.lstm3(x)\n",
    "#         # x = self.selu(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leakyrelu(x)\n",
    "#         x = self.fc_2(x[:,-1,:]) # [:,:,-1]\n",
    "        \n",
    "#         # x = self.fc_2(x[:,-1,:])\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.scinet(x)\n",
    "#         x = self.scinet_decompose(x)\n",
    "#         x1,x2 = x[0],x[1]\n",
    "#         x = torch.cat([x1,x2],dim=1)\n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.scinet(x)\n",
    "        x = self.scinet_decompose(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31adc92f-7d3b-4a78-9898-3ad69b0406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "input_dataset = CustomDataset(input=input_df, label=label_df, infer_mode=False, seq_length=seq_length)\n",
    "input_loader  = DataLoader(input_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True, seq_length=seq_length)\n",
    "test_loader  = DataLoader(test_dataset  , batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6928024e-e648-45da-9492-385f06326c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.size() for x,y in iter(input_loader)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c132bc04-4a20-43ad-aec5-648acc3fd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f7f09-5c0c-412f-ac80-7983fea8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cure-lab/LTSF-Linear/blob/main/models/NLinear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cff7d35-f90f-42a1-b9bb-b07ee20d6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization-Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len):\n",
    "        super(NLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
    "        # Use this line if you want to visualize the weights\n",
    "        # self.Linear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seq_last = x[:,-1:,:].detach()\n",
    "        x = x - seq_last\n",
    "        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = x + seq_last\n",
    "        return x # [Batch, Output length, Channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b20c40ea-b3ea-40e3-8873-e55db8878907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9267c5e601264901a8153dc5a33cb9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    if kf_iter==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79343771-ef42-403a-9ae7-d472d202a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "(1/10)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'-'*100)\n",
    "print(f'({kf_iter}/{n_splits})')\n",
    "print(f'-'*100)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# (1) train validation split\n",
    "#------------------------------------------------------------------------------------\n",
    "tr_case_num = case_num[tr_idx]\n",
    "va_case_num = case_num[va_idx]\n",
    "\n",
    "X_train = input_df[input_df.case_num.isin(tr_case_num)]\n",
    "X_valid = input_df[input_df.case_num.isin(va_case_num)]\n",
    "\n",
    "y_train = label_df[label_df.case_num.isin(tr_case_num)]\n",
    "y_valid = label_df[label_df.case_num.isin(va_case_num)]\n",
    "# print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# (2) custom dataset\n",
    "#------------------------------------------------------------------------------------\n",
    "train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False, seq_length=seq_length)\n",
    "train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False, seq_length=seq_length)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "# [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "# [y for x,y in iter(train_loader)]\n",
    "# sum([y.size(0) for x,y in iter(train_loader)])\n",
    "\n",
    "# len([x for x,y in iter(train_loader)])\n",
    "\n",
    "# [(x[0].size(),x[1].size()) for x in train_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5faa1458-e142-4ac4-803f-e1144cae9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLinear_Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, input_size):\n",
    "        super(NLinear_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        dropout=0.2\n",
    "        \n",
    "        self.nlinear = NLinear(seq_len=seq_len,pred_len=pred_len)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.nlinear(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f79fe58-76a1-46a0-9b26-8f731cca94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[0100/8192] tr_loss : 34.26193, val_loss : 30.41315, elapsed : 4.34s, total : 4.34s, remaining : 35105.26s\n",
      "*[0200/8192] tr_loss : 29.93769, val_loss : 26.67021, elapsed : 4.36s, total : 8.70s, remaining : 34873.74s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mES_PATIENCE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m     25\u001b[0m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mES_VERBOSE\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#scheduler,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, valid_loader, scheduler, device, early_stopping, epochs, metric_period, best_model_only, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y)\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(loss) \u001b[38;5;66;03m# MSE -> RMSE\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Getting gradients\u001b[39;00m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Updating parameters\u001b[39;00m\n\u001b[1;32m     43\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/_tensor.py:484\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    476\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    477\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 484\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "# (3) modeling\n",
    "#------------------------------------------------------------------------------------\n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "model = NLinear_Model(seq_len=1, pred_len=1, input_size=input_size)\n",
    "# model = SCINet_Model(input_size = input_size)\n",
    "# model = BaseModel(\n",
    "#     input_size = input_size,\n",
    "#     hidden_sizes=[400,300],\n",
    "#     dropout_rates=[0.2,0.2],\n",
    "#     num_classes=seq_length,\n",
    "#     num_layers=2,\n",
    "#     bidirectional=True,\n",
    "# )\n",
    "\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=100, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "CFG['ES_PATIENCE'] = 400\n",
    "CFG['ES_VERBOSE']  = 0\n",
    "best_model = train(\n",
    "    model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    scheduler=None,#scheduler,\n",
    "    device=device,\n",
    "    early_stopping=True,\n",
    "    metric_period=100,\n",
    "    epochs=8192,\n",
    "    best_model_only=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c16ec8b-cfb7-4535-bbd9-922d254e34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "# (4-1) predict : input dataset\n",
    "#------------------------------------------------------------------------------------\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "#true_list = []\n",
    "with torch.no_grad():\n",
    "    for X,y in iter(input_loader): # train_loader, valid_loader\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X)\n",
    "        # model_pred = torch.exp(model_pred)\n",
    "\n",
    "        pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "        #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "pred_input_df[f'pred_{kf_iter}'] = pred_list\n",
    "# pred_input_df.to_csv(f'./out/kf_lstm/pred_input_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7c4bf1d-b781-4bf0-83f3-402241fcd08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.27548250988552"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=pred_input_df['pred_1'],y=pred_input_df['predicted_weight_g'])\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(pred_input_df['pred_1'],pred_input_df['predicted_weight_g']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2407-3006-4dd8-873f-ddabbfc9d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "# (4-2) predict : test dataset\n",
    "#------------------------------------------------------------------------------------\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "#true_list = []\n",
    "with torch.no_grad():\n",
    "    for X in iter(test_loader): # train_loader, valid_loader\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X)\n",
    "        # model_pred = torch.exp(model_pred)\n",
    "\n",
    "        pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "        #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "pred_test_df[f'pred_{kf_iter}'] = pred_list\n",
    "# pred_test_df.to_csv(f'./out/kf_lstm/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf5a68-454d-457d-9db5-98056e39363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = case_num[tr_idx]\n",
    "    va_case_num = case_num[va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)]\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)]\n",
    "\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (2) custom dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False, seq_length=seq_length)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "    valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False, seq_length=seq_length)\n",
    "    valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "    \n",
    "    # [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "    # [y for x,y in iter(train_loader)]\n",
    "    # sum([y.size(0) for x,y in iter(train_loader)])\n",
    "\n",
    "    # len([x for x,y in iter(train_loader)])\n",
    "\n",
    "    # [(x[0].size(),x[1].size()) for x in train_loader]\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (3) modeling\n",
    "    #------------------------------------------------------------------------------------\n",
    "    seed_everything(CFG['SEED'])\n",
    "\n",
    "    input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "    model = Model(input_size = input_size)\n",
    "    # model = SCINet_Model(input_size = input_size)\n",
    "    # model = BaseModel(\n",
    "    #     input_size = input_size,\n",
    "    #     hidden_sizes=[400,300],\n",
    "    #     dropout_rates=[0.2,0.2],\n",
    "    #     num_classes=seq_length,\n",
    "    #     num_layers=2,\n",
    "    #     bidirectional=True,\n",
    "    # )\n",
    "\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=100, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "    CFG['ES_PATIENCE'] = 400\n",
    "    CFG['ES_VERBOSE']  = 0\n",
    "    best_model = train(\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        scheduler=None,#scheduler,\n",
    "        device=device,\n",
    "        early_stopping=True,\n",
    "        metric_period=100,\n",
    "        epochs=8192,\n",
    "        best_model_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-1) predict : input dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X,y in iter(input_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_input_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_input_df.to_csv(f'./out/kf_lstm/pred_input_df_{kf_iter}.csv',index=False)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-2) predict : test dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_test_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_test_df.to_csv(f'./out/kf_lstm/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89196afd-c692-4b6c-854b-372373e553cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cure-lab/SCINet\n",
    "# https://github.com/fd17/SciNet_PyTorch/blob/master/models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SciNet(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, latent_dim, layer_dim):\n",
    "\t\t\"\"\"Initialize SciNet Model.\n",
    "\t\t\n",
    "\t\tParams\n",
    "\t\t======\n",
    "\t\t\tinput_dim (int): number of inputs\n",
    "\t\t\toutput_dim (int): number of outputs\n",
    "\t\t\tlatent_dim (int): number of latent neurons\n",
    "\t\t\tLayer_dim (int): number of neurons in hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(SciNet, self).__init__()\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\t\tself.enc1 = nn.Linear(input_dim, layer_dim)\n",
    "\t\tself.enc2 = nn.Linear(layer_dim, layer_dim)\n",
    "\t\tself.latent = nn.Linear(layer_dim, latent_dim*2)\n",
    "\t\tself.dec1 = nn.Linear(latent_dim+1, layer_dim)\n",
    "\t\tself.dec2 = nn.Linear(layer_dim,layer_dim)\n",
    "\t\tself.out = nn.Linear(layer_dim, output_dim)       \n",
    "\t  \n",
    "\tdef encoder(self, x):\n",
    "\t\tz = F.elu(self.enc1(x))\n",
    "\t\tz = F.elu(self.enc2(z))\n",
    "\t\tz = self.latent(z)\n",
    "\t\tself.mu = z[:, 0:self.latent_dim]\n",
    "\t\tself.log_sigma = z[:, self.latent_dim:]\n",
    "\t\tself.sigma = torch.exp(self.log_sigma)        \n",
    "\n",
    "\t\t# Use reparametrization trick to sample from gaussian\n",
    "\t\teps = torch.randn(x.size(0), self.latent_dim)\n",
    "\t\tz_sample = self.mu + self.sigma * eps        \n",
    "\n",
    "\t\t# Compute KL loss\n",
    "\t\tself.kl_loss = kl_divergence(self.mu, self.log_sigma, dim=self.latent_dim)\n",
    "\n",
    "\t\treturn z_sample\n",
    "\t\n",
    "\tdef decoder(self, z):\n",
    "\t\tx = F.elu(self.dec1(z))\n",
    "\t\tx = F.elu(self.dec2(x))        \n",
    "\t\treturn self.out(x)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\tq = obs[:,-1].reshape(obs.size(0),1)\n",
    "\t\tobs = obs[:,0:-1]\n",
    "\t\tself.latent_r = self.encoder(obs) \n",
    "\t\tdec_input = torch.cat( (q, self.latent_r), 1)\n",
    "\n",
    "\t\treturn self.decoder(dec_input)\n",
    "\n",
    "\n",
    "def kl_divergence(means, log_sigma, dim, target_sigma=0.1):\n",
    "\t\"\"\"\n",
    "\tComputes Kullback–Leibler divergence for arrays of mean and log(sigma)\n",
    "\t\"\"\"\n",
    "\ttarget_sigma = torch.Tensor([target_sigma])\n",
    "\treturn 1 / 2. * torch.mean(torch.mean(1 / target_sigma**2 * means**2 +\n",
    "\t\t\ttorch.exp(2 * log_sigma) / target_sigma**2 - 2 * log_sigma + 2 * torch.log(target_sigma), dim=1) - dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acf626-e892-4194-b2ed-5f802c85e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def pendulum(t, A0, delta0, k, b, m):\n",
    "\t\"\"\"\n",
    "\tSolution x(t) for pendulum differential equation\n",
    "\t\tmx'' = -kx + bx'\n",
    "\tReturns position at time t\n",
    "\tParameters:\n",
    "\t\t- t: time\n",
    "\t\t- A0: starting amplitude\n",
    "\t\t- delta0: phase\n",
    "\t\t- k: spring constant\n",
    "\t\t- b: damping factor\n",
    "\t\"\"\"\n",
    "\tA = 1 - b**2 / (4 * m * k)\n",
    "\tif A < 0:\n",
    "\t\treturn None\n",
    "\tw = np.sqrt(k/m)* np.sqrt(A)\n",
    "\tresult = A0 * np.exp( - t * b / (2. * m) ) * np.cos(w * t + delta0)\n",
    "\treturn result\n",
    "\n",
    "def target_loss(pred,answer):\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tpred = pred[:,0]\n",
    "\t\n",
    "\treturn torch.mean(torch.sum((pred - answer)**2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d7a3e-599a-4a26-87e4-de8339ea32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scinet = SciNet(input_dim=X_train.shape[1], output_dim=1, latent_dim=3, layer_dim=64)\n",
    "scinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ba876-adf6-4da2-97f6-ce8debd72be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "optimizer = optim.Adam(scinet.parameters())\n",
    "hist_error = []\n",
    "hist_loss = []\n",
    "beta = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    epoch_error = []\n",
    "    epoch_loss = []\n",
    "    for i_batch, minibatch in enumerate(train_loader):\n",
    "\n",
    "        inputs, outputs = minibatch\n",
    "        optimizer.zero_grad()\n",
    "        pred = scinet.forward(inputs)\n",
    "        \n",
    "        loss = target_loss(pred, outputs) + beta * scinet.kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        error = torch.mean(torch.sqrt((pred[:,0]-outputs)**2)).detach().numpy()\n",
    "        epoch_error.append(error)\n",
    "        epoch_loss.append(loss.data.detach().numpy())\n",
    "    hist_error.append(np.mean(epoch_error))\n",
    "    hist_loss.append(np.mean(epoch_loss))\n",
    "    print(\"Epoch %d -- loss %f, RMS error %f \" % (epoch+1, hist_loss[-1], hist_error[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d9fee-508f-4d4f-b2d5-4e7a68691887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "# model = BaseModel(\n",
    "#     input_size = X_train.shape[1],\n",
    "#     hidden_sizes=[32,16],\n",
    "#     dropout_rates=[0.2,0.2],\n",
    "#     num_classes=1,\n",
    "#     num_layers=1,\n",
    "#     bidirectional=False,\n",
    "# )\n",
    "# # model = GRUModel(input_dim=X_train.shape[1], hidden_dim=64, layer_dim=1, output_dim=1)\n",
    "\n",
    "# model.eval()\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-6)# lr = CFG[\"LEARNING_RATE\"], weight_decay=1e-5)\n",
    "# # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=False)\n",
    "\n",
    "# best_model = train(\n",
    "#     model,\n",
    "#     optimizer=optimizer,\n",
    "#     train_loader=train_loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     early_stopping=False,\n",
    "#     metric_period=1,\n",
    "#     epochs=1024,\n",
    "# )\n",
    "\n",
    "# test_df = test_df[X_train.columns]\n",
    "# test_dataset = CustomTestDataset(input=test_df)\n",
    "# test_loader  = DataLoader(test_dataset, shuffle=False, num_workers=0)\n",
    "\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# test_pred = []\n",
    "# with torch.no_grad():\n",
    "#     for X in iter(test_loader):\n",
    "#         X = X.float().to(device)\n",
    "\n",
    "#         model_pred = model(X)\n",
    "#         # model_pred = torch.exp(model_pred)\n",
    "#         model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "#         test_pred += model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5189c-9130-4ab4-8d64-ffc051c7667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b36a1-ca35-499f-ac31-9e1de876df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted_weight_g'] = test_pred\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8c26e-2811-4d5e-9737-be9e9982741c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f1d1e-d7b4-430b-9293-a4b4d060d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GRUCell(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size,  3*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3*hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self) :\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters() : \n",
    "            w.data.uniform_(-std, std)\n",
    "            \n",
    "    def forward(self, x, hidden) : \n",
    "        x = x.view(-1, x.size(1))\n",
    "        \n",
    "        gate_x = self.x2h(x)\n",
    "        gate_h = self.h2h(hidden)\n",
    "        \n",
    "        gate_x = gate_x.squeeze()\n",
    "        gate_h = gate_h.squeeze()\n",
    "        \n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        \n",
    "        resetgate = F.sigmoid(i_r + h_r)\n",
    "        inputgate = F.sigmoid(i_i + h_i)\n",
    "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
    "        \n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "    \n",
    "class GRUModel(nn.Module) :\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True) :\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        if torch.cuda.is_available() :\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else :\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        outs = []\n",
    "        hn = h0[0, :, :]\n",
    "        \n",
    "        for seq in range(x.size(1)) : \n",
    "            hn = self.gru_cell(x[:, seq, :], hn)\n",
    "            outs.append(hn)\n",
    "        out = outs[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86409d-8a20-4a3b-97c7-067e4a2956d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cd5d5-3c8c-463c-9544-b08ff83ace31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "path = f'./model/best_model_999.pt'\n",
    "# path = f'./model/best_model.pt'\n",
    "\n",
    "torch.save(best_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f48a7-6517-40ac-9024-adc1e99b8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Model(input_size = input_size) # SCINet_Model\n",
    "best_model.load_state_dict(torch.load(path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fd45c-dbf6-41c2-a2b4-997084b88a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3eddf-1719-4355-ab47-3989ffbe0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x.shape for x,y in iter(train_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c750e-6ffc-44ca-959a-565416951e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f46acc-77b7-48fa-b5ef-22ee051ddd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in np.array(pred_list)]),len([x for x in np.array(true_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66844fb9-0ea7-4bc3-a491-bb4887a80e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len([x for x in np.array(pred_list)])/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cd058-96cf-4058-a483-cb03354cad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(size): # 3,25\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        'pred' : pred_list,\n",
    "        'true' : true_list,\n",
    "    })[i*28:(i+1)*28].reset_index(drop=True)\n",
    "    # plot_df = np.exp(plot_df)\n",
    "\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.true,color='black')\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.pred,color='red')\n",
    "    # plt.title(random_num[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e4b32-cebe-4f62-815c-dd1cce12a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(plot_df.pred,plot_df.true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc1a96-e2f7-422f-82b6-559a8370770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X in iter(test_loader):\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X)\n",
    "        # model_pred = torch.exp(model_pred)\n",
    "\n",
    "        pred_list += model_pred.cpu().numpy().reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96f71-2ae7-444e-9437-7bbb251a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test_label_df.sort_values(['case_num','DAT'])\n",
    "sub['predicted_weight_g'] = pred_list\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num].drop('case_num',axis=1)\n",
    "    s.to_csv(f'./out/lstm/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(\"./out/lstm/\")\n",
    "submission = zipfile.ZipFile(\"../lstm.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
