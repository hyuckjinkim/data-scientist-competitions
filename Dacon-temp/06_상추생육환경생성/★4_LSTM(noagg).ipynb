{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfd3826-7ff1-4a26-b431-6f34d34b5125",
   "metadata": {},
   "source": [
    "- kmeans 등으로 y의 max랑 구분지어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4041a5-44ba-4987-a6e7-3c3f40f10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 torch 다운로드\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e5953c-7a86-4c51-a887-423f7beb6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fp = fm.FontProperties(fname='/home/studio-lab-user/Dacon/tools/NanumFont/NanumGothic.ttf', size=10)\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "# # cuda (not Mac)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # mps (Mac)\n",
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47cef-a496-4dc7-9a8b-0f363f65cdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':128,#1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':0.05,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0fc924-3361-45c3-9f77-a764ef8fbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x,size):\n",
    "\n",
    "    ma = []\n",
    "    for i in range(len(x)):\n",
    "        if i<size:\n",
    "            values = x.values[:(i+1)]\n",
    "        else:\n",
    "            values = x.values[(i-size+1):(i+1)]\n",
    "        ma_value = values.mean()\n",
    "        ma.append(ma_value)\n",
    "        \n",
    "    return ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, input_paths, label_paths, test_input_paths, test_label_paths):\n",
    "        \n",
    "        self.input, self.label, self.test_input, self.test_label = None, None, None, None\n",
    "        \n",
    "        self.X_train, self.X_valid = None, None\n",
    "        self.y_train, self.y_valid = None, None\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, label_path in zip(input_paths, label_paths):\n",
    "            case_num = input_path.replace('./data/train_input/CASE_','').replace('.csv','')\n",
    "            \n",
    "            input_df = pd.read_csv(input_path)\n",
    "            label_df = pd.read_csv(label_path)\n",
    "\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_df['case_num'] = case_num\n",
    "            label_df['case_num'] = case_num\n",
    "            \n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(label_df)\n",
    "        \n",
    "        test_input_fn = []\n",
    "        test_label_fn = []\n",
    "        for test_input_path, test_label_path in zip(test_input_paths, test_label_paths):\n",
    "            case_num = test_input_path.replace('./data/test_input/TEST_','').replace('.csv','')\n",
    "            \n",
    "            test_input_df = pd.read_csv(test_input_path)\n",
    "            test_label_df = pd.read_csv(test_label_path)\n",
    "            \n",
    "            test_input_df['case_num'] = case_num\n",
    "            test_label_df['case_num'] = case_num\n",
    "            \n",
    "            test_input_fn.append(test_input_df)\n",
    "            test_label_fn.append(test_label_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0).sort_values(['case_num','DAT','obs_time'])\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_fn ,axis=0)\n",
    "        self.test_label  = pd.concat(test_label_fn ,axis=0)\n",
    "        \n",
    "        self.input     .obs_time = list(np.arange(0,24))*int(self.input     .shape[0]/24)\n",
    "        self.test_input.obs_time = list(np.arange(0,24))*int(self.test_input.shape[0]/24)\n",
    "        \n",
    "    def _data_return(self):\n",
    "        return self.input,self.label,self.test_input,self.test_label\n",
    "            \n",
    "    def _target_log(self):\n",
    "        self.label['predicted_weight_g'] = np.log(self.label['predicted_weight_g'])\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        # 1. time 추가 : 1~672 (24시간 x 28일)\n",
    "        self.input     ['time'] = [i+1 for i in range(28*24)]*self.input     .case_num.nunique()\n",
    "        self.test_input['time'] = [i+1 for i in range(28*24)]*self.test_input.case_num.nunique()\n",
    "\n",
    "        features = [\n",
    "            'DAT', 'obs_time', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'case_num', 'time'\n",
    "        ]\n",
    "        # del_features = [\n",
    "        #     '시간당분무량','시간당백색광량','시간당적색광량','시간당청색광량','시간당총광량',\n",
    "        #     # '일간누적분무량','일간누적백색광량','일간누적적색광량','일간누적청색광량','일간누적총광량'\n",
    "        # ]\n",
    "        # self.input     .drop(columns=del_features,inplace=True)\n",
    "        # self.test_input.drop(columns=del_features,inplace=True)\n",
    "        \n",
    "        # 2. 각 컬럼들의 파생변수\n",
    "        input_df       = []\n",
    "        test_input_df  = []\n",
    "        for case_num in self.input.case_num.unique():\n",
    "            i_df = self.input     [self.input     .case_num==case_num]\n",
    "            t_df = self.test_input[self.test_input.case_num==case_num]\n",
    "            \n",
    "            for col in list(set(self.input.columns)-set(['case_num','DAT','obs_time','time'])):\n",
    "                for i in range(4):\n",
    "                    # (1) 이전시간 값\n",
    "                    i_df[f'{col}_bf{i+1}'] = i_df[col].shift(i+1).fillna(0)\n",
    "                    t_df[f'{col}_bf{i+1}'] = t_df[col].shift(i+1).fillna(0)\n",
    "                \n",
    "                    # (2) 전시간대 대비 상승했는지 여부\n",
    "                    i_df[f'{col}_higher_than_{i+1}d'] = np.where(i_df[col]>i_df[col].shift(i+1),1,0)\n",
    "                    t_df[f'{col}_higher_than_{i+1}d'] = np.where(t_df[col]>t_df[col].shift(i+1),1,0)\n",
    "\n",
    "                    # (3) 전시간대 대비 상승률 -> 넣으면 NaN 발생\n",
    "                    if i_df[col].min()<=0:\n",
    "                        offset = i_df[col].min()\n",
    "                        i_df[col] = i_df[col] + offset\n",
    "                        t_df[col] = t_df[col] + offset\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'] = (i_df[col] - i_df[col].shift(i+1)) / i_df[col]\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'] = (t_df[col] - t_df[col].shift(i+1)) / t_df[col]\n",
    "\n",
    "                    # -inf -> min, inf -> max\n",
    "                    tmp = i_df[f'{col}_{i+1}d_rise_rate'].copy()\n",
    "                    tmp = tmp[(tmp!=-np.inf) & (tmp!=np.inf)]\n",
    "                    min_info, max_info = tmp.min(), tmp.max()\n",
    "\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    \n",
    "                    # fill nan to zero\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    \n",
    "                    # (4) moving average\n",
    "                    for size in [2,4,7]:\n",
    "                        i_df[f'{col}_ma{size}'] = moving_average(i_df[col],size=size)\n",
    "                        t_df[f'{col}_ma{size}'] = moving_average(t_df[col],size=size)\n",
    "                \n",
    "                # (5) cumulative sum\n",
    "                i_df[f'{col}_cumsum'] = i_df[col].cumsum()\n",
    "                t_df[f'{col}_cumsum'] = t_df[col].cumsum()\n",
    "\n",
    "            input_df     .append(i_df)\n",
    "            test_input_df.append(t_df)\n",
    "        \n",
    "        # concat\n",
    "        self.input       = pd.concat(input_df     ,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_df,axis=0)\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # https://dacon.io/competitions/official/236033/talkboard/407304?page=1&dtype=recent\n",
    "    def _scale_dataset(self,outlier):\n",
    "        \n",
    "        minmax_info = {\n",
    "            #'None':[0,0],\n",
    "            'DAT':[1,28],\n",
    "            'obs_time':[0,23],\n",
    "            'time':[0,28*24],\n",
    "            '내부온도관측치':[4,40],\n",
    "            '내부습도관측치':[0,100],\n",
    "            'co2관측치':[0,1200],\n",
    "            'ec관측치':[0,8],\n",
    "            '시간당분무량':[0,3000],\n",
    "            '일간누적분무량':[0,72000],\n",
    "            '시간당백색광량':[0,120000],\n",
    "            '일간누적백색광량':[0,2880000],\n",
    "            '시간당적색광량':[0,120000],\n",
    "            '일간누적적색광량':[0,2880000],\n",
    "            '시간당청색광량':[0,120000],\n",
    "            '일간누적청색광량':[0,2880000],\n",
    "            '시간당총광량':[0,120000],\n",
    "            '일간누적총광량':[0,2880000],\n",
    "        }\n",
    "            \n",
    "        scale_feature = [feature for feature,(min_info,max_info) in minmax_info.items() if feature in self.input.columns]\n",
    "        \n",
    "        # for train dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.input[col] = (self.input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.input[col][self.input[col]<0] = 0\n",
    "                self.input[col][self.input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.input[col][(self.input[col]<0) | (self.input[col]>1)] = np.nan\n",
    "            \n",
    "        # for test dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.test_input[col][self.test_input[col]<0] = 0\n",
    "                self.test_input[col][self.test_input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.test_input[col][(self.test_input[col]<0) | (self.test_input[col]>1)] = np.nan\n",
    "        \n",
    "        # another features\n",
    "        another_features = list(set(self.input.select_dtypes(exclude=[object]).columns)-set(scale_feature))\n",
    "        for col in another_features:\n",
    "            if self.input[col].min()==0:\n",
    "                offset=1e-4\n",
    "                self.input[col]      = self.input     [col]+offset\n",
    "                self.test_input[col] = self.test_input[col]+offset\n",
    "            \n",
    "            min_info,max_info = self.input[col].min(),self.input[col].max()    \n",
    "            self.input[col]      = (self.input[col]     -min_info) / (max_info-min_info)\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "        \n",
    "    def _interaction_term(self):\n",
    "        # num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        num_features = [\n",
    "            'DAT','time',\n",
    "            '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량',\n",
    "        ]\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    self.input     [f'{num_features[i]}*{num_features[j]}'] = self.input     [num_features[i]]*self.input     [num_features[j]]\n",
    "                    self.test_input[f'{num_features[i]}*{num_features[j]}'] = self.test_input[num_features[i]]*self.test_input[num_features[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46afb772-6e30-42b5-b09b-f0cdc584fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf407c58-fe03-456d-9bad-ec0ff6b18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# val_rate = 0.05\n",
    "\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# dataset._preprocess()\n",
    "# dataset._scale_dataset()\n",
    "# input_df, label_df = dataset._data_return()\n",
    "\n",
    "# for case_num in tqdm(sorted(input_df.case_num.unique())):\n",
    "\n",
    "#     input = input_df[input_df.case_num==case_num].drop('case_num',axis=1)\n",
    "#     label = label_df[label_df.case_num==case_num].drop('case_num',axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize=(20,15))\n",
    "#     nrow = 3\n",
    "#     ncol = 5\n",
    "\n",
    "#     iter = 0\n",
    "#     total = len(input.columns)-3\n",
    "#     for col in input.columns:\n",
    "#         if col not in ['time','DAT','obs_time']:\n",
    "#             iter+=1\n",
    "\n",
    "#             y1 = input[col]\n",
    "#             #y1 = (y1-y1.min())/(y1.max()-y1.min())\n",
    "\n",
    "#             y2 = label['predicted_weight_g']\n",
    "#             y2 = (y2-y2.min())/(y2.max()-y2.min())\n",
    "\n",
    "#             y3 = input.groupby('DAT')[col].mean().values\n",
    "\n",
    "#             corr, pvalue = pearsonr(y2,y3)\n",
    "\n",
    "#             fig.add_subplot(ncol,nrow,iter)\n",
    "#             sns.scatterplot(x=input.time  ,y=y1)\n",
    "#             sns.scatterplot(x=label.DAT*24,y=y2,color='red')\n",
    "#             sns.lineplot   (x=label.DAT*24,y=y3,color='blue',linestyle='--',alpha=0.7)\n",
    "#             plt.ylabel('')\n",
    "\n",
    "#             plt.title(f'{col}(corr={corr:.3f}(pvalue={pvalue:.3f}))',fontproperties=fp)\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'./fig/{case_num}.png',dpi=100)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731d783-38cb-43e3-8f4f-1f9c6d187cdc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d57fd0-ea32-4fb4-a626-1006236b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_label_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "all_test_label_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d65ad91-4f38-4423-99f9-c1e8089c2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 s, sys: 1.59 s, total: 57.3 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess Class\n",
    "dataset = Preprocess(\n",
    "    input_paths = all_input_list,\n",
    "    label_paths = all_label_list,\n",
    "    test_input_paths = all_test_input_list,\n",
    "    test_label_paths = all_test_label_list,\n",
    ")\n",
    "\n",
    "# (1) preprocessing + scaling + interaction term\n",
    "dataset._preprocess()\n",
    "# dataset._target_log()\n",
    "dataset._scale_dataset(outlier='keep')\n",
    "# dataset._interaction_term()\n",
    "\n",
    "# (2) Data Return for check\n",
    "input_df, label_df, test_input_df, test_label_df = dataset._data_return()\n",
    "\n",
    "# # (3) Delete Std zero features\n",
    "# std_zero_features = []\n",
    "# for case_num in input_df.case_num.unique():\n",
    "#     tmp = input_df[input_df.case_num==case_num]\n",
    "#     std_zero_feature = tmp.std().index[tmp.std()==0].tolist()\n",
    "#     std_zero_features += std_zero_feature\n",
    "    \n",
    "# std_zero_features = pd.unique(std_zero_features)\n",
    "\n",
    "# input_df = input_df.drop(std_zero_features,axis=1)\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ec50457-eded-4068-a3ef-8487af93b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_info = input_df.isnull().sum()\n",
    "null_info[null_info!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "521202eb-ce6f-4e3c-a229-ffb2f5bf7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = input_df.copy()\n",
    "\n",
    "# cols = [col for col in d.columns if col.find('_higher_than_')>=0]\n",
    "# std_zero_cols = d[cols].std()==0\n",
    "# del_cols = std_zero_cols[std_zero_cols].index.tolist()\n",
    "# print(del_cols)\n",
    "\n",
    "# # for i in range(len(cols)):\n",
    "# #     print(f'({i}/{len(cols)}) {cols[i]}')\n",
    "# #     plt.figure(figsize=(8,7))\n",
    "# #     sns.lineplot(x=d.time,y=d[cols[i]])\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8979bc0a-1589-44b6-9cc6-6733be3e1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18816, 242)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_df.isnull().sum()\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f20a6772-c516-46e3-b30c-e9c4041134b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAT</th>\n",
       "      <th>obs_time</th>\n",
       "      <th>내부온도관측치</th>\n",
       "      <th>내부습도관측치</th>\n",
       "      <th>co2관측치</th>\n",
       "      <th>ec관측치</th>\n",
       "      <th>시간당분무량</th>\n",
       "      <th>일간누적분무량</th>\n",
       "      <th>시간당백색광량</th>\n",
       "      <th>일간누적백색광량</th>\n",
       "      <th>시간당적색광량</th>\n",
       "      <th>일간누적적색광량</th>\n",
       "      <th>시간당청색광량</th>\n",
       "      <th>일간누적청색광량</th>\n",
       "      <th>시간당총광량</th>\n",
       "      <th>일간누적총광량</th>\n",
       "      <th>case_num</th>\n",
       "      <th>time</th>\n",
       "      <th>시간당총광량_bf1</th>\n",
       "      <th>시간당총광량_higher_than_1d</th>\n",
       "      <th>시간당총광량_1d_rise_rate</th>\n",
       "      <th>시간당총광량_ma2</th>\n",
       "      <th>시간당총광량_ma4</th>\n",
       "      <th>시간당총광량_ma7</th>\n",
       "      <th>시간당총광량_bf2</th>\n",
       "      <th>시간당총광량_higher_than_2d</th>\n",
       "      <th>시간당총광량_2d_rise_rate</th>\n",
       "      <th>시간당총광량_bf3</th>\n",
       "      <th>시간당총광량_higher_than_3d</th>\n",
       "      <th>시간당총광량_3d_rise_rate</th>\n",
       "      <th>시간당총광량_bf4</th>\n",
       "      <th>시간당총광량_higher_than_4d</th>\n",
       "      <th>시간당총광량_4d_rise_rate</th>\n",
       "      <th>시간당총광량_cumsum</th>\n",
       "      <th>내부온도관측치_bf1</th>\n",
       "      <th>내부온도관측치_higher_than_1d</th>\n",
       "      <th>내부온도관측치_1d_rise_rate</th>\n",
       "      <th>내부온도관측치_ma2</th>\n",
       "      <th>내부온도관측치_ma4</th>\n",
       "      <th>내부온도관측치_ma7</th>\n",
       "      <th>내부온도관측치_bf2</th>\n",
       "      <th>내부온도관측치_higher_than_2d</th>\n",
       "      <th>내부온도관측치_2d_rise_rate</th>\n",
       "      <th>내부온도관측치_bf3</th>\n",
       "      <th>내부온도관측치_higher_than_3d</th>\n",
       "      <th>내부온도관측치_3d_rise_rate</th>\n",
       "      <th>내부온도관측치_bf4</th>\n",
       "      <th>내부온도관측치_higher_than_4d</th>\n",
       "      <th>내부온도관측치_4d_rise_rate</th>\n",
       "      <th>내부온도관측치_cumsum</th>\n",
       "      <th>일간누적총광량_bf1</th>\n",
       "      <th>일간누적총광량_higher_than_1d</th>\n",
       "      <th>일간누적총광량_1d_rise_rate</th>\n",
       "      <th>일간누적총광량_ma2</th>\n",
       "      <th>일간누적총광량_ma4</th>\n",
       "      <th>일간누적총광량_ma7</th>\n",
       "      <th>일간누적총광량_bf2</th>\n",
       "      <th>일간누적총광량_higher_than_2d</th>\n",
       "      <th>일간누적총광량_2d_rise_rate</th>\n",
       "      <th>일간누적총광량_bf3</th>\n",
       "      <th>일간누적총광량_higher_than_3d</th>\n",
       "      <th>일간누적총광량_3d_rise_rate</th>\n",
       "      <th>일간누적총광량_bf4</th>\n",
       "      <th>일간누적총광량_higher_than_4d</th>\n",
       "      <th>일간누적총광량_4d_rise_rate</th>\n",
       "      <th>일간누적총광량_cumsum</th>\n",
       "      <th>일간누적분무량_bf1</th>\n",
       "      <th>일간누적분무량_higher_than_1d</th>\n",
       "      <th>일간누적분무량_1d_rise_rate</th>\n",
       "      <th>일간누적분무량_ma2</th>\n",
       "      <th>일간누적분무량_ma4</th>\n",
       "      <th>일간누적분무량_ma7</th>\n",
       "      <th>일간누적분무량_bf2</th>\n",
       "      <th>일간누적분무량_higher_than_2d</th>\n",
       "      <th>일간누적분무량_2d_rise_rate</th>\n",
       "      <th>일간누적분무량_bf3</th>\n",
       "      <th>일간누적분무량_higher_than_3d</th>\n",
       "      <th>일간누적분무량_3d_rise_rate</th>\n",
       "      <th>일간누적분무량_bf4</th>\n",
       "      <th>일간누적분무량_higher_than_4d</th>\n",
       "      <th>일간누적분무량_4d_rise_rate</th>\n",
       "      <th>일간누적분무량_cumsum</th>\n",
       "      <th>시간당적색광량_bf1</th>\n",
       "      <th>시간당적색광량_higher_than_1d</th>\n",
       "      <th>시간당적색광량_1d_rise_rate</th>\n",
       "      <th>시간당적색광량_ma2</th>\n",
       "      <th>시간당적색광량_ma4</th>\n",
       "      <th>시간당적색광량_ma7</th>\n",
       "      <th>시간당적색광량_bf2</th>\n",
       "      <th>시간당적색광량_higher_than_2d</th>\n",
       "      <th>시간당적색광량_2d_rise_rate</th>\n",
       "      <th>시간당적색광량_bf3</th>\n",
       "      <th>시간당적색광량_higher_than_3d</th>\n",
       "      <th>시간당적색광량_3d_rise_rate</th>\n",
       "      <th>시간당적색광량_bf4</th>\n",
       "      <th>시간당적색광량_higher_than_4d</th>\n",
       "      <th>시간당적색광량_4d_rise_rate</th>\n",
       "      <th>시간당적색광량_cumsum</th>\n",
       "      <th>내부습도관측치_bf1</th>\n",
       "      <th>내부습도관측치_higher_than_1d</th>\n",
       "      <th>내부습도관측치_1d_rise_rate</th>\n",
       "      <th>내부습도관측치_ma2</th>\n",
       "      <th>내부습도관측치_ma4</th>\n",
       "      <th>내부습도관측치_ma7</th>\n",
       "      <th>내부습도관측치_bf2</th>\n",
       "      <th>내부습도관측치_higher_than_2d</th>\n",
       "      <th>내부습도관측치_2d_rise_rate</th>\n",
       "      <th>내부습도관측치_bf3</th>\n",
       "      <th>내부습도관측치_higher_than_3d</th>\n",
       "      <th>내부습도관측치_3d_rise_rate</th>\n",
       "      <th>내부습도관측치_bf4</th>\n",
       "      <th>내부습도관측치_higher_than_4d</th>\n",
       "      <th>내부습도관측치_4d_rise_rate</th>\n",
       "      <th>내부습도관측치_cumsum</th>\n",
       "      <th>시간당분무량_bf1</th>\n",
       "      <th>시간당분무량_higher_than_1d</th>\n",
       "      <th>시간당분무량_1d_rise_rate</th>\n",
       "      <th>시간당분무량_ma2</th>\n",
       "      <th>시간당분무량_ma4</th>\n",
       "      <th>시간당분무량_ma7</th>\n",
       "      <th>시간당분무량_bf2</th>\n",
       "      <th>시간당분무량_higher_than_2d</th>\n",
       "      <th>시간당분무량_2d_rise_rate</th>\n",
       "      <th>시간당분무량_bf3</th>\n",
       "      <th>시간당분무량_higher_than_3d</th>\n",
       "      <th>시간당분무량_3d_rise_rate</th>\n",
       "      <th>시간당분무량_bf4</th>\n",
       "      <th>시간당분무량_higher_than_4d</th>\n",
       "      <th>시간당분무량_4d_rise_rate</th>\n",
       "      <th>시간당분무량_cumsum</th>\n",
       "      <th>일간누적백색광량_bf1</th>\n",
       "      <th>일간누적백색광량_higher_than_1d</th>\n",
       "      <th>일간누적백색광량_1d_rise_rate</th>\n",
       "      <th>일간누적백색광량_ma2</th>\n",
       "      <th>일간누적백색광량_ma4</th>\n",
       "      <th>일간누적백색광량_ma7</th>\n",
       "      <th>일간누적백색광량_bf2</th>\n",
       "      <th>일간누적백색광량_higher_than_2d</th>\n",
       "      <th>일간누적백색광량_2d_rise_rate</th>\n",
       "      <th>일간누적백색광량_bf3</th>\n",
       "      <th>일간누적백색광량_higher_than_3d</th>\n",
       "      <th>일간누적백색광량_3d_rise_rate</th>\n",
       "      <th>일간누적백색광량_bf4</th>\n",
       "      <th>일간누적백색광량_higher_than_4d</th>\n",
       "      <th>일간누적백색광량_4d_rise_rate</th>\n",
       "      <th>일간누적백색광량_cumsum</th>\n",
       "      <th>ec관측치_bf1</th>\n",
       "      <th>ec관측치_higher_than_1d</th>\n",
       "      <th>ec관측치_1d_rise_rate</th>\n",
       "      <th>ec관측치_ma2</th>\n",
       "      <th>ec관측치_ma4</th>\n",
       "      <th>ec관측치_ma7</th>\n",
       "      <th>ec관측치_bf2</th>\n",
       "      <th>ec관측치_higher_than_2d</th>\n",
       "      <th>ec관측치_2d_rise_rate</th>\n",
       "      <th>ec관측치_bf3</th>\n",
       "      <th>ec관측치_higher_than_3d</th>\n",
       "      <th>ec관측치_3d_rise_rate</th>\n",
       "      <th>ec관측치_bf4</th>\n",
       "      <th>ec관측치_higher_than_4d</th>\n",
       "      <th>ec관측치_4d_rise_rate</th>\n",
       "      <th>ec관측치_cumsum</th>\n",
       "      <th>시간당백색광량_bf1</th>\n",
       "      <th>시간당백색광량_higher_than_1d</th>\n",
       "      <th>시간당백색광량_1d_rise_rate</th>\n",
       "      <th>시간당백색광량_ma2</th>\n",
       "      <th>시간당백색광량_ma4</th>\n",
       "      <th>시간당백색광량_ma7</th>\n",
       "      <th>시간당백색광량_bf2</th>\n",
       "      <th>시간당백색광량_higher_than_2d</th>\n",
       "      <th>시간당백색광량_2d_rise_rate</th>\n",
       "      <th>시간당백색광량_bf3</th>\n",
       "      <th>시간당백색광량_higher_than_3d</th>\n",
       "      <th>시간당백색광량_3d_rise_rate</th>\n",
       "      <th>시간당백색광량_bf4</th>\n",
       "      <th>시간당백색광량_higher_than_4d</th>\n",
       "      <th>시간당백색광량_4d_rise_rate</th>\n",
       "      <th>시간당백색광량_cumsum</th>\n",
       "      <th>시간당청색광량_bf1</th>\n",
       "      <th>시간당청색광량_higher_than_1d</th>\n",
       "      <th>시간당청색광량_1d_rise_rate</th>\n",
       "      <th>시간당청색광량_ma2</th>\n",
       "      <th>시간당청색광량_ma4</th>\n",
       "      <th>시간당청색광량_ma7</th>\n",
       "      <th>시간당청색광량_bf2</th>\n",
       "      <th>시간당청색광량_higher_than_2d</th>\n",
       "      <th>시간당청색광량_2d_rise_rate</th>\n",
       "      <th>시간당청색광량_bf3</th>\n",
       "      <th>시간당청색광량_higher_than_3d</th>\n",
       "      <th>시간당청색광량_3d_rise_rate</th>\n",
       "      <th>시간당청색광량_bf4</th>\n",
       "      <th>시간당청색광량_higher_than_4d</th>\n",
       "      <th>시간당청색광량_4d_rise_rate</th>\n",
       "      <th>시간당청색광량_cumsum</th>\n",
       "      <th>일간누적청색광량_bf1</th>\n",
       "      <th>일간누적청색광량_higher_than_1d</th>\n",
       "      <th>일간누적청색광량_1d_rise_rate</th>\n",
       "      <th>일간누적청색광량_ma2</th>\n",
       "      <th>일간누적청색광량_ma4</th>\n",
       "      <th>일간누적청색광량_ma7</th>\n",
       "      <th>일간누적청색광량_bf2</th>\n",
       "      <th>일간누적청색광량_higher_than_2d</th>\n",
       "      <th>일간누적청색광량_2d_rise_rate</th>\n",
       "      <th>일간누적청색광량_bf3</th>\n",
       "      <th>일간누적청색광량_higher_than_3d</th>\n",
       "      <th>일간누적청색광량_3d_rise_rate</th>\n",
       "      <th>일간누적청색광량_bf4</th>\n",
       "      <th>일간누적청색광량_higher_than_4d</th>\n",
       "      <th>일간누적청색광량_4d_rise_rate</th>\n",
       "      <th>일간누적청색광량_cumsum</th>\n",
       "      <th>일간누적적색광량_bf1</th>\n",
       "      <th>일간누적적색광량_higher_than_1d</th>\n",
       "      <th>일간누적적색광량_1d_rise_rate</th>\n",
       "      <th>일간누적적색광량_ma2</th>\n",
       "      <th>일간누적적색광량_ma4</th>\n",
       "      <th>일간누적적색광량_ma7</th>\n",
       "      <th>일간누적적색광량_bf2</th>\n",
       "      <th>일간누적적색광량_higher_than_2d</th>\n",
       "      <th>일간누적적색광량_2d_rise_rate</th>\n",
       "      <th>일간누적적색광량_bf3</th>\n",
       "      <th>일간누적적색광량_higher_than_3d</th>\n",
       "      <th>일간누적적색광량_3d_rise_rate</th>\n",
       "      <th>일간누적적색광량_bf4</th>\n",
       "      <th>일간누적적색광량_higher_than_4d</th>\n",
       "      <th>일간누적적색광량_4d_rise_rate</th>\n",
       "      <th>일간누적적색광량_cumsum</th>\n",
       "      <th>co2관측치_bf1</th>\n",
       "      <th>co2관측치_higher_than_1d</th>\n",
       "      <th>co2관측치_1d_rise_rate</th>\n",
       "      <th>co2관측치_ma2</th>\n",
       "      <th>co2관측치_ma4</th>\n",
       "      <th>co2관측치_ma7</th>\n",
       "      <th>co2관측치_bf2</th>\n",
       "      <th>co2관측치_higher_than_2d</th>\n",
       "      <th>co2관측치_2d_rise_rate</th>\n",
       "      <th>co2관측치_bf3</th>\n",
       "      <th>co2관측치_higher_than_3d</th>\n",
       "      <th>co2관측치_3d_rise_rate</th>\n",
       "      <th>co2관측치_bf4</th>\n",
       "      <th>co2관측치_higher_than_4d</th>\n",
       "      <th>co2관측치_4d_rise_rate</th>\n",
       "      <th>co2관측치_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.818350</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.175930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.734724</td>\n",
       "      <td>0.567709</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.424022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748157</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756583</td>\n",
       "      <td>0.870398</td>\n",
       "      <td>0.850697</td>\n",
       "      <td>0.850592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400607</td>\n",
       "      <td>0.815584</td>\n",
       "      <td>0.833827</td>\n",
       "      <td>0.837769</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.884755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.259206</td>\n",
       "      <td>0.260748</td>\n",
       "      <td>0.261681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472981</td>\n",
       "      <td>0.289605</td>\n",
       "      <td>0.296091</td>\n",
       "      <td>0.314353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564111</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.602232</td>\n",
       "      <td>0.812643</td>\n",
       "      <td>0.440580</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738653</td>\n",
       "      <td>0.572653</td>\n",
       "      <td>0.479162</td>\n",
       "      <td>0.431122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748157</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.877540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.754874</td>\n",
       "      <td>0.867173</td>\n",
       "      <td>0.846946</td>\n",
       "      <td>0.846810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405704</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.843155</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.884774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.259350</td>\n",
       "      <td>0.260893</td>\n",
       "      <td>0.261826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459046</td>\n",
       "      <td>0.287377</td>\n",
       "      <td>0.293813</td>\n",
       "      <td>0.311934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564111</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.590926</td>\n",
       "      <td>0.814717</td>\n",
       "      <td>0.444028</td>\n",
       "      <td>0.175864</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730452</td>\n",
       "      <td>0.572306</td>\n",
       "      <td>0.476757</td>\n",
       "      <td>0.428423</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748157</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.871420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757203</td>\n",
       "      <td>0.865120</td>\n",
       "      <td>0.846604</td>\n",
       "      <td>0.846465</td>\n",
       "      <td>0.878576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.773351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.205209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394962</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.841360</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.884774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.259301</td>\n",
       "      <td>0.260812</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.480795</td>\n",
       "      <td>0.286408</td>\n",
       "      <td>0.293912</td>\n",
       "      <td>0.312039</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564111</td>\n",
       "      <td>0.003222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.593194</td>\n",
       "      <td>0.813983</td>\n",
       "      <td>0.454639</td>\n",
       "      <td>0.175836</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.592668</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735578</td>\n",
       "      <td>0.568077</td>\n",
       "      <td>0.476234</td>\n",
       "      <td>0.427836</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728510</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748157</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.873644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756364</td>\n",
       "      <td>0.865878</td>\n",
       "      <td>0.846192</td>\n",
       "      <td>0.846050</td>\n",
       "      <td>0.872449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.774728</td>\n",
       "      <td>0.879694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.739822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405704</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.843155</td>\n",
       "      <td>0.348337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.884793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.259088</td>\n",
       "      <td>0.260761</td>\n",
       "      <td>0.261694</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>0.291544</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>0.314196</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543618</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564111</td>\n",
       "      <td>0.004365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.594213</td>\n",
       "      <td>0.814833</td>\n",
       "      <td>0.465486</td>\n",
       "      <td>0.176384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.594583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735107</td>\n",
       "      <td>0.569615</td>\n",
       "      <td>0.476996</td>\n",
       "      <td>0.427758</td>\n",
       "      <td>0.592668</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.733199</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732477</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749067</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756837</td>\n",
       "      <td>0.865944</td>\n",
       "      <td>0.845037</td>\n",
       "      <td>0.846026</td>\n",
       "      <td>0.874676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.873559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741906</td>\n",
       "      <td>0.879946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749934</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.205209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394962</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.842078</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.505453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.884793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999437</td>\n",
       "      <td>0.259471</td>\n",
       "      <td>0.260929</td>\n",
       "      <td>0.261827</td>\n",
       "      <td>0.257567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496433</td>\n",
       "      <td>0.299383</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.317210</td>\n",
       "      <td>0.310717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554169</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584409</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.585725</td>\n",
       "      <td>0.005536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAT  obs_time   내부온도관측치   내부습도관측치    co2관측치     ec관측치  시간당분무량  일간누적분무량  \\\n",
       "0  0.0  0.000000  0.591667  0.818350  0.446681  0.175930   0.000  0.00000   \n",
       "1  0.0  0.043478  0.602232  0.812643  0.440580  0.176125   0.042  0.00175   \n",
       "2  0.0  0.086957  0.590926  0.814717  0.444028  0.175864   0.000  0.00175   \n",
       "3  0.0  0.130435  0.593194  0.813983  0.454639  0.175836   0.042  0.00350   \n",
       "4  0.0  0.173913  0.594213  0.814833  0.465486  0.176384   0.000  0.00350   \n",
       "\n",
       "   시간당백색광량  일간누적백색광량  시간당적색광량  일간누적적색광량  시간당청색광량  일간누적청색광량  시간당총광량  일간누적총광량  \\\n",
       "0      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "1      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "2      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "3      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "4      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "\n",
       "  case_num      time  시간당총광량_bf1  시간당총광량_higher_than_1d  시간당총광량_1d_rise_rate  \\\n",
       "0       01  0.001488    0.813914                    0.0             0.999784   \n",
       "1       01  0.002976    0.813914                    0.0             0.999784   \n",
       "2       01  0.004464    0.813914                    0.0             0.999784   \n",
       "3       01  0.005952    0.813914                    0.0             0.999784   \n",
       "4       01  0.007440    0.813914                    0.0             0.999784   \n",
       "\n",
       "   시간당총광량_ma2  시간당총광량_ma4  시간당총광량_ma7  시간당총광량_bf2  시간당총광량_higher_than_2d  \\\n",
       "0    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "1    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "2    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "3    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "4    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "\n",
       "   시간당총광량_2d_rise_rate  시간당총광량_bf3  시간당총광량_higher_than_3d  \\\n",
       "0             0.999887    0.945933                    0.0   \n",
       "1             0.999887    0.945933                    0.0   \n",
       "2             0.999887    0.945933                    0.0   \n",
       "3             0.999887    0.945933                    0.0   \n",
       "4             0.999887    0.945933                    0.0   \n",
       "\n",
       "   시간당총광량_3d_rise_rate  시간당총광량_bf4  시간당총광량_higher_than_4d  \\\n",
       "0             0.999887    0.972215                    0.0   \n",
       "1             0.999887    0.972215                    0.0   \n",
       "2             0.999887    0.972215                    0.0   \n",
       "3             0.999887    0.972215                    0.0   \n",
       "4             0.999887    0.972215                    0.0   \n",
       "\n",
       "   시간당총광량_4d_rise_rate  시간당총광량_cumsum  내부온도관측치_bf1  내부온도관측치_higher_than_1d  \\\n",
       "0             0.999887       0.990831     0.000000                     0.0   \n",
       "1             0.999887       0.990831     0.593293                     1.0   \n",
       "2             0.999887       0.990831     0.602213                     0.0   \n",
       "3             0.999887       0.990831     0.592668                     1.0   \n",
       "4             0.999887       0.990831     0.594583                     1.0   \n",
       "\n",
       "   내부온도관측치_1d_rise_rate  내부온도관측치_ma2  내부온도관측치_ma4  내부온도관측치_ma7  내부온도관측치_bf2  \\\n",
       "0              0.734724     0.567709     0.472835     0.424022     0.000000   \n",
       "1              0.738653     0.572653     0.479162     0.431122     0.000000   \n",
       "2              0.730452     0.572306     0.476757     0.428423     0.593293   \n",
       "3              0.735578     0.568077     0.476234     0.427836     0.602213   \n",
       "4              0.735107     0.569615     0.476996     0.427758     0.592668   \n",
       "\n",
       "   내부온도관측치_higher_than_2d  내부온도관측치_2d_rise_rate  내부온도관측치_bf3  \\\n",
       "0                     0.0              0.731950     0.000000   \n",
       "1                     0.0              0.731950     0.000000   \n",
       "2                     0.0              0.731667     0.000000   \n",
       "3                     0.0              0.728510     0.593293   \n",
       "4                     1.0              0.733199     0.602213   \n",
       "\n",
       "   내부온도관측치_higher_than_3d  내부온도관측치_3d_rise_rate  내부온도관측치_bf4  \\\n",
       "0                     0.0              0.735484     0.000000   \n",
       "1                     0.0              0.735484     0.000000   \n",
       "2                     0.0              0.735484     0.000000   \n",
       "3                     1.0              0.736058     0.000000   \n",
       "4                     0.0              0.732477     0.593293   \n",
       "\n",
       "   내부온도관측치_higher_than_4d  내부온도관측치_4d_rise_rate  내부온도관측치_cumsum  일간누적총광량_bf1  \\\n",
       "0                     0.0              0.748157        0.000565          0.0   \n",
       "1                     0.0              0.748157        0.001844          0.0   \n",
       "2                     0.0              0.748157        0.003102          0.0   \n",
       "3                     0.0              0.748157        0.004365          0.0   \n",
       "4                     1.0              0.749067        0.005629          0.0   \n",
       "\n",
       "   일간누적총광량_higher_than_1d  일간누적총광량_1d_rise_rate  일간누적총광량_ma2  일간누적총광량_ma4  \\\n",
       "0                     0.0              0.999989          0.0          0.0   \n",
       "1                     0.0              0.999989          0.0          0.0   \n",
       "2                     0.0              0.999989          0.0          0.0   \n",
       "3                     0.0              0.999989          0.0          0.0   \n",
       "4                     0.0              0.999989          0.0          0.0   \n",
       "\n",
       "   일간누적총광량_ma7  일간누적총광량_bf2  일간누적총광량_higher_than_2d  일간누적총광량_2d_rise_rate  \\\n",
       "0          0.0          0.0                     0.0              0.999989   \n",
       "1          0.0          0.0                     0.0              0.999989   \n",
       "2          0.0          0.0                     0.0              0.999989   \n",
       "3          0.0          0.0                     0.0              0.999989   \n",
       "4          0.0          0.0                     0.0              0.999989   \n",
       "\n",
       "   일간누적총광량_bf3  일간누적총광량_higher_than_3d  일간누적총광량_3d_rise_rate  일간누적총광량_bf4  \\\n",
       "0          0.0                     0.0              0.999989          0.0   \n",
       "1          0.0                     0.0              0.999989          0.0   \n",
       "2          0.0                     0.0              0.999989          0.0   \n",
       "3          0.0                     0.0              0.999989          0.0   \n",
       "4          0.0                     0.0              0.999989          0.0   \n",
       "\n",
       "   일간누적총광량_higher_than_4d  일간누적총광량_4d_rise_rate  일간누적총광량_cumsum  일간누적분무량_bf1  \\\n",
       "0                     0.0              0.999989             0.0      0.00000   \n",
       "1                     0.0              0.999989             0.0      0.00000   \n",
       "2                     0.0              0.999989             0.0      0.00294   \n",
       "3                     0.0              0.999989             0.0      0.00294   \n",
       "4                     0.0              0.999989             0.0      0.00588   \n",
       "\n",
       "   일간누적분무량_higher_than_1d  일간누적분무량_1d_rise_rate  일간누적분무량_ma2  일간누적분무량_ma4  \\\n",
       "0                     0.0              0.995783     0.000000     0.000000   \n",
       "1                     1.0              1.000000     0.001503     0.001573   \n",
       "2                     0.0              0.995783     0.003006     0.002097   \n",
       "3                     1.0              0.997892     0.004509     0.003145   \n",
       "4                     0.0              0.995783     0.006012     0.004718   \n",
       "\n",
       "   일간누적분무량_ma7  일간누적분무량_bf2  일간누적분무량_higher_than_2d  일간누적분무량_2d_rise_rate  \\\n",
       "0     0.000000      0.00000                     0.0              0.995783   \n",
       "1     0.001685      0.00000                     0.0              0.995783   \n",
       "2     0.002247      0.00000                     1.0              1.000000   \n",
       "3     0.003371      0.00294                     1.0              0.997892   \n",
       "4     0.004045      0.00294                     1.0              0.997892   \n",
       "\n",
       "   일간누적분무량_bf3  일간누적분무량_higher_than_3d  일간누적분무량_3d_rise_rate  일간누적분무량_bf4  \\\n",
       "0      0.00000                     0.0              0.995783          0.0   \n",
       "1      0.00000                     0.0              0.995783          0.0   \n",
       "2      0.00000                     0.0              0.995783          0.0   \n",
       "3      0.00000                     1.0              1.000000          0.0   \n",
       "4      0.00294                     1.0              0.997892          0.0   \n",
       "\n",
       "   일간누적분무량_higher_than_4d  일간누적분무량_4d_rise_rate  일간누적분무량_cumsum  시간당적색광량_bf1  \\\n",
       "0                     0.0              0.995783        0.000000     0.469684   \n",
       "1                     0.0              0.995783        0.000015     0.469684   \n",
       "2                     0.0              0.995783        0.000029     0.469684   \n",
       "3                     0.0              0.995783        0.000058     0.469684   \n",
       "4                     1.0              1.000000        0.000087     0.469684   \n",
       "\n",
       "   시간당적색광량_higher_than_1d  시간당적색광량_1d_rise_rate  시간당적색광량_ma2  시간당적색광량_ma4  \\\n",
       "0                     0.0              0.998938     0.932244     0.931334   \n",
       "1                     0.0              0.998938     0.932244     0.931334   \n",
       "2                     0.0              0.998938     0.932244     0.931334   \n",
       "3                     0.0              0.998938     0.932244     0.931334   \n",
       "4                     0.0              0.998938     0.932244     0.931334   \n",
       "\n",
       "   시간당적색광량_ma7  시간당적색광량_bf2  시간당적색광량_higher_than_2d  시간당적색광량_2d_rise_rate  \\\n",
       "0     0.931104     0.639163                     0.0              0.999552   \n",
       "1     0.931104     0.639163                     0.0              0.999552   \n",
       "2     0.931104     0.639163                     0.0              0.999552   \n",
       "3     0.931104     0.639163                     0.0              0.999552   \n",
       "4     0.931104     0.639163                     0.0              0.999552   \n",
       "\n",
       "   시간당적색광량_bf3  시간당적색광량_higher_than_3d  시간당적색광량_3d_rise_rate  시간당적색광량_bf4  \\\n",
       "0     0.779865                     0.0              0.999661     0.876319   \n",
       "1     0.779865                     0.0              0.999661     0.876319   \n",
       "2     0.779865                     0.0              0.999661     0.876319   \n",
       "3     0.779865                     0.0              0.999661     0.876319   \n",
       "4     0.779865                     0.0              0.999661     0.876319   \n",
       "\n",
       "   시간당적색광량_higher_than_4d  시간당적색광량_4d_rise_rate  시간당적색광량_cumsum  내부습도관측치_bf1  \\\n",
       "0                     0.0              0.999661        0.956626     0.000000   \n",
       "1                     0.0              0.999661        0.956626     0.877540   \n",
       "2                     0.0              0.999661        0.956626     0.871420   \n",
       "3                     0.0              0.999661        0.956626     0.873644   \n",
       "4                     0.0              0.999661        0.956626     0.872858   \n",
       "\n",
       "   내부습도관측치_higher_than_1d  내부습도관측치_1d_rise_rate  내부습도관측치_ma2  내부습도관측치_ma4  \\\n",
       "0                     0.0              0.756583     0.870398     0.850697   \n",
       "1                     0.0              0.754874     0.867173     0.846946   \n",
       "2                     1.0              0.757203     0.865120     0.846604   \n",
       "3                     0.0              0.756364     0.865878     0.846192   \n",
       "4                     1.0              0.756837     0.865944     0.845037   \n",
       "\n",
       "   내부습도관측치_ma7  내부습도관측치_bf2  내부습도관측치_higher_than_2d  내부습도관측치_2d_rise_rate  \\\n",
       "0     0.850592     0.000000                     0.0              0.774357   \n",
       "1     0.846810     0.000000                     0.0              0.774357   \n",
       "2     0.846465     0.878576                     0.0              0.773351   \n",
       "3     0.846050     0.872449                     1.0              0.774728   \n",
       "4     0.846026     0.874676                     1.0              0.774389   \n",
       "\n",
       "   내부습도관측치_bf3  내부습도관측치_higher_than_3d  내부습도관측치_3d_rise_rate  내부습도관측치_bf4  \\\n",
       "0     0.000000                     0.0              0.741211     0.000000   \n",
       "1     0.000000                     0.0              0.741211     0.000000   \n",
       "2     0.000000                     0.0              0.741211     0.000000   \n",
       "3     0.879694                     0.0              0.739822     0.000000   \n",
       "4     0.873559                     1.0              0.741906     0.879946   \n",
       "\n",
       "   내부습도관측치_higher_than_4d  내부습도관측치_4d_rise_rate  내부습도관측치_cumsum  시간당분무량_bf1  \\\n",
       "0                     0.0              0.751008        0.001149    0.172029   \n",
       "1                     0.0              0.751008        0.002658    0.172029   \n",
       "2                     0.0              0.751008        0.004170    0.205209   \n",
       "3                     0.0              0.751008        0.005682    0.172029   \n",
       "4                     0.0              0.749934        0.007195    0.205209   \n",
       "\n",
       "   시간당분무량_higher_than_1d  시간당분무량_1d_rise_rate  시간당분무량_ma2  시간당분무량_ma4  \\\n",
       "0                    0.0             0.400607    0.815584    0.833827   \n",
       "1                    1.0             0.405704    0.820828    0.839187   \n",
       "2                    0.0             0.394962    0.820828    0.837400   \n",
       "3                    1.0             0.405704    0.820828    0.839187   \n",
       "4                    0.0             0.394962    0.820828    0.839187   \n",
       "\n",
       "   시간당분무량_ma7  시간당분무량_bf2  시간당분무량_higher_than_2d  시간당분무량_2d_rise_rate  \\\n",
       "0    0.837769    0.317698                    0.0             0.499335   \n",
       "1    0.843155    0.317698                    0.0             0.499335   \n",
       "2    0.841360    0.317698                    0.0             0.499335   \n",
       "3    0.843155    0.348337                    0.0             0.499335   \n",
       "4    0.842078    0.317698                    0.0             0.499335   \n",
       "\n",
       "   시간당분무량_bf3  시간당분무량_higher_than_3d  시간당분무량_3d_rise_rate  시간당분무량_bf4  \\\n",
       "0    0.482201                    0.0             0.932916    0.650656   \n",
       "1    0.482201                    0.0             0.932916    0.650656   \n",
       "2    0.482201                    0.0             0.932916    0.650656   \n",
       "3    0.482201                    1.0             1.000000    0.650656   \n",
       "4    0.505453                    0.0             0.858616    0.650656   \n",
       "\n",
       "   시간당분무량_higher_than_4d  시간당분무량_4d_rise_rate  시간당분무량_cumsum  일간누적백색광량_bf1  \\\n",
       "0                    0.0             0.919927       0.884755           0.0   \n",
       "1                    0.0             0.919927       0.884774           0.0   \n",
       "2                    0.0             0.919927       0.884774           0.0   \n",
       "3                    0.0             0.919927       0.884793           0.0   \n",
       "4                    0.0             0.919927       0.884793           0.0   \n",
       "\n",
       "   일간누적백색광량_higher_than_1d  일간누적백색광량_1d_rise_rate  일간누적백색광량_ma2  일간누적백색광량_ma4  \\\n",
       "0                      0.0               0.999989           0.0           0.0   \n",
       "1                      0.0               0.999989           0.0           0.0   \n",
       "2                      0.0               0.999989           0.0           0.0   \n",
       "3                      0.0               0.999989           0.0           0.0   \n",
       "4                      0.0               0.999989           0.0           0.0   \n",
       "\n",
       "   일간누적백색광량_ma7  일간누적백색광량_bf2  일간누적백색광량_higher_than_2d  일간누적백색광량_2d_rise_rate  \\\n",
       "0           0.0           0.0                      0.0               0.999989   \n",
       "1           0.0           0.0                      0.0               0.999989   \n",
       "2           0.0           0.0                      0.0               0.999989   \n",
       "3           0.0           0.0                      0.0               0.999989   \n",
       "4           0.0           0.0                      0.0               0.999989   \n",
       "\n",
       "   일간누적백색광량_bf3  일간누적백색광량_higher_than_3d  일간누적백색광량_3d_rise_rate  일간누적백색광량_bf4  \\\n",
       "0           0.0                      0.0               0.999989           0.0   \n",
       "1           0.0                      0.0               0.999989           0.0   \n",
       "2           0.0                      0.0               0.999989           0.0   \n",
       "3           0.0                      0.0               0.999989           0.0   \n",
       "4           0.0                      0.0               0.999989           0.0   \n",
       "\n",
       "   일간누적백색광량_higher_than_4d  일간누적백색광량_4d_rise_rate  일간누적백색광량_cumsum  ec관측치_bf1  \\\n",
       "0                      0.0               0.999989              0.0   0.000000   \n",
       "1                      0.0               0.999989              0.0   0.257663   \n",
       "2                      0.0               0.999989              0.0   0.257949   \n",
       "3                      0.0               0.999989              0.0   0.257567   \n",
       "4                      0.0               0.999989              0.0   0.257526   \n",
       "\n",
       "   ec관측치_higher_than_1d  ec관측치_1d_rise_rate  ec관측치_ma2  ec관측치_ma4  ec관측치_ma7  \\\n",
       "0                   0.0            0.999435   0.259206   0.260748   0.261681   \n",
       "1                   1.0            0.999436   0.259350   0.260893   0.261826   \n",
       "2                   0.0            0.999434   0.259301   0.260812   0.261745   \n",
       "3                   0.0            0.999435   0.259088   0.260761   0.261694   \n",
       "4                   1.0            0.999437   0.259471   0.260929   0.261827   \n",
       "\n",
       "   ec관측치_bf2  ec관측치_higher_than_2d  ec관측치_2d_rise_rate  ec관측치_bf3  \\\n",
       "0   0.000000                   0.0            0.999683   0.000000   \n",
       "1   0.000000                   0.0            0.999683   0.000000   \n",
       "2   0.257663                   0.0            0.999683   0.000000   \n",
       "3   0.257949                   0.0            0.999683   0.257663   \n",
       "4   0.257567                   1.0            0.999684   0.257949   \n",
       "\n",
       "   ec관측치_higher_than_3d  ec관측치_3d_rise_rate  ec관측치_bf4  ec관측치_higher_than_4d  \\\n",
       "0                   0.0            0.999686   0.000000                   0.0   \n",
       "1                   0.0            0.999686   0.000000                   0.0   \n",
       "2                   0.0            0.999686   0.000000                   0.0   \n",
       "3                   0.0            0.999686   0.000000                   0.0   \n",
       "4                   1.0            0.999686   0.257663                   1.0   \n",
       "\n",
       "   ec관측치_4d_rise_rate  ec관측치_cumsum  시간당백색광량_bf1  시간당백색광량_higher_than_1d  \\\n",
       "0            0.999851      0.000708     0.881706                     0.0   \n",
       "1            0.999851      0.001417     0.881706                     0.0   \n",
       "2            0.999851      0.002125     0.881706                     0.0   \n",
       "3            0.999851      0.002832     0.881706                     0.0   \n",
       "4            0.999851      0.003542     0.881706                     0.0   \n",
       "\n",
       "   시간당백색광량_1d_rise_rate  시간당백색광량_ma2  시간당백색광량_ma4  시간당백색광량_ma7  시간당백색광량_bf2  \\\n",
       "0              0.999635     0.991383     0.991271     0.991275     0.937135   \n",
       "1              0.999635     0.991383     0.991271     0.991275     0.937135   \n",
       "2              0.999635     0.991383     0.991271     0.991275     0.937135   \n",
       "3              0.999635     0.991383     0.991271     0.991275     0.937135   \n",
       "4              0.999635     0.991383     0.991271     0.991275     0.937135   \n",
       "\n",
       "   시간당백색광량_higher_than_2d  시간당백색광량_2d_rise_rate  시간당백색광량_bf3  \\\n",
       "0                     0.0              0.999819     0.967547   \n",
       "1                     0.0              0.999819     0.967547   \n",
       "2                     0.0              0.999819     0.967547   \n",
       "3                     0.0              0.999819     0.967547   \n",
       "4                     0.0              0.999819     0.967547   \n",
       "\n",
       "   시간당백색광량_higher_than_3d  시간당백색광량_3d_rise_rate  시간당백색광량_bf4  \\\n",
       "0                     0.0              0.999831     0.983506   \n",
       "1                     0.0              0.999831     0.983506   \n",
       "2                     0.0              0.999831     0.983506   \n",
       "3                     0.0              0.999831     0.983506   \n",
       "4                     0.0              0.999831     0.983506   \n",
       "\n",
       "   시간당백색광량_higher_than_4d  시간당백색광량_4d_rise_rate  시간당백색광량_cumsum  시간당청색광량_bf1  \\\n",
       "0                     0.0              0.999831         0.99446      0.66396   \n",
       "1                     0.0              0.999831         0.99446      0.66396   \n",
       "2                     0.0              0.999831         0.99446      0.66396   \n",
       "3                     0.0              0.999831         0.99446      0.66396   \n",
       "4                     0.0              0.999831         0.99446      0.66396   \n",
       "\n",
       "   시간당청색광량_higher_than_1d  시간당청색광량_1d_rise_rate  시간당청색광량_ma2  시간당청색광량_ma4  \\\n",
       "0                     0.0              0.998681     0.968464      0.96809   \n",
       "1                     0.0              0.998681     0.968464      0.96809   \n",
       "2                     0.0              0.998681     0.968464      0.96809   \n",
       "3                     0.0              0.998681     0.968464      0.96809   \n",
       "4                     0.0              0.998681     0.968464      0.96809   \n",
       "\n",
       "   시간당청색광량_ma7  시간당청색광량_bf2  시간당청색광량_higher_than_2d  시간당청색광량_2d_rise_rate  \\\n",
       "0     0.968003     0.798048                     0.0              0.999552   \n",
       "1     0.968003     0.798048                     0.0              0.999552   \n",
       "2     0.968003     0.798048                     0.0              0.999552   \n",
       "3     0.968003     0.798048                     0.0              0.999552   \n",
       "4     0.968003     0.798048                     0.0              0.999552   \n",
       "\n",
       "   시간당청색광량_bf3  시간당청색광량_higher_than_3d  시간당청색광량_3d_rise_rate  시간당청색광량_bf4  \\\n",
       "0     0.887683                     0.0              0.999661       0.9405   \n",
       "1     0.887683                     0.0              0.999661       0.9405   \n",
       "2     0.887683                     0.0              0.999661       0.9405   \n",
       "3     0.887683                     0.0              0.999661       0.9405   \n",
       "4     0.887683                     0.0              0.999661       0.9405   \n",
       "\n",
       "   시간당청색광량_higher_than_4d  시간당청색광량_4d_rise_rate  시간당청색광량_cumsum  일간누적청색광량_bf1  \\\n",
       "0                     0.0              0.999661        0.980104           0.0   \n",
       "1                     0.0              0.999661        0.980104           0.0   \n",
       "2                     0.0              0.999661        0.980104           0.0   \n",
       "3                     0.0              0.999661        0.980104           0.0   \n",
       "4                     0.0              0.999661        0.980104           0.0   \n",
       "\n",
       "   일간누적청색광량_higher_than_1d  일간누적청색광량_1d_rise_rate  일간누적청색광량_ma2  일간누적청색광량_ma4  \\\n",
       "0                      0.0               0.999166           0.0           0.0   \n",
       "1                      0.0               0.999166           0.0           0.0   \n",
       "2                      0.0               0.999166           0.0           0.0   \n",
       "3                      0.0               0.999166           0.0           0.0   \n",
       "4                      0.0               0.999166           0.0           0.0   \n",
       "\n",
       "   일간누적청색광량_ma7  일간누적청색광량_bf2  일간누적청색광량_higher_than_2d  일간누적청색광량_2d_rise_rate  \\\n",
       "0           0.0           0.0                      0.0               0.999166   \n",
       "1           0.0           0.0                      0.0               0.999166   \n",
       "2           0.0           0.0                      0.0               0.999166   \n",
       "3           0.0           0.0                      0.0               0.999166   \n",
       "4           0.0           0.0                      0.0               0.999166   \n",
       "\n",
       "   일간누적청색광량_bf3  일간누적청색광량_higher_than_3d  일간누적청색광량_3d_rise_rate  일간누적청색광량_bf4  \\\n",
       "0           0.0                      0.0               0.999166           0.0   \n",
       "1           0.0                      0.0               0.999166           0.0   \n",
       "2           0.0                      0.0               0.999166           0.0   \n",
       "3           0.0                      0.0               0.999166           0.0   \n",
       "4           0.0                      0.0               0.999166           0.0   \n",
       "\n",
       "   일간누적청색광량_higher_than_4d  일간누적청색광량_4d_rise_rate  일간누적청색광량_cumsum  \\\n",
       "0                      0.0               0.999151              0.0   \n",
       "1                      0.0               0.999151              0.0   \n",
       "2                      0.0               0.999151              0.0   \n",
       "3                      0.0               0.999151              0.0   \n",
       "4                      0.0               0.999151              0.0   \n",
       "\n",
       "   일간누적적색광량_bf1  일간누적적색광량_higher_than_1d  일간누적적색광량_1d_rise_rate  일간누적적색광량_ma2  \\\n",
       "0           0.0                      0.0               0.999293           0.0   \n",
       "1           0.0                      0.0               0.999293           0.0   \n",
       "2           0.0                      0.0               0.999293           0.0   \n",
       "3           0.0                      0.0               0.999293           0.0   \n",
       "4           0.0                      0.0               0.999293           0.0   \n",
       "\n",
       "   일간누적적색광량_ma4  일간누적적색광량_ma7  일간누적적색광량_bf2  일간누적적색광량_higher_than_2d  \\\n",
       "0           0.0           0.0           0.0                      0.0   \n",
       "1           0.0           0.0           0.0                      0.0   \n",
       "2           0.0           0.0           0.0                      0.0   \n",
       "3           0.0           0.0           0.0                      0.0   \n",
       "4           0.0           0.0           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_2d_rise_rate  일간누적적색광량_bf3  일간누적적색광량_higher_than_3d  \\\n",
       "0               0.999293           0.0                      0.0   \n",
       "1               0.999293           0.0                      0.0   \n",
       "2               0.999293           0.0                      0.0   \n",
       "3               0.999293           0.0                      0.0   \n",
       "4               0.999293           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_3d_rise_rate  일간누적적색광량_bf4  일간누적적색광량_higher_than_4d  \\\n",
       "0               0.999293           0.0                      0.0   \n",
       "1               0.999293           0.0                      0.0   \n",
       "2               0.999293           0.0                      0.0   \n",
       "3               0.999293           0.0                      0.0   \n",
       "4               0.999293           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_4d_rise_rate  일간누적적색광량_cumsum  co2관측치_bf1  co2관측치_higher_than_1d  \\\n",
       "0               0.999293              0.0    0.000000                    0.0   \n",
       "1               0.999293              0.0    0.312574                    0.0   \n",
       "2               0.999293              0.0    0.308305                    1.0   \n",
       "3               0.999293              0.0    0.310717                    1.0   \n",
       "4               0.999293              0.0    0.318143                    1.0   \n",
       "\n",
       "   co2관측치_1d_rise_rate  co2관측치_ma2  co2관측치_ma4  co2관측치_ma7  co2관측치_bf2  \\\n",
       "0             0.472981    0.289605    0.296091    0.314353    0.000000   \n",
       "1             0.459046    0.287377    0.293813    0.311934    0.000000   \n",
       "2             0.480795    0.286408    0.293912    0.312039    0.312574   \n",
       "3             0.496470    0.291544    0.295943    0.314196    0.308305   \n",
       "4             0.496433    0.299383    0.299455    0.317210    0.310717   \n",
       "\n",
       "   co2관측치_higher_than_2d  co2관측치_2d_rise_rate  co2관측치_bf3  \\\n",
       "0                    0.0             0.522119    0.000000   \n",
       "1                    0.0             0.522119    0.000000   \n",
       "2                    0.0             0.517966    0.000000   \n",
       "3                    1.0             0.543618    0.312574   \n",
       "4                    1.0             0.554169    0.308305   \n",
       "\n",
       "   co2관측치_higher_than_3d  co2관측치_3d_rise_rate  co2관측치_bf4  \\\n",
       "0                    0.0             0.553259    0.000000   \n",
       "1                    0.0             0.553259    0.000000   \n",
       "2                    0.0             0.553259    0.000000   \n",
       "3                    1.0             0.563450    0.000000   \n",
       "4                    1.0             0.584409    0.312574   \n",
       "\n",
       "   co2관측치_higher_than_4d  co2관측치_4d_rise_rate  co2관측치_cumsum  \n",
       "0                    0.0             0.564111       0.000997  \n",
       "1                    0.0             0.564111       0.002105  \n",
       "2                    0.0             0.564111       0.003222  \n",
       "3                    0.0             0.564111       0.004365  \n",
       "4                    1.0             0.585725       0.005536  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f93b40f-3e0c-4f7e-b84c-8c0958395769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DAT', 'obs_time', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'case_num', 'time', '시간당총광량_bf1', '시간당총광량_higher_than_1d', '시간당총광량_1d_rise_rate', '시간당총광량_ma2', '시간당총광량_ma4', '시간당총광량_ma7', '시간당총광량_bf2', '시간당총광량_higher_than_2d', '시간당총광량_2d_rise_rate', '시간당총광량_bf3', '시간당총광량_higher_than_3d', '시간당총광량_3d_rise_rate', '시간당총광량_bf4', '시간당총광량_higher_than_4d', '시간당총광량_4d_rise_rate', '시간당총광량_cumsum', '내부온도관측치_bf1', '내부온도관측치_higher_than_1d', '내부온도관측치_1d_rise_rate', '내부온도관측치_ma2', '내부온도관측치_ma4', '내부온도관측치_ma7', '내부온도관측치_bf2', '내부온도관측치_higher_than_2d', '내부온도관측치_2d_rise_rate', '내부온도관측치_bf3', '내부온도관측치_higher_than_3d', '내부온도관측치_3d_rise_rate', '내부온도관측치_bf4', '내부온도관측치_higher_than_4d', '내부온도관측치_4d_rise_rate', '내부온도관측치_cumsum', '일간누적총광량_bf1', '일간누적총광량_higher_than_1d', '일간누적총광량_1d_rise_rate', '일간누적총광량_ma2', '일간누적총광량_ma4', '일간누적총광량_ma7', '일간누적총광량_bf2', '일간누적총광량_higher_than_2d', '일간누적총광량_2d_rise_rate', '일간누적총광량_bf3', '일간누적총광량_higher_than_3d', '일간누적총광량_3d_rise_rate', '일간누적총광량_bf4', '일간누적총광량_higher_than_4d', '일간누적총광량_4d_rise_rate', '일간누적총광량_cumsum', '일간누적분무량_bf1', '일간누적분무량_higher_than_1d', '일간누적분무량_1d_rise_rate', '일간누적분무량_ma2', '일간누적분무량_ma4', '일간누적분무량_ma7', '일간누적분무량_bf2', '일간누적분무량_higher_than_2d', '일간누적분무량_2d_rise_rate', '일간누적분무량_bf3', '일간누적분무량_higher_than_3d', '일간누적분무량_3d_rise_rate', '일간누적분무량_bf4', '일간누적분무량_higher_than_4d', '일간누적분무량_4d_rise_rate', '일간누적분무량_cumsum', '시간당적색광량_bf1', '시간당적색광량_higher_than_1d', '시간당적색광량_1d_rise_rate', '시간당적색광량_ma2', '시간당적색광량_ma4', '시간당적색광량_ma7', '시간당적색광량_bf2', '시간당적색광량_higher_than_2d', '시간당적색광량_2d_rise_rate', '시간당적색광량_bf3', '시간당적색광량_higher_than_3d', '시간당적색광량_3d_rise_rate', '시간당적색광량_bf4', '시간당적색광량_higher_than_4d', '시간당적색광량_4d_rise_rate', '시간당적색광량_cumsum', '내부습도관측치_bf1', '내부습도관측치_higher_than_1d', '내부습도관측치_1d_rise_rate', '내부습도관측치_ma2', '내부습도관측치_ma4', '내부습도관측치_ma7', '내부습도관측치_bf2', '내부습도관측치_higher_than_2d', '내부습도관측치_2d_rise_rate', '내부습도관측치_bf3', '내부습도관측치_higher_than_3d', '내부습도관측치_3d_rise_rate', '내부습도관측치_bf4', '내부습도관측치_higher_than_4d', '내부습도관측치_4d_rise_rate', '내부습도관측치_cumsum', '시간당분무량_bf1', '시간당분무량_higher_than_1d', '시간당분무량_1d_rise_rate', '시간당분무량_ma2', '시간당분무량_ma4', '시간당분무량_ma7', '시간당분무량_bf2', '시간당분무량_higher_than_2d', '시간당분무량_2d_rise_rate', '시간당분무량_bf3', '시간당분무량_higher_than_3d', '시간당분무량_3d_rise_rate', '시간당분무량_bf4', '시간당분무량_higher_than_4d', '시간당분무량_4d_rise_rate', '시간당분무량_cumsum', '일간누적백색광량_bf1', '일간누적백색광량_higher_than_1d', '일간누적백색광량_1d_rise_rate', '일간누적백색광량_ma2', '일간누적백색광량_ma4', '일간누적백색광량_ma7', '일간누적백색광량_bf2', '일간누적백색광량_higher_than_2d', '일간누적백색광량_2d_rise_rate', '일간누적백색광량_bf3', '일간누적백색광량_higher_than_3d', '일간누적백색광량_3d_rise_rate', '일간누적백색광량_bf4', '일간누적백색광량_higher_than_4d', '일간누적백색광량_4d_rise_rate', '일간누적백색광량_cumsum', 'ec관측치_bf1', 'ec관측치_higher_than_1d', 'ec관측치_1d_rise_rate', 'ec관측치_ma2', 'ec관측치_ma4', 'ec관측치_ma7', 'ec관측치_bf2', 'ec관측치_higher_than_2d', 'ec관측치_2d_rise_rate', 'ec관측치_bf3', 'ec관측치_higher_than_3d', 'ec관측치_3d_rise_rate', 'ec관측치_bf4', 'ec관측치_higher_than_4d', 'ec관측치_4d_rise_rate', 'ec관측치_cumsum', '시간당백색광량_bf1', '시간당백색광량_higher_than_1d', '시간당백색광량_1d_rise_rate', '시간당백색광량_ma2', '시간당백색광량_ma4', '시간당백색광량_ma7', '시간당백색광량_bf2', '시간당백색광량_higher_than_2d', '시간당백색광량_2d_rise_rate', '시간당백색광량_bf3', '시간당백색광량_higher_than_3d', '시간당백색광량_3d_rise_rate', '시간당백색광량_bf4', '시간당백색광량_higher_than_4d', '시간당백색광량_4d_rise_rate', '시간당백색광량_cumsum', '시간당청색광량_bf1', '시간당청색광량_higher_than_1d', '시간당청색광량_1d_rise_rate', '시간당청색광량_ma2', '시간당청색광량_ma4', '시간당청색광량_ma7', '시간당청색광량_bf2', '시간당청색광량_higher_than_2d', '시간당청색광량_2d_rise_rate', '시간당청색광량_bf3', '시간당청색광량_higher_than_3d', '시간당청색광량_3d_rise_rate', '시간당청색광량_bf4', '시간당청색광량_higher_than_4d', '시간당청색광량_4d_rise_rate', '시간당청색광량_cumsum', '일간누적청색광량_bf1', '일간누적청색광량_higher_than_1d', '일간누적청색광량_1d_rise_rate', '일간누적청색광량_ma2', '일간누적청색광량_ma4', '일간누적청색광량_ma7', '일간누적청색광량_bf2', '일간누적청색광량_higher_than_2d', '일간누적청색광량_2d_rise_rate', '일간누적청색광량_bf3', '일간누적청색광량_higher_than_3d', '일간누적청색광량_3d_rise_rate', '일간누적청색광량_bf4', '일간누적청색광량_higher_than_4d', '일간누적청색광량_4d_rise_rate', '일간누적청색광량_cumsum', '일간누적적색광량_bf1', '일간누적적색광량_higher_than_1d', '일간누적적색광량_1d_rise_rate', '일간누적적색광량_ma2', '일간누적적색광량_ma4', '일간누적적색광량_ma7', '일간누적적색광량_bf2', '일간누적적색광량_higher_than_2d', '일간누적적색광량_2d_rise_rate', '일간누적적색광량_bf3', '일간누적적색광량_higher_than_3d', '일간누적적색광량_3d_rise_rate', '일간누적적색광량_bf4', '일간누적적색광량_higher_than_4d', '일간누적적색광량_4d_rise_rate', '일간누적적색광량_cumsum', 'co2관측치_bf1', 'co2관측치_higher_than_1d', 'co2관측치_1d_rise_rate', 'co2관측치_ma2', 'co2관측치_ma4', 'co2관측치_ma7', 'co2관측치_bf2', 'co2관측치_higher_than_2d', 'co2관측치_2d_rise_rate', 'co2관측치_bf3', 'co2관측치_higher_than_3d', 'co2관측치_3d_rise_rate', 'co2관측치_bf4', 'co2관측치_higher_than_4d', 'co2관측치_4d_rise_rate', 'co2관측치_cumsum']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in input_df.columns if col.find('*')<0])\n",
    "# print(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62d3f562-5196-4fc2-a3bb-7c45455a14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_df = []\n",
    "for col in input_df.drop(columns=['case_num','DAT','obs_time']).columns:\n",
    "    _x = input_df[col]\n",
    "    _y = np.array([[y]*24 for y in label_df['predicted_weight_g']]).flatten()\n",
    "    r_value, p_value = pearsonr(_x,_y)\n",
    "    corr_df.append([col,r_value,p_value])\n",
    "    \n",
    "corr_df = pd.DataFrame(corr_df,columns=['feature','r_value','p_value'])\n",
    "# corr_df.sort_values('p_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dccd58fa-b0bf-4263-8494-7b8c94a34a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['시간당총광량_higher_than_3d' '내부온도관측치_higher_than_1d' '내부온도관측치_1d_rise_rate'\n",
      " '내부온도관측치_higher_than_2d' '내부온도관측치_2d_rise_rate' '내부온도관측치_higher_than_3d'\n",
      " '내부온도관측치_3d_rise_rate' '내부온도관측치_higher_than_4d' '내부온도관측치_4d_rise_rate'\n",
      " '일간누적총광량_1d_rise_rate' '일간누적총광량_2d_rise_rate' '일간누적총광량_3d_rise_rate'\n",
      " '일간누적총광량_4d_rise_rate' '일간누적분무량_1d_rise_rate' '일간누적분무량_2d_rise_rate'\n",
      " '일간누적분무량_3d_rise_rate' '일간누적분무량_higher_than_4d' '일간누적분무량_4d_rise_rate'\n",
      " '시간당적색광량_higher_than_1d' '시간당적색광량_higher_than_2d'\n",
      " '시간당적색광량_higher_than_3d' '시간당적색광량_higher_than_4d'\n",
      " '내부습도관측치_higher_than_1d' '내부습도관측치_1d_rise_rate' '내부습도관측치_higher_than_2d'\n",
      " '내부습도관측치_2d_rise_rate' '내부습도관측치_higher_than_3d' '내부습도관측치_3d_rise_rate'\n",
      " '내부습도관측치_higher_than_4d' '내부습도관측치_4d_rise_rate' '시간당분무량_1d_rise_rate'\n",
      " '시간당분무량_2d_rise_rate' '시간당분무량_4d_rise_rate' '일간누적백색광량_1d_rise_rate'\n",
      " '일간누적백색광량_2d_rise_rate' '일간누적백색광량_3d_rise_rate' '일간누적백색광량_4d_rise_rate'\n",
      " 'ec관측치_higher_than_1d' 'ec관측치_1d_rise_rate' 'ec관측치_higher_than_2d'\n",
      " 'ec관측치_2d_rise_rate' 'ec관측치_higher_than_3d' 'ec관측치_3d_rise_rate'\n",
      " 'ec관측치_higher_than_4d' '시간당청색광량_1d_rise_rate' 'co2관측치_higher_than_1d'\n",
      " 'co2관측치_1d_rise_rate' 'co2관측치_higher_than_2d' 'co2관측치_2d_rise_rate'\n",
      " 'co2관측치_higher_than_3d' 'co2관측치_3d_rise_rate' 'co2관측치_higher_than_4d'\n",
      " 'co2관측치_4d_rise_rate']\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "del_feature = corr_df.feature[corr_df.p_value>alpha].tolist()\n",
    "len(del_feature)\n",
    "print(np.array(del_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "290fe567-1d52-4079-bb25-9016f212a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df     .drop(columns=del_feature,inplace=True)\n",
    "test_input_df.drop(columns=del_feature,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a2304db-0c27-4e98-b2b0-941e29a8d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asis(242) -> tobe(189)\n"
     ]
    }
   ],
   "source": [
    "print(f'asis({input_df.shape[1]+len(del_feature)}) -> tobe({input_df.shape[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "81b11b1c-1b93-46a1-8450-b41ddd93e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAT</th>\n",
       "      <th>obs_time</th>\n",
       "      <th>내부온도관측치</th>\n",
       "      <th>내부습도관측치</th>\n",
       "      <th>co2관측치</th>\n",
       "      <th>ec관측치</th>\n",
       "      <th>시간당분무량</th>\n",
       "      <th>일간누적분무량</th>\n",
       "      <th>시간당백색광량</th>\n",
       "      <th>일간누적백색광량</th>\n",
       "      <th>시간당적색광량</th>\n",
       "      <th>일간누적적색광량</th>\n",
       "      <th>시간당청색광량</th>\n",
       "      <th>일간누적청색광량</th>\n",
       "      <th>시간당총광량</th>\n",
       "      <th>일간누적총광량</th>\n",
       "      <th>case_num</th>\n",
       "      <th>time</th>\n",
       "      <th>시간당총광량_bf1</th>\n",
       "      <th>시간당총광량_higher_than_1d</th>\n",
       "      <th>시간당총광량_1d_rise_rate</th>\n",
       "      <th>시간당총광량_ma2</th>\n",
       "      <th>시간당총광량_ma4</th>\n",
       "      <th>시간당총광량_ma7</th>\n",
       "      <th>시간당총광량_bf2</th>\n",
       "      <th>시간당총광량_higher_than_2d</th>\n",
       "      <th>시간당총광량_2d_rise_rate</th>\n",
       "      <th>시간당총광량_bf3</th>\n",
       "      <th>시간당총광량_3d_rise_rate</th>\n",
       "      <th>시간당총광량_bf4</th>\n",
       "      <th>시간당총광량_higher_than_4d</th>\n",
       "      <th>시간당총광량_4d_rise_rate</th>\n",
       "      <th>시간당총광량_cumsum</th>\n",
       "      <th>내부온도관측치_bf1</th>\n",
       "      <th>내부온도관측치_ma2</th>\n",
       "      <th>내부온도관측치_ma4</th>\n",
       "      <th>내부온도관측치_ma7</th>\n",
       "      <th>내부온도관측치_bf2</th>\n",
       "      <th>내부온도관측치_bf3</th>\n",
       "      <th>내부온도관측치_bf4</th>\n",
       "      <th>내부온도관측치_cumsum</th>\n",
       "      <th>일간누적총광량_bf1</th>\n",
       "      <th>일간누적총광량_higher_than_1d</th>\n",
       "      <th>일간누적총광량_ma2</th>\n",
       "      <th>일간누적총광량_ma4</th>\n",
       "      <th>일간누적총광량_ma7</th>\n",
       "      <th>일간누적총광량_bf2</th>\n",
       "      <th>일간누적총광량_higher_than_2d</th>\n",
       "      <th>일간누적총광량_bf3</th>\n",
       "      <th>일간누적총광량_higher_than_3d</th>\n",
       "      <th>일간누적총광량_bf4</th>\n",
       "      <th>일간누적총광량_higher_than_4d</th>\n",
       "      <th>일간누적총광량_cumsum</th>\n",
       "      <th>일간누적분무량_bf1</th>\n",
       "      <th>일간누적분무량_higher_than_1d</th>\n",
       "      <th>일간누적분무량_ma2</th>\n",
       "      <th>일간누적분무량_ma4</th>\n",
       "      <th>일간누적분무량_ma7</th>\n",
       "      <th>일간누적분무량_bf2</th>\n",
       "      <th>일간누적분무량_higher_than_2d</th>\n",
       "      <th>일간누적분무량_bf3</th>\n",
       "      <th>일간누적분무량_higher_than_3d</th>\n",
       "      <th>일간누적분무량_bf4</th>\n",
       "      <th>일간누적분무량_cumsum</th>\n",
       "      <th>시간당적색광량_bf1</th>\n",
       "      <th>시간당적색광량_1d_rise_rate</th>\n",
       "      <th>시간당적색광량_ma2</th>\n",
       "      <th>시간당적색광량_ma4</th>\n",
       "      <th>시간당적색광량_ma7</th>\n",
       "      <th>시간당적색광량_bf2</th>\n",
       "      <th>시간당적색광량_2d_rise_rate</th>\n",
       "      <th>시간당적색광량_bf3</th>\n",
       "      <th>시간당적색광량_3d_rise_rate</th>\n",
       "      <th>시간당적색광량_bf4</th>\n",
       "      <th>시간당적색광량_4d_rise_rate</th>\n",
       "      <th>시간당적색광량_cumsum</th>\n",
       "      <th>내부습도관측치_bf1</th>\n",
       "      <th>내부습도관측치_ma2</th>\n",
       "      <th>내부습도관측치_ma4</th>\n",
       "      <th>내부습도관측치_ma7</th>\n",
       "      <th>내부습도관측치_bf2</th>\n",
       "      <th>내부습도관측치_bf3</th>\n",
       "      <th>내부습도관측치_bf4</th>\n",
       "      <th>내부습도관측치_cumsum</th>\n",
       "      <th>시간당분무량_bf1</th>\n",
       "      <th>시간당분무량_higher_than_1d</th>\n",
       "      <th>시간당분무량_ma2</th>\n",
       "      <th>시간당분무량_ma4</th>\n",
       "      <th>시간당분무량_ma7</th>\n",
       "      <th>시간당분무량_bf2</th>\n",
       "      <th>시간당분무량_higher_than_2d</th>\n",
       "      <th>시간당분무량_bf3</th>\n",
       "      <th>시간당분무량_higher_than_3d</th>\n",
       "      <th>시간당분무량_3d_rise_rate</th>\n",
       "      <th>시간당분무량_bf4</th>\n",
       "      <th>시간당분무량_higher_than_4d</th>\n",
       "      <th>시간당분무량_cumsum</th>\n",
       "      <th>일간누적백색광량_bf1</th>\n",
       "      <th>일간누적백색광량_higher_than_1d</th>\n",
       "      <th>일간누적백색광량_ma2</th>\n",
       "      <th>일간누적백색광량_ma4</th>\n",
       "      <th>일간누적백색광량_ma7</th>\n",
       "      <th>일간누적백색광량_bf2</th>\n",
       "      <th>일간누적백색광량_higher_than_2d</th>\n",
       "      <th>일간누적백색광량_bf3</th>\n",
       "      <th>일간누적백색광량_higher_than_3d</th>\n",
       "      <th>일간누적백색광량_bf4</th>\n",
       "      <th>일간누적백색광량_higher_than_4d</th>\n",
       "      <th>일간누적백색광량_cumsum</th>\n",
       "      <th>ec관측치_bf1</th>\n",
       "      <th>ec관측치_ma2</th>\n",
       "      <th>ec관측치_ma4</th>\n",
       "      <th>ec관측치_ma7</th>\n",
       "      <th>ec관측치_bf2</th>\n",
       "      <th>ec관측치_bf3</th>\n",
       "      <th>ec관측치_bf4</th>\n",
       "      <th>ec관측치_4d_rise_rate</th>\n",
       "      <th>ec관측치_cumsum</th>\n",
       "      <th>시간당백색광량_bf1</th>\n",
       "      <th>시간당백색광량_higher_than_1d</th>\n",
       "      <th>시간당백색광량_1d_rise_rate</th>\n",
       "      <th>시간당백색광량_ma2</th>\n",
       "      <th>시간당백색광량_ma4</th>\n",
       "      <th>시간당백색광량_ma7</th>\n",
       "      <th>시간당백색광량_bf2</th>\n",
       "      <th>시간당백색광량_higher_than_2d</th>\n",
       "      <th>시간당백색광량_2d_rise_rate</th>\n",
       "      <th>시간당백색광량_bf3</th>\n",
       "      <th>시간당백색광량_higher_than_3d</th>\n",
       "      <th>시간당백색광량_3d_rise_rate</th>\n",
       "      <th>시간당백색광량_bf4</th>\n",
       "      <th>시간당백색광량_higher_than_4d</th>\n",
       "      <th>시간당백색광량_4d_rise_rate</th>\n",
       "      <th>시간당백색광량_cumsum</th>\n",
       "      <th>시간당청색광량_bf1</th>\n",
       "      <th>시간당청색광량_higher_than_1d</th>\n",
       "      <th>시간당청색광량_ma2</th>\n",
       "      <th>시간당청색광량_ma4</th>\n",
       "      <th>시간당청색광량_ma7</th>\n",
       "      <th>시간당청색광량_bf2</th>\n",
       "      <th>시간당청색광량_higher_than_2d</th>\n",
       "      <th>시간당청색광량_2d_rise_rate</th>\n",
       "      <th>시간당청색광량_bf3</th>\n",
       "      <th>시간당청색광량_higher_than_3d</th>\n",
       "      <th>시간당청색광량_3d_rise_rate</th>\n",
       "      <th>시간당청색광량_bf4</th>\n",
       "      <th>시간당청색광량_higher_than_4d</th>\n",
       "      <th>시간당청색광량_4d_rise_rate</th>\n",
       "      <th>시간당청색광량_cumsum</th>\n",
       "      <th>일간누적청색광량_bf1</th>\n",
       "      <th>일간누적청색광량_higher_than_1d</th>\n",
       "      <th>일간누적청색광량_1d_rise_rate</th>\n",
       "      <th>일간누적청색광량_ma2</th>\n",
       "      <th>일간누적청색광량_ma4</th>\n",
       "      <th>일간누적청색광량_ma7</th>\n",
       "      <th>일간누적청색광량_bf2</th>\n",
       "      <th>일간누적청색광량_higher_than_2d</th>\n",
       "      <th>일간누적청색광량_2d_rise_rate</th>\n",
       "      <th>일간누적청색광량_bf3</th>\n",
       "      <th>일간누적청색광량_higher_than_3d</th>\n",
       "      <th>일간누적청색광량_3d_rise_rate</th>\n",
       "      <th>일간누적청색광량_bf4</th>\n",
       "      <th>일간누적청색광량_higher_than_4d</th>\n",
       "      <th>일간누적청색광량_4d_rise_rate</th>\n",
       "      <th>일간누적청색광량_cumsum</th>\n",
       "      <th>일간누적적색광량_bf1</th>\n",
       "      <th>일간누적적색광량_higher_than_1d</th>\n",
       "      <th>일간누적적색광량_1d_rise_rate</th>\n",
       "      <th>일간누적적색광량_ma2</th>\n",
       "      <th>일간누적적색광량_ma4</th>\n",
       "      <th>일간누적적색광량_ma7</th>\n",
       "      <th>일간누적적색광량_bf2</th>\n",
       "      <th>일간누적적색광량_higher_than_2d</th>\n",
       "      <th>일간누적적색광량_2d_rise_rate</th>\n",
       "      <th>일간누적적색광량_bf3</th>\n",
       "      <th>일간누적적색광량_higher_than_3d</th>\n",
       "      <th>일간누적적색광량_3d_rise_rate</th>\n",
       "      <th>일간누적적색광량_bf4</th>\n",
       "      <th>일간누적적색광량_higher_than_4d</th>\n",
       "      <th>일간누적적색광량_4d_rise_rate</th>\n",
       "      <th>일간누적적색광량_cumsum</th>\n",
       "      <th>co2관측치_bf1</th>\n",
       "      <th>co2관측치_ma2</th>\n",
       "      <th>co2관측치_ma4</th>\n",
       "      <th>co2관측치_ma7</th>\n",
       "      <th>co2관측치_bf2</th>\n",
       "      <th>co2관측치_bf3</th>\n",
       "      <th>co2관측치_bf4</th>\n",
       "      <th>co2관측치_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.818350</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.175930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567709</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.424022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870398</td>\n",
       "      <td>0.850697</td>\n",
       "      <td>0.850592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.815584</td>\n",
       "      <td>0.833827</td>\n",
       "      <td>0.837769</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259206</td>\n",
       "      <td>0.260748</td>\n",
       "      <td>0.261681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289605</td>\n",
       "      <td>0.296091</td>\n",
       "      <td>0.314353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.602232</td>\n",
       "      <td>0.812643</td>\n",
       "      <td>0.440580</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>0.572653</td>\n",
       "      <td>0.479162</td>\n",
       "      <td>0.431122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.877540</td>\n",
       "      <td>0.867173</td>\n",
       "      <td>0.846946</td>\n",
       "      <td>0.846810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.843155</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.259350</td>\n",
       "      <td>0.260893</td>\n",
       "      <td>0.261826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.287377</td>\n",
       "      <td>0.293813</td>\n",
       "      <td>0.311934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.590926</td>\n",
       "      <td>0.814717</td>\n",
       "      <td>0.444028</td>\n",
       "      <td>0.175864</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.572306</td>\n",
       "      <td>0.476757</td>\n",
       "      <td>0.428423</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.871420</td>\n",
       "      <td>0.865120</td>\n",
       "      <td>0.846604</td>\n",
       "      <td>0.846465</td>\n",
       "      <td>0.878576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.205209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.841360</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932916</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>0.259301</td>\n",
       "      <td>0.260812</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>0.286408</td>\n",
       "      <td>0.293912</td>\n",
       "      <td>0.312039</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.593194</td>\n",
       "      <td>0.813983</td>\n",
       "      <td>0.454639</td>\n",
       "      <td>0.175836</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.592668</td>\n",
       "      <td>0.568077</td>\n",
       "      <td>0.476234</td>\n",
       "      <td>0.427836</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.873644</td>\n",
       "      <td>0.865878</td>\n",
       "      <td>0.846192</td>\n",
       "      <td>0.846050</td>\n",
       "      <td>0.872449</td>\n",
       "      <td>0.879694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.172029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.843155</td>\n",
       "      <td>0.348337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257567</td>\n",
       "      <td>0.259088</td>\n",
       "      <td>0.260761</td>\n",
       "      <td>0.261694</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310717</td>\n",
       "      <td>0.291544</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>0.314196</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.594213</td>\n",
       "      <td>0.814833</td>\n",
       "      <td>0.465486</td>\n",
       "      <td>0.176384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>0.813914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.985331</td>\n",
       "      <td>0.985289</td>\n",
       "      <td>0.897412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.972215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.594583</td>\n",
       "      <td>0.569615</td>\n",
       "      <td>0.476996</td>\n",
       "      <td>0.427758</td>\n",
       "      <td>0.592668</td>\n",
       "      <td>0.602213</td>\n",
       "      <td>0.593293</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.469684</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.779865</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.876319</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.865944</td>\n",
       "      <td>0.845037</td>\n",
       "      <td>0.846026</td>\n",
       "      <td>0.874676</td>\n",
       "      <td>0.873559</td>\n",
       "      <td>0.879946</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.205209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.842078</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>0.650656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257526</td>\n",
       "      <td>0.259471</td>\n",
       "      <td>0.260929</td>\n",
       "      <td>0.261827</td>\n",
       "      <td>0.257567</td>\n",
       "      <td>0.257949</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.881706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.967547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.99446</td>\n",
       "      <td>0.66396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.968464</td>\n",
       "      <td>0.96809</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.798048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.980104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318143</td>\n",
       "      <td>0.299383</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.317210</td>\n",
       "      <td>0.310717</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.005536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAT  obs_time   내부온도관측치   내부습도관측치    co2관측치     ec관측치  시간당분무량  일간누적분무량  \\\n",
       "0  0.0  0.000000  0.591667  0.818350  0.446681  0.175930   0.000  0.00000   \n",
       "1  0.0  0.043478  0.602232  0.812643  0.440580  0.176125   0.042  0.00175   \n",
       "2  0.0  0.086957  0.590926  0.814717  0.444028  0.175864   0.000  0.00175   \n",
       "3  0.0  0.130435  0.593194  0.813983  0.454639  0.175836   0.042  0.00350   \n",
       "4  0.0  0.173913  0.594213  0.814833  0.465486  0.176384   0.000  0.00350   \n",
       "\n",
       "   시간당백색광량  일간누적백색광량  시간당적색광량  일간누적적색광량  시간당청색광량  일간누적청색광량  시간당총광량  일간누적총광량  \\\n",
       "0      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "1      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "2      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "3      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "4      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
       "\n",
       "  case_num      time  시간당총광량_bf1  시간당총광량_higher_than_1d  시간당총광량_1d_rise_rate  \\\n",
       "0       01  0.001488    0.813914                    0.0             0.999784   \n",
       "1       01  0.002976    0.813914                    0.0             0.999784   \n",
       "2       01  0.004464    0.813914                    0.0             0.999784   \n",
       "3       01  0.005952    0.813914                    0.0             0.999784   \n",
       "4       01  0.007440    0.813914                    0.0             0.999784   \n",
       "\n",
       "   시간당총광량_ma2  시간당총광량_ma4  시간당총광량_ma7  시간당총광량_bf2  시간당총광량_higher_than_2d  \\\n",
       "0    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "1    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "2    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "3    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "4    0.985519    0.985331    0.985289    0.897412                    0.0   \n",
       "\n",
       "   시간당총광량_2d_rise_rate  시간당총광량_bf3  시간당총광량_3d_rise_rate  시간당총광량_bf4  \\\n",
       "0             0.999887    0.945933             0.999887    0.972215   \n",
       "1             0.999887    0.945933             0.999887    0.972215   \n",
       "2             0.999887    0.945933             0.999887    0.972215   \n",
       "3             0.999887    0.945933             0.999887    0.972215   \n",
       "4             0.999887    0.945933             0.999887    0.972215   \n",
       "\n",
       "   시간당총광량_higher_than_4d  시간당총광량_4d_rise_rate  시간당총광량_cumsum  내부온도관측치_bf1  \\\n",
       "0                    0.0             0.999887       0.990831     0.000000   \n",
       "1                    0.0             0.999887       0.990831     0.593293   \n",
       "2                    0.0             0.999887       0.990831     0.602213   \n",
       "3                    0.0             0.999887       0.990831     0.592668   \n",
       "4                    0.0             0.999887       0.990831     0.594583   \n",
       "\n",
       "   내부온도관측치_ma2  내부온도관측치_ma4  내부온도관측치_ma7  내부온도관측치_bf2  내부온도관측치_bf3  \\\n",
       "0     0.567709     0.472835     0.424022     0.000000     0.000000   \n",
       "1     0.572653     0.479162     0.431122     0.000000     0.000000   \n",
       "2     0.572306     0.476757     0.428423     0.593293     0.000000   \n",
       "3     0.568077     0.476234     0.427836     0.602213     0.593293   \n",
       "4     0.569615     0.476996     0.427758     0.592668     0.602213   \n",
       "\n",
       "   내부온도관측치_bf4  내부온도관측치_cumsum  일간누적총광량_bf1  일간누적총광량_higher_than_1d  \\\n",
       "0     0.000000        0.000565          0.0                     0.0   \n",
       "1     0.000000        0.001844          0.0                     0.0   \n",
       "2     0.000000        0.003102          0.0                     0.0   \n",
       "3     0.000000        0.004365          0.0                     0.0   \n",
       "4     0.593293        0.005629          0.0                     0.0   \n",
       "\n",
       "   일간누적총광량_ma2  일간누적총광량_ma4  일간누적총광량_ma7  일간누적총광량_bf2  일간누적총광량_higher_than_2d  \\\n",
       "0          0.0          0.0          0.0          0.0                     0.0   \n",
       "1          0.0          0.0          0.0          0.0                     0.0   \n",
       "2          0.0          0.0          0.0          0.0                     0.0   \n",
       "3          0.0          0.0          0.0          0.0                     0.0   \n",
       "4          0.0          0.0          0.0          0.0                     0.0   \n",
       "\n",
       "   일간누적총광량_bf3  일간누적총광량_higher_than_3d  일간누적총광량_bf4  일간누적총광량_higher_than_4d  \\\n",
       "0          0.0                     0.0          0.0                     0.0   \n",
       "1          0.0                     0.0          0.0                     0.0   \n",
       "2          0.0                     0.0          0.0                     0.0   \n",
       "3          0.0                     0.0          0.0                     0.0   \n",
       "4          0.0                     0.0          0.0                     0.0   \n",
       "\n",
       "   일간누적총광량_cumsum  일간누적분무량_bf1  일간누적분무량_higher_than_1d  일간누적분무량_ma2  \\\n",
       "0             0.0      0.00000                     0.0     0.000000   \n",
       "1             0.0      0.00000                     1.0     0.001503   \n",
       "2             0.0      0.00294                     0.0     0.003006   \n",
       "3             0.0      0.00294                     1.0     0.004509   \n",
       "4             0.0      0.00588                     0.0     0.006012   \n",
       "\n",
       "   일간누적분무량_ma4  일간누적분무량_ma7  일간누적분무량_bf2  일간누적분무량_higher_than_2d  일간누적분무량_bf3  \\\n",
       "0     0.000000     0.000000      0.00000                     0.0      0.00000   \n",
       "1     0.001573     0.001685      0.00000                     0.0      0.00000   \n",
       "2     0.002097     0.002247      0.00000                     1.0      0.00000   \n",
       "3     0.003145     0.003371      0.00294                     1.0      0.00000   \n",
       "4     0.004718     0.004045      0.00294                     1.0      0.00294   \n",
       "\n",
       "   일간누적분무량_higher_than_3d  일간누적분무량_bf4  일간누적분무량_cumsum  시간당적색광량_bf1  \\\n",
       "0                     0.0          0.0        0.000000     0.469684   \n",
       "1                     0.0          0.0        0.000015     0.469684   \n",
       "2                     0.0          0.0        0.000029     0.469684   \n",
       "3                     1.0          0.0        0.000058     0.469684   \n",
       "4                     1.0          0.0        0.000087     0.469684   \n",
       "\n",
       "   시간당적색광량_1d_rise_rate  시간당적색광량_ma2  시간당적색광량_ma4  시간당적색광량_ma7  시간당적색광량_bf2  \\\n",
       "0              0.998938     0.932244     0.931334     0.931104     0.639163   \n",
       "1              0.998938     0.932244     0.931334     0.931104     0.639163   \n",
       "2              0.998938     0.932244     0.931334     0.931104     0.639163   \n",
       "3              0.998938     0.932244     0.931334     0.931104     0.639163   \n",
       "4              0.998938     0.932244     0.931334     0.931104     0.639163   \n",
       "\n",
       "   시간당적색광량_2d_rise_rate  시간당적색광량_bf3  시간당적색광량_3d_rise_rate  시간당적색광량_bf4  \\\n",
       "0              0.999552     0.779865              0.999661     0.876319   \n",
       "1              0.999552     0.779865              0.999661     0.876319   \n",
       "2              0.999552     0.779865              0.999661     0.876319   \n",
       "3              0.999552     0.779865              0.999661     0.876319   \n",
       "4              0.999552     0.779865              0.999661     0.876319   \n",
       "\n",
       "   시간당적색광량_4d_rise_rate  시간당적색광량_cumsum  내부습도관측치_bf1  내부습도관측치_ma2  \\\n",
       "0              0.999661        0.956626     0.000000     0.870398   \n",
       "1              0.999661        0.956626     0.877540     0.867173   \n",
       "2              0.999661        0.956626     0.871420     0.865120   \n",
       "3              0.999661        0.956626     0.873644     0.865878   \n",
       "4              0.999661        0.956626     0.872858     0.865944   \n",
       "\n",
       "   내부습도관측치_ma4  내부습도관측치_ma7  내부습도관측치_bf2  내부습도관측치_bf3  내부습도관측치_bf4  \\\n",
       "0     0.850697     0.850592     0.000000     0.000000     0.000000   \n",
       "1     0.846946     0.846810     0.000000     0.000000     0.000000   \n",
       "2     0.846604     0.846465     0.878576     0.000000     0.000000   \n",
       "3     0.846192     0.846050     0.872449     0.879694     0.000000   \n",
       "4     0.845037     0.846026     0.874676     0.873559     0.879946   \n",
       "\n",
       "   내부습도관측치_cumsum  시간당분무량_bf1  시간당분무량_higher_than_1d  시간당분무량_ma2  시간당분무량_ma4  \\\n",
       "0        0.001149    0.172029                    0.0    0.815584    0.833827   \n",
       "1        0.002658    0.172029                    1.0    0.820828    0.839187   \n",
       "2        0.004170    0.205209                    0.0    0.820828    0.837400   \n",
       "3        0.005682    0.172029                    1.0    0.820828    0.839187   \n",
       "4        0.007195    0.205209                    0.0    0.820828    0.839187   \n",
       "\n",
       "   시간당분무량_ma7  시간당분무량_bf2  시간당분무량_higher_than_2d  시간당분무량_bf3  \\\n",
       "0    0.837769    0.317698                    0.0    0.482201   \n",
       "1    0.843155    0.317698                    0.0    0.482201   \n",
       "2    0.841360    0.317698                    0.0    0.482201   \n",
       "3    0.843155    0.348337                    0.0    0.482201   \n",
       "4    0.842078    0.317698                    0.0    0.505453   \n",
       "\n",
       "   시간당분무량_higher_than_3d  시간당분무량_3d_rise_rate  시간당분무량_bf4  \\\n",
       "0                    0.0             0.932916    0.650656   \n",
       "1                    0.0             0.932916    0.650656   \n",
       "2                    0.0             0.932916    0.650656   \n",
       "3                    1.0             1.000000    0.650656   \n",
       "4                    0.0             0.858616    0.650656   \n",
       "\n",
       "   시간당분무량_higher_than_4d  시간당분무량_cumsum  일간누적백색광량_bf1  \\\n",
       "0                    0.0       0.884755           0.0   \n",
       "1                    0.0       0.884774           0.0   \n",
       "2                    0.0       0.884774           0.0   \n",
       "3                    0.0       0.884793           0.0   \n",
       "4                    0.0       0.884793           0.0   \n",
       "\n",
       "   일간누적백색광량_higher_than_1d  일간누적백색광량_ma2  일간누적백색광량_ma4  일간누적백색광량_ma7  \\\n",
       "0                      0.0           0.0           0.0           0.0   \n",
       "1                      0.0           0.0           0.0           0.0   \n",
       "2                      0.0           0.0           0.0           0.0   \n",
       "3                      0.0           0.0           0.0           0.0   \n",
       "4                      0.0           0.0           0.0           0.0   \n",
       "\n",
       "   일간누적백색광량_bf2  일간누적백색광량_higher_than_2d  일간누적백색광량_bf3  \\\n",
       "0           0.0                      0.0           0.0   \n",
       "1           0.0                      0.0           0.0   \n",
       "2           0.0                      0.0           0.0   \n",
       "3           0.0                      0.0           0.0   \n",
       "4           0.0                      0.0           0.0   \n",
       "\n",
       "   일간누적백색광량_higher_than_3d  일간누적백색광량_bf4  일간누적백색광량_higher_than_4d  \\\n",
       "0                      0.0           0.0                      0.0   \n",
       "1                      0.0           0.0                      0.0   \n",
       "2                      0.0           0.0                      0.0   \n",
       "3                      0.0           0.0                      0.0   \n",
       "4                      0.0           0.0                      0.0   \n",
       "\n",
       "   일간누적백색광량_cumsum  ec관측치_bf1  ec관측치_ma2  ec관측치_ma4  ec관측치_ma7  ec관측치_bf2  \\\n",
       "0              0.0   0.000000   0.259206   0.260748   0.261681   0.000000   \n",
       "1              0.0   0.257663   0.259350   0.260893   0.261826   0.000000   \n",
       "2              0.0   0.257949   0.259301   0.260812   0.261745   0.257663   \n",
       "3              0.0   0.257567   0.259088   0.260761   0.261694   0.257949   \n",
       "4              0.0   0.257526   0.259471   0.260929   0.261827   0.257567   \n",
       "\n",
       "   ec관측치_bf3  ec관측치_bf4  ec관측치_4d_rise_rate  ec관측치_cumsum  시간당백색광량_bf1  \\\n",
       "0   0.000000   0.000000            0.999851      0.000708     0.881706   \n",
       "1   0.000000   0.000000            0.999851      0.001417     0.881706   \n",
       "2   0.000000   0.000000            0.999851      0.002125     0.881706   \n",
       "3   0.257663   0.000000            0.999851      0.002832     0.881706   \n",
       "4   0.257949   0.257663            0.999851      0.003542     0.881706   \n",
       "\n",
       "   시간당백색광량_higher_than_1d  시간당백색광량_1d_rise_rate  시간당백색광량_ma2  시간당백색광량_ma4  \\\n",
       "0                     0.0              0.999635     0.991383     0.991271   \n",
       "1                     0.0              0.999635     0.991383     0.991271   \n",
       "2                     0.0              0.999635     0.991383     0.991271   \n",
       "3                     0.0              0.999635     0.991383     0.991271   \n",
       "4                     0.0              0.999635     0.991383     0.991271   \n",
       "\n",
       "   시간당백색광량_ma7  시간당백색광량_bf2  시간당백색광량_higher_than_2d  시간당백색광량_2d_rise_rate  \\\n",
       "0     0.991275     0.937135                     0.0              0.999819   \n",
       "1     0.991275     0.937135                     0.0              0.999819   \n",
       "2     0.991275     0.937135                     0.0              0.999819   \n",
       "3     0.991275     0.937135                     0.0              0.999819   \n",
       "4     0.991275     0.937135                     0.0              0.999819   \n",
       "\n",
       "   시간당백색광량_bf3  시간당백색광량_higher_than_3d  시간당백색광량_3d_rise_rate  시간당백색광량_bf4  \\\n",
       "0     0.967547                     0.0              0.999831     0.983506   \n",
       "1     0.967547                     0.0              0.999831     0.983506   \n",
       "2     0.967547                     0.0              0.999831     0.983506   \n",
       "3     0.967547                     0.0              0.999831     0.983506   \n",
       "4     0.967547                     0.0              0.999831     0.983506   \n",
       "\n",
       "   시간당백색광량_higher_than_4d  시간당백색광량_4d_rise_rate  시간당백색광량_cumsum  시간당청색광량_bf1  \\\n",
       "0                     0.0              0.999831         0.99446      0.66396   \n",
       "1                     0.0              0.999831         0.99446      0.66396   \n",
       "2                     0.0              0.999831         0.99446      0.66396   \n",
       "3                     0.0              0.999831         0.99446      0.66396   \n",
       "4                     0.0              0.999831         0.99446      0.66396   \n",
       "\n",
       "   시간당청색광량_higher_than_1d  시간당청색광량_ma2  시간당청색광량_ma4  시간당청색광량_ma7  시간당청색광량_bf2  \\\n",
       "0                     0.0     0.968464      0.96809     0.968003     0.798048   \n",
       "1                     0.0     0.968464      0.96809     0.968003     0.798048   \n",
       "2                     0.0     0.968464      0.96809     0.968003     0.798048   \n",
       "3                     0.0     0.968464      0.96809     0.968003     0.798048   \n",
       "4                     0.0     0.968464      0.96809     0.968003     0.798048   \n",
       "\n",
       "   시간당청색광량_higher_than_2d  시간당청색광량_2d_rise_rate  시간당청색광량_bf3  \\\n",
       "0                     0.0              0.999552     0.887683   \n",
       "1                     0.0              0.999552     0.887683   \n",
       "2                     0.0              0.999552     0.887683   \n",
       "3                     0.0              0.999552     0.887683   \n",
       "4                     0.0              0.999552     0.887683   \n",
       "\n",
       "   시간당청색광량_higher_than_3d  시간당청색광량_3d_rise_rate  시간당청색광량_bf4  \\\n",
       "0                     0.0              0.999661       0.9405   \n",
       "1                     0.0              0.999661       0.9405   \n",
       "2                     0.0              0.999661       0.9405   \n",
       "3                     0.0              0.999661       0.9405   \n",
       "4                     0.0              0.999661       0.9405   \n",
       "\n",
       "   시간당청색광량_higher_than_4d  시간당청색광량_4d_rise_rate  시간당청색광량_cumsum  일간누적청색광량_bf1  \\\n",
       "0                     0.0              0.999661        0.980104           0.0   \n",
       "1                     0.0              0.999661        0.980104           0.0   \n",
       "2                     0.0              0.999661        0.980104           0.0   \n",
       "3                     0.0              0.999661        0.980104           0.0   \n",
       "4                     0.0              0.999661        0.980104           0.0   \n",
       "\n",
       "   일간누적청색광량_higher_than_1d  일간누적청색광량_1d_rise_rate  일간누적청색광량_ma2  일간누적청색광량_ma4  \\\n",
       "0                      0.0               0.999166           0.0           0.0   \n",
       "1                      0.0               0.999166           0.0           0.0   \n",
       "2                      0.0               0.999166           0.0           0.0   \n",
       "3                      0.0               0.999166           0.0           0.0   \n",
       "4                      0.0               0.999166           0.0           0.0   \n",
       "\n",
       "   일간누적청색광량_ma7  일간누적청색광량_bf2  일간누적청색광량_higher_than_2d  일간누적청색광량_2d_rise_rate  \\\n",
       "0           0.0           0.0                      0.0               0.999166   \n",
       "1           0.0           0.0                      0.0               0.999166   \n",
       "2           0.0           0.0                      0.0               0.999166   \n",
       "3           0.0           0.0                      0.0               0.999166   \n",
       "4           0.0           0.0                      0.0               0.999166   \n",
       "\n",
       "   일간누적청색광량_bf3  일간누적청색광량_higher_than_3d  일간누적청색광량_3d_rise_rate  일간누적청색광량_bf4  \\\n",
       "0           0.0                      0.0               0.999166           0.0   \n",
       "1           0.0                      0.0               0.999166           0.0   \n",
       "2           0.0                      0.0               0.999166           0.0   \n",
       "3           0.0                      0.0               0.999166           0.0   \n",
       "4           0.0                      0.0               0.999166           0.0   \n",
       "\n",
       "   일간누적청색광량_higher_than_4d  일간누적청색광량_4d_rise_rate  일간누적청색광량_cumsum  \\\n",
       "0                      0.0               0.999151              0.0   \n",
       "1                      0.0               0.999151              0.0   \n",
       "2                      0.0               0.999151              0.0   \n",
       "3                      0.0               0.999151              0.0   \n",
       "4                      0.0               0.999151              0.0   \n",
       "\n",
       "   일간누적적색광량_bf1  일간누적적색광량_higher_than_1d  일간누적적색광량_1d_rise_rate  일간누적적색광량_ma2  \\\n",
       "0           0.0                      0.0               0.999293           0.0   \n",
       "1           0.0                      0.0               0.999293           0.0   \n",
       "2           0.0                      0.0               0.999293           0.0   \n",
       "3           0.0                      0.0               0.999293           0.0   \n",
       "4           0.0                      0.0               0.999293           0.0   \n",
       "\n",
       "   일간누적적색광량_ma4  일간누적적색광량_ma7  일간누적적색광량_bf2  일간누적적색광량_higher_than_2d  \\\n",
       "0           0.0           0.0           0.0                      0.0   \n",
       "1           0.0           0.0           0.0                      0.0   \n",
       "2           0.0           0.0           0.0                      0.0   \n",
       "3           0.0           0.0           0.0                      0.0   \n",
       "4           0.0           0.0           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_2d_rise_rate  일간누적적색광량_bf3  일간누적적색광량_higher_than_3d  \\\n",
       "0               0.999293           0.0                      0.0   \n",
       "1               0.999293           0.0                      0.0   \n",
       "2               0.999293           0.0                      0.0   \n",
       "3               0.999293           0.0                      0.0   \n",
       "4               0.999293           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_3d_rise_rate  일간누적적색광량_bf4  일간누적적색광량_higher_than_4d  \\\n",
       "0               0.999293           0.0                      0.0   \n",
       "1               0.999293           0.0                      0.0   \n",
       "2               0.999293           0.0                      0.0   \n",
       "3               0.999293           0.0                      0.0   \n",
       "4               0.999293           0.0                      0.0   \n",
       "\n",
       "   일간누적적색광량_4d_rise_rate  일간누적적색광량_cumsum  co2관측치_bf1  co2관측치_ma2  co2관측치_ma4  \\\n",
       "0               0.999293              0.0    0.000000    0.289605    0.296091   \n",
       "1               0.999293              0.0    0.312574    0.287377    0.293813   \n",
       "2               0.999293              0.0    0.308305    0.286408    0.293912   \n",
       "3               0.999293              0.0    0.310717    0.291544    0.295943   \n",
       "4               0.999293              0.0    0.318143    0.299383    0.299455   \n",
       "\n",
       "   co2관측치_ma7  co2관측치_bf2  co2관측치_bf3  co2관측치_bf4  co2관측치_cumsum  \n",
       "0    0.314353    0.000000    0.000000    0.000000       0.000997  \n",
       "1    0.311934    0.000000    0.000000    0.000000       0.002105  \n",
       "2    0.312039    0.312574    0.000000    0.000000       0.003222  \n",
       "3    0.314196    0.308305    0.312574    0.000000       0.004365  \n",
       "4    0.317210    0.310717    0.308305    0.312574       0.005536  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dff77-20cc-4880-9341-8835231ba4b5",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec8035-20fe-4240-8016-9395b0ba39f3",
   "metadata": {},
   "source": [
    "# Pre-Fit Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "052f31db-7bd7-48fb-9285-1f85ddfa1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     input_df.drop(columns=['case_num']),\n",
    "#     label_df['predicted_weight_g'],\n",
    "#     test_size=0.05,\n",
    "#     random_state=42\n",
    "# )\n",
    "# print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "20aec8b4-538e-451c-b044-1871d0036b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# model = CatBoostRegressor(iterations=5000,metric_period=1000,random_state=42)\n",
    "# model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "# tr_pred = model.predict(X_train)\n",
    "# va_pred = model.predict(X_valid)\n",
    "\n",
    "# # X_train['cat_pred'] = np.exp(X_train['cat_pred'])\n",
    "# # X_valid['cat_pred'] = np.exp(X_valid['cat_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73529957-9cb6-4760-881a-b5268109a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# tr_mse = np.sqrt(mean_squared_error(tr_pred,y_train))\n",
    "# va_mse = np.sqrt(mean_squared_error(va_pred,y_valid))\n",
    "\n",
    "# print(tr_mse,va_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a08-c07c-42bd-9b38-18eccfeba517",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a523935d-af75-45ce-976c-c78f41578e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        # print(x.shape,x_reshape.shape)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446b0f-9dba-4a2b-b25e-5e20da605124",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "194faee7-d316-4c85-9cad-8629cffdacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden, dropout, num_layers, bidirectional):\n",
    "\n",
    "        if bidirectional:\n",
    "            offset = 2\n",
    "        else:\n",
    "            offset = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden[0],\n",
    "            dropout=dropout[0],\n",
    "            num_layers=num_layers[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=offset*hidden[0],\n",
    "            hidden_size=hidden[1],\n",
    "            dropout=dropout[1],\n",
    "            num_layers=num_layers[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=offset*hidden[1],\n",
    "            hidden_size=hidden[2],\n",
    "            dropout=dropout[2],\n",
    "            num_layers=num_layers[2],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=offset*hidden[2],\n",
    "            hidden_size=hidden[3],\n",
    "            dropout=dropout[3],\n",
    "            num_layers=num_layers[3],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.elu  = nn.ELU()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.fc = nn.Linear(offset * hidden[3], 1)\n",
    "        self.fc = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st\n",
    "        x, _ = self.lstm1(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 2nd\n",
    "        x, _ = self.lstm2(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 3rd\n",
    "        x, _ = self.lstm3(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 4th\n",
    "        x, _ = self.lstm4(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # fully connected layer\n",
    "        x    = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac7bec-be69-4cf4-a663-2c5d0e813db0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fcb61ab-2360-4805-8407-1005e3ca5b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update)\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                 kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update)\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x)\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd =SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "    \n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen):\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: \n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCINet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, num_decoder_layer = 1, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.num_decoder_layer = num_decoder_layer\n",
    "\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        self.div_projection = nn.ModuleList()\n",
    "        self.overlap_len = self.input_len//4\n",
    "        self.div_len = self.input_len//6\n",
    "\n",
    "        if self.num_decoder_layer > 1:\n",
    "            self.projection1 = nn.Linear(self.input_len, self.output_len)\n",
    "            for layer_idx in range(self.num_decoder_layer-1):\n",
    "                div_projection = nn.ModuleList()\n",
    "                for i in range(6):\n",
    "                    lens = min(i*self.div_len+self.overlap_len,self.input_len) - i*self.div_len\n",
    "                    div_projection.append(nn.Linear(lens, self.div_len))\n",
    "                self.div_projection.append(div_projection)\n",
    "\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x += pe[:, :, :-1]\n",
    "            else:\n",
    "                x += self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        if self.RIN:\n",
    "            print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            #mean\n",
    "            x = x - means\n",
    "            #var\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # affine\n",
    "            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x += res1\n",
    "        if self.num_decoder_layer == 1:\n",
    "            x = self.projection1(x)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    "            for div_projection in self.div_projection:\n",
    "                output = torch.zeros(x.shape,dtype=x.dtype).cuda()\n",
    "                for i, div_layer in enumerate(div_projection):\n",
    "                    div_x = x[:,:,i*self.div_len:min(i*self.div_len+self.overlap_len,self.input_len)]\n",
    "                    output[:,:,i*self.div_len:(i+1)*self.div_len] = div_layer(div_x)\n",
    "                x = output\n",
    "            x = self.projection1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            res2 = x\n",
    "            x = self.blocks2(x)\n",
    "            x += res2\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x, MidOutPut\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b693071-44da-46f4-a007-ccbfa2a9b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class SCINet_decompose(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet_decompose, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.decomp = series_decomp(25)\n",
    "        self.trend = nn.Linear(input_len,input_len)\n",
    "        self.trend_dec = nn.Linear(input_len,output_len)\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        x, trend = self.decomp(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # seq_means = x[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means = x[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # x = x - seq_means\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "            # print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means2 = trend.mean(1, keepdim=True).detach()\n",
    "            trend = trend - means2\n",
    "            stdev2 = torch.sqrt(torch.var(trend, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            trend /= stdev2\n",
    "            # seq_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # trend = trend - seq_means2 \n",
    "            trend = trend * self.affine_weight2 + self.affine_bias2\n",
    "        \n",
    "\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x = x + pe[:, :, :-1]\n",
    "            else:\n",
    "                x = x + self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        \n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x = self.projection1(x)\n",
    "\n",
    "        trend = trend.permute(0,2,1)\n",
    "        trend = self.trend(trend)  \n",
    "        trend = self.trend_dec(trend).permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                # x = x + pred_means\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            x = self.blocks2(x)\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend, MidOutPut\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e41047c2-5bff-4835-9c26-6f523fbbf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCINet_Model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(SCINet_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # 24,4,1,1,2,0.5,False,1,True,1\n",
    "        window_size = 24 # in (fixed)\n",
    "        horizon = 4      # out\n",
    "        hidden_size = 1\n",
    "        groups = 1\n",
    "        kernel = 2\n",
    "        dropout = 0.05\n",
    "        single_step_output_One = False\n",
    "        num_levels = 1\n",
    "        positionalEcoding = True\n",
    "        num_stacks = 1\n",
    "        self.scinet = SCINet(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        self.scinet_decompose = SCINet_decompose(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        \n",
    "        # hidden  = [64, 64, 64]\n",
    "        # dropout = [0.2, 0.5, 0.5]\n",
    "        # num_layers = [1,1,1]\n",
    "        # self.lstm1 = nn.LSTM(\n",
    "        #     input_size=input_size,\n",
    "        #     hidden_size=hidden[0],\n",
    "        #     dropout=dropout[0],\n",
    "        #     num_layers=num_layers[0],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm2 = nn.LSTM(\n",
    "        #     input_size=2*hidden[0],\n",
    "        #     hidden_size=hidden[1],\n",
    "        #     dropout=dropout[1],\n",
    "        #     num_layers=num_layers[1],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm3 = nn.LSTM(\n",
    "        #     input_size=2*hidden[1],\n",
    "        #     hidden_size=hidden[2],\n",
    "        #     dropout=dropout[2],\n",
    "        #     num_layers=num_layers[2],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(horizon)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.fc = nn.Linear(2*hidden[0], 1)\n",
    "        self.fc_1 = nn.Linear(input_size, 16)\n",
    "        self.fc_1 = TimeDistributed(self.fc_1)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        self.fc_2 = TimeDistributed(self.fc_2)\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        \n",
    "        self.nlinear = NLinear(input_size,1)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.scinet(x)\n",
    "#         # x = self.bn(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm1(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm2(x)\n",
    "#         # x = self.selu(x)\n",
    "#         # x,_ = self.lstm3(x)\n",
    "#         # x = self.selu(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leakyrelu(x)\n",
    "#         x = self.fc_2(x[:,-1,:]) # [:,:,-1]\n",
    "        \n",
    "#         # x = self.fc_2(x[:,-1,:])\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.scinet(x)\n",
    "#         x = self.scinet_decompose(x)\n",
    "#         x1,x2 = x[0],x[1]\n",
    "#         x = torch.cat([x1,x2],dim=1)\n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.scinet(x)\n",
    "        x = self.scinet_decompose(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        # x = self.nlinear(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fe908-42f5-43ec-b343-e9d60b7bdce1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a341e468-e635-460c-b7f9-ab5475e77b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization-Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len, td=True):\n",
    "        super(NLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
    "        if td:\n",
    "            self.Linear = TimeDistributed(self.Linear)\n",
    "        # Use this line if you want to visualize the weights\n",
    "        self.Linear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "        self._reinitialize()\n",
    "    \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'Linear' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seq_last = x[:,-1:,:].detach()\n",
    "        x = x - seq_last\n",
    "        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = x + seq_last\n",
    "        return x # [Batch, Output length, Channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "12749617-0499-43b8-8499-4236f292e471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLinear_Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, input_size):\n",
    "        super(NLinear_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.BatchNorm1d(nodes[1]) , \n",
    "        nodes = [24]*4\n",
    "        dropout = 0.05\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            NLinear(seq_len ,nodes[0]), self.dropout, nn.BatchNorm1d(nodes[0]), self.activation,\n",
    "            NLinear(nodes[0],nodes[1]), self.dropout, nn.BatchNorm1d(nodes[1]), self.activation,\n",
    "            NLinear(nodes[1],nodes[2]), self.dropout, nn.BatchNorm1d(nodes[2]), self.activation,\n",
    "            NLinear(nodes[2],nodes[3]), self.dropout, nn.BatchNorm1d(nodes[3]), self.activation,\n",
    "            NLinear(nodes[3],pred_len),\n",
    "        )\n",
    "\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear_1(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.nlinear_2(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train, Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "620d419b-7332-4e64-a6e5-68aed8e5b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_fn(output, target):\n",
    "    return torch.sqrt(torch.mean((output-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e26ee98e-b3ae-4d8e-a18e-8b166686c493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, scheduler, device, \n",
    "    early_stopping, epochs, metric_period=1, best_model_only=True, verbose=True,\n",
    "):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['ES_PATIENCE'], verbose = CFG['ES_VERBOSE'], path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            # print(output.shape,Y.shape) # torch.Size([4, 28, 1]) torch.Size([4, 24])\n",
    "            # print(output[:5],Y[:5])\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5],output.shape,Y.shape)\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % metric_period == 0:\n",
    "                epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "                progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "                    .format(\n",
    "                        epoch_str,\n",
    "                        epochs,np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)\n",
    "                    )\n",
    "                epoch_s = time.time()\n",
    "\n",
    "                if best_loss > valid_loss:\n",
    "                    mark = '*'\n",
    "                else:\n",
    "                    mark = ' '\n",
    "            \n",
    "                print(mark+progress)\n",
    "            \n",
    "        if best_model_only:\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model = model\n",
    "                \n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec6fe-a87d-4fe0-8f91-97136bd96e57",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5cc3e9c6-5ca0-43cf-99d4-4826cedf9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,input,label,infer_mode):\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        input = input.sort_values(['case_num','DAT','obs_time'])\n",
    "        label = label.sort_values(['case_num','DAT'])\n",
    "\n",
    "        input_window_size = 24*28\n",
    "        label_window_size = 28\n",
    "\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        for i in range(int(input.shape[0]/input_window_size)):\n",
    "            # input_df = self.input.iloc[i]\n",
    "            input_df = input.iloc[i*input_window_size : (i+1)*input_window_size,:]\n",
    "            label_df = label.iloc[i*label_window_size : (i+1)*label_window_size  ]\n",
    "\n",
    "            # seq_length = 12\n",
    "            for j in range(label_df.shape[0]-seq_length+1):\n",
    "                x_seq = input_df.iloc[j*24:(j+seq_length)*24,:].drop(columns=['case_num']) # 'DAT','obs_time','time'\n",
    "                y_seq = label_df.iloc[j   : j+seq_length]      ['predicted_weight_g']\n",
    "\n",
    "                self.input_list.append(torch.Tensor(x_seq.values))\n",
    "                self.label_list.append(torch.Tensor(y_seq.values))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data  = self.input_list[index]\n",
    "        label = self.label_list[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "31adc92f-7d3b-4a78-9898-3ad69b0406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "input_dataset = CustomDataset(input=input_df, label=label_df, infer_mode=False)\n",
    "input_loader  = DataLoader(input_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True)\n",
    "test_loader  = DataLoader(test_dataset  , batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a0afddd8-1ac4-4c94-bca6-2f7a520d23d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18816, 189), (784, 3))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.shape, label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c132bc04-4a20-43ad-aec5-648acc3fd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "# pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d155e407-de26-47e5-ac93-233efdde56b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188]),\n",
       " torch.Size([16, 24, 188])]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.size() for x,y in input_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fe9bd444-65c1-4f0e-9559-75e3d99c0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '4'\n",
    "\n",
    "paths = [f'./out/kf_lstm_{save_mark}',f'./out/kf_lstm_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dbbf5a68-454d-457d-9db5-98056e39363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a594b56784cc4545bf2133d6b1dbb0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "(1/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*[0001/5000] tr_loss : 44.95973, val_loss : 41.78373, elapsed : 0.33s, total : 0.33s, remaining : 1658.76s\n",
      "*[0002/5000] tr_loss : 44.90810, val_loss : 41.75090, elapsed : 0.33s, total : 0.67s, remaining : 1671.01s\n",
      "*[0003/5000] tr_loss : 44.86072, val_loss : 41.72053, elapsed : 0.33s, total : 1.00s, remaining : 1648.95s\n",
      "*[0004/5000] tr_loss : 44.81866, val_loss : 41.68634, elapsed : 0.33s, total : 1.33s, remaining : 1656.65s\n",
      "*[0005/5000] tr_loss : 44.77864, val_loss : 41.64815, elapsed : 0.33s, total : 1.66s, remaining : 1635.44s\n",
      "*[0006/5000] tr_loss : 44.73691, val_loss : 41.60605, elapsed : 0.33s, total : 1.98s, remaining : 1631.21s\n",
      "*[0007/5000] tr_loss : 44.69910, val_loss : 41.57472, elapsed : 0.33s, total : 2.31s, remaining : 1634.48s\n",
      "*[0008/5000] tr_loss : 44.66396, val_loss : 41.53954, elapsed : 0.32s, total : 2.63s, remaining : 1621.49s\n",
      "*[0009/5000] tr_loss : 44.62923, val_loss : 41.50762, elapsed : 0.32s, total : 2.96s, remaining : 1621.00s\n",
      "*[0010/5000] tr_loss : 44.59596, val_loss : 41.48302, elapsed : 0.33s, total : 3.29s, remaining : 1658.96s\n",
      "*[0011/5000] tr_loss : 44.56148, val_loss : 41.45194, elapsed : 0.33s, total : 3.62s, remaining : 1626.10s\n",
      "*[0012/5000] tr_loss : 44.52883, val_loss : 41.42505, elapsed : 0.32s, total : 3.94s, remaining : 1611.34s\n",
      "*[0013/5000] tr_loss : 44.49425, val_loss : 41.39622, elapsed : 0.33s, total : 4.27s, remaining : 1623.86s\n",
      "*[0014/5000] tr_loss : 44.46503, val_loss : 41.36847, elapsed : 0.33s, total : 4.60s, remaining : 1639.13s\n",
      "*[0015/5000] tr_loss : 44.43104, val_loss : 41.33928, elapsed : 0.33s, total : 4.92s, remaining : 1623.65s\n",
      "*[0016/5000] tr_loss : 44.39750, val_loss : 41.31376, elapsed : 0.33s, total : 5.25s, remaining : 1623.50s\n",
      "*[0017/5000] tr_loss : 44.36585, val_loss : 41.28474, elapsed : 0.33s, total : 5.57s, remaining : 1621.82s\n",
      "*[0018/5000] tr_loss : 44.33149, val_loss : 41.26001, elapsed : 0.32s, total : 5.90s, remaining : 1614.28s\n",
      "*[0019/5000] tr_loss : 44.30117, val_loss : 41.22580, elapsed : 0.33s, total : 6.22s, remaining : 1623.02s\n",
      "*[0020/5000] tr_loss : 44.27041, val_loss : 41.20064, elapsed : 0.32s, total : 6.55s, remaining : 1608.79s\n",
      "*[0021/5000] tr_loss : 44.23698, val_loss : 41.16443, elapsed : 0.33s, total : 6.87s, remaining : 1641.25s\n",
      "*[0022/5000] tr_loss : 44.20744, val_loss : 41.13871, elapsed : 0.32s, total : 7.20s, remaining : 1613.08s\n",
      "*[0023/5000] tr_loss : 44.17480, val_loss : 41.11157, elapsed : 0.33s, total : 7.52s, remaining : 1620.32s\n",
      "*[0024/5000] tr_loss : 44.14089, val_loss : 41.07745, elapsed : 0.34s, total : 7.86s, remaining : 1693.13s\n",
      "*[0025/5000] tr_loss : 44.11129, val_loss : 41.05176, elapsed : 0.33s, total : 8.19s, remaining : 1632.78s\n",
      "*[0026/5000] tr_loss : 44.07388, val_loss : 41.01558, elapsed : 0.33s, total : 8.52s, remaining : 1632.98s\n",
      "*[0027/5000] tr_loss : 44.04330, val_loss : 40.99267, elapsed : 0.32s, total : 8.84s, remaining : 1600.18s\n",
      "*[0028/5000] tr_loss : 44.00653, val_loss : 40.96189, elapsed : 0.33s, total : 9.17s, remaining : 1625.97s\n",
      "*[0029/5000] tr_loss : 43.97557, val_loss : 40.92495, elapsed : 0.33s, total : 9.50s, remaining : 1624.39s\n",
      "*[0030/5000] tr_loss : 43.93783, val_loss : 40.90011, elapsed : 0.32s, total : 9.82s, remaining : 1613.37s\n",
      "*[0031/5000] tr_loss : 43.90449, val_loss : 40.87329, elapsed : 0.33s, total : 10.15s, remaining : 1634.52s\n",
      "*[0032/5000] tr_loss : 43.86817, val_loss : 40.84829, elapsed : 0.33s, total : 10.48s, remaining : 1635.46s\n",
      "*[0033/5000] tr_loss : 43.83660, val_loss : 40.82059, elapsed : 0.33s, total : 10.81s, remaining : 1626.52s\n",
      "*[0034/5000] tr_loss : 43.79713, val_loss : 40.78503, elapsed : 0.33s, total : 11.14s, remaining : 1635.42s\n",
      "*[0035/5000] tr_loss : 43.75679, val_loss : 40.75727, elapsed : 0.32s, total : 11.46s, remaining : 1603.28s\n",
      "*[0036/5000] tr_loss : 43.72274, val_loss : 40.72579, elapsed : 0.33s, total : 11.79s, remaining : 1623.84s\n",
      "*[0037/5000] tr_loss : 43.69005, val_loss : 40.70375, elapsed : 0.33s, total : 12.11s, remaining : 1617.00s\n",
      "*[0038/5000] tr_loss : 43.64965, val_loss : 40.67824, elapsed : 0.33s, total : 12.44s, remaining : 1620.94s\n",
      "*[0039/5000] tr_loss : 43.61762, val_loss : 40.64607, elapsed : 0.32s, total : 12.76s, remaining : 1609.40s\n",
      "*[0040/5000] tr_loss : 43.57898, val_loss : 40.61798, elapsed : 0.33s, total : 13.09s, remaining : 1623.02s\n",
      "*[0041/5000] tr_loss : 43.54265, val_loss : 40.59078, elapsed : 0.34s, total : 13.43s, remaining : 1684.59s\n",
      "*[0042/5000] tr_loss : 43.50076, val_loss : 40.55608, elapsed : 0.33s, total : 13.76s, remaining : 1620.08s\n",
      "*[0043/5000] tr_loss : 43.46668, val_loss : 40.52796, elapsed : 0.33s, total : 14.08s, remaining : 1611.76s\n",
      "*[0044/5000] tr_loss : 43.42547, val_loss : 40.49576, elapsed : 0.33s, total : 14.42s, remaining : 1658.38s\n",
      "*[0045/5000] tr_loss : 43.39208, val_loss : 40.46671, elapsed : 0.33s, total : 14.75s, remaining : 1624.28s\n",
      "*[0046/5000] tr_loss : 43.35361, val_loss : 40.43593, elapsed : 0.32s, total : 15.07s, remaining : 1600.24s\n",
      "*[0047/5000] tr_loss : 43.31621, val_loss : 40.40852, elapsed : 0.33s, total : 15.40s, remaining : 1650.15s\n",
      "*[0048/5000] tr_loss : 43.27757, val_loss : 40.36831, elapsed : 0.33s, total : 15.73s, remaining : 1635.87s\n",
      "*[0049/5000] tr_loss : 43.23802, val_loss : 40.33214, elapsed : 0.33s, total : 16.06s, remaining : 1611.63s\n",
      "*[0050/5000] tr_loss : 43.20042, val_loss : 40.29975, elapsed : 0.33s, total : 16.38s, remaining : 1616.85s\n",
      "*[0051/5000] tr_loss : 43.16460, val_loss : 40.27229, elapsed : 0.33s, total : 16.71s, remaining : 1625.21s\n",
      "*[0052/5000] tr_loss : 43.12879, val_loss : 40.23672, elapsed : 0.33s, total : 17.04s, remaining : 1620.55s\n",
      "*[0053/5000] tr_loss : 43.09255, val_loss : 40.20748, elapsed : 0.33s, total : 17.37s, remaining : 1613.57s\n",
      "*[0054/5000] tr_loss : 43.05236, val_loss : 40.16150, elapsed : 0.33s, total : 17.70s, remaining : 1635.66s\n",
      "*[0055/5000] tr_loss : 43.00971, val_loss : 40.13675, elapsed : 0.33s, total : 18.03s, remaining : 1619.97s\n",
      "*[0056/5000] tr_loss : 42.98171, val_loss : 40.10426, elapsed : 0.33s, total : 18.35s, remaining : 1618.03s\n",
      "*[0057/5000] tr_loss : 42.93707, val_loss : 40.05264, elapsed : 0.33s, total : 18.68s, remaining : 1608.74s\n",
      "*[0058/5000] tr_loss : 42.89939, val_loss : 40.02628, elapsed : 0.33s, total : 19.00s, remaining : 1610.34s\n",
      "*[0059/5000] tr_loss : 42.86027, val_loss : 39.98916, elapsed : 0.33s, total : 19.33s, remaining : 1615.20s\n",
      "*[0060/5000] tr_loss : 42.82554, val_loss : 39.95535, elapsed : 0.33s, total : 19.66s, remaining : 1605.87s\n",
      "*[0061/5000] tr_loss : 42.78780, val_loss : 39.91656, elapsed : 0.33s, total : 19.98s, remaining : 1613.11s\n",
      "*[0062/5000] tr_loss : 42.75721, val_loss : 39.88471, elapsed : 0.36s, total : 20.34s, remaining : 1767.37s\n",
      "*[0063/5000] tr_loss : 42.71241, val_loss : 39.84604, elapsed : 0.33s, total : 20.67s, remaining : 1628.58s\n",
      "*[0064/5000] tr_loss : 42.68080, val_loss : 39.80770, elapsed : 0.33s, total : 21.00s, remaining : 1627.37s\n",
      "*[0065/5000] tr_loss : 42.64322, val_loss : 39.76420, elapsed : 0.33s, total : 21.33s, remaining : 1615.18s\n",
      "*[0066/5000] tr_loss : 42.60742, val_loss : 39.74117, elapsed : 0.33s, total : 21.65s, remaining : 1605.68s\n",
      "*[0067/5000] tr_loss : 42.57804, val_loss : 39.70003, elapsed : 0.33s, total : 21.98s, remaining : 1618.06s\n",
      "*[0068/5000] tr_loss : 42.54139, val_loss : 39.67021, elapsed : 0.33s, total : 22.31s, remaining : 1618.99s\n",
      "*[0069/5000] tr_loss : 42.50021, val_loss : 39.62251, elapsed : 0.33s, total : 22.64s, remaining : 1615.65s\n",
      "*[0070/5000] tr_loss : 42.46553, val_loss : 39.59862, elapsed : 0.33s, total : 22.96s, remaining : 1605.36s\n",
      "*[0071/5000] tr_loss : 42.43452, val_loss : 39.55475, elapsed : 0.32s, total : 23.29s, remaining : 1600.11s\n",
      "*[0072/5000] tr_loss : 42.39517, val_loss : 39.51939, elapsed : 0.33s, total : 23.61s, remaining : 1609.51s\n",
      "*[0073/5000] tr_loss : 42.36222, val_loss : 39.48133, elapsed : 0.33s, total : 23.94s, remaining : 1605.50s\n",
      "*[0074/5000] tr_loss : 42.33527, val_loss : 39.44461, elapsed : 0.33s, total : 24.27s, remaining : 1628.22s\n",
      "*[0075/5000] tr_loss : 42.30342, val_loss : 39.40297, elapsed : 0.33s, total : 24.60s, remaining : 1640.54s\n",
      "*[0076/5000] tr_loss : 42.26528, val_loss : 39.36157, elapsed : 0.33s, total : 24.93s, remaining : 1610.40s\n",
      "*[0077/5000] tr_loss : 42.22906, val_loss : 39.32623, elapsed : 0.33s, total : 25.26s, remaining : 1604.16s\n",
      "*[0078/5000] tr_loss : 42.19591, val_loss : 39.28753, elapsed : 0.33s, total : 25.58s, remaining : 1607.90s\n",
      "*[0079/5000] tr_loss : 42.16413, val_loss : 39.25106, elapsed : 0.33s, total : 25.91s, remaining : 1609.72s\n",
      "*[0080/5000] tr_loss : 42.12816, val_loss : 39.21643, elapsed : 0.33s, total : 26.24s, remaining : 1603.20s\n",
      "*[0081/5000] tr_loss : 42.09522, val_loss : 39.18090, elapsed : 0.33s, total : 26.56s, remaining : 1614.56s\n",
      "*[0082/5000] tr_loss : 42.06150, val_loss : 39.14250, elapsed : 0.33s, total : 26.90s, remaining : 1634.08s\n",
      "*[0083/5000] tr_loss : 42.02765, val_loss : 39.10590, elapsed : 0.33s, total : 27.22s, remaining : 1610.14s\n",
      "*[0084/5000] tr_loss : 41.99363, val_loss : 39.07078, elapsed : 0.33s, total : 27.56s, remaining : 1630.14s\n",
      "*[0085/5000] tr_loss : 41.96132, val_loss : 39.02951, elapsed : 0.33s, total : 27.88s, remaining : 1610.85s\n",
      "*[0086/5000] tr_loss : 41.92328, val_loss : 39.00259, elapsed : 0.33s, total : 28.21s, remaining : 1601.44s\n",
      "*[0087/5000] tr_loss : 41.88929, val_loss : 38.96156, elapsed : 0.33s, total : 28.54s, remaining : 1604.29s\n",
      "*[0088/5000] tr_loss : 41.85391, val_loss : 38.92031, elapsed : 0.33s, total : 28.86s, remaining : 1607.09s\n",
      "*[0089/5000] tr_loss : 41.81966, val_loss : 38.87673, elapsed : 0.33s, total : 29.19s, remaining : 1617.19s\n",
      "*[0090/5000] tr_loss : 41.78352, val_loss : 38.83600, elapsed : 0.33s, total : 29.52s, remaining : 1612.91s\n",
      "*[0091/5000] tr_loss : 41.74774, val_loss : 38.79944, elapsed : 0.33s, total : 29.85s, remaining : 1603.64s\n",
      "*[0092/5000] tr_loss : 41.71646, val_loss : 38.75147, elapsed : 0.33s, total : 30.18s, remaining : 1609.35s\n",
      "*[0093/5000] tr_loss : 41.67633, val_loss : 38.71534, elapsed : 0.33s, total : 30.50s, remaining : 1597.91s\n",
      "*[0094/5000] tr_loss : 41.63354, val_loss : 38.67105, elapsed : 0.34s, total : 30.84s, remaining : 1654.64s\n",
      "*[0095/5000] tr_loss : 41.59350, val_loss : 38.62798, elapsed : 0.35s, total : 31.19s, remaining : 1714.55s\n",
      "*[0096/5000] tr_loss : 41.55718, val_loss : 38.59057, elapsed : 0.34s, total : 31.52s, remaining : 1645.22s\n",
      "*[0097/5000] tr_loss : 41.51936, val_loss : 38.55347, elapsed : 0.34s, total : 31.86s, remaining : 1665.06s\n",
      "*[0098/5000] tr_loss : 41.48029, val_loss : 38.50429, elapsed : 0.33s, total : 32.19s, remaining : 1610.95s\n",
      "*[0099/5000] tr_loss : 41.44069, val_loss : 38.46853, elapsed : 0.33s, total : 32.52s, remaining : 1626.41s\n",
      "*[0100/5000] tr_loss : 41.38863, val_loss : 38.42059, elapsed : 0.34s, total : 32.87s, remaining : 1670.29s\n",
      "*[0101/5000] tr_loss : 41.35063, val_loss : 38.36528, elapsed : 0.33s, total : 33.20s, remaining : 1615.09s\n",
      "*[0102/5000] tr_loss : 41.31714, val_loss : 38.32084, elapsed : 0.34s, total : 33.53s, remaining : 1660.55s\n",
      "*[0103/5000] tr_loss : 41.27242, val_loss : 38.27999, elapsed : 0.34s, total : 33.87s, remaining : 1655.62s\n",
      "*[0104/5000] tr_loss : 41.22618, val_loss : 38.22962, elapsed : 0.33s, total : 34.21s, remaining : 1630.99s\n",
      "*[0105/5000] tr_loss : 41.19999, val_loss : 38.18433, elapsed : 0.33s, total : 34.53s, remaining : 1598.96s\n",
      "*[0106/5000] tr_loss : 41.15104, val_loss : 38.12937, elapsed : 0.34s, total : 34.87s, remaining : 1648.88s\n",
      "*[0107/5000] tr_loss : 41.11715, val_loss : 38.07934, elapsed : 0.33s, total : 35.20s, remaining : 1614.50s\n",
      "*[0108/5000] tr_loss : 41.06745, val_loss : 38.03501, elapsed : 0.33s, total : 35.53s, remaining : 1606.27s\n",
      "*[0109/5000] tr_loss : 41.02425, val_loss : 37.99100, elapsed : 0.36s, total : 35.89s, remaining : 1757.40s\n",
      "*[0110/5000] tr_loss : 40.99518, val_loss : 37.94124, elapsed : 0.34s, total : 36.23s, remaining : 1657.12s\n",
      "*[0111/5000] tr_loss : 40.94336, val_loss : 37.88066, elapsed : 0.33s, total : 36.56s, remaining : 1620.89s\n",
      "*[0112/5000] tr_loss : 40.90228, val_loss : 37.83736, elapsed : 0.34s, total : 36.90s, remaining : 1654.14s\n",
      "*[0113/5000] tr_loss : 40.87077, val_loss : 37.79103, elapsed : 0.34s, total : 37.24s, remaining : 1667.33s\n",
      "*[0114/5000] tr_loss : 40.81559, val_loss : 37.73600, elapsed : 0.33s, total : 37.57s, remaining : 1634.58s\n",
      "*[0115/5000] tr_loss : 40.78890, val_loss : 37.69181, elapsed : 0.33s, total : 37.90s, remaining : 1613.92s\n",
      "*[0116/5000] tr_loss : 40.74148, val_loss : 37.62844, elapsed : 0.34s, total : 38.24s, remaining : 1642.31s\n",
      "*[0117/5000] tr_loss : 40.70678, val_loss : 37.58646, elapsed : 0.33s, total : 38.57s, remaining : 1627.47s\n",
      "*[0118/5000] tr_loss : 40.67825, val_loss : 37.54217, elapsed : 0.33s, total : 38.91s, remaining : 1631.30s\n",
      "*[0119/5000] tr_loss : 40.63722, val_loss : 37.49889, elapsed : 0.34s, total : 39.24s, remaining : 1653.16s\n",
      "*[0120/5000] tr_loss : 40.58865, val_loss : 37.44354, elapsed : 0.33s, total : 39.58s, remaining : 1628.98s\n",
      "*[0121/5000] tr_loss : 40.53933, val_loss : 37.40239, elapsed : 0.35s, total : 39.92s, remaining : 1683.35s\n",
      "*[0122/5000] tr_loss : 40.51346, val_loss : 37.35499, elapsed : 0.34s, total : 40.26s, remaining : 1659.67s\n",
      "*[0123/5000] tr_loss : 40.46206, val_loss : 37.33508, elapsed : 0.35s, total : 40.61s, remaining : 1693.88s\n",
      "*[0124/5000] tr_loss : 40.41596, val_loss : 37.30224, elapsed : 0.38s, total : 40.99s, remaining : 1844.87s\n",
      "*[0125/5000] tr_loss : 40.37891, val_loss : 37.26020, elapsed : 0.34s, total : 41.33s, remaining : 1674.34s\n",
      "*[0126/5000] tr_loss : 40.33085, val_loss : 37.21887, elapsed : 0.34s, total : 41.67s, remaining : 1659.10s\n",
      "*[0127/5000] tr_loss : 40.28033, val_loss : 37.19303, elapsed : 0.33s, total : 42.01s, remaining : 1624.41s\n",
      "*[0128/5000] tr_loss : 40.23576, val_loss : 37.16570, elapsed : 0.34s, total : 42.34s, remaining : 1641.09s\n",
      "*[0129/5000] tr_loss : 40.19220, val_loss : 37.13867, elapsed : 0.33s, total : 42.67s, remaining : 1600.52s\n",
      "*[0130/5000] tr_loss : 40.15056, val_loss : 37.08611, elapsed : 0.33s, total : 43.01s, remaining : 1623.52s\n",
      "*[0131/5000] tr_loss : 40.10973, val_loss : 37.04775, elapsed : 0.34s, total : 43.34s, remaining : 1636.45s\n",
      "*[0132/5000] tr_loss : 40.07410, val_loss : 37.00295, elapsed : 0.33s, total : 43.68s, remaining : 1620.63s\n",
      "*[0133/5000] tr_loss : 40.03566, val_loss : 36.96847, elapsed : 0.34s, total : 44.01s, remaining : 1651.50s\n",
      "*[0134/5000] tr_loss : 40.00044, val_loss : 36.91793, elapsed : 0.34s, total : 44.35s, remaining : 1645.68s\n",
      "*[0135/5000] tr_loss : 39.95972, val_loss : 36.87674, elapsed : 0.34s, total : 44.70s, remaining : 1673.87s\n",
      "*[0136/5000] tr_loss : 39.91435, val_loss : 36.83658, elapsed : 0.37s, total : 45.07s, remaining : 1803.87s\n",
      "*[0137/5000] tr_loss : 39.85982, val_loss : 36.81056, elapsed : 0.34s, total : 45.40s, remaining : 1638.98s\n",
      "*[0138/5000] tr_loss : 39.83235, val_loss : 36.77772, elapsed : 0.34s, total : 45.74s, remaining : 1634.39s\n",
      "*[0139/5000] tr_loss : 39.78918, val_loss : 36.75513, elapsed : 0.35s, total : 46.09s, remaining : 1691.40s\n",
      "*[0140/5000] tr_loss : 39.73714, val_loss : 36.72365, elapsed : 0.35s, total : 46.44s, remaining : 1704.84s\n",
      "*[0141/5000] tr_loss : 39.69395, val_loss : 36.69001, elapsed : 0.37s, total : 46.81s, remaining : 1802.01s\n",
      "*[0142/5000] tr_loss : 39.62948, val_loss : 36.66334, elapsed : 0.35s, total : 47.16s, remaining : 1691.26s\n",
      "*[0143/5000] tr_loss : 39.57457, val_loss : 36.61015, elapsed : 0.33s, total : 47.49s, remaining : 1625.48s\n",
      "*[0144/5000] tr_loss : 39.52755, val_loss : 36.57386, elapsed : 0.36s, total : 47.85s, remaining : 1724.10s\n",
      "*[0145/5000] tr_loss : 39.46650, val_loss : 36.50126, elapsed : 0.33s, total : 48.18s, remaining : 1611.87s\n",
      "*[0146/5000] tr_loss : 39.42387, val_loss : 36.40904, elapsed : 0.34s, total : 48.52s, remaining : 1626.64s\n",
      "*[0147/5000] tr_loss : 39.34645, val_loss : 36.31255, elapsed : 0.34s, total : 48.85s, remaining : 1643.29s\n",
      "*[0148/5000] tr_loss : 39.27793, val_loss : 36.21395, elapsed : 0.34s, total : 49.19s, remaining : 1637.80s\n",
      "*[0149/5000] tr_loss : 39.23269, val_loss : 36.11297, elapsed : 0.34s, total : 49.53s, remaining : 1651.15s\n",
      "*[0150/5000] tr_loss : 39.15785, val_loss : 36.00627, elapsed : 0.34s, total : 49.87s, remaining : 1639.59s\n",
      "*[0151/5000] tr_loss : 39.09774, val_loss : 35.92374, elapsed : 0.33s, total : 50.20s, remaining : 1585.64s\n",
      "*[0152/5000] tr_loss : 39.05168, val_loss : 35.82914, elapsed : 0.33s, total : 50.53s, remaining : 1586.76s\n",
      "*[0153/5000] tr_loss : 38.99665, val_loss : 35.76596, elapsed : 0.33s, total : 50.86s, remaining : 1608.38s\n",
      "*[0154/5000] tr_loss : 38.95669, val_loss : 35.67954, elapsed : 0.34s, total : 51.19s, remaining : 1624.00s\n",
      "*[0155/5000] tr_loss : 38.90725, val_loss : 35.61576, elapsed : 0.33s, total : 51.52s, remaining : 1586.72s\n",
      "*[0156/5000] tr_loss : 38.85656, val_loss : 35.55778, elapsed : 0.33s, total : 51.85s, remaining : 1618.62s\n",
      "*[0157/5000] tr_loss : 38.81893, val_loss : 35.48970, elapsed : 0.35s, total : 52.21s, remaining : 1705.81s\n",
      "*[0158/5000] tr_loss : 38.77587, val_loss : 35.43378, elapsed : 0.34s, total : 52.55s, remaining : 1654.00s\n",
      "*[0159/5000] tr_loss : 38.73365, val_loss : 35.36520, elapsed : 0.34s, total : 52.89s, remaining : 1633.91s\n",
      "*[0160/5000] tr_loss : 38.69935, val_loss : 35.32386, elapsed : 0.33s, total : 53.22s, remaining : 1613.91s\n",
      "*[0161/5000] tr_loss : 38.65590, val_loss : 35.26797, elapsed : 0.33s, total : 53.55s, remaining : 1600.05s\n",
      "*[0162/5000] tr_loss : 38.63297, val_loss : 35.22121, elapsed : 0.34s, total : 53.89s, remaining : 1650.56s\n",
      "*[0163/5000] tr_loss : 38.59185, val_loss : 35.16271, elapsed : 0.34s, total : 54.23s, remaining : 1630.14s\n",
      "*[0164/5000] tr_loss : 38.55147, val_loss : 35.11064, elapsed : 0.36s, total : 54.58s, remaining : 1717.98s\n",
      "*[0165/5000] tr_loss : 38.51832, val_loss : 35.05224, elapsed : 0.34s, total : 54.92s, remaining : 1624.08s\n",
      "*[0166/5000] tr_loss : 38.49076, val_loss : 35.02603, elapsed : 0.34s, total : 55.25s, remaining : 1621.78s\n",
      "*[0167/5000] tr_loss : 38.44883, val_loss : 34.97322, elapsed : 0.34s, total : 55.59s, remaining : 1621.41s\n",
      "*[0168/5000] tr_loss : 38.42047, val_loss : 34.92354, elapsed : 0.33s, total : 55.93s, remaining : 1618.58s\n",
      "*[0169/5000] tr_loss : 38.40339, val_loss : 34.89143, elapsed : 0.34s, total : 56.26s, remaining : 1626.79s\n",
      "*[0170/5000] tr_loss : 38.36640, val_loss : 34.83620, elapsed : 0.34s, total : 56.60s, remaining : 1653.04s\n",
      "*[0171/5000] tr_loss : 38.33821, val_loss : 34.81114, elapsed : 0.34s, total : 56.94s, remaining : 1637.24s\n",
      "*[0172/5000] tr_loss : 38.30609, val_loss : 34.76631, elapsed : 0.34s, total : 57.28s, remaining : 1628.43s\n",
      "*[0173/5000] tr_loss : 38.28019, val_loss : 34.71320, elapsed : 0.33s, total : 57.61s, remaining : 1603.36s\n",
      "*[0174/5000] tr_loss : 38.25216, val_loss : 34.69180, elapsed : 0.33s, total : 57.94s, remaining : 1582.95s\n",
      "*[0175/5000] tr_loss : 38.22735, val_loss : 34.64797, elapsed : 0.33s, total : 58.27s, remaining : 1574.22s\n",
      "*[0176/5000] tr_loss : 38.19281, val_loss : 34.61351, elapsed : 0.37s, total : 58.64s, remaining : 1800.62s\n",
      "*[0177/5000] tr_loss : 38.16952, val_loss : 34.57560, elapsed : 0.34s, total : 58.98s, remaining : 1622.75s\n",
      "*[0178/5000] tr_loss : 38.14253, val_loss : 34.53884, elapsed : 0.33s, total : 59.31s, remaining : 1596.87s\n",
      "*[0179/5000] tr_loss : 38.12917, val_loss : 34.49269, elapsed : 0.33s, total : 59.64s, remaining : 1580.28s\n",
      "*[0180/5000] tr_loss : 38.09480, val_loss : 34.45602, elapsed : 0.33s, total : 59.96s, remaining : 1574.61s\n",
      "*[0181/5000] tr_loss : 38.07964, val_loss : 34.42955, elapsed : 0.33s, total : 60.29s, remaining : 1597.76s\n",
      "*[0182/5000] tr_loss : 38.05291, val_loss : 34.40297, elapsed : 0.33s, total : 60.63s, remaining : 1613.05s\n",
      "*[0183/5000] tr_loss : 38.02909, val_loss : 34.36287, elapsed : 0.33s, total : 60.96s, remaining : 1605.84s\n",
      "*[0184/5000] tr_loss : 38.00498, val_loss : 34.33239, elapsed : 0.33s, total : 61.29s, remaining : 1599.96s\n",
      "*[0185/5000] tr_loss : 37.98886, val_loss : 34.29946, elapsed : 0.33s, total : 61.63s, remaining : 1597.62s\n",
      "*[0186/5000] tr_loss : 37.96418, val_loss : 34.28032, elapsed : 0.34s, total : 61.97s, remaining : 1629.57s\n",
      "*[0187/5000] tr_loss : 37.94615, val_loss : 34.25241, elapsed : 0.33s, total : 62.30s, remaining : 1607.88s\n",
      "*[0188/5000] tr_loss : 37.91895, val_loss : 34.22639, elapsed : 0.33s, total : 62.63s, remaining : 1592.72s\n",
      "*[0189/5000] tr_loss : 37.90408, val_loss : 34.19912, elapsed : 0.33s, total : 62.96s, remaining : 1582.82s\n",
      "*[0190/5000] tr_loss : 37.86853, val_loss : 34.16399, elapsed : 0.35s, total : 63.30s, remaining : 1660.97s\n",
      "*[0191/5000] tr_loss : 37.87294, val_loss : 34.14238, elapsed : 0.35s, total : 63.65s, remaining : 1684.12s\n",
      "*[0192/5000] tr_loss : 37.83292, val_loss : 34.10982, elapsed : 0.34s, total : 63.99s, remaining : 1624.83s\n",
      "*[0193/5000] tr_loss : 37.82287, val_loss : 34.08574, elapsed : 0.35s, total : 64.34s, remaining : 1677.31s\n",
      "*[0194/5000] tr_loss : 37.80004, val_loss : 34.05991, elapsed : 0.33s, total : 64.67s, remaining : 1567.40s\n",
      "*[0195/5000] tr_loss : 37.77708, val_loss : 34.02451, elapsed : 0.33s, total : 65.00s, remaining : 1574.48s\n",
      "*[0196/5000] tr_loss : 37.75659, val_loss : 34.00073, elapsed : 0.33s, total : 65.33s, remaining : 1594.82s\n",
      "*[0197/5000] tr_loss : 37.73954, val_loss : 33.99079, elapsed : 0.32s, total : 65.65s, remaining : 1557.45s\n",
      "*[0198/5000] tr_loss : 37.72457, val_loss : 33.95609, elapsed : 0.33s, total : 65.98s, remaining : 1592.65s\n",
      "*[0199/5000] tr_loss : 37.70947, val_loss : 33.92650, elapsed : 0.33s, total : 66.31s, remaining : 1564.26s\n",
      "*[0200/5000] tr_loss : 37.69795, val_loss : 33.90389, elapsed : 0.34s, total : 66.65s, remaining : 1609.95s\n",
      "*[0201/5000] tr_loss : 37.66634, val_loss : 33.87849, elapsed : 0.33s, total : 66.98s, remaining : 1592.74s\n",
      "*[0202/5000] tr_loss : 37.65463, val_loss : 33.85433, elapsed : 0.33s, total : 67.31s, remaining : 1602.30s\n",
      "*[0203/5000] tr_loss : 37.63919, val_loss : 33.83193, elapsed : 0.34s, total : 67.65s, remaining : 1616.69s\n",
      "*[0204/5000] tr_loss : 37.62729, val_loss : 33.80285, elapsed : 0.33s, total : 67.98s, remaining : 1596.06s\n",
      "*[0205/5000] tr_loss : 37.60126, val_loss : 33.78008, elapsed : 0.33s, total : 68.31s, remaining : 1583.84s\n",
      "*[0206/5000] tr_loss : 37.58894, val_loss : 33.75590, elapsed : 0.33s, total : 68.64s, remaining : 1567.56s\n",
      "*[0207/5000] tr_loss : 37.58133, val_loss : 33.72932, elapsed : 0.33s, total : 68.97s, remaining : 1579.04s\n",
      "*[0208/5000] tr_loss : 37.55794, val_loss : 33.70317, elapsed : 0.33s, total : 69.29s, remaining : 1564.91s\n",
      "*[0209/5000] tr_loss : 37.55155, val_loss : 33.68534, elapsed : 0.33s, total : 69.62s, remaining : 1560.84s\n",
      "*[0210/5000] tr_loss : 37.53078, val_loss : 33.67418, elapsed : 0.33s, total : 69.95s, remaining : 1563.84s\n",
      "*[0211/5000] tr_loss : 37.50831, val_loss : 33.64004, elapsed : 0.33s, total : 70.28s, remaining : 1573.21s\n",
      "*[0212/5000] tr_loss : 37.49193, val_loss : 33.61395, elapsed : 0.32s, total : 70.60s, remaining : 1548.45s\n",
      "*[0213/5000] tr_loss : 37.47777, val_loss : 33.60251, elapsed : 0.34s, total : 70.94s, remaining : 1609.37s\n",
      "*[0214/5000] tr_loss : 37.46811, val_loss : 33.57504, elapsed : 0.33s, total : 71.27s, remaining : 1585.06s\n",
      "*[0215/5000] tr_loss : 37.46767, val_loss : 33.55279, elapsed : 0.33s, total : 71.59s, remaining : 1562.68s\n",
      "*[0216/5000] tr_loss : 37.46955, val_loss : 33.53533, elapsed : 0.37s, total : 71.96s, remaining : 1752.23s\n",
      "*[0217/5000] tr_loss : 37.44311, val_loss : 33.51932, elapsed : 0.33s, total : 72.29s, remaining : 1570.07s\n",
      "*[0218/5000] tr_loss : 37.42018, val_loss : 33.49581, elapsed : 0.33s, total : 72.62s, remaining : 1572.40s\n",
      "*[0219/5000] tr_loss : 37.40312, val_loss : 33.47260, elapsed : 0.33s, total : 72.95s, remaining : 1586.86s\n",
      "*[0220/5000] tr_loss : 37.39670, val_loss : 33.45523, elapsed : 0.33s, total : 73.28s, remaining : 1564.65s\n",
      "*[0221/5000] tr_loss : 37.37419, val_loss : 33.43557, elapsed : 0.33s, total : 73.60s, remaining : 1553.92s\n",
      "*[0222/5000] tr_loss : 37.37091, val_loss : 33.41085, elapsed : 0.33s, total : 73.93s, remaining : 1577.15s\n",
      "*[0223/5000] tr_loss : 37.36012, val_loss : 33.39962, elapsed : 0.33s, total : 74.26s, remaining : 1575.83s\n",
      "*[0224/5000] tr_loss : 37.35001, val_loss : 33.37950, elapsed : 0.33s, total : 74.59s, remaining : 1553.54s\n",
      "*[0225/5000] tr_loss : 37.32852, val_loss : 33.36376, elapsed : 0.33s, total : 74.92s, remaining : 1577.04s\n",
      "*[0226/5000] tr_loss : 37.31729, val_loss : 33.33418, elapsed : 0.33s, total : 75.25s, remaining : 1571.19s\n",
      "*[0227/5000] tr_loss : 37.32130, val_loss : 33.31909, elapsed : 0.33s, total : 75.58s, remaining : 1583.59s\n",
      "*[0228/5000] tr_loss : 37.30676, val_loss : 33.30482, elapsed : 0.33s, total : 75.91s, remaining : 1580.89s\n",
      "*[0229/5000] tr_loss : 37.29505, val_loss : 33.28736, elapsed : 0.34s, total : 76.24s, remaining : 1602.07s\n",
      "*[0230/5000] tr_loss : 37.28459, val_loss : 33.26384, elapsed : 0.33s, total : 76.57s, remaining : 1572.93s\n",
      "*[0231/5000] tr_loss : 37.26781, val_loss : 33.25283, elapsed : 0.33s, total : 76.90s, remaining : 1572.06s\n",
      "*[0232/5000] tr_loss : 37.25686, val_loss : 33.23122, elapsed : 0.33s, total : 77.23s, remaining : 1567.15s\n",
      "*[0233/5000] tr_loss : 37.25851, val_loss : 33.21820, elapsed : 0.33s, total : 77.56s, remaining : 1571.74s\n",
      "*[0234/5000] tr_loss : 37.23815, val_loss : 33.20619, elapsed : 0.33s, total : 77.90s, remaining : 1594.58s\n",
      "*[0235/5000] tr_loss : 37.22269, val_loss : 33.18125, elapsed : 0.33s, total : 78.23s, remaining : 1562.58s\n",
      "*[0236/5000] tr_loss : 37.21504, val_loss : 33.16994, elapsed : 0.32s, total : 78.55s, remaining : 1543.80s\n",
      "*[0237/5000] tr_loss : 37.20972, val_loss : 33.15377, elapsed : 0.33s, total : 78.88s, remaining : 1582.20s\n",
      "*[0238/5000] tr_loss : 37.20362, val_loss : 33.14179, elapsed : 0.32s, total : 79.21s, remaining : 1543.58s\n",
      "*[0239/5000] tr_loss : 37.18743, val_loss : 33.12406, elapsed : 0.33s, total : 79.53s, remaining : 1552.74s\n",
      "*[0240/5000] tr_loss : 37.18413, val_loss : 33.09930, elapsed : 0.33s, total : 79.86s, remaining : 1566.10s\n",
      "*[0241/5000] tr_loss : 37.18226, val_loss : 33.09004, elapsed : 0.33s, total : 80.19s, remaining : 1575.66s\n",
      "*[0242/5000] tr_loss : 37.16726, val_loss : 33.07855, elapsed : 0.33s, total : 80.52s, remaining : 1554.25s\n",
      "*[0243/5000] tr_loss : 37.15119, val_loss : 33.06168, elapsed : 0.34s, total : 80.86s, remaining : 1600.66s\n",
      "*[0244/5000] tr_loss : 37.14999, val_loss : 33.05488, elapsed : 0.34s, total : 81.20s, remaining : 1616.33s\n",
      "*[0245/5000] tr_loss : 37.12618, val_loss : 33.03072, elapsed : 0.32s, total : 81.52s, remaining : 1544.41s\n",
      "*[0246/5000] tr_loss : 37.13035, val_loss : 33.00887, elapsed : 0.33s, total : 81.85s, remaining : 1570.52s\n",
      "*[0247/5000] tr_loss : 37.12400, val_loss : 33.00359, elapsed : 0.34s, total : 82.19s, remaining : 1624.24s\n",
      "*[0248/5000] tr_loss : 37.11224, val_loss : 32.99162, elapsed : 0.33s, total : 82.52s, remaining : 1551.47s\n",
      "*[0249/5000] tr_loss : 37.11704, val_loss : 32.97069, elapsed : 0.34s, total : 82.86s, remaining : 1598.79s\n",
      "*[0250/5000] tr_loss : 37.09115, val_loss : 32.95932, elapsed : 0.34s, total : 83.19s, remaining : 1604.18s\n",
      "*[0251/5000] tr_loss : 37.08999, val_loss : 32.94910, elapsed : 0.33s, total : 83.52s, remaining : 1570.54s\n",
      "*[0252/5000] tr_loss : 37.06738, val_loss : 32.94172, elapsed : 0.34s, total : 83.86s, remaining : 1611.90s\n",
      "*[0253/5000] tr_loss : 37.06883, val_loss : 32.92232, elapsed : 0.34s, total : 84.20s, remaining : 1594.02s\n",
      "*[0254/5000] tr_loss : 37.07165, val_loss : 32.91200, elapsed : 0.34s, total : 84.54s, remaining : 1615.10s\n",
      "*[0255/5000] tr_loss : 37.05472, val_loss : 32.90369, elapsed : 0.33s, total : 84.87s, remaining : 1579.77s\n",
      "*[0256/5000] tr_loss : 37.05261, val_loss : 32.88088, elapsed : 0.34s, total : 85.21s, remaining : 1607.01s\n",
      "*[0257/5000] tr_loss : 37.03649, val_loss : 32.86612, elapsed : 0.34s, total : 85.55s, remaining : 1593.50s\n",
      "*[0258/5000] tr_loss : 37.03803, val_loss : 32.86103, elapsed : 0.34s, total : 85.88s, remaining : 1590.17s\n",
      "*[0259/5000] tr_loss : 37.02344, val_loss : 32.84845, elapsed : 0.33s, total : 86.21s, remaining : 1574.31s\n",
      "*[0260/5000] tr_loss : 37.03539, val_loss : 32.83615, elapsed : 0.33s, total : 86.55s, remaining : 1575.61s\n",
      "*[0261/5000] tr_loss : 37.01358, val_loss : 32.81587, elapsed : 0.33s, total : 86.88s, remaining : 1568.28s\n",
      "*[0262/5000] tr_loss : 36.99984, val_loss : 32.80999, elapsed : 0.33s, total : 87.21s, remaining : 1552.86s\n",
      "*[0263/5000] tr_loss : 37.00197, val_loss : 32.79399, elapsed : 0.33s, total : 87.54s, remaining : 1577.13s\n",
      "*[0264/5000] tr_loss : 36.99707, val_loss : 32.78315, elapsed : 0.34s, total : 87.88s, remaining : 1620.28s\n",
      "*[0265/5000] tr_loss : 37.00840, val_loss : 32.77318, elapsed : 0.33s, total : 88.21s, remaining : 1572.17s\n",
      "*[0266/5000] tr_loss : 36.98484, val_loss : 32.75715, elapsed : 0.33s, total : 88.55s, remaining : 1580.95s\n",
      "*[0267/5000] tr_loss : 36.98501, val_loss : 32.74739, elapsed : 0.33s, total : 88.87s, remaining : 1539.00s\n",
      "*[0268/5000] tr_loss : 36.96170, val_loss : 32.73032, elapsed : 0.33s, total : 89.21s, remaining : 1576.32s\n",
      "*[0269/5000] tr_loss : 36.95734, val_loss : 32.71869, elapsed : 0.33s, total : 89.53s, remaining : 1550.22s\n",
      "*[0270/5000] tr_loss : 36.96346, val_loss : 32.70589, elapsed : 0.33s, total : 89.86s, remaining : 1542.77s\n",
      "*[0271/5000] tr_loss : 36.95147, val_loss : 32.69828, elapsed : 0.34s, total : 90.20s, remaining : 1589.93s\n",
      "*[0272/5000] tr_loss : 36.94563, val_loss : 32.69295, elapsed : 0.33s, total : 90.53s, remaining : 1556.12s\n",
      "*[0273/5000] tr_loss : 36.94808, val_loss : 32.66578, elapsed : 0.35s, total : 90.88s, remaining : 1664.59s\n",
      "*[0274/5000] tr_loss : 36.94186, val_loss : 32.66407, elapsed : 0.33s, total : 91.21s, remaining : 1563.70s\n",
      "*[0275/5000] tr_loss : 36.93946, val_loss : 32.64973, elapsed : 0.33s, total : 91.54s, remaining : 1552.77s\n",
      "*[0276/5000] tr_loss : 36.91308, val_loss : 32.63673, elapsed : 0.33s, total : 91.87s, remaining : 1576.03s\n",
      "*[0277/5000] tr_loss : 36.92234, val_loss : 32.62809, elapsed : 0.33s, total : 92.20s, remaining : 1557.22s\n",
      "*[0278/5000] tr_loss : 36.90570, val_loss : 32.62035, elapsed : 0.33s, total : 92.53s, remaining : 1567.95s\n",
      "*[0279/5000] tr_loss : 36.90915, val_loss : 32.60748, elapsed : 0.33s, total : 92.87s, remaining : 1574.60s\n",
      "*[0280/5000] tr_loss : 36.90463, val_loss : 32.60338, elapsed : 0.32s, total : 93.19s, remaining : 1533.33s\n",
      "*[0281/5000] tr_loss : 36.90353, val_loss : 32.58768, elapsed : 0.33s, total : 93.52s, remaining : 1549.28s\n",
      "*[0282/5000] tr_loss : 36.90044, val_loss : 32.58004, elapsed : 0.33s, total : 93.85s, remaining : 1543.85s\n",
      "*[0283/5000] tr_loss : 36.88726, val_loss : 32.57320, elapsed : 0.33s, total : 94.18s, remaining : 1553.53s\n",
      "*[0284/5000] tr_loss : 36.88538, val_loss : 32.56403, elapsed : 0.34s, total : 94.52s, remaining : 1606.91s\n",
      "*[0285/5000] tr_loss : 36.87407, val_loss : 32.54888, elapsed : 0.33s, total : 94.85s, remaining : 1560.84s\n",
      "*[0286/5000] tr_loss : 36.87094, val_loss : 32.53389, elapsed : 0.33s, total : 95.17s, remaining : 1540.72s\n",
      "*[0287/5000] tr_loss : 36.87270, val_loss : 32.52809, elapsed : 0.33s, total : 95.50s, remaining : 1555.54s\n",
      "*[0288/5000] tr_loss : 36.86502, val_loss : 32.52179, elapsed : 0.33s, total : 95.83s, remaining : 1554.80s\n",
      "*[0289/5000] tr_loss : 36.86633, val_loss : 32.50964, elapsed : 0.33s, total : 96.16s, remaining : 1539.26s\n",
      "*[0290/5000] tr_loss : 36.85151, val_loss : 32.49252, elapsed : 0.32s, total : 96.49s, remaining : 1524.89s\n",
      "*[0291/5000] tr_loss : 36.84500, val_loss : 32.48836, elapsed : 0.33s, total : 96.81s, remaining : 1542.84s\n",
      "*[0292/5000] tr_loss : 36.85122, val_loss : 32.48060, elapsed : 0.33s, total : 97.14s, remaining : 1548.33s\n",
      "*[0293/5000] tr_loss : 36.84916, val_loss : 32.47716, elapsed : 0.33s, total : 97.47s, remaining : 1546.67s\n",
      "*[0294/5000] tr_loss : 36.83458, val_loss : 32.46651, elapsed : 0.33s, total : 97.80s, remaining : 1552.23s\n",
      "*[0295/5000] tr_loss : 36.82896, val_loss : 32.46020, elapsed : 0.33s, total : 98.13s, remaining : 1537.35s\n",
      "*[0296/5000] tr_loss : 36.82647, val_loss : 32.45526, elapsed : 0.33s, total : 98.45s, remaining : 1538.55s\n",
      "*[0297/5000] tr_loss : 36.81863, val_loss : 32.44308, elapsed : 0.33s, total : 98.78s, remaining : 1544.32s\n",
      "*[0298/5000] tr_loss : 36.81966, val_loss : 32.43376, elapsed : 0.33s, total : 99.11s, remaining : 1539.43s\n",
      "*[0299/5000] tr_loss : 36.82025, val_loss : 32.43102, elapsed : 0.33s, total : 99.44s, remaining : 1563.21s\n",
      "*[0300/5000] tr_loss : 36.81328, val_loss : 32.42091, elapsed : 0.33s, total : 99.77s, remaining : 1537.65s\n",
      "*[0301/5000] tr_loss : 36.81768, val_loss : 32.41480, elapsed : 0.33s, total : 100.10s, remaining : 1561.88s\n",
      "*[0302/5000] tr_loss : 36.81393, val_loss : 32.39948, elapsed : 0.33s, total : 100.43s, remaining : 1540.95s\n",
      "*[0303/5000] tr_loss : 36.80900, val_loss : 32.39576, elapsed : 0.34s, total : 100.77s, remaining : 1601.55s\n",
      "*[0304/5000] tr_loss : 36.78420, val_loss : 32.38710, elapsed : 0.35s, total : 101.12s, remaining : 1642.89s\n",
      "*[0305/5000] tr_loss : 36.79290, val_loss : 32.37932, elapsed : 0.35s, total : 101.47s, remaining : 1626.28s\n",
      "*[0306/5000] tr_loss : 36.78034, val_loss : 32.36973, elapsed : 0.34s, total : 101.81s, remaining : 1611.17s\n",
      "*[0307/5000] tr_loss : 36.78629, val_loss : 32.36175, elapsed : 0.34s, total : 102.15s, remaining : 1583.84s\n",
      "*[0308/5000] tr_loss : 36.79613, val_loss : 32.36011, elapsed : 0.33s, total : 102.48s, remaining : 1560.05s\n",
      "*[0309/5000] tr_loss : 36.77729, val_loss : 32.34821, elapsed : 0.33s, total : 102.81s, remaining : 1545.08s\n",
      "*[0310/5000] tr_loss : 36.76983, val_loss : 32.34196, elapsed : 0.34s, total : 103.15s, remaining : 1591.02s\n",
      "*[0311/5000] tr_loss : 36.77260, val_loss : 32.34026, elapsed : 0.33s, total : 103.48s, remaining : 1535.88s\n",
      "*[0312/5000] tr_loss : 36.77235, val_loss : 32.33073, elapsed : 0.34s, total : 103.82s, remaining : 1590.15s\n",
      "*[0313/5000] tr_loss : 36.75999, val_loss : 32.32644, elapsed : 0.33s, total : 104.15s, remaining : 1540.50s\n",
      "*[0314/5000] tr_loss : 36.77050, val_loss : 32.31137, elapsed : 0.34s, total : 104.49s, remaining : 1602.13s\n",
      "*[0315/5000] tr_loss : 36.76976, val_loss : 32.30315, elapsed : 0.35s, total : 104.83s, remaining : 1620.83s\n",
      " [0316/5000] tr_loss : 36.75561, val_loss : 32.30321, elapsed : 0.36s, total : 105.19s, remaining : 1666.48s\n",
      "*[0317/5000] tr_loss : 36.75647, val_loss : 32.28681, elapsed : 0.33s, total : 105.52s, remaining : 1557.48s\n",
      "*[0318/5000] tr_loss : 36.75563, val_loss : 32.27763, elapsed : 0.33s, total : 105.85s, remaining : 1542.46s\n",
      "*[0319/5000] tr_loss : 36.74751, val_loss : 32.27386, elapsed : 0.33s, total : 106.18s, remaining : 1543.07s\n",
      "*[0320/5000] tr_loss : 36.74404, val_loss : 32.26733, elapsed : 0.34s, total : 106.53s, remaining : 1612.14s\n",
      "*[0321/5000] tr_loss : 36.74929, val_loss : 32.26699, elapsed : 0.35s, total : 106.87s, remaining : 1632.37s\n",
      "*[0322/5000] tr_loss : 36.74301, val_loss : 32.25341, elapsed : 0.38s, total : 107.25s, remaining : 1778.07s\n",
      "*[0323/5000] tr_loss : 36.73183, val_loss : 32.24630, elapsed : 0.35s, total : 107.60s, remaining : 1623.09s\n",
      "*[0324/5000] tr_loss : 36.73130, val_loss : 32.24401, elapsed : 0.35s, total : 107.95s, remaining : 1640.51s\n",
      "*[0325/5000] tr_loss : 36.73049, val_loss : 32.23547, elapsed : 0.34s, total : 108.29s, remaining : 1597.25s\n",
      "*[0326/5000] tr_loss : 36.73565, val_loss : 32.23355, elapsed : 0.34s, total : 108.64s, remaining : 1610.43s\n",
      "*[0327/5000] tr_loss : 36.74155, val_loss : 32.21897, elapsed : 0.35s, total : 108.98s, remaining : 1617.79s\n",
      "*[0328/5000] tr_loss : 36.72529, val_loss : 32.21745, elapsed : 0.34s, total : 109.33s, remaining : 1592.67s\n",
      "*[0329/5000] tr_loss : 36.71806, val_loss : 32.20806, elapsed : 0.33s, total : 109.66s, remaining : 1548.26s\n",
      "*[0330/5000] tr_loss : 36.70890, val_loss : 32.20076, elapsed : 0.35s, total : 110.01s, remaining : 1623.72s\n",
      "*[0331/5000] tr_loss : 36.72026, val_loss : 32.19580, elapsed : 0.34s, total : 110.34s, remaining : 1579.34s\n",
      "*[0332/5000] tr_loss : 36.70249, val_loss : 32.18628, elapsed : 0.35s, total : 110.69s, remaining : 1625.01s\n",
      " [0333/5000] tr_loss : 36.70853, val_loss : 32.18809, elapsed : 0.34s, total : 111.03s, remaining : 1588.35s\n",
      "*[0334/5000] tr_loss : 36.70287, val_loss : 32.18019, elapsed : 0.33s, total : 111.36s, remaining : 1553.47s\n",
      "*[0335/5000] tr_loss : 36.70224, val_loss : 32.16930, elapsed : 0.34s, total : 111.71s, remaining : 1602.42s\n",
      "*[0336/5000] tr_loss : 36.69116, val_loss : 32.15951, elapsed : 0.34s, total : 112.05s, remaining : 1603.78s\n",
      "*[0337/5000] tr_loss : 36.69130, val_loss : 32.15888, elapsed : 0.33s, total : 112.39s, remaining : 1558.80s\n",
      "*[0338/5000] tr_loss : 36.69139, val_loss : 32.15095, elapsed : 0.33s, total : 112.72s, remaining : 1554.42s\n",
      "*[0339/5000] tr_loss : 36.68949, val_loss : 32.14362, elapsed : 0.38s, total : 113.10s, remaining : 1767.05s\n",
      "*[0340/5000] tr_loss : 36.69327, val_loss : 32.14200, elapsed : 0.38s, total : 113.48s, remaining : 1786.09s\n",
      "*[0341/5000] tr_loss : 36.68409, val_loss : 32.13135, elapsed : 0.37s, total : 113.86s, remaining : 1738.38s\n",
      " [0342/5000] tr_loss : 36.68488, val_loss : 32.13281, elapsed : 0.35s, total : 114.21s, remaining : 1629.67s\n",
      "*[0343/5000] tr_loss : 36.68140, val_loss : 32.12249, elapsed : 0.36s, total : 114.57s, remaining : 1674.97s\n",
      "*[0344/5000] tr_loss : 36.68405, val_loss : 32.12072, elapsed : 0.34s, total : 114.90s, remaining : 1564.58s\n",
      "*[0345/5000] tr_loss : 36.67483, val_loss : 32.10971, elapsed : 0.34s, total : 115.25s, remaining : 1602.57s\n",
      "*[0346/5000] tr_loss : 36.67010, val_loss : 32.10782, elapsed : 0.35s, total : 115.59s, remaining : 1612.05s\n",
      "*[0347/5000] tr_loss : 36.67503, val_loss : 32.10243, elapsed : 0.33s, total : 115.93s, remaining : 1558.31s\n",
      "*[0348/5000] tr_loss : 36.65209, val_loss : 32.09436, elapsed : 0.34s, total : 116.27s, remaining : 1593.15s\n",
      "*[0349/5000] tr_loss : 36.66017, val_loss : 32.08561, elapsed : 0.34s, total : 116.61s, remaining : 1581.60s\n",
      "*[0350/5000] tr_loss : 36.65946, val_loss : 32.08411, elapsed : 0.33s, total : 116.94s, remaining : 1523.36s\n",
      "*[0351/5000] tr_loss : 36.65814, val_loss : 32.08112, elapsed : 0.35s, total : 117.29s, remaining : 1625.83s\n",
      "*[0352/5000] tr_loss : 36.66870, val_loss : 32.07451, elapsed : 0.34s, total : 117.62s, remaining : 1565.65s\n",
      "*[0353/5000] tr_loss : 36.65522, val_loss : 32.05705, elapsed : 0.33s, total : 117.96s, remaining : 1548.13s\n",
      " [0354/5000] tr_loss : 36.65913, val_loss : 32.06165, elapsed : 0.33s, total : 118.28s, remaining : 1519.55s\n",
      " [0355/5000] tr_loss : 36.65452, val_loss : 32.05941, elapsed : 0.32s, total : 118.61s, remaining : 1508.12s\n",
      "*[0356/5000] tr_loss : 36.65444, val_loss : 32.05623, elapsed : 0.33s, total : 118.93s, remaining : 1510.04s\n",
      "*[0357/5000] tr_loss : 36.64849, val_loss : 32.04205, elapsed : 0.33s, total : 119.26s, remaining : 1529.28s\n",
      "*[0358/5000] tr_loss : 36.64766, val_loss : 32.03936, elapsed : 0.33s, total : 119.59s, remaining : 1525.54s\n",
      " [0359/5000] tr_loss : 36.65027, val_loss : 32.04103, elapsed : 0.33s, total : 119.92s, remaining : 1508.33s\n",
      "*[0360/5000] tr_loss : 36.65560, val_loss : 32.03182, elapsed : 0.33s, total : 120.25s, remaining : 1519.85s\n",
      "*[0361/5000] tr_loss : 36.65105, val_loss : 32.02644, elapsed : 0.33s, total : 120.58s, remaining : 1543.28s\n",
      "*[0362/5000] tr_loss : 36.63467, val_loss : 32.02354, elapsed : 0.33s, total : 120.90s, remaining : 1517.46s\n",
      "*[0363/5000] tr_loss : 36.63115, val_loss : 32.02017, elapsed : 0.33s, total : 121.24s, remaining : 1532.79s\n",
      "*[0364/5000] tr_loss : 36.62888, val_loss : 32.01715, elapsed : 0.35s, total : 121.58s, remaining : 1606.19s\n",
      "*[0365/5000] tr_loss : 36.63934, val_loss : 32.00705, elapsed : 0.34s, total : 121.92s, remaining : 1587.21s\n",
      "*[0366/5000] tr_loss : 36.64173, val_loss : 31.99934, elapsed : 0.33s, total : 122.26s, remaining : 1545.32s\n",
      "*[0367/5000] tr_loss : 36.64324, val_loss : 31.99229, elapsed : 0.33s, total : 122.59s, remaining : 1524.11s\n",
      " [0368/5000] tr_loss : 36.63195, val_loss : 31.99822, elapsed : 0.33s, total : 122.92s, remaining : 1532.36s\n",
      "*[0369/5000] tr_loss : 36.62793, val_loss : 31.99216, elapsed : 0.33s, total : 123.25s, remaining : 1533.44s\n",
      "*[0370/5000] tr_loss : 36.63017, val_loss : 31.98871, elapsed : 0.33s, total : 123.58s, remaining : 1538.15s\n",
      "*[0371/5000] tr_loss : 36.61770, val_loss : 31.98371, elapsed : 0.33s, total : 123.91s, remaining : 1521.22s\n",
      "*[0372/5000] tr_loss : 36.62025, val_loss : 31.97788, elapsed : 0.33s, total : 124.24s, remaining : 1511.77s\n",
      "*[0373/5000] tr_loss : 36.62002, val_loss : 31.97637, elapsed : 0.33s, total : 124.57s, remaining : 1525.47s\n",
      "*[0374/5000] tr_loss : 36.61629, val_loss : 31.97013, elapsed : 0.34s, total : 124.90s, remaining : 1553.47s\n",
      "*[0375/5000] tr_loss : 36.60928, val_loss : 31.96175, elapsed : 0.35s, total : 125.25s, remaining : 1620.19s\n",
      "*[0376/5000] tr_loss : 36.61904, val_loss : 31.96050, elapsed : 0.38s, total : 125.63s, remaining : 1744.09s\n",
      "*[0377/5000] tr_loss : 36.60729, val_loss : 31.96032, elapsed : 0.37s, total : 126.00s, remaining : 1724.02s\n",
      "*[0378/5000] tr_loss : 36.61158, val_loss : 31.95149, elapsed : 0.36s, total : 126.37s, remaining : 1686.84s\n",
      "*[0379/5000] tr_loss : 36.61548, val_loss : 31.95064, elapsed : 0.34s, total : 126.71s, remaining : 1592.01s\n",
      "*[0380/5000] tr_loss : 36.59635, val_loss : 31.95036, elapsed : 0.34s, total : 127.05s, remaining : 1555.08s\n",
      "*[0381/5000] tr_loss : 36.59412, val_loss : 31.94170, elapsed : 0.34s, total : 127.39s, remaining : 1569.56s\n",
      " [0382/5000] tr_loss : 36.60155, val_loss : 31.94236, elapsed : 0.33s, total : 127.72s, remaining : 1541.49s\n",
      "*[0383/5000] tr_loss : 36.59206, val_loss : 31.93839, elapsed : 0.33s, total : 128.05s, remaining : 1529.49s\n",
      "*[0384/5000] tr_loss : 36.59860, val_loss : 31.92988, elapsed : 0.33s, total : 128.39s, remaining : 1535.19s\n",
      "*[0385/5000] tr_loss : 36.59041, val_loss : 31.92729, elapsed : 0.34s, total : 128.73s, remaining : 1575.09s\n",
      "*[0386/5000] tr_loss : 36.58658, val_loss : 31.92562, elapsed : 0.34s, total : 129.07s, remaining : 1565.78s\n",
      "*[0387/5000] tr_loss : 36.59033, val_loss : 31.91842, elapsed : 0.34s, total : 129.40s, remaining : 1552.69s\n",
      "*[0388/5000] tr_loss : 36.59528, val_loss : 31.91645, elapsed : 0.34s, total : 129.74s, remaining : 1552.03s\n",
      "*[0389/5000] tr_loss : 36.59045, val_loss : 31.91129, elapsed : 0.34s, total : 130.08s, remaining : 1572.76s\n",
      "*[0390/5000] tr_loss : 36.58790, val_loss : 31.89936, elapsed : 0.34s, total : 130.42s, remaining : 1561.43s\n",
      " [0391/5000] tr_loss : 36.58771, val_loss : 31.90146, elapsed : 0.34s, total : 130.76s, remaining : 1582.96s\n",
      " [0392/5000] tr_loss : 36.58658, val_loss : 31.90481, elapsed : 0.34s, total : 131.10s, remaining : 1555.79s\n",
      "*[0393/5000] tr_loss : 36.59421, val_loss : 31.89602, elapsed : 0.33s, total : 131.43s, remaining : 1532.92s\n",
      "*[0394/5000] tr_loss : 36.58552, val_loss : 31.89261, elapsed : 0.33s, total : 131.76s, remaining : 1521.00s\n",
      "*[0395/5000] tr_loss : 36.57759, val_loss : 31.89059, elapsed : 0.33s, total : 132.10s, remaining : 1523.10s\n",
      "*[0396/5000] tr_loss : 36.57700, val_loss : 31.88059, elapsed : 0.33s, total : 132.43s, remaining : 1540.92s\n",
      "*[0397/5000] tr_loss : 36.57721, val_loss : 31.87603, elapsed : 0.33s, total : 132.76s, remaining : 1528.58s\n",
      "*[0398/5000] tr_loss : 36.57835, val_loss : 31.87338, elapsed : 0.34s, total : 133.10s, remaining : 1556.33s\n",
      " [0399/5000] tr_loss : 36.57288, val_loss : 31.87488, elapsed : 0.34s, total : 133.44s, remaining : 1571.82s\n",
      " [0400/5000] tr_loss : 36.57582, val_loss : 31.87769, elapsed : 0.33s, total : 133.77s, remaining : 1529.88s\n",
      "*[0401/5000] tr_loss : 36.57831, val_loss : 31.87010, elapsed : 0.32s, total : 134.10s, remaining : 1486.95s\n",
      " [0402/5000] tr_loss : 36.58270, val_loss : 31.87133, elapsed : 0.37s, total : 134.47s, remaining : 1703.09s\n",
      " [0403/5000] tr_loss : 36.57600, val_loss : 31.87382, elapsed : 0.34s, total : 134.81s, remaining : 1572.54s\n",
      " [0404/5000] tr_loss : 36.56915, val_loss : 31.87044, elapsed : 0.33s, total : 135.15s, remaining : 1538.49s\n",
      " [0405/5000] tr_loss : 36.57990, val_loss : 31.87391, elapsed : 0.33s, total : 135.47s, remaining : 1511.68s\n",
      " [0406/5000] tr_loss : 36.57552, val_loss : 31.87162, elapsed : 0.33s, total : 135.81s, remaining : 1531.07s\n",
      " [0407/5000] tr_loss : 36.57261, val_loss : 31.87138, elapsed : 0.33s, total : 136.14s, remaining : 1514.72s\n",
      " [0408/5000] tr_loss : 36.57791, val_loss : 31.87127, elapsed : 0.33s, total : 136.47s, remaining : 1521.13s\n",
      "*[0409/5000] tr_loss : 36.56874, val_loss : 31.86924, elapsed : 0.33s, total : 136.80s, remaining : 1528.78s\n",
      " [0410/5000] tr_loss : 36.56990, val_loss : 31.87452, elapsed : 0.34s, total : 137.14s, remaining : 1539.96s\n",
      "*[0411/5000] tr_loss : 36.57101, val_loss : 31.86550, elapsed : 0.33s, total : 137.47s, remaining : 1510.06s\n",
      "*[0412/5000] tr_loss : 36.56136, val_loss : 31.86469, elapsed : 0.36s, total : 137.82s, remaining : 1632.88s\n",
      " [0413/5000] tr_loss : 36.56312, val_loss : 31.86642, elapsed : 0.33s, total : 138.15s, remaining : 1498.26s\n",
      " [0414/5000] tr_loss : 36.56412, val_loss : 31.86608, elapsed : 0.34s, total : 138.49s, remaining : 1567.24s\n",
      " [0415/5000] tr_loss : 36.56762, val_loss : 31.86782, elapsed : 0.32s, total : 138.82s, remaining : 1486.46s\n",
      "*[0416/5000] tr_loss : 36.57473, val_loss : 31.86260, elapsed : 0.33s, total : 139.14s, remaining : 1499.33s\n",
      "*[0417/5000] tr_loss : 36.56962, val_loss : 31.85994, elapsed : 0.33s, total : 139.47s, remaining : 1501.13s\n",
      " [0418/5000] tr_loss : 36.57005, val_loss : 31.86766, elapsed : 0.34s, total : 139.81s, remaining : 1540.71s\n",
      " [0419/5000] tr_loss : 36.56972, val_loss : 31.86068, elapsed : 0.33s, total : 140.13s, remaining : 1496.78s\n",
      " [0420/5000] tr_loss : 36.55319, val_loss : 31.86086, elapsed : 0.33s, total : 140.46s, remaining : 1503.39s\n",
      "*[0421/5000] tr_loss : 36.54836, val_loss : 31.85867, elapsed : 0.33s, total : 140.80s, remaining : 1531.62s\n",
      "*[0422/5000] tr_loss : 36.55654, val_loss : 31.85170, elapsed : 0.33s, total : 141.13s, remaining : 1528.11s\n",
      " [0423/5000] tr_loss : 36.55226, val_loss : 31.85883, elapsed : 0.35s, total : 141.48s, remaining : 1595.01s\n",
      " [0424/5000] tr_loss : 36.57084, val_loss : 31.85788, elapsed : 0.34s, total : 141.82s, remaining : 1549.25s\n",
      " [0425/5000] tr_loss : 36.57626, val_loss : 31.85383, elapsed : 0.34s, total : 142.15s, remaining : 1533.05s\n",
      " [0426/5000] tr_loss : 36.56048, val_loss : 31.85548, elapsed : 0.33s, total : 142.48s, remaining : 1509.87s\n",
      " [0427/5000] tr_loss : 36.55895, val_loss : 31.85257, elapsed : 0.34s, total : 142.82s, remaining : 1542.33s\n",
      "*[0428/5000] tr_loss : 36.55751, val_loss : 31.84725, elapsed : 0.34s, total : 143.16s, remaining : 1540.76s\n",
      " [0429/5000] tr_loss : 36.55801, val_loss : 31.84858, elapsed : 0.33s, total : 143.49s, remaining : 1508.41s\n",
      "*[0430/5000] tr_loss : 36.56004, val_loss : 31.84441, elapsed : 0.33s, total : 143.81s, remaining : 1492.86s\n",
      " [0431/5000] tr_loss : 36.57062, val_loss : 31.84978, elapsed : 0.33s, total : 144.14s, remaining : 1505.69s\n",
      " [0432/5000] tr_loss : 36.56121, val_loss : 31.84588, elapsed : 0.33s, total : 144.47s, remaining : 1487.94s\n",
      "*[0433/5000] tr_loss : 36.56398, val_loss : 31.84236, elapsed : 0.33s, total : 144.80s, remaining : 1511.66s\n",
      " [0434/5000] tr_loss : 36.56515, val_loss : 31.84824, elapsed : 0.33s, total : 145.13s, remaining : 1513.47s\n",
      "*[0435/5000] tr_loss : 36.56119, val_loss : 31.83495, elapsed : 0.33s, total : 145.46s, remaining : 1498.95s\n",
      " [0436/5000] tr_loss : 36.56454, val_loss : 31.83620, elapsed : 0.33s, total : 145.79s, remaining : 1514.81s\n",
      "*[0437/5000] tr_loss : 36.55882, val_loss : 31.83242, elapsed : 0.33s, total : 146.12s, remaining : 1498.15s\n",
      "*[0438/5000] tr_loss : 36.56697, val_loss : 31.82895, elapsed : 0.33s, total : 146.45s, remaining : 1514.92s\n",
      " [0439/5000] tr_loss : 36.56482, val_loss : 31.83441, elapsed : 0.33s, total : 146.78s, remaining : 1489.14s\n",
      "*[0440/5000] tr_loss : 36.56161, val_loss : 31.82725, elapsed : 0.33s, total : 147.11s, remaining : 1491.15s\n",
      " [0441/5000] tr_loss : 36.55613, val_loss : 31.82805, elapsed : 0.33s, total : 147.43s, remaining : 1495.58s\n",
      " [0442/5000] tr_loss : 36.56081, val_loss : 31.82823, elapsed : 0.33s, total : 147.76s, remaining : 1496.15s\n",
      "*[0443/5000] tr_loss : 36.55287, val_loss : 31.82559, elapsed : 0.33s, total : 148.09s, remaining : 1509.72s\n",
      "*[0444/5000] tr_loss : 36.55960, val_loss : 31.82512, elapsed : 0.33s, total : 148.42s, remaining : 1490.84s\n",
      "*[0445/5000] tr_loss : 36.56074, val_loss : 31.82397, elapsed : 0.33s, total : 148.75s, remaining : 1492.50s\n",
      " [0446/5000] tr_loss : 36.56175, val_loss : 31.82573, elapsed : 0.35s, total : 149.09s, remaining : 1577.57s\n",
      "*[0447/5000] tr_loss : 36.57565, val_loss : 31.81812, elapsed : 0.33s, total : 149.42s, remaining : 1491.72s\n",
      " [0448/5000] tr_loss : 36.55845, val_loss : 31.82154, elapsed : 0.33s, total : 149.76s, remaining : 1515.99s\n",
      "*[0449/5000] tr_loss : 36.56632, val_loss : 31.81453, elapsed : 0.33s, total : 150.08s, remaining : 1495.63s\n",
      "*[0450/5000] tr_loss : 36.56765, val_loss : 31.80977, elapsed : 0.33s, total : 150.42s, remaining : 1508.46s\n",
      "*[0451/5000] tr_loss : 36.56706, val_loss : 31.80778, elapsed : 0.34s, total : 150.75s, remaining : 1538.86s\n",
      " [0452/5000] tr_loss : 36.56073, val_loss : 31.80972, elapsed : 0.34s, total : 151.09s, remaining : 1526.49s\n",
      " [0453/5000] tr_loss : 36.55299, val_loss : 31.81174, elapsed : 0.34s, total : 151.43s, remaining : 1532.25s\n",
      " [0454/5000] tr_loss : 36.54868, val_loss : 31.80861, elapsed : 0.34s, total : 151.76s, remaining : 1531.57s\n",
      "*[0455/5000] tr_loss : 36.56001, val_loss : 31.80776, elapsed : 0.33s, total : 152.10s, remaining : 1514.51s\n",
      "*[0456/5000] tr_loss : 36.55239, val_loss : 31.80046, elapsed : 0.33s, total : 152.43s, remaining : 1504.10s\n",
      " [0457/5000] tr_loss : 36.55859, val_loss : 31.80202, elapsed : 0.33s, total : 152.76s, remaining : 1514.33s\n",
      " [0458/5000] tr_loss : 36.54839, val_loss : 31.80316, elapsed : 0.33s, total : 153.09s, remaining : 1510.47s\n",
      "*[0459/5000] tr_loss : 36.53762, val_loss : 31.79744, elapsed : 0.33s, total : 153.43s, remaining : 1508.27s\n",
      "*[0460/5000] tr_loss : 36.53970, val_loss : 31.79328, elapsed : 0.33s, total : 153.76s, remaining : 1519.45s\n",
      " [0461/5000] tr_loss : 36.54581, val_loss : 31.79677, elapsed : 0.34s, total : 154.10s, remaining : 1532.70s\n",
      "*[0462/5000] tr_loss : 36.55182, val_loss : 31.78831, elapsed : 0.34s, total : 154.43s, remaining : 1522.46s\n",
      "*[0463/5000] tr_loss : 36.54438, val_loss : 31.78273, elapsed : 0.34s, total : 154.77s, remaining : 1534.73s\n",
      " [0464/5000] tr_loss : 36.54205, val_loss : 31.78429, elapsed : 0.35s, total : 155.12s, remaining : 1577.26s\n",
      "*[0465/5000] tr_loss : 36.54684, val_loss : 31.78122, elapsed : 0.34s, total : 155.46s, remaining : 1535.82s\n",
      "*[0466/5000] tr_loss : 36.54059, val_loss : 31.77840, elapsed : 0.34s, total : 155.80s, remaining : 1552.32s\n",
      "*[0467/5000] tr_loss : 36.53695, val_loss : 31.77168, elapsed : 0.34s, total : 156.14s, remaining : 1553.21s\n",
      " [0468/5000] tr_loss : 36.54991, val_loss : 31.77280, elapsed : 0.33s, total : 156.48s, remaining : 1503.64s\n",
      "*[0469/5000] tr_loss : 36.53005, val_loss : 31.76677, elapsed : 0.33s, total : 156.81s, remaining : 1512.12s\n",
      "*[0470/5000] tr_loss : 36.53335, val_loss : 31.76324, elapsed : 0.36s, total : 157.17s, remaining : 1624.55s\n",
      "*[0471/5000] tr_loss : 36.54138, val_loss : 31.76056, elapsed : 0.33s, total : 157.50s, remaining : 1486.29s\n",
      "*[0472/5000] tr_loss : 36.53453, val_loss : 31.75740, elapsed : 0.33s, total : 157.83s, remaining : 1487.26s\n",
      "*[0473/5000] tr_loss : 36.53229, val_loss : 31.75135, elapsed : 0.36s, total : 158.19s, remaining : 1640.61s\n",
      "*[0474/5000] tr_loss : 36.52885, val_loss : 31.74614, elapsed : 0.33s, total : 158.52s, remaining : 1494.91s\n",
      " [0475/5000] tr_loss : 36.53876, val_loss : 31.74699, elapsed : 0.33s, total : 158.85s, remaining : 1485.31s\n",
      "*[0476/5000] tr_loss : 36.52933, val_loss : 31.74056, elapsed : 0.33s, total : 159.18s, remaining : 1488.55s\n",
      "*[0477/5000] tr_loss : 36.52192, val_loss : 31.73677, elapsed : 0.33s, total : 159.51s, remaining : 1501.50s\n",
      "*[0478/5000] tr_loss : 36.51848, val_loss : 31.72772, elapsed : 0.33s, total : 159.83s, remaining : 1469.71s\n",
      "*[0479/5000] tr_loss : 36.51585, val_loss : 31.72098, elapsed : 0.33s, total : 160.16s, remaining : 1487.50s\n",
      " [0480/5000] tr_loss : 36.51811, val_loss : 31.72423, elapsed : 0.34s, total : 160.50s, remaining : 1533.66s\n",
      "*[0481/5000] tr_loss : 36.51837, val_loss : 31.72068, elapsed : 0.33s, total : 160.83s, remaining : 1490.66s\n",
      "*[0482/5000] tr_loss : 36.51635, val_loss : 31.71618, elapsed : 0.34s, total : 161.17s, remaining : 1530.84s\n",
      "*[0483/5000] tr_loss : 36.50745, val_loss : 31.70897, elapsed : 0.34s, total : 161.51s, remaining : 1535.15s\n",
      "*[0484/5000] tr_loss : 36.51308, val_loss : 31.69787, elapsed : 0.33s, total : 161.84s, remaining : 1499.00s\n",
      "*[0485/5000] tr_loss : 36.50515, val_loss : 31.69710, elapsed : 0.33s, total : 162.17s, remaining : 1479.65s\n",
      " [0486/5000] tr_loss : 36.51190, val_loss : 31.69919, elapsed : 0.33s, total : 162.50s, remaining : 1484.43s\n",
      "*[0487/5000] tr_loss : 36.50648, val_loss : 31.68995, elapsed : 0.33s, total : 162.82s, remaining : 1474.17s\n",
      "*[0488/5000] tr_loss : 36.50404, val_loss : 31.68688, elapsed : 0.33s, total : 163.16s, remaining : 1495.55s\n",
      "*[0489/5000] tr_loss : 36.49696, val_loss : 31.68302, elapsed : 0.33s, total : 163.49s, remaining : 1492.62s\n",
      "*[0490/5000] tr_loss : 36.50885, val_loss : 31.67955, elapsed : 0.34s, total : 163.82s, remaining : 1518.57s\n",
      "*[0491/5000] tr_loss : 36.49744, val_loss : 31.67348, elapsed : 0.33s, total : 164.15s, remaining : 1482.00s\n",
      " [0492/5000] tr_loss : 36.49433, val_loss : 31.67625, elapsed : 0.33s, total : 164.48s, remaining : 1481.28s\n",
      "*[0493/5000] tr_loss : 36.49035, val_loss : 31.67024, elapsed : 0.33s, total : 164.81s, remaining : 1481.79s\n",
      "*[0494/5000] tr_loss : 36.49665, val_loss : 31.66839, elapsed : 0.33s, total : 165.14s, remaining : 1490.15s\n",
      "*[0495/5000] tr_loss : 36.50035, val_loss : 31.66711, elapsed : 0.36s, total : 165.50s, remaining : 1602.24s\n",
      "*[0496/5000] tr_loss : 36.49932, val_loss : 31.66259, elapsed : 0.34s, total : 165.84s, remaining : 1535.75s\n",
      "*[0497/5000] tr_loss : 36.49768, val_loss : 31.65870, elapsed : 0.36s, total : 166.20s, remaining : 1631.84s\n",
      "*[0498/5000] tr_loss : 36.49598, val_loss : 31.65365, elapsed : 0.35s, total : 166.55s, remaining : 1555.86s\n",
      "*[0499/5000] tr_loss : 36.47995, val_loss : 31.65341, elapsed : 0.36s, total : 166.90s, remaining : 1613.26s\n",
      "*[0500/5000] tr_loss : 36.49593, val_loss : 31.64957, elapsed : 0.34s, total : 167.25s, remaining : 1550.55s\n",
      "*[0501/5000] tr_loss : 36.49641, val_loss : 31.63600, elapsed : 0.36s, total : 167.61s, remaining : 1641.51s\n",
      " [0502/5000] tr_loss : 36.48543, val_loss : 31.63710, elapsed : 0.34s, total : 167.95s, remaining : 1530.78s\n",
      "*[0503/5000] tr_loss : 36.49099, val_loss : 31.63517, elapsed : 0.35s, total : 168.30s, remaining : 1559.36s\n",
      " [0504/5000] tr_loss : 36.48109, val_loss : 31.63734, elapsed : 0.35s, total : 168.65s, remaining : 1569.23s\n",
      "*[0505/5000] tr_loss : 36.47576, val_loss : 31.62714, elapsed : 0.35s, total : 169.00s, remaining : 1560.82s\n",
      "*[0506/5000] tr_loss : 36.47215, val_loss : 31.62411, elapsed : 0.35s, total : 169.34s, remaining : 1560.76s\n",
      " [0507/5000] tr_loss : 36.48157, val_loss : 31.62763, elapsed : 0.34s, total : 169.69s, remaining : 1533.65s\n",
      "*[0508/5000] tr_loss : 36.46472, val_loss : 31.62354, elapsed : 0.36s, total : 170.05s, remaining : 1629.61s\n",
      "*[0509/5000] tr_loss : 36.48095, val_loss : 31.61082, elapsed : 0.35s, total : 170.40s, remaining : 1570.45s\n",
      " [0510/5000] tr_loss : 36.47455, val_loss : 31.61472, elapsed : 0.35s, total : 170.75s, remaining : 1576.51s\n",
      "*[0511/5000] tr_loss : 36.47448, val_loss : 31.60891, elapsed : 0.35s, total : 171.10s, remaining : 1565.37s\n",
      "*[0512/5000] tr_loss : 36.47400, val_loss : 31.60659, elapsed : 0.35s, total : 171.45s, remaining : 1569.96s\n",
      "*[0513/5000] tr_loss : 36.46125, val_loss : 31.60649, elapsed : 0.35s, total : 171.79s, remaining : 1550.89s\n",
      "*[0514/5000] tr_loss : 36.47023, val_loss : 31.60351, elapsed : 0.35s, total : 172.14s, remaining : 1555.45s\n",
      "*[0515/5000] tr_loss : 36.45866, val_loss : 31.59582, elapsed : 0.35s, total : 172.49s, remaining : 1548.16s\n",
      "*[0516/5000] tr_loss : 36.46809, val_loss : 31.59332, elapsed : 0.35s, total : 172.83s, remaining : 1554.66s\n",
      "*[0517/5000] tr_loss : 36.46641, val_loss : 31.59034, elapsed : 0.38s, total : 173.21s, remaining : 1701.49s\n",
      "*[0518/5000] tr_loss : 36.45648, val_loss : 31.58530, elapsed : 0.37s, total : 173.59s, remaining : 1674.74s\n",
      " [0519/5000] tr_loss : 36.45296, val_loss : 31.58797, elapsed : 0.34s, total : 173.93s, remaining : 1535.27s\n",
      "*[0520/5000] tr_loss : 36.45474, val_loss : 31.58141, elapsed : 0.35s, total : 174.28s, remaining : 1557.01s\n",
      "*[0521/5000] tr_loss : 36.46426, val_loss : 31.57985, elapsed : 0.34s, total : 174.62s, remaining : 1520.58s\n",
      "*[0522/5000] tr_loss : 36.46386, val_loss : 31.57860, elapsed : 0.34s, total : 174.96s, remaining : 1543.74s\n",
      "*[0523/5000] tr_loss : 36.45577, val_loss : 31.57329, elapsed : 0.34s, total : 175.30s, remaining : 1515.99s\n",
      "*[0524/5000] tr_loss : 36.43951, val_loss : 31.56976, elapsed : 0.36s, total : 175.66s, remaining : 1619.28s\n",
      "*[0525/5000] tr_loss : 36.45292, val_loss : 31.56216, elapsed : 0.40s, total : 176.06s, remaining : 1798.47s\n",
      " [0526/5000] tr_loss : 36.44542, val_loss : 31.56273, elapsed : 0.42s, total : 176.48s, remaining : 1875.43s\n",
      "*[0527/5000] tr_loss : 36.44136, val_loss : 31.56101, elapsed : 0.41s, total : 176.89s, remaining : 1820.39s\n",
      "*[0528/5000] tr_loss : 36.44157, val_loss : 31.55272, elapsed : 0.39s, total : 177.28s, remaining : 1732.81s\n",
      "*[0529/5000] tr_loss : 36.44926, val_loss : 31.54860, elapsed : 0.39s, total : 177.67s, remaining : 1754.68s\n",
      " [0530/5000] tr_loss : 36.44656, val_loss : 31.55280, elapsed : 0.33s, total : 178.00s, remaining : 1491.83s\n",
      " [0531/5000] tr_loss : 36.44258, val_loss : 31.55421, elapsed : 0.34s, total : 178.34s, remaining : 1520.43s\n",
      " [0532/5000] tr_loss : 36.43875, val_loss : 31.54885, elapsed : 0.34s, total : 178.68s, remaining : 1500.13s\n",
      "*[0533/5000] tr_loss : 36.43651, val_loss : 31.54520, elapsed : 0.34s, total : 179.02s, remaining : 1537.09s\n",
      "*[0534/5000] tr_loss : 36.43330, val_loss : 31.54174, elapsed : 0.35s, total : 179.37s, remaining : 1557.84s\n",
      "*[0535/5000] tr_loss : 36.43390, val_loss : 31.54029, elapsed : 0.38s, total : 179.75s, remaining : 1683.18s\n",
      "*[0536/5000] tr_loss : 36.43046, val_loss : 31.53739, elapsed : 0.33s, total : 180.08s, remaining : 1489.76s\n",
      " [0537/5000] tr_loss : 36.42867, val_loss : 31.54185, elapsed : 0.34s, total : 180.42s, remaining : 1523.28s\n",
      "*[0538/5000] tr_loss : 36.42782, val_loss : 31.53353, elapsed : 0.33s, total : 180.75s, remaining : 1475.73s\n",
      "*[0539/5000] tr_loss : 36.42150, val_loss : 31.52745, elapsed : 0.34s, total : 181.09s, remaining : 1496.26s\n",
      " [0540/5000] tr_loss : 36.42643, val_loss : 31.53032, elapsed : 0.34s, total : 181.43s, remaining : 1512.85s\n",
      "*[0541/5000] tr_loss : 36.41329, val_loss : 31.52639, elapsed : 0.38s, total : 181.81s, remaining : 1677.73s\n",
      "*[0542/5000] tr_loss : 36.42087, val_loss : 31.52226, elapsed : 0.37s, total : 182.17s, remaining : 1628.49s\n",
      "*[0543/5000] tr_loss : 36.41938, val_loss : 31.51953, elapsed : 0.38s, total : 182.55s, remaining : 1686.56s\n",
      "*[0544/5000] tr_loss : 36.41279, val_loss : 31.51638, elapsed : 0.36s, total : 182.91s, remaining : 1599.51s\n",
      "*[0545/5000] tr_loss : 36.41895, val_loss : 31.51489, elapsed : 0.38s, total : 183.29s, remaining : 1698.82s\n",
      " [0546/5000] tr_loss : 36.40631, val_loss : 31.51530, elapsed : 0.37s, total : 183.66s, remaining : 1662.51s\n",
      "*[0547/5000] tr_loss : 36.41610, val_loss : 31.50730, elapsed : 0.36s, total : 184.03s, remaining : 1620.05s\n",
      "*[0548/5000] tr_loss : 36.41344, val_loss : 31.50647, elapsed : 0.37s, total : 184.40s, remaining : 1668.28s\n",
      "*[0549/5000] tr_loss : 36.41791, val_loss : 31.50555, elapsed : 0.37s, total : 184.77s, remaining : 1639.26s\n",
      " [0550/5000] tr_loss : 36.41443, val_loss : 31.50620, elapsed : 0.38s, total : 185.15s, remaining : 1704.12s\n",
      "*[0551/5000] tr_loss : 36.41400, val_loss : 31.50317, elapsed : 0.36s, total : 185.51s, remaining : 1591.50s\n",
      "*[0552/5000] tr_loss : 36.40948, val_loss : 31.49888, elapsed : 0.36s, total : 185.87s, remaining : 1609.73s\n",
      "*[0553/5000] tr_loss : 36.40381, val_loss : 31.49761, elapsed : 0.38s, total : 186.26s, remaining : 1707.59s\n",
      "*[0554/5000] tr_loss : 36.39850, val_loss : 31.49689, elapsed : 0.36s, total : 186.62s, remaining : 1616.83s\n",
      "*[0555/5000] tr_loss : 36.39638, val_loss : 31.49410, elapsed : 0.37s, total : 186.99s, remaining : 1636.73s\n",
      "*[0556/5000] tr_loss : 36.39707, val_loss : 31.49061, elapsed : 0.36s, total : 187.35s, remaining : 1596.18s\n",
      "*[0557/5000] tr_loss : 36.39651, val_loss : 31.48679, elapsed : 0.39s, total : 187.74s, remaining : 1722.27s\n",
      "*[0558/5000] tr_loss : 36.40488, val_loss : 31.48291, elapsed : 0.38s, total : 188.11s, remaining : 1681.50s\n",
      "*[0559/5000] tr_loss : 36.39006, val_loss : 31.48140, elapsed : 0.39s, total : 188.50s, remaining : 1718.65s\n",
      "*[0560/5000] tr_loss : 36.39689, val_loss : 31.47915, elapsed : 0.40s, total : 188.90s, remaining : 1790.91s\n",
      "*[0561/5000] tr_loss : 36.40028, val_loss : 31.47732, elapsed : 0.39s, total : 189.29s, remaining : 1717.21s\n",
      " [0562/5000] tr_loss : 36.41219, val_loss : 31.47861, elapsed : 0.39s, total : 189.69s, remaining : 1747.39s\n",
      "*[0563/5000] tr_loss : 36.40701, val_loss : 31.47346, elapsed : 0.40s, total : 190.09s, remaining : 1779.53s\n",
      " [0564/5000] tr_loss : 36.38780, val_loss : 31.47448, elapsed : 0.38s, total : 190.47s, remaining : 1678.88s\n",
      "*[0565/5000] tr_loss : 36.38991, val_loss : 31.46722, elapsed : 0.39s, total : 190.85s, remaining : 1726.32s\n",
      " [0566/5000] tr_loss : 36.38606, val_loss : 31.47020, elapsed : 0.39s, total : 191.24s, remaining : 1722.51s\n",
      " [0567/5000] tr_loss : 36.37315, val_loss : 31.46805, elapsed : 0.38s, total : 191.62s, remaining : 1664.95s\n",
      "*[0568/5000] tr_loss : 36.37820, val_loss : 31.46233, elapsed : 0.39s, total : 192.00s, remaining : 1709.96s\n",
      " [0569/5000] tr_loss : 36.38550, val_loss : 31.46422, elapsed : 0.38s, total : 192.39s, remaining : 1703.07s\n",
      "*[0570/5000] tr_loss : 36.38356, val_loss : 31.46226, elapsed : 0.39s, total : 192.77s, remaining : 1708.62s\n",
      "*[0571/5000] tr_loss : 36.37521, val_loss : 31.45264, elapsed : 0.38s, total : 193.15s, remaining : 1683.45s\n",
      " [0572/5000] tr_loss : 36.37847, val_loss : 31.45497, elapsed : 0.38s, total : 193.54s, remaining : 1697.93s\n",
      " [0573/5000] tr_loss : 36.38857, val_loss : 31.45295, elapsed : 0.39s, total : 193.93s, remaining : 1717.53s\n",
      "*[0574/5000] tr_loss : 36.37927, val_loss : 31.44996, elapsed : 0.38s, total : 194.30s, remaining : 1662.95s\n",
      " [0575/5000] tr_loss : 36.37768, val_loss : 31.45027, elapsed : 0.38s, total : 194.68s, remaining : 1684.51s\n",
      "*[0576/5000] tr_loss : 36.37607, val_loss : 31.44630, elapsed : 0.34s, total : 195.02s, remaining : 1500.27s\n",
      " [0577/5000] tr_loss : 36.37378, val_loss : 31.44686, elapsed : 0.36s, total : 195.39s, remaining : 1610.15s\n",
      "*[0578/5000] tr_loss : 36.37878, val_loss : 31.44309, elapsed : 0.34s, total : 195.73s, remaining : 1502.64s\n",
      "*[0579/5000] tr_loss : 36.37028, val_loss : 31.44201, elapsed : 0.35s, total : 196.07s, remaining : 1529.50s\n",
      " [0580/5000] tr_loss : 36.36654, val_loss : 31.44217, elapsed : 0.36s, total : 196.43s, remaining : 1572.02s\n",
      "*[0581/5000] tr_loss : 36.36516, val_loss : 31.43806, elapsed : 0.34s, total : 196.77s, remaining : 1513.15s\n",
      "*[0582/5000] tr_loss : 36.37903, val_loss : 31.43544, elapsed : 0.34s, total : 197.11s, remaining : 1519.38s\n",
      " [0583/5000] tr_loss : 36.37690, val_loss : 31.43562, elapsed : 0.35s, total : 197.47s, remaining : 1560.06s\n",
      "*[0584/5000] tr_loss : 36.37038, val_loss : 31.42814, elapsed : 0.34s, total : 197.81s, remaining : 1516.64s\n",
      "*[0585/5000] tr_loss : 36.37047, val_loss : 31.42629, elapsed : 0.33s, total : 198.14s, remaining : 1463.74s\n",
      " [0586/5000] tr_loss : 36.36640, val_loss : 31.42919, elapsed : 0.33s, total : 198.47s, remaining : 1465.86s\n",
      "*[0587/5000] tr_loss : 36.35840, val_loss : 31.42548, elapsed : 0.33s, total : 198.81s, remaining : 1463.86s\n",
      "*[0588/5000] tr_loss : 36.36581, val_loss : 31.42158, elapsed : 0.33s, total : 199.14s, remaining : 1452.10s\n",
      " [0589/5000] tr_loss : 36.36224, val_loss : 31.42296, elapsed : 0.34s, total : 199.47s, remaining : 1485.46s\n",
      "*[0590/5000] tr_loss : 36.36655, val_loss : 31.41940, elapsed : 0.37s, total : 199.84s, remaining : 1643.76s\n",
      " [0591/5000] tr_loss : 36.36073, val_loss : 31.42045, elapsed : 0.33s, total : 200.18s, remaining : 1455.59s\n",
      "*[0592/5000] tr_loss : 36.36038, val_loss : 31.41454, elapsed : 0.34s, total : 200.51s, remaining : 1484.13s\n",
      "*[0593/5000] tr_loss : 36.36730, val_loss : 31.41029, elapsed : 0.33s, total : 200.85s, remaining : 1471.20s\n",
      "*[0594/5000] tr_loss : 36.35568, val_loss : 31.40986, elapsed : 0.34s, total : 201.18s, remaining : 1483.92s\n",
      "*[0595/5000] tr_loss : 36.36205, val_loss : 31.40854, elapsed : 0.33s, total : 201.52s, remaining : 1472.13s\n",
      "*[0596/5000] tr_loss : 36.34504, val_loss : 31.40310, elapsed : 0.34s, total : 201.85s, remaining : 1482.86s\n",
      "*[0597/5000] tr_loss : 36.35172, val_loss : 31.39886, elapsed : 0.34s, total : 202.19s, remaining : 1488.84s\n",
      " [0598/5000] tr_loss : 36.35845, val_loss : 31.39977, elapsed : 0.34s, total : 202.53s, remaining : 1501.38s\n",
      "*[0599/5000] tr_loss : 36.35934, val_loss : 31.39579, elapsed : 0.34s, total : 202.87s, remaining : 1487.22s\n",
      " [0600/5000] tr_loss : 36.34656, val_loss : 31.39808, elapsed : 0.33s, total : 203.20s, remaining : 1461.64s\n",
      "*[0601/5000] tr_loss : 36.34815, val_loss : 31.39016, elapsed : 0.33s, total : 203.53s, remaining : 1459.65s\n",
      "*[0602/5000] tr_loss : 36.34281, val_loss : 31.38858, elapsed : 0.33s, total : 203.86s, remaining : 1448.32s\n",
      "*[0603/5000] tr_loss : 36.34053, val_loss : 31.38414, elapsed : 0.34s, total : 204.20s, remaining : 1479.71s\n",
      " [0604/5000] tr_loss : 36.35111, val_loss : 31.38633, elapsed : 0.33s, total : 204.53s, remaining : 1453.45s\n",
      " [0605/5000] tr_loss : 36.34581, val_loss : 31.38554, elapsed : 0.34s, total : 204.87s, remaining : 1503.35s\n",
      "*[0606/5000] tr_loss : 36.34392, val_loss : 31.38385, elapsed : 0.33s, total : 205.20s, remaining : 1444.93s\n",
      "*[0607/5000] tr_loss : 36.34911, val_loss : 31.37918, elapsed : 0.33s, total : 205.54s, remaining : 1460.86s\n",
      "*[0608/5000] tr_loss : 36.33852, val_loss : 31.37891, elapsed : 0.33s, total : 205.87s, remaining : 1455.62s\n",
      "*[0609/5000] tr_loss : 36.34542, val_loss : 31.37814, elapsed : 0.33s, total : 206.20s, remaining : 1464.78s\n",
      "*[0610/5000] tr_loss : 36.33712, val_loss : 31.37673, elapsed : 0.33s, total : 206.53s, remaining : 1468.48s\n",
      "*[0611/5000] tr_loss : 36.34216, val_loss : 31.37128, elapsed : 0.34s, total : 206.88s, remaining : 1502.67s\n",
      " [0612/5000] tr_loss : 36.33593, val_loss : 31.37261, elapsed : 0.34s, total : 207.22s, remaining : 1486.06s\n",
      "*[0613/5000] tr_loss : 36.34646, val_loss : 31.36680, elapsed : 0.34s, total : 207.56s, remaining : 1489.41s\n",
      " [0614/5000] tr_loss : 36.34326, val_loss : 31.36897, elapsed : 0.34s, total : 207.89s, remaining : 1485.12s\n",
      " [0615/5000] tr_loss : 36.33396, val_loss : 31.36706, elapsed : 0.33s, total : 208.22s, remaining : 1448.81s\n",
      "*[0616/5000] tr_loss : 36.33889, val_loss : 31.36585, elapsed : 0.33s, total : 208.56s, remaining : 1450.85s\n",
      " [0617/5000] tr_loss : 36.32919, val_loss : 31.36639, elapsed : 0.34s, total : 208.90s, remaining : 1491.49s\n",
      "*[0618/5000] tr_loss : 36.32765, val_loss : 31.36215, elapsed : 0.33s, total : 209.23s, remaining : 1453.30s\n",
      "*[0619/5000] tr_loss : 36.33359, val_loss : 31.36122, elapsed : 0.34s, total : 209.56s, remaining : 1472.91s\n",
      "*[0620/5000] tr_loss : 36.32334, val_loss : 31.35726, elapsed : 0.34s, total : 209.90s, remaining : 1481.64s\n",
      " [0621/5000] tr_loss : 36.32725, val_loss : 31.35807, elapsed : 0.33s, total : 210.24s, remaining : 1461.58s\n",
      " [0622/5000] tr_loss : 36.32779, val_loss : 31.35733, elapsed : 0.33s, total : 210.57s, remaining : 1461.50s\n",
      "*[0623/5000] tr_loss : 36.32762, val_loss : 31.35346, elapsed : 0.34s, total : 210.91s, remaining : 1476.59s\n",
      " [0624/5000] tr_loss : 36.32482, val_loss : 31.35537, elapsed : 0.37s, total : 211.28s, remaining : 1613.05s\n",
      " [0625/5000] tr_loss : 36.30779, val_loss : 31.35436, elapsed : 0.34s, total : 211.61s, remaining : 1470.13s\n",
      "*[0626/5000] tr_loss : 36.31395, val_loss : 31.35217, elapsed : 0.33s, total : 211.95s, remaining : 1462.48s\n",
      "*[0627/5000] tr_loss : 36.32458, val_loss : 31.34837, elapsed : 0.35s, total : 212.29s, remaining : 1524.65s\n",
      " [0628/5000] tr_loss : 36.31570, val_loss : 31.35114, elapsed : 0.34s, total : 212.63s, remaining : 1479.31s\n",
      "*[0629/5000] tr_loss : 36.31734, val_loss : 31.34705, elapsed : 0.35s, total : 212.98s, remaining : 1510.48s\n",
      " [0630/5000] tr_loss : 36.31263, val_loss : 31.34762, elapsed : 0.34s, total : 213.32s, remaining : 1471.64s\n",
      "*[0631/5000] tr_loss : 36.31373, val_loss : 31.34551, elapsed : 0.34s, total : 213.66s, remaining : 1504.64s\n",
      "*[0632/5000] tr_loss : 36.31637, val_loss : 31.34108, elapsed : 0.33s, total : 213.99s, remaining : 1458.46s\n",
      " [0633/5000] tr_loss : 36.32800, val_loss : 31.34167, elapsed : 0.34s, total : 214.33s, remaining : 1486.37s\n",
      " [0634/5000] tr_loss : 36.31360, val_loss : 31.34540, elapsed : 0.33s, total : 214.67s, remaining : 1456.59s\n",
      "*[0635/5000] tr_loss : 36.31846, val_loss : 31.34021, elapsed : 0.33s, total : 215.00s, remaining : 1452.22s\n",
      " [0636/5000] tr_loss : 36.31575, val_loss : 31.34050, elapsed : 0.34s, total : 215.34s, remaining : 1478.36s\n",
      " [0637/5000] tr_loss : 36.30742, val_loss : 31.34188, elapsed : 0.34s, total : 215.68s, remaining : 1469.46s\n",
      "*[0638/5000] tr_loss : 36.30174, val_loss : 31.33688, elapsed : 0.34s, total : 216.01s, remaining : 1464.30s\n",
      "*[0639/5000] tr_loss : 36.31339, val_loss : 31.33391, elapsed : 0.33s, total : 216.35s, remaining : 1452.44s\n",
      " [0640/5000] tr_loss : 36.30715, val_loss : 31.33480, elapsed : 0.34s, total : 216.69s, remaining : 1481.08s\n",
      "*[0641/5000] tr_loss : 36.30425, val_loss : 31.32653, elapsed : 0.37s, total : 217.06s, remaining : 1619.59s\n",
      " [0642/5000] tr_loss : 36.30336, val_loss : 31.32896, elapsed : 0.34s, total : 217.40s, remaining : 1482.04s\n",
      "*[0643/5000] tr_loss : 36.29335, val_loss : 31.32562, elapsed : 0.33s, total : 217.73s, remaining : 1456.47s\n",
      " [0644/5000] tr_loss : 36.30758, val_loss : 31.32582, elapsed : 0.34s, total : 218.07s, remaining : 1482.72s\n",
      "*[0645/5000] tr_loss : 36.30548, val_loss : 31.32506, elapsed : 0.33s, total : 218.40s, remaining : 1429.08s\n",
      "*[0646/5000] tr_loss : 36.31337, val_loss : 31.32239, elapsed : 0.33s, total : 218.73s, remaining : 1455.65s\n",
      "*[0647/5000] tr_loss : 36.30526, val_loss : 31.32060, elapsed : 0.34s, total : 219.07s, remaining : 1480.54s\n",
      "*[0648/5000] tr_loss : 36.29119, val_loss : 31.31985, elapsed : 0.34s, total : 219.41s, remaining : 1468.41s\n",
      "*[0649/5000] tr_loss : 36.28718, val_loss : 31.31817, elapsed : 0.34s, total : 219.75s, remaining : 1466.92s\n",
      "*[0650/5000] tr_loss : 36.30055, val_loss : 31.31503, elapsed : 0.34s, total : 220.09s, remaining : 1469.10s\n",
      " [0651/5000] tr_loss : 36.30370, val_loss : 31.31608, elapsed : 0.34s, total : 220.43s, remaining : 1472.03s\n",
      "*[0652/5000] tr_loss : 36.28883, val_loss : 31.31408, elapsed : 0.34s, total : 220.76s, remaining : 1466.16s\n",
      " [0653/5000] tr_loss : 36.30134, val_loss : 31.31747, elapsed : 0.34s, total : 221.10s, remaining : 1457.76s\n",
      "*[0654/5000] tr_loss : 36.29713, val_loss : 31.31178, elapsed : 0.34s, total : 221.43s, remaining : 1458.35s\n",
      " [0655/5000] tr_loss : 36.29255, val_loss : 31.31244, elapsed : 0.34s, total : 221.78s, remaining : 1489.73s\n",
      "*[0656/5000] tr_loss : 36.29138, val_loss : 31.30732, elapsed : 0.34s, total : 222.12s, remaining : 1491.29s\n",
      " [0657/5000] tr_loss : 36.28241, val_loss : 31.31141, elapsed : 0.33s, total : 222.45s, remaining : 1444.12s\n",
      " [0658/5000] tr_loss : 36.28532, val_loss : 31.31606, elapsed : 0.34s, total : 222.79s, remaining : 1461.03s\n",
      " [0659/5000] tr_loss : 36.29785, val_loss : 31.31653, elapsed : 0.33s, total : 223.12s, remaining : 1436.01s\n",
      "*[0660/5000] tr_loss : 36.29059, val_loss : 31.30671, elapsed : 0.33s, total : 223.45s, remaining : 1450.54s\n",
      " [0661/5000] tr_loss : 36.27678, val_loss : 31.30847, elapsed : 0.34s, total : 223.79s, remaining : 1476.71s\n",
      " [0662/5000] tr_loss : 36.28843, val_loss : 31.31274, elapsed : 0.34s, total : 224.13s, remaining : 1466.44s\n",
      " [0663/5000] tr_loss : 36.28387, val_loss : 31.31211, elapsed : 0.34s, total : 224.47s, remaining : 1459.93s\n",
      " [0664/5000] tr_loss : 36.27255, val_loss : 31.31238, elapsed : 0.34s, total : 224.81s, remaining : 1464.57s\n",
      "*[0665/5000] tr_loss : 36.28711, val_loss : 31.30646, elapsed : 0.33s, total : 225.14s, remaining : 1440.94s\n",
      " [0666/5000] tr_loss : 36.27534, val_loss : 31.31153, elapsed : 0.34s, total : 225.48s, remaining : 1466.25s\n",
      " [0667/5000] tr_loss : 36.27458, val_loss : 31.31574, elapsed : 0.33s, total : 225.81s, remaining : 1437.62s\n",
      " [0668/5000] tr_loss : 36.28274, val_loss : 31.31077, elapsed : 0.34s, total : 226.15s, remaining : 1463.07s\n",
      " [0669/5000] tr_loss : 36.28086, val_loss : 31.31211, elapsed : 0.33s, total : 226.48s, remaining : 1435.67s\n",
      " [0670/5000] tr_loss : 36.27512, val_loss : 31.31182, elapsed : 0.33s, total : 226.81s, remaining : 1435.11s\n",
      " [0671/5000] tr_loss : 36.28019, val_loss : 31.31097, elapsed : 0.34s, total : 227.15s, remaining : 1460.69s\n",
      " [0672/5000] tr_loss : 36.28431, val_loss : 31.31625, elapsed : 0.33s, total : 227.48s, remaining : 1448.87s\n",
      " [0673/5000] tr_loss : 36.27870, val_loss : 31.31369, elapsed : 0.34s, total : 227.82s, remaining : 1463.11s\n",
      " [0674/5000] tr_loss : 36.26849, val_loss : 31.31043, elapsed : 0.33s, total : 228.15s, remaining : 1435.23s\n",
      " [0675/5000] tr_loss : 36.27391, val_loss : 31.30876, elapsed : 0.33s, total : 228.48s, remaining : 1429.40s\n",
      " [0676/5000] tr_loss : 36.26387, val_loss : 31.31649, elapsed : 0.33s, total : 228.81s, remaining : 1424.46s\n",
      " [0677/5000] tr_loss : 36.26207, val_loss : 31.31229, elapsed : 0.33s, total : 229.15s, remaining : 1442.51s\n",
      " [0678/5000] tr_loss : 36.25192, val_loss : 31.31255, elapsed : 0.33s, total : 229.48s, remaining : 1422.01s\n",
      " [0679/5000] tr_loss : 36.26697, val_loss : 31.31422, elapsed : 0.35s, total : 229.82s, remaining : 1505.84s\n",
      " [0680/5000] tr_loss : 36.26591, val_loss : 31.31522, elapsed : 0.33s, total : 230.16s, remaining : 1443.26s\n",
      " [0681/5000] tr_loss : 36.25809, val_loss : 31.31303, elapsed : 0.34s, total : 230.50s, remaining : 1464.73s\n",
      " [0682/5000] tr_loss : 36.27638, val_loss : 31.30775, elapsed : 0.37s, total : 230.87s, remaining : 1605.60s\n",
      " [0683/5000] tr_loss : 36.27709, val_loss : 31.31447, elapsed : 0.39s, total : 231.26s, remaining : 1681.22s\n",
      " [0684/5000] tr_loss : 36.25803, val_loss : 31.31195, elapsed : 0.35s, total : 231.61s, remaining : 1506.32s\n",
      " [0685/5000] tr_loss : 36.25141, val_loss : 31.31292, elapsed : 0.35s, total : 231.95s, remaining : 1495.14s\n",
      " [0686/5000] tr_loss : 36.26899, val_loss : 31.31119, elapsed : 0.35s, total : 232.30s, remaining : 1505.21s\n",
      " [0687/5000] tr_loss : 36.26858, val_loss : 31.30753, elapsed : 0.36s, total : 232.66s, remaining : 1553.09s\n",
      " [0688/5000] tr_loss : 36.25086, val_loss : 31.31575, elapsed : 0.34s, total : 233.00s, remaining : 1471.32s\n",
      " [0689/5000] tr_loss : 36.25848, val_loss : 31.31310, elapsed : 0.35s, total : 233.36s, remaining : 1517.37s\n",
      " [0690/5000] tr_loss : 36.25827, val_loss : 31.30904, elapsed : 0.34s, total : 233.70s, remaining : 1461.04s\n",
      "*[0691/5000] tr_loss : 36.26118, val_loss : 31.30505, elapsed : 0.35s, total : 234.04s, remaining : 1495.24s\n",
      " [0692/5000] tr_loss : 36.24444, val_loss : 31.31109, elapsed : 0.37s, total : 234.41s, remaining : 1586.92s\n",
      " [0693/5000] tr_loss : 36.26161, val_loss : 31.31352, elapsed : 0.35s, total : 234.76s, remaining : 1514.86s\n",
      " [0694/5000] tr_loss : 36.25585, val_loss : 31.31445, elapsed : 0.37s, total : 235.13s, remaining : 1589.00s\n",
      " [0695/5000] tr_loss : 36.25835, val_loss : 31.31195, elapsed : 0.34s, total : 235.48s, remaining : 1483.33s\n",
      " [0696/5000] tr_loss : 36.26107, val_loss : 31.31330, elapsed : 0.34s, total : 235.82s, remaining : 1479.19s\n",
      " [0697/5000] tr_loss : 36.25364, val_loss : 31.30936, elapsed : 0.35s, total : 236.17s, remaining : 1525.28s\n",
      " [0698/5000] tr_loss : 36.25814, val_loss : 31.30965, elapsed : 0.34s, total : 236.52s, remaining : 1472.91s\n",
      " [0699/5000] tr_loss : 36.27092, val_loss : 31.31228, elapsed : 0.35s, total : 236.86s, remaining : 1488.36s\n",
      " [0700/5000] tr_loss : 36.25140, val_loss : 31.31541, elapsed : 0.36s, total : 237.22s, remaining : 1532.46s\n",
      " [0701/5000] tr_loss : 36.25245, val_loss : 31.31455, elapsed : 0.34s, total : 237.56s, remaining : 1473.90s\n",
      " [0702/5000] tr_loss : 36.25306, val_loss : 31.31146, elapsed : 0.34s, total : 237.91s, remaining : 1481.20s\n",
      " [0703/5000] tr_loss : 36.25109, val_loss : 31.31134, elapsed : 0.36s, total : 238.27s, remaining : 1545.31s\n",
      " [0704/5000] tr_loss : 36.26155, val_loss : 31.31060, elapsed : 0.35s, total : 238.61s, remaining : 1483.53s\n",
      " [0705/5000] tr_loss : 36.25295, val_loss : 31.31039, elapsed : 0.35s, total : 238.96s, remaining : 1495.92s\n",
      " [0706/5000] tr_loss : 36.24992, val_loss : 31.31457, elapsed : 0.36s, total : 239.32s, remaining : 1526.12s\n",
      " [0707/5000] tr_loss : 36.25421, val_loss : 31.31508, elapsed : 0.34s, total : 239.66s, remaining : 1457.74s\n",
      " [0708/5000] tr_loss : 36.26299, val_loss : 31.30749, elapsed : 0.33s, total : 239.99s, remaining : 1425.13s\n",
      " [0709/5000] tr_loss : 36.26295, val_loss : 31.31160, elapsed : 0.33s, total : 240.32s, remaining : 1428.68s\n",
      " [0710/5000] tr_loss : 36.25463, val_loss : 31.31009, elapsed : 0.33s, total : 240.66s, remaining : 1436.76s\n",
      " [0711/5000] tr_loss : 36.25764, val_loss : 31.31387, elapsed : 0.34s, total : 241.00s, remaining : 1461.67s\n",
      " [0712/5000] tr_loss : 36.25351, val_loss : 31.31023, elapsed : 0.34s, total : 241.33s, remaining : 1449.37s\n",
      " [0713/5000] tr_loss : 36.25978, val_loss : 31.31162, elapsed : 0.34s, total : 241.67s, remaining : 1457.24s\n",
      " [0714/5000] tr_loss : 36.25073, val_loss : 31.31422, elapsed : 0.34s, total : 242.01s, remaining : 1456.93s\n",
      " [0715/5000] tr_loss : 36.25170, val_loss : 31.31496, elapsed : 0.34s, total : 242.35s, remaining : 1453.54s\n",
      " [0716/5000] tr_loss : 36.24808, val_loss : 31.30956, elapsed : 0.34s, total : 242.69s, remaining : 1457.44s\n",
      " [0717/5000] tr_loss : 36.23716, val_loss : 31.30918, elapsed : 0.34s, total : 243.03s, remaining : 1453.88s\n",
      " [0718/5000] tr_loss : 36.24840, val_loss : 31.31325, elapsed : 0.34s, total : 243.37s, remaining : 1445.93s\n",
      " [0719/5000] tr_loss : 36.26617, val_loss : 31.31412, elapsed : 0.33s, total : 243.70s, remaining : 1407.74s\n",
      " [0720/5000] tr_loss : 36.25344, val_loss : 31.31352, elapsed : 0.34s, total : 244.04s, remaining : 1447.56s\n",
      " [0721/5000] tr_loss : 36.25898, val_loss : 31.30814, elapsed : 0.34s, total : 244.38s, remaining : 1458.20s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(2/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*[0001/5000] tr_loss : 44.87771, val_loss : 24.79412, elapsed : 0.35s, total : 0.35s, remaining : 1745.99s\n",
      "*[0002/5000] tr_loss : 44.82316, val_loss : 24.71703, elapsed : 0.35s, total : 0.70s, remaining : 1747.65s\n",
      "*[0003/5000] tr_loss : 44.77344, val_loss : 24.65466, elapsed : 0.35s, total : 1.05s, remaining : 1730.35s\n",
      "*[0004/5000] tr_loss : 44.73076, val_loss : 24.59797, elapsed : 0.35s, total : 1.39s, remaining : 1742.75s\n",
      "*[0005/5000] tr_loss : 44.68590, val_loss : 24.56181, elapsed : 0.35s, total : 1.75s, remaining : 1761.13s\n",
      "*[0006/5000] tr_loss : 44.64609, val_loss : 24.52640, elapsed : 0.36s, total : 2.11s, remaining : 1795.31s\n",
      "*[0007/5000] tr_loss : 44.60420, val_loss : 24.50051, elapsed : 0.36s, total : 2.46s, remaining : 1773.46s\n",
      "*[0008/5000] tr_loss : 44.56793, val_loss : 24.47083, elapsed : 0.36s, total : 2.82s, remaining : 1778.89s\n",
      "*[0009/5000] tr_loss : 44.53023, val_loss : 24.44186, elapsed : 0.40s, total : 3.22s, remaining : 1991.07s\n",
      "*[0010/5000] tr_loss : 44.49642, val_loss : 24.41350, elapsed : 0.35s, total : 3.57s, remaining : 1747.34s\n",
      "*[0011/5000] tr_loss : 44.46293, val_loss : 24.38623, elapsed : 0.35s, total : 3.92s, remaining : 1737.03s\n",
      "*[0012/5000] tr_loss : 44.42499, val_loss : 24.36512, elapsed : 0.35s, total : 4.26s, remaining : 1725.55s\n",
      "*[0013/5000] tr_loss : 44.39518, val_loss : 24.34067, elapsed : 0.34s, total : 4.60s, remaining : 1700.11s\n",
      "*[0014/5000] tr_loss : 44.35792, val_loss : 24.31242, elapsed : 0.35s, total : 4.95s, remaining : 1724.19s\n",
      "*[0015/5000] tr_loss : 44.32364, val_loss : 24.29190, elapsed : 0.35s, total : 5.30s, remaining : 1745.61s\n",
      "*[0016/5000] tr_loss : 44.29001, val_loss : 24.27082, elapsed : 0.35s, total : 5.65s, remaining : 1742.25s\n",
      "*[0017/5000] tr_loss : 44.25438, val_loss : 24.24388, elapsed : 0.35s, total : 6.00s, remaining : 1745.31s\n",
      "*[0018/5000] tr_loss : 44.22113, val_loss : 24.21834, elapsed : 0.35s, total : 6.35s, remaining : 1740.59s\n",
      "*[0019/5000] tr_loss : 44.19137, val_loss : 24.19692, elapsed : 0.34s, total : 6.69s, remaining : 1709.61s\n",
      "*[0020/5000] tr_loss : 44.15753, val_loss : 24.16471, elapsed : 0.34s, total : 7.04s, remaining : 1713.41s\n",
      "*[0021/5000] tr_loss : 44.11922, val_loss : 24.14010, elapsed : 0.35s, total : 7.38s, remaining : 1719.65s\n",
      "*[0022/5000] tr_loss : 44.08764, val_loss : 24.11930, elapsed : 0.35s, total : 7.73s, remaining : 1720.52s\n",
      "*[0023/5000] tr_loss : 44.05032, val_loss : 24.08475, elapsed : 0.35s, total : 8.07s, remaining : 1732.60s\n",
      "*[0024/5000] tr_loss : 44.01945, val_loss : 24.05779, elapsed : 0.34s, total : 8.42s, remaining : 1716.07s\n",
      "*[0025/5000] tr_loss : 43.98136, val_loss : 24.02956, elapsed : 0.35s, total : 8.77s, remaining : 1736.74s\n",
      "*[0026/5000] tr_loss : 43.94629, val_loss : 24.00947, elapsed : 0.34s, total : 9.11s, remaining : 1713.57s\n",
      "*[0027/5000] tr_loss : 43.91238, val_loss : 23.97555, elapsed : 0.34s, total : 9.46s, remaining : 1702.84s\n",
      "*[0028/5000] tr_loss : 43.87623, val_loss : 23.94825, elapsed : 0.35s, total : 9.80s, remaining : 1717.72s\n",
      "*[0029/5000] tr_loss : 43.83430, val_loss : 23.91796, elapsed : 0.34s, total : 10.14s, remaining : 1706.18s\n",
      "*[0030/5000] tr_loss : 43.80112, val_loss : 23.90032, elapsed : 0.35s, total : 10.49s, remaining : 1741.02s\n",
      "*[0031/5000] tr_loss : 43.76085, val_loss : 23.87032, elapsed : 0.35s, total : 10.84s, remaining : 1718.63s\n",
      "*[0032/5000] tr_loss : 43.72272, val_loss : 23.83548, elapsed : 0.34s, total : 11.18s, remaining : 1669.01s\n",
      "*[0033/5000] tr_loss : 43.68693, val_loss : 23.79910, elapsed : 0.34s, total : 11.52s, remaining : 1713.12s\n",
      "*[0034/5000] tr_loss : 43.64692, val_loss : 23.77394, elapsed : 0.34s, total : 11.86s, remaining : 1692.60s\n",
      "*[0035/5000] tr_loss : 43.61045, val_loss : 23.73648, elapsed : 0.34s, total : 12.20s, remaining : 1693.65s\n",
      "*[0036/5000] tr_loss : 43.56700, val_loss : 23.71365, elapsed : 0.35s, total : 12.55s, remaining : 1713.38s\n",
      "*[0037/5000] tr_loss : 43.52744, val_loss : 23.66291, elapsed : 0.35s, total : 12.90s, remaining : 1726.74s\n",
      "*[0038/5000] tr_loss : 43.49197, val_loss : 23.62659, elapsed : 0.35s, total : 13.24s, remaining : 1713.89s\n",
      "*[0039/5000] tr_loss : 43.44804, val_loss : 23.59173, elapsed : 0.34s, total : 13.58s, remaining : 1697.99s\n",
      "*[0040/5000] tr_loss : 43.40491, val_loss : 23.54520, elapsed : 0.35s, total : 13.93s, remaining : 1730.72s\n",
      "*[0041/5000] tr_loss : 43.36457, val_loss : 23.51624, elapsed : 0.35s, total : 14.28s, remaining : 1731.57s\n",
      "*[0042/5000] tr_loss : 43.32636, val_loss : 23.46993, elapsed : 0.37s, total : 14.66s, remaining : 1858.21s\n",
      "*[0043/5000] tr_loss : 43.28598, val_loss : 23.43201, elapsed : 0.38s, total : 15.04s, remaining : 1905.07s\n",
      "*[0044/5000] tr_loss : 43.24774, val_loss : 23.39304, elapsed : 0.36s, total : 15.40s, remaining : 1766.34s\n",
      "*[0045/5000] tr_loss : 43.20215, val_loss : 23.35646, elapsed : 0.35s, total : 15.75s, remaining : 1735.61s\n",
      "*[0046/5000] tr_loss : 43.15611, val_loss : 23.31314, elapsed : 0.37s, total : 16.12s, remaining : 1857.11s\n",
      "*[0047/5000] tr_loss : 43.12093, val_loss : 23.27353, elapsed : 0.35s, total : 16.48s, remaining : 1751.90s\n",
      "*[0048/5000] tr_loss : 43.07780, val_loss : 23.23916, elapsed : 0.35s, total : 16.83s, remaining : 1754.40s\n",
      "*[0049/5000] tr_loss : 43.03386, val_loss : 23.20648, elapsed : 0.34s, total : 17.18s, remaining : 1706.70s\n",
      "*[0050/5000] tr_loss : 42.99754, val_loss : 23.17030, elapsed : 0.36s, total : 17.54s, remaining : 1787.89s\n",
      "*[0051/5000] tr_loss : 42.95960, val_loss : 23.13338, elapsed : 0.41s, total : 17.94s, remaining : 2012.58s\n",
      "*[0052/5000] tr_loss : 42.91760, val_loss : 23.10389, elapsed : 0.36s, total : 18.30s, remaining : 1761.53s\n",
      "*[0053/5000] tr_loss : 42.87381, val_loss : 23.06905, elapsed : 0.38s, total : 18.68s, remaining : 1875.03s\n",
      "*[0054/5000] tr_loss : 42.83165, val_loss : 23.03549, elapsed : 0.46s, total : 19.14s, remaining : 2263.27s\n",
      "*[0055/5000] tr_loss : 42.79605, val_loss : 23.00293, elapsed : 0.37s, total : 19.51s, remaining : 1821.77s\n",
      "*[0056/5000] tr_loss : 42.75750, val_loss : 22.97712, elapsed : 0.35s, total : 19.86s, remaining : 1739.42s\n",
      "*[0057/5000] tr_loss : 42.71862, val_loss : 22.93958, elapsed : 0.38s, total : 20.23s, remaining : 1856.74s\n",
      "*[0058/5000] tr_loss : 42.67423, val_loss : 22.91031, elapsed : 0.37s, total : 20.60s, remaining : 1828.10s\n",
      "*[0059/5000] tr_loss : 42.63560, val_loss : 22.88460, elapsed : 0.34s, total : 20.94s, remaining : 1668.06s\n",
      "*[0060/5000] tr_loss : 42.59881, val_loss : 22.85080, elapsed : 0.36s, total : 21.30s, remaining : 1786.80s\n",
      "*[0061/5000] tr_loss : 42.55494, val_loss : 22.82629, elapsed : 0.34s, total : 21.64s, remaining : 1682.66s\n",
      "*[0062/5000] tr_loss : 42.51805, val_loss : 22.79517, elapsed : 0.36s, total : 22.00s, remaining : 1786.44s\n",
      "*[0063/5000] tr_loss : 42.47499, val_loss : 22.76985, elapsed : 0.35s, total : 22.36s, remaining : 1742.74s\n",
      "*[0064/5000] tr_loss : 42.43279, val_loss : 22.74330, elapsed : 0.38s, total : 22.74s, remaining : 1866.35s\n",
      "*[0065/5000] tr_loss : 42.40231, val_loss : 22.71646, elapsed : 0.34s, total : 23.08s, remaining : 1701.55s\n",
      "*[0066/5000] tr_loss : 42.35753, val_loss : 22.69450, elapsed : 0.36s, total : 23.44s, remaining : 1789.68s\n",
      "*[0067/5000] tr_loss : 42.32436, val_loss : 22.66788, elapsed : 0.35s, total : 23.79s, remaining : 1726.81s\n",
      "*[0068/5000] tr_loss : 42.28571, val_loss : 22.64560, elapsed : 0.37s, total : 24.16s, remaining : 1830.23s\n",
      "*[0069/5000] tr_loss : 42.24110, val_loss : 22.62422, elapsed : 0.35s, total : 24.52s, remaining : 1729.62s\n",
      "*[0070/5000] tr_loss : 42.20891, val_loss : 22.60477, elapsed : 0.37s, total : 24.88s, remaining : 1800.99s\n",
      "*[0071/5000] tr_loss : 42.17281, val_loss : 22.58721, elapsed : 0.34s, total : 25.22s, remaining : 1681.70s\n",
      "*[0072/5000] tr_loss : 42.13572, val_loss : 22.56586, elapsed : 0.36s, total : 25.59s, remaining : 1795.99s\n",
      "*[0073/5000] tr_loss : 42.09879, val_loss : 22.55181, elapsed : 0.35s, total : 25.94s, remaining : 1737.83s\n",
      "*[0074/5000] tr_loss : 42.06043, val_loss : 22.52853, elapsed : 0.36s, total : 26.30s, remaining : 1759.24s\n",
      "*[0075/5000] tr_loss : 42.02113, val_loss : 22.51106, elapsed : 0.36s, total : 26.66s, remaining : 1776.76s\n",
      "*[0076/5000] tr_loss : 41.98185, val_loss : 22.49613, elapsed : 0.37s, total : 27.03s, remaining : 1819.38s\n",
      "*[0077/5000] tr_loss : 41.95352, val_loss : 22.48543, elapsed : 0.35s, total : 27.38s, remaining : 1742.14s\n",
      "*[0078/5000] tr_loss : 41.90564, val_loss : 22.46968, elapsed : 0.35s, total : 27.74s, remaining : 1745.70s\n",
      "*[0079/5000] tr_loss : 41.87563, val_loss : 22.45323, elapsed : 0.34s, total : 28.08s, remaining : 1690.52s\n",
      "*[0080/5000] tr_loss : 41.82732, val_loss : 22.44388, elapsed : 0.37s, total : 28.45s, remaining : 1802.42s\n",
      "*[0081/5000] tr_loss : 41.78727, val_loss : 22.43284, elapsed : 0.35s, total : 28.79s, remaining : 1698.32s\n",
      "*[0082/5000] tr_loss : 41.74997, val_loss : 22.41857, elapsed : 0.37s, total : 29.16s, remaining : 1795.31s\n",
      "*[0083/5000] tr_loss : 41.70695, val_loss : 22.40187, elapsed : 0.36s, total : 29.52s, remaining : 1766.13s\n",
      "*[0084/5000] tr_loss : 41.67237, val_loss : 22.38314, elapsed : 0.37s, total : 29.89s, remaining : 1820.10s\n",
      "*[0085/5000] tr_loss : 41.62674, val_loss : 22.36951, elapsed : 0.35s, total : 30.23s, remaining : 1705.23s\n",
      "*[0086/5000] tr_loss : 41.57941, val_loss : 22.33947, elapsed : 0.36s, total : 30.59s, remaining : 1761.43s\n",
      "*[0087/5000] tr_loss : 41.53054, val_loss : 22.30527, elapsed : 0.34s, total : 30.94s, remaining : 1690.84s\n",
      "*[0088/5000] tr_loss : 41.48176, val_loss : 22.28333, elapsed : 0.37s, total : 31.30s, remaining : 1809.75s\n",
      "*[0089/5000] tr_loss : 41.42517, val_loss : 22.24159, elapsed : 0.35s, total : 31.65s, remaining : 1704.91s\n",
      "*[0090/5000] tr_loss : 41.39151, val_loss : 22.20083, elapsed : 0.35s, total : 32.00s, remaining : 1720.83s\n",
      "*[0091/5000] tr_loss : 41.33961, val_loss : 22.16747, elapsed : 0.35s, total : 32.35s, remaining : 1701.46s\n",
      "*[0092/5000] tr_loss : 41.28466, val_loss : 22.12026, elapsed : 0.43s, total : 32.78s, remaining : 2111.46s\n",
      "*[0093/5000] tr_loss : 41.23480, val_loss : 22.07808, elapsed : 0.36s, total : 33.14s, remaining : 1766.34s\n",
      "*[0094/5000] tr_loss : 41.18411, val_loss : 22.04217, elapsed : 0.37s, total : 33.51s, remaining : 1814.28s\n",
      "*[0095/5000] tr_loss : 41.15516, val_loss : 21.99465, elapsed : 0.36s, total : 33.87s, remaining : 1776.82s\n",
      "*[0096/5000] tr_loss : 41.10807, val_loss : 21.95182, elapsed : 0.39s, total : 34.26s, remaining : 1903.52s\n",
      "*[0097/5000] tr_loss : 41.04846, val_loss : 21.91362, elapsed : 0.36s, total : 34.62s, remaining : 1766.06s\n",
      "*[0098/5000] tr_loss : 41.00176, val_loss : 21.87323, elapsed : 0.37s, total : 34.99s, remaining : 1834.99s\n",
      "*[0099/5000] tr_loss : 40.95681, val_loss : 21.83494, elapsed : 0.36s, total : 35.35s, remaining : 1772.83s\n",
      "*[0100/5000] tr_loss : 40.91293, val_loss : 21.79926, elapsed : 0.38s, total : 35.74s, remaining : 1881.07s\n",
      "*[0101/5000] tr_loss : 40.86545, val_loss : 21.76429, elapsed : 0.36s, total : 36.10s, remaining : 1785.30s\n",
      "*[0102/5000] tr_loss : 40.82616, val_loss : 21.73003, elapsed : 0.37s, total : 36.48s, remaining : 1835.52s\n",
      "*[0103/5000] tr_loss : 40.77186, val_loss : 21.70125, elapsed : 0.36s, total : 36.84s, remaining : 1772.82s\n",
      "*[0104/5000] tr_loss : 40.73653, val_loss : 21.66445, elapsed : 0.39s, total : 37.23s, remaining : 1900.93s\n",
      "*[0105/5000] tr_loss : 40.70039, val_loss : 21.63778, elapsed : 0.36s, total : 37.59s, remaining : 1762.33s\n",
      "*[0106/5000] tr_loss : 40.64412, val_loss : 21.61204, elapsed : 0.38s, total : 37.96s, remaining : 1837.79s\n",
      "*[0107/5000] tr_loss : 40.60176, val_loss : 21.58144, elapsed : 0.36s, total : 38.33s, remaining : 1765.72s\n",
      "*[0108/5000] tr_loss : 40.55444, val_loss : 21.55362, elapsed : 0.38s, total : 38.71s, remaining : 1878.73s\n",
      "*[0109/5000] tr_loss : 40.51033, val_loss : 21.52336, elapsed : 0.36s, total : 39.07s, remaining : 1774.19s\n",
      "*[0110/5000] tr_loss : 40.46964, val_loss : 21.49405, elapsed : 0.38s, total : 39.46s, remaining : 1878.58s\n",
      "*[0111/5000] tr_loss : 40.42454, val_loss : 21.46211, elapsed : 0.36s, total : 39.81s, remaining : 1750.26s\n",
      "*[0112/5000] tr_loss : 40.37554, val_loss : 21.43251, elapsed : 0.39s, total : 40.20s, remaining : 1893.33s\n",
      "*[0113/5000] tr_loss : 40.33463, val_loss : 21.40158, elapsed : 0.36s, total : 40.56s, remaining : 1771.08s\n",
      "*[0114/5000] tr_loss : 40.29188, val_loss : 21.37267, elapsed : 0.37s, total : 40.93s, remaining : 1786.29s\n",
      "*[0115/5000] tr_loss : 40.23509, val_loss : 21.33657, elapsed : 0.36s, total : 41.29s, remaining : 1779.24s\n",
      "*[0116/5000] tr_loss : 40.20437, val_loss : 21.30620, elapsed : 0.38s, total : 41.68s, remaining : 1873.99s\n",
      "*[0117/5000] tr_loss : 40.15920, val_loss : 21.27580, elapsed : 0.36s, total : 42.04s, remaining : 1769.79s\n",
      "*[0118/5000] tr_loss : 40.11873, val_loss : 21.24916, elapsed : 0.37s, total : 42.41s, remaining : 1816.21s\n",
      "*[0119/5000] tr_loss : 40.07367, val_loss : 21.21965, elapsed : 0.36s, total : 42.77s, remaining : 1755.20s\n",
      "*[0120/5000] tr_loss : 40.02275, val_loss : 21.19798, elapsed : 0.38s, total : 43.16s, remaining : 1872.58s\n",
      "*[0121/5000] tr_loss : 39.98914, val_loss : 21.16939, elapsed : 0.35s, total : 43.51s, remaining : 1722.22s\n",
      "*[0122/5000] tr_loss : 39.94648, val_loss : 21.14348, elapsed : 0.37s, total : 43.88s, remaining : 1819.66s\n",
      "*[0123/5000] tr_loss : 39.88320, val_loss : 21.12113, elapsed : 0.36s, total : 44.24s, remaining : 1769.56s\n",
      "*[0124/5000] tr_loss : 39.85374, val_loss : 21.09537, elapsed : 0.38s, total : 44.63s, remaining : 1876.81s\n",
      "*[0125/5000] tr_loss : 39.80565, val_loss : 21.06929, elapsed : 0.36s, total : 44.99s, remaining : 1747.94s\n",
      "*[0126/5000] tr_loss : 39.76654, val_loss : 21.04375, elapsed : 0.37s, total : 45.36s, remaining : 1816.23s\n",
      "*[0127/5000] tr_loss : 39.72205, val_loss : 21.01054, elapsed : 0.36s, total : 45.72s, remaining : 1766.13s\n",
      "*[0128/5000] tr_loss : 39.66366, val_loss : 20.98154, elapsed : 0.37s, total : 46.10s, remaining : 1821.70s\n",
      "*[0129/5000] tr_loss : 39.62484, val_loss : 20.94832, elapsed : 0.39s, total : 46.48s, remaining : 1876.69s\n",
      "*[0130/5000] tr_loss : 39.57441, val_loss : 20.91182, elapsed : 0.41s, total : 46.89s, remaining : 1977.82s\n",
      "*[0131/5000] tr_loss : 39.50689, val_loss : 20.86707, elapsed : 0.37s, total : 47.26s, remaining : 1806.70s\n",
      "*[0132/5000] tr_loss : 39.45856, val_loss : 20.82399, elapsed : 0.36s, total : 47.62s, remaining : 1753.67s\n",
      "*[0133/5000] tr_loss : 39.41714, val_loss : 20.77432, elapsed : 0.38s, total : 48.00s, remaining : 1840.69s\n",
      "*[0134/5000] tr_loss : 39.34669, val_loss : 20.71935, elapsed : 0.35s, total : 48.34s, remaining : 1681.36s\n",
      "*[0135/5000] tr_loss : 39.28098, val_loss : 20.66729, elapsed : 0.37s, total : 48.71s, remaining : 1802.04s\n",
      "*[0136/5000] tr_loss : 39.23368, val_loss : 20.60848, elapsed : 0.35s, total : 49.06s, remaining : 1688.78s\n",
      "*[0137/5000] tr_loss : 39.15218, val_loss : 20.54100, elapsed : 0.38s, total : 49.44s, remaining : 1850.52s\n",
      "*[0138/5000] tr_loss : 39.08206, val_loss : 20.47928, elapsed : 0.35s, total : 49.79s, remaining : 1698.27s\n",
      "*[0139/5000] tr_loss : 39.02287, val_loss : 20.42172, elapsed : 0.37s, total : 50.16s, remaining : 1788.28s\n",
      "*[0140/5000] tr_loss : 38.94721, val_loss : 20.36101, elapsed : 0.36s, total : 50.52s, remaining : 1742.33s\n",
      "*[0141/5000] tr_loss : 38.87953, val_loss : 20.31419, elapsed : 0.35s, total : 50.87s, remaining : 1698.00s\n",
      "*[0142/5000] tr_loss : 38.81890, val_loss : 20.26665, elapsed : 0.37s, total : 51.23s, remaining : 1782.47s\n",
      "*[0143/5000] tr_loss : 38.77413, val_loss : 20.22572, elapsed : 0.36s, total : 51.60s, remaining : 1754.25s\n",
      "*[0144/5000] tr_loss : 38.70130, val_loss : 20.20078, elapsed : 0.35s, total : 51.95s, remaining : 1696.55s\n",
      "*[0145/5000] tr_loss : 38.65702, val_loss : 20.17337, elapsed : 0.35s, total : 52.29s, remaining : 1693.68s\n",
      "*[0146/5000] tr_loss : 38.60399, val_loss : 20.15890, elapsed : 0.39s, total : 52.68s, remaining : 1889.93s\n",
      "*[0147/5000] tr_loss : 38.55990, val_loss : 20.13495, elapsed : 0.37s, total : 53.05s, remaining : 1801.39s\n",
      "*[0148/5000] tr_loss : 38.50860, val_loss : 20.12229, elapsed : 0.35s, total : 53.40s, remaining : 1693.69s\n",
      "*[0149/5000] tr_loss : 38.45447, val_loss : 20.11013, elapsed : 0.38s, total : 53.78s, remaining : 1843.96s\n",
      "*[0150/5000] tr_loss : 38.41074, val_loss : 20.09948, elapsed : 0.35s, total : 54.13s, remaining : 1694.50s\n",
      "*[0151/5000] tr_loss : 38.36718, val_loss : 20.09559, elapsed : 0.36s, total : 54.50s, remaining : 1763.35s\n",
      "*[0152/5000] tr_loss : 38.31986, val_loss : 20.09096, elapsed : 0.34s, total : 54.84s, remaining : 1652.50s\n",
      "*[0153/5000] tr_loss : 38.26844, val_loss : 20.08603, elapsed : 0.36s, total : 55.20s, remaining : 1731.42s\n",
      "*[0154/5000] tr_loss : 38.24190, val_loss : 20.08562, elapsed : 0.38s, total : 55.58s, remaining : 1858.02s\n",
      "*[0155/5000] tr_loss : 38.21288, val_loss : 20.07882, elapsed : 0.35s, total : 55.93s, remaining : 1678.00s\n",
      "*[0156/5000] tr_loss : 38.16904, val_loss : 20.07108, elapsed : 0.37s, total : 56.29s, remaining : 1783.23s\n",
      " [0157/5000] tr_loss : 38.13000, val_loss : 20.07418, elapsed : 0.35s, total : 56.64s, remaining : 1689.85s\n",
      " [0158/5000] tr_loss : 38.10116, val_loss : 20.07698, elapsed : 0.37s, total : 57.01s, remaining : 1803.75s\n",
      " [0159/5000] tr_loss : 38.06105, val_loss : 20.07679, elapsed : 0.35s, total : 57.37s, remaining : 1713.08s\n",
      " [0160/5000] tr_loss : 38.02382, val_loss : 20.07870, elapsed : 0.37s, total : 57.73s, remaining : 1767.20s\n",
      " [0161/5000] tr_loss : 37.99092, val_loss : 20.07429, elapsed : 0.35s, total : 58.08s, remaining : 1686.29s\n",
      " [0162/5000] tr_loss : 37.96796, val_loss : 20.07792, elapsed : 0.34s, total : 58.42s, remaining : 1647.53s\n",
      " [0163/5000] tr_loss : 37.94173, val_loss : 20.07753, elapsed : 0.38s, total : 58.80s, remaining : 1832.51s\n",
      " [0164/5000] tr_loss : 37.90789, val_loss : 20.08580, elapsed : 0.34s, total : 59.15s, remaining : 1661.91s\n",
      " [0165/5000] tr_loss : 37.85860, val_loss : 20.08702, elapsed : 0.36s, total : 59.50s, remaining : 1723.03s\n",
      " [0166/5000] tr_loss : 37.83407, val_loss : 20.08462, elapsed : 0.34s, total : 59.85s, remaining : 1663.87s\n",
      " [0167/5000] tr_loss : 37.80972, val_loss : 20.09376, elapsed : 0.35s, total : 60.20s, remaining : 1690.99s\n",
      " [0168/5000] tr_loss : 37.78898, val_loss : 20.09113, elapsed : 0.36s, total : 60.56s, remaining : 1748.05s\n",
      " [0169/5000] tr_loss : 37.77397, val_loss : 20.09394, elapsed : 0.35s, total : 60.91s, remaining : 1689.47s\n",
      " [0170/5000] tr_loss : 37.75503, val_loss : 20.09929, elapsed : 0.40s, total : 61.31s, remaining : 1926.59s\n",
      " [0171/5000] tr_loss : 37.74218, val_loss : 20.09494, elapsed : 0.34s, total : 61.65s, remaining : 1646.62s\n",
      " [0172/5000] tr_loss : 37.73278, val_loss : 20.10600, elapsed : 0.38s, total : 62.03s, remaining : 1836.41s\n",
      " [0173/5000] tr_loss : 37.71616, val_loss : 20.10719, elapsed : 0.35s, total : 62.38s, remaining : 1711.80s\n",
      " [0174/5000] tr_loss : 37.71232, val_loss : 20.10731, elapsed : 0.36s, total : 62.74s, remaining : 1719.27s\n",
      " [0175/5000] tr_loss : 37.68714, val_loss : 20.11174, elapsed : 0.35s, total : 63.09s, remaining : 1689.15s\n",
      " [0176/5000] tr_loss : 37.67600, val_loss : 20.11169, elapsed : 0.34s, total : 63.43s, remaining : 1651.73s\n",
      " [0177/5000] tr_loss : 37.65826, val_loss : 20.11119, elapsed : 0.37s, total : 63.81s, remaining : 1801.66s\n",
      " [0178/5000] tr_loss : 37.66106, val_loss : 20.11533, elapsed : 0.34s, total : 64.14s, remaining : 1637.30s\n",
      " [0179/5000] tr_loss : 37.63805, val_loss : 20.11751, elapsed : 0.35s, total : 64.50s, remaining : 1709.74s\n",
      " [0180/5000] tr_loss : 37.63590, val_loss : 20.11825, elapsed : 0.36s, total : 64.86s, remaining : 1719.64s\n",
      " [0181/5000] tr_loss : 37.61819, val_loss : 20.12070, elapsed : 0.36s, total : 65.21s, remaining : 1728.09s\n",
      " [0182/5000] tr_loss : 37.61852, val_loss : 20.12199, elapsed : 0.38s, total : 65.59s, remaining : 1825.24s\n",
      " [0183/5000] tr_loss : 37.61281, val_loss : 20.12094, elapsed : 0.34s, total : 65.94s, remaining : 1657.16s\n",
      " [0184/5000] tr_loss : 37.61686, val_loss : 20.12635, elapsed : 0.37s, total : 66.31s, remaining : 1784.97s\n",
      " [0185/5000] tr_loss : 37.60361, val_loss : 20.12405, elapsed : 0.34s, total : 66.65s, remaining : 1636.63s\n",
      " [0186/5000] tr_loss : 37.59889, val_loss : 20.12417, elapsed : 0.37s, total : 67.02s, remaining : 1785.03s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(3/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*[0001/5000] tr_loss : 42.54345, val_loss : 49.76885, elapsed : 0.36s, total : 0.36s, remaining : 1805.36s\n",
      "*[0002/5000] tr_loss : 42.49321, val_loss : 49.73794, elapsed : 0.34s, total : 0.70s, remaining : 1698.52s\n",
      "*[0003/5000] tr_loss : 42.44991, val_loss : 49.71370, elapsed : 0.34s, total : 1.04s, remaining : 1695.03s\n",
      "*[0004/5000] tr_loss : 42.40582, val_loss : 49.68444, elapsed : 0.36s, total : 1.40s, remaining : 1802.20s\n",
      "*[0005/5000] tr_loss : 42.36596, val_loss : 49.65194, elapsed : 0.33s, total : 1.74s, remaining : 1670.90s\n",
      "*[0006/5000] tr_loss : 42.32493, val_loss : 49.61389, elapsed : 0.36s, total : 2.09s, remaining : 1778.24s\n",
      "*[0007/5000] tr_loss : 42.28592, val_loss : 49.58875, elapsed : 0.33s, total : 2.42s, remaining : 1657.82s\n",
      "*[0008/5000] tr_loss : 42.25012, val_loss : 49.55984, elapsed : 0.34s, total : 2.76s, remaining : 1701.95s\n",
      "*[0009/5000] tr_loss : 42.21644, val_loss : 49.52868, elapsed : 0.37s, total : 3.13s, remaining : 1845.05s\n",
      "*[0010/5000] tr_loss : 42.18401, val_loss : 49.50547, elapsed : 0.33s, total : 3.47s, remaining : 1664.00s\n",
      "*[0011/5000] tr_loss : 42.14840, val_loss : 49.47617, elapsed : 0.35s, total : 3.82s, remaining : 1750.88s\n",
      "*[0012/5000] tr_loss : 42.11551, val_loss : 49.44659, elapsed : 0.33s, total : 4.15s, remaining : 1646.24s\n",
      "*[0013/5000] tr_loss : 42.07954, val_loss : 49.41521, elapsed : 0.33s, total : 4.48s, remaining : 1627.23s\n",
      "*[0014/5000] tr_loss : 42.05128, val_loss : 49.38596, elapsed : 0.37s, total : 4.84s, remaining : 1827.48s\n",
      "*[0015/5000] tr_loss : 42.01474, val_loss : 49.35295, elapsed : 0.34s, total : 5.18s, remaining : 1709.67s\n",
      "*[0016/5000] tr_loss : 41.98190, val_loss : 49.32293, elapsed : 0.35s, total : 5.54s, remaining : 1749.91s\n",
      "*[0017/5000] tr_loss : 41.95290, val_loss : 49.29270, elapsed : 0.34s, total : 5.87s, remaining : 1672.27s\n",
      "*[0018/5000] tr_loss : 41.91771, val_loss : 49.26511, elapsed : 0.38s, total : 6.26s, remaining : 1916.89s\n",
      "*[0019/5000] tr_loss : 41.88776, val_loss : 49.22748, elapsed : 0.36s, total : 6.61s, remaining : 1779.61s\n",
      "*[0020/5000] tr_loss : 41.85446, val_loss : 49.19776, elapsed : 0.34s, total : 6.96s, remaining : 1699.81s\n",
      "*[0021/5000] tr_loss : 41.81920, val_loss : 49.15836, elapsed : 0.35s, total : 7.31s, remaining : 1758.64s\n",
      "*[0022/5000] tr_loss : 41.78896, val_loss : 49.13034, elapsed : 0.34s, total : 7.65s, remaining : 1697.64s\n",
      "*[0023/5000] tr_loss : 41.75693, val_loss : 49.09925, elapsed : 0.36s, total : 8.01s, remaining : 1784.32s\n",
      "*[0024/5000] tr_loss : 41.72000, val_loss : 49.06265, elapsed : 0.36s, total : 8.37s, remaining : 1804.21s\n",
      "*[0025/5000] tr_loss : 41.69053, val_loss : 49.03756, elapsed : 0.33s, total : 8.71s, remaining : 1662.79s\n",
      "*[0026/5000] tr_loss : 41.65011, val_loss : 48.99920, elapsed : 0.33s, total : 9.04s, remaining : 1648.57s\n",
      "*[0027/5000] tr_loss : 41.61887, val_loss : 48.97325, elapsed : 0.35s, total : 9.39s, remaining : 1742.58s\n",
      "*[0028/5000] tr_loss : 41.58054, val_loss : 48.94811, elapsed : 0.34s, total : 9.73s, remaining : 1680.90s\n",
      "*[0029/5000] tr_loss : 41.54896, val_loss : 48.90380, elapsed : 0.33s, total : 10.06s, remaining : 1656.83s\n",
      "*[0030/5000] tr_loss : 41.51006, val_loss : 48.88232, elapsed : 0.37s, total : 10.43s, remaining : 1824.68s\n",
      "*[0031/5000] tr_loss : 41.47520, val_loss : 48.85470, elapsed : 0.33s, total : 10.75s, remaining : 1617.75s\n",
      "*[0032/5000] tr_loss : 41.43879, val_loss : 48.82976, elapsed : 0.35s, total : 11.10s, remaining : 1720.70s\n",
      "*[0033/5000] tr_loss : 41.40500, val_loss : 48.79929, elapsed : 0.33s, total : 11.43s, remaining : 1661.90s\n",
      "*[0034/5000] tr_loss : 41.36295, val_loss : 48.76337, elapsed : 0.34s, total : 11.77s, remaining : 1700.44s\n",
      "*[0035/5000] tr_loss : 41.32290, val_loss : 48.73927, elapsed : 0.34s, total : 12.12s, remaining : 1699.23s\n",
      "*[0036/5000] tr_loss : 41.28670, val_loss : 48.70079, elapsed : 0.34s, total : 12.45s, remaining : 1664.47s\n",
      "*[0037/5000] tr_loss : 41.25063, val_loss : 48.67694, elapsed : 0.33s, total : 12.78s, remaining : 1614.90s\n",
      "*[0038/5000] tr_loss : 41.20685, val_loss : 48.65222, elapsed : 0.33s, total : 13.10s, remaining : 1615.31s\n",
      "*[0039/5000] tr_loss : 41.17375, val_loss : 48.61498, elapsed : 0.32s, total : 13.43s, remaining : 1597.74s\n",
      "*[0040/5000] tr_loss : 41.13538, val_loss : 48.58438, elapsed : 0.32s, total : 13.75s, remaining : 1587.34s\n",
      "*[0041/5000] tr_loss : 41.09407, val_loss : 48.55120, elapsed : 0.32s, total : 14.06s, remaining : 1580.57s\n",
      "*[0042/5000] tr_loss : 41.04887, val_loss : 48.50479, elapsed : 0.32s, total : 14.39s, remaining : 1591.11s\n",
      "*[0043/5000] tr_loss : 41.01223, val_loss : 48.47641, elapsed : 0.32s, total : 14.71s, remaining : 1598.08s\n",
      "*[0044/5000] tr_loss : 40.97152, val_loss : 48.43883, elapsed : 0.32s, total : 15.03s, remaining : 1601.97s\n",
      "*[0045/5000] tr_loss : 40.93415, val_loss : 48.40423, elapsed : 0.32s, total : 15.35s, remaining : 1579.94s\n",
      "*[0046/5000] tr_loss : 40.89713, val_loss : 48.36764, elapsed : 0.32s, total : 15.67s, remaining : 1573.41s\n",
      "*[0047/5000] tr_loss : 40.85491, val_loss : 48.34015, elapsed : 0.32s, total : 15.98s, remaining : 1568.76s\n",
      "*[0048/5000] tr_loss : 40.81294, val_loss : 48.29535, elapsed : 0.32s, total : 16.30s, remaining : 1567.03s\n",
      "*[0049/5000] tr_loss : 40.77083, val_loss : 48.24727, elapsed : 0.32s, total : 16.62s, remaining : 1562.27s\n",
      "*[0050/5000] tr_loss : 40.73442, val_loss : 48.21249, elapsed : 0.32s, total : 16.93s, remaining : 1567.77s\n",
      "*[0051/5000] tr_loss : 40.69214, val_loss : 48.18206, elapsed : 0.35s, total : 17.28s, remaining : 1736.14s\n",
      "*[0052/5000] tr_loss : 40.65799, val_loss : 48.13871, elapsed : 0.32s, total : 17.61s, remaining : 1591.34s\n",
      "*[0053/5000] tr_loss : 40.61620, val_loss : 48.10585, elapsed : 0.32s, total : 17.92s, remaining : 1567.73s\n",
      "*[0054/5000] tr_loss : 40.57619, val_loss : 48.04754, elapsed : 0.32s, total : 18.24s, remaining : 1563.63s\n",
      "*[0055/5000] tr_loss : 40.53430, val_loss : 48.01744, elapsed : 0.32s, total : 18.55s, remaining : 1560.96s\n",
      "*[0056/5000] tr_loss : 40.49926, val_loss : 47.97440, elapsed : 0.32s, total : 18.88s, remaining : 1604.47s\n",
      "*[0057/5000] tr_loss : 40.45344, val_loss : 47.92002, elapsed : 0.32s, total : 19.20s, remaining : 1590.92s\n",
      "*[0058/5000] tr_loss : 40.41175, val_loss : 47.88985, elapsed : 0.32s, total : 19.52s, remaining : 1559.90s\n",
      "*[0059/5000] tr_loss : 40.36788, val_loss : 47.84382, elapsed : 0.32s, total : 19.83s, remaining : 1561.70s\n",
      "*[0060/5000] tr_loss : 40.32914, val_loss : 47.80194, elapsed : 0.31s, total : 20.15s, remaining : 1555.20s\n",
      "*[0061/5000] tr_loss : 40.28872, val_loss : 47.75343, elapsed : 0.32s, total : 20.47s, remaining : 1569.47s\n",
      "*[0062/5000] tr_loss : 40.25910, val_loss : 47.71807, elapsed : 0.32s, total : 20.78s, remaining : 1567.00s\n",
      "*[0063/5000] tr_loss : 40.21507, val_loss : 47.66872, elapsed : 0.32s, total : 21.10s, remaining : 1572.56s\n",
      "*[0064/5000] tr_loss : 40.17866, val_loss : 47.62194, elapsed : 0.32s, total : 21.42s, remaining : 1561.01s\n",
      "*[0065/5000] tr_loss : 40.13631, val_loss : 47.57700, elapsed : 0.32s, total : 21.73s, remaining : 1561.74s\n",
      "*[0066/5000] tr_loss : 40.09563, val_loss : 47.54008, elapsed : 0.32s, total : 22.05s, remaining : 1560.76s\n",
      "*[0067/5000] tr_loss : 40.06699, val_loss : 47.50059, elapsed : 0.32s, total : 22.37s, remaining : 1569.66s\n",
      "*[0068/5000] tr_loss : 40.02852, val_loss : 47.45641, elapsed : 0.32s, total : 22.69s, remaining : 1575.20s\n",
      "*[0069/5000] tr_loss : 39.98038, val_loss : 47.40467, elapsed : 0.32s, total : 23.01s, remaining : 1570.16s\n",
      "*[0070/5000] tr_loss : 39.94585, val_loss : 47.36938, elapsed : 0.32s, total : 23.32s, remaining : 1559.03s\n",
      "*[0071/5000] tr_loss : 39.91013, val_loss : 47.31910, elapsed : 0.32s, total : 23.65s, remaining : 1589.34s\n",
      "*[0072/5000] tr_loss : 39.86812, val_loss : 47.27677, elapsed : 0.32s, total : 23.96s, remaining : 1568.20s\n",
      "*[0073/5000] tr_loss : 39.83479, val_loss : 47.23366, elapsed : 0.32s, total : 24.28s, remaining : 1571.29s\n",
      "*[0074/5000] tr_loss : 39.80609, val_loss : 47.18914, elapsed : 0.32s, total : 24.60s, remaining : 1571.34s\n",
      "*[0075/5000] tr_loss : 39.77019, val_loss : 47.14122, elapsed : 0.32s, total : 24.92s, remaining : 1566.07s\n",
      "*[0076/5000] tr_loss : 39.73032, val_loss : 47.09585, elapsed : 0.32s, total : 25.24s, remaining : 1580.94s\n",
      "*[0077/5000] tr_loss : 39.69130, val_loss : 47.04490, elapsed : 0.32s, total : 25.56s, remaining : 1575.11s\n",
      "*[0078/5000] tr_loss : 39.65171, val_loss : 47.00156, elapsed : 0.32s, total : 25.88s, remaining : 1577.02s\n",
      "*[0079/5000] tr_loss : 39.61975, val_loss : 46.95925, elapsed : 0.35s, total : 26.23s, remaining : 1720.44s\n",
      "*[0080/5000] tr_loss : 39.58037, val_loss : 46.91683, elapsed : 0.32s, total : 26.55s, remaining : 1571.38s\n",
      "*[0081/5000] tr_loss : 39.54603, val_loss : 46.87788, elapsed : 0.32s, total : 26.87s, remaining : 1576.13s\n",
      "*[0082/5000] tr_loss : 39.50413, val_loss : 46.83224, elapsed : 0.32s, total : 27.19s, remaining : 1568.60s\n",
      "*[0083/5000] tr_loss : 39.46694, val_loss : 46.79041, elapsed : 0.32s, total : 27.51s, remaining : 1569.80s\n",
      "*[0084/5000] tr_loss : 39.42925, val_loss : 46.74976, elapsed : 0.32s, total : 27.83s, remaining : 1573.19s\n",
      "*[0085/5000] tr_loss : 39.39261, val_loss : 46.70178, elapsed : 0.32s, total : 28.15s, remaining : 1566.63s\n",
      "*[0086/5000] tr_loss : 39.34828, val_loss : 46.67261, elapsed : 0.32s, total : 28.46s, remaining : 1556.65s\n",
      "*[0087/5000] tr_loss : 39.30583, val_loss : 46.62076, elapsed : 0.32s, total : 28.78s, remaining : 1563.57s\n",
      "*[0088/5000] tr_loss : 39.26546, val_loss : 46.57196, elapsed : 0.32s, total : 29.10s, remaining : 1568.65s\n",
      "*[0089/5000] tr_loss : 39.22914, val_loss : 46.51983, elapsed : 0.32s, total : 29.42s, remaining : 1569.13s\n",
      "*[0090/5000] tr_loss : 39.18170, val_loss : 46.46479, elapsed : 0.32s, total : 29.74s, remaining : 1563.14s\n",
      "*[0091/5000] tr_loss : 39.14271, val_loss : 46.42368, elapsed : 0.32s, total : 30.06s, remaining : 1571.41s\n",
      "*[0092/5000] tr_loss : 39.10483, val_loss : 46.35533, elapsed : 0.32s, total : 30.38s, remaining : 1581.39s\n",
      "*[0093/5000] tr_loss : 39.06434, val_loss : 46.31343, elapsed : 0.32s, total : 30.70s, remaining : 1564.86s\n",
      "*[0094/5000] tr_loss : 39.01219, val_loss : 46.25600, elapsed : 0.32s, total : 31.02s, remaining : 1557.95s\n",
      "*[0095/5000] tr_loss : 38.96559, val_loss : 46.19497, elapsed : 0.32s, total : 31.34s, remaining : 1564.84s\n",
      "*[0096/5000] tr_loss : 38.92219, val_loss : 46.12546, elapsed : 0.32s, total : 31.66s, remaining : 1570.74s\n",
      "*[0097/5000] tr_loss : 38.87651, val_loss : 46.06882, elapsed : 0.32s, total : 31.98s, remaining : 1560.87s\n",
      "*[0098/5000] tr_loss : 38.82914, val_loss : 45.99512, elapsed : 0.32s, total : 32.30s, remaining : 1564.95s\n",
      "*[0099/5000] tr_loss : 38.78433, val_loss : 45.94088, elapsed : 0.32s, total : 32.62s, remaining : 1571.45s\n",
      "*[0100/5000] tr_loss : 38.73662, val_loss : 45.86936, elapsed : 0.32s, total : 32.94s, remaining : 1568.07s\n",
      "*[0101/5000] tr_loss : 38.69330, val_loss : 45.78786, elapsed : 0.32s, total : 33.26s, remaining : 1574.09s\n",
      "*[0102/5000] tr_loss : 38.65130, val_loss : 45.72751, elapsed : 0.32s, total : 33.58s, remaining : 1563.94s\n",
      "*[0103/5000] tr_loss : 38.60135, val_loss : 45.67014, elapsed : 0.32s, total : 33.90s, remaining : 1571.08s\n",
      "*[0104/5000] tr_loss : 38.56042, val_loss : 45.59592, elapsed : 0.32s, total : 34.22s, remaining : 1575.64s\n",
      "*[0105/5000] tr_loss : 38.52524, val_loss : 45.54221, elapsed : 0.32s, total : 34.54s, remaining : 1566.55s\n",
      "*[0106/5000] tr_loss : 38.47818, val_loss : 45.47174, elapsed : 0.32s, total : 34.86s, remaining : 1570.92s\n",
      "*[0107/5000] tr_loss : 38.43655, val_loss : 45.40165, elapsed : 0.32s, total : 35.19s, remaining : 1582.84s\n",
      "*[0108/5000] tr_loss : 38.38753, val_loss : 45.32787, elapsed : 0.32s, total : 35.50s, remaining : 1562.71s\n",
      "*[0109/5000] tr_loss : 38.34744, val_loss : 45.26281, elapsed : 0.32s, total : 35.83s, remaining : 1575.52s\n",
      "*[0110/5000] tr_loss : 38.31364, val_loss : 45.19605, elapsed : 0.32s, total : 36.15s, remaining : 1563.42s\n",
      "*[0111/5000] tr_loss : 38.25901, val_loss : 45.11923, elapsed : 0.32s, total : 36.47s, remaining : 1558.03s\n",
      "*[0112/5000] tr_loss : 38.21386, val_loss : 45.05308, elapsed : 0.35s, total : 36.82s, remaining : 1720.55s\n",
      "*[0113/5000] tr_loss : 38.18199, val_loss : 44.98967, elapsed : 0.33s, total : 37.14s, remaining : 1595.66s\n",
      "*[0114/5000] tr_loss : 38.11581, val_loss : 44.91288, elapsed : 0.32s, total : 37.46s, remaining : 1564.55s\n",
      "*[0115/5000] tr_loss : 38.09118, val_loss : 44.85618, elapsed : 0.34s, total : 37.80s, remaining : 1647.41s\n",
      "*[0116/5000] tr_loss : 38.04074, val_loss : 44.77568, elapsed : 0.33s, total : 38.13s, remaining : 1593.52s\n",
      "*[0117/5000] tr_loss : 38.00329, val_loss : 44.72216, elapsed : 0.32s, total : 38.45s, remaining : 1561.81s\n",
      "*[0118/5000] tr_loss : 37.97221, val_loss : 44.67119, elapsed : 0.33s, total : 38.77s, remaining : 1589.20s\n",
      "*[0119/5000] tr_loss : 37.93030, val_loss : 44.62895, elapsed : 0.32s, total : 39.10s, remaining : 1579.45s\n",
      "*[0120/5000] tr_loss : 37.88922, val_loss : 44.57630, elapsed : 0.32s, total : 39.42s, remaining : 1573.91s\n",
      "*[0121/5000] tr_loss : 37.82734, val_loss : 44.52844, elapsed : 0.32s, total : 39.74s, remaining : 1563.76s\n",
      "*[0122/5000] tr_loss : 37.81028, val_loss : 44.46083, elapsed : 0.32s, total : 40.06s, remaining : 1572.24s\n",
      "*[0123/5000] tr_loss : 37.75550, val_loss : 44.42975, elapsed : 0.32s, total : 40.38s, remaining : 1559.15s\n",
      "*[0124/5000] tr_loss : 37.71477, val_loss : 44.38537, elapsed : 0.32s, total : 40.70s, remaining : 1566.98s\n",
      "*[0125/5000] tr_loss : 37.67954, val_loss : 44.32104, elapsed : 0.32s, total : 41.02s, remaining : 1567.16s\n",
      "*[0126/5000] tr_loss : 37.63394, val_loss : 44.25940, elapsed : 0.32s, total : 41.34s, remaining : 1559.84s\n",
      "*[0127/5000] tr_loss : 37.58877, val_loss : 44.20991, elapsed : 0.32s, total : 41.66s, remaining : 1539.41s\n",
      "*[0128/5000] tr_loss : 37.54162, val_loss : 44.16880, elapsed : 0.33s, total : 41.99s, remaining : 1585.72s\n",
      "*[0129/5000] tr_loss : 37.49690, val_loss : 44.14020, elapsed : 0.32s, total : 42.31s, remaining : 1564.19s\n",
      "*[0130/5000] tr_loss : 37.46167, val_loss : 44.07201, elapsed : 0.32s, total : 42.63s, remaining : 1568.08s\n",
      "*[0131/5000] tr_loss : 37.40928, val_loss : 44.02849, elapsed : 0.32s, total : 42.95s, remaining : 1562.34s\n",
      "*[0132/5000] tr_loss : 37.36179, val_loss : 43.99327, elapsed : 0.32s, total : 43.27s, remaining : 1554.26s\n",
      "*[0133/5000] tr_loss : 37.31493, val_loss : 43.95855, elapsed : 0.35s, total : 43.62s, remaining : 1714.49s\n",
      "*[0134/5000] tr_loss : 37.26002, val_loss : 43.90026, elapsed : 0.33s, total : 43.95s, remaining : 1583.39s\n",
      "*[0135/5000] tr_loss : 37.20946, val_loss : 43.85606, elapsed : 0.32s, total : 44.27s, remaining : 1559.98s\n",
      "*[0136/5000] tr_loss : 37.14970, val_loss : 43.81126, elapsed : 0.32s, total : 44.59s, remaining : 1560.10s\n",
      "*[0137/5000] tr_loss : 37.08907, val_loss : 43.76937, elapsed : 0.32s, total : 44.91s, remaining : 1567.39s\n",
      "*[0138/5000] tr_loss : 37.03418, val_loss : 43.72378, elapsed : 0.32s, total : 45.24s, remaining : 1578.69s\n",
      "*[0139/5000] tr_loss : 36.97093, val_loss : 43.66865, elapsed : 0.32s, total : 45.55s, remaining : 1549.01s\n",
      "*[0140/5000] tr_loss : 36.90031, val_loss : 43.62372, elapsed : 0.32s, total : 45.88s, remaining : 1558.63s\n",
      "*[0141/5000] tr_loss : 36.83977, val_loss : 43.55283, elapsed : 0.32s, total : 46.20s, remaining : 1561.10s\n",
      "*[0142/5000] tr_loss : 36.75568, val_loss : 43.48007, elapsed : 0.32s, total : 46.52s, remaining : 1560.73s\n",
      "*[0143/5000] tr_loss : 36.68404, val_loss : 43.37922, elapsed : 0.32s, total : 46.84s, remaining : 1563.57s\n",
      "*[0144/5000] tr_loss : 36.63071, val_loss : 43.30716, elapsed : 0.32s, total : 47.16s, remaining : 1550.76s\n",
      "*[0145/5000] tr_loss : 36.56342, val_loss : 43.21141, elapsed : 0.32s, total : 47.48s, remaining : 1561.31s\n",
      "*[0146/5000] tr_loss : 36.52029, val_loss : 43.11658, elapsed : 0.32s, total : 47.80s, remaining : 1561.88s\n",
      "*[0147/5000] tr_loss : 36.44575, val_loss : 43.03658, elapsed : 0.32s, total : 48.12s, remaining : 1543.48s\n",
      "*[0148/5000] tr_loss : 36.38304, val_loss : 42.94470, elapsed : 0.32s, total : 48.44s, remaining : 1548.44s\n",
      "*[0149/5000] tr_loss : 36.34845, val_loss : 42.84892, elapsed : 0.32s, total : 48.76s, remaining : 1548.22s\n",
      "*[0150/5000] tr_loss : 36.27667, val_loss : 42.76860, elapsed : 0.32s, total : 49.08s, remaining : 1556.38s\n",
      "*[0151/5000] tr_loss : 36.22548, val_loss : 42.69728, elapsed : 0.32s, total : 49.40s, remaining : 1550.42s\n",
      "*[0152/5000] tr_loss : 36.18458, val_loss : 42.62244, elapsed : 0.32s, total : 49.72s, remaining : 1552.01s\n",
      "*[0153/5000] tr_loss : 36.13433, val_loss : 42.57211, elapsed : 0.32s, total : 50.04s, remaining : 1553.22s\n",
      "*[0154/5000] tr_loss : 36.08909, val_loss : 42.48474, elapsed : 0.32s, total : 50.36s, remaining : 1553.18s\n",
      "*[0155/5000] tr_loss : 36.04946, val_loss : 42.43810, elapsed : 0.32s, total : 50.68s, remaining : 1536.09s\n",
      "*[0156/5000] tr_loss : 36.00557, val_loss : 42.38936, elapsed : 0.32s, total : 51.00s, remaining : 1558.70s\n",
      "*[0157/5000] tr_loss : 35.97475, val_loss : 42.32768, elapsed : 0.32s, total : 51.32s, remaining : 1548.86s\n",
      "*[0158/5000] tr_loss : 35.94100, val_loss : 42.27460, elapsed : 0.32s, total : 51.64s, remaining : 1549.22s\n",
      "*[0159/5000] tr_loss : 35.89797, val_loss : 42.20768, elapsed : 0.32s, total : 51.96s, remaining : 1560.40s\n",
      "*[0160/5000] tr_loss : 35.87240, val_loss : 42.17782, elapsed : 0.32s, total : 52.28s, remaining : 1541.29s\n",
      "*[0161/5000] tr_loss : 35.83365, val_loss : 42.12935, elapsed : 0.32s, total : 52.60s, remaining : 1551.18s\n",
      "*[0162/5000] tr_loss : 35.81337, val_loss : 42.08675, elapsed : 0.32s, total : 52.92s, remaining : 1553.53s\n",
      "*[0163/5000] tr_loss : 35.77646, val_loss : 42.03346, elapsed : 0.32s, total : 53.24s, remaining : 1550.32s\n",
      "*[0164/5000] tr_loss : 35.74878, val_loss : 41.98127, elapsed : 0.32s, total : 53.56s, remaining : 1550.17s\n",
      "*[0165/5000] tr_loss : 35.71180, val_loss : 41.93376, elapsed : 0.35s, total : 53.92s, remaining : 1714.30s\n",
      "*[0166/5000] tr_loss : 35.69705, val_loss : 41.90530, elapsed : 0.32s, total : 54.24s, remaining : 1569.47s\n",
      "*[0167/5000] tr_loss : 35.65568, val_loss : 41.86499, elapsed : 0.32s, total : 54.56s, remaining : 1546.45s\n",
      "*[0168/5000] tr_loss : 35.62447, val_loss : 41.80966, elapsed : 0.32s, total : 54.89s, remaining : 1564.20s\n",
      "*[0169/5000] tr_loss : 35.61082, val_loss : 41.78608, elapsed : 0.32s, total : 55.21s, remaining : 1567.77s\n",
      "*[0170/5000] tr_loss : 35.58530, val_loss : 41.72624, elapsed : 0.32s, total : 55.53s, remaining : 1556.11s\n",
      "*[0171/5000] tr_loss : 35.55637, val_loss : 41.70813, elapsed : 0.32s, total : 55.86s, remaining : 1567.90s\n",
      "*[0172/5000] tr_loss : 35.52495, val_loss : 41.66020, elapsed : 0.32s, total : 56.18s, remaining : 1547.46s\n",
      "*[0173/5000] tr_loss : 35.50744, val_loss : 41.60442, elapsed : 0.32s, total : 56.50s, remaining : 1549.29s\n",
      "*[0174/5000] tr_loss : 35.47949, val_loss : 41.58174, elapsed : 0.32s, total : 56.82s, remaining : 1553.02s\n",
      "*[0175/5000] tr_loss : 35.45792, val_loss : 41.54093, elapsed : 0.32s, total : 57.14s, remaining : 1547.70s\n",
      "*[0176/5000] tr_loss : 35.43322, val_loss : 41.51058, elapsed : 0.32s, total : 57.47s, remaining : 1559.33s\n",
      "*[0177/5000] tr_loss : 35.40830, val_loss : 41.47105, elapsed : 0.32s, total : 57.79s, remaining : 1547.61s\n",
      "*[0178/5000] tr_loss : 35.38349, val_loss : 41.43290, elapsed : 0.32s, total : 58.11s, remaining : 1547.76s\n",
      "*[0179/5000] tr_loss : 35.36544, val_loss : 41.38760, elapsed : 0.32s, total : 58.43s, remaining : 1546.58s\n",
      "*[0180/5000] tr_loss : 35.33762, val_loss : 41.34553, elapsed : 0.33s, total : 58.76s, remaining : 1583.63s\n",
      "*[0181/5000] tr_loss : 35.32375, val_loss : 41.32358, elapsed : 0.32s, total : 59.08s, remaining : 1547.70s\n",
      "*[0182/5000] tr_loss : 35.30380, val_loss : 41.29784, elapsed : 0.33s, total : 59.41s, remaining : 1572.02s\n",
      "*[0183/5000] tr_loss : 35.27690, val_loss : 41.25623, elapsed : 0.32s, total : 59.73s, remaining : 1556.31s\n",
      "*[0184/5000] tr_loss : 35.25420, val_loss : 41.21319, elapsed : 0.32s, total : 60.05s, remaining : 1564.31s\n",
      "*[0185/5000] tr_loss : 35.24715, val_loss : 41.18681, elapsed : 0.32s, total : 60.38s, remaining : 1552.53s\n",
      "*[0186/5000] tr_loss : 35.22147, val_loss : 41.16255, elapsed : 0.32s, total : 60.70s, remaining : 1564.45s\n",
      "*[0187/5000] tr_loss : 35.20582, val_loss : 41.12629, elapsed : 0.32s, total : 61.02s, remaining : 1560.09s\n",
      "*[0188/5000] tr_loss : 35.17892, val_loss : 41.09887, elapsed : 0.33s, total : 61.35s, remaining : 1575.77s\n",
      "*[0189/5000] tr_loss : 35.17464, val_loss : 41.07364, elapsed : 0.33s, total : 61.68s, remaining : 1576.35s\n",
      "*[0190/5000] tr_loss : 35.13960, val_loss : 41.02825, elapsed : 0.32s, total : 62.00s, remaining : 1560.00s\n",
      "*[0191/5000] tr_loss : 35.14133, val_loss : 41.00779, elapsed : 0.33s, total : 62.33s, remaining : 1570.66s\n",
      "*[0192/5000] tr_loss : 35.11203, val_loss : 40.97300, elapsed : 0.32s, total : 62.65s, remaining : 1541.86s\n",
      "*[0193/5000] tr_loss : 35.09966, val_loss : 40.94244, elapsed : 0.32s, total : 62.97s, remaining : 1553.73s\n",
      "*[0194/5000] tr_loss : 35.08209, val_loss : 40.91863, elapsed : 0.32s, total : 63.30s, remaining : 1544.94s\n",
      "*[0195/5000] tr_loss : 35.06682, val_loss : 40.88271, elapsed : 0.35s, total : 63.65s, remaining : 1697.12s\n",
      "*[0196/5000] tr_loss : 35.04623, val_loss : 40.85423, elapsed : 0.33s, total : 63.98s, remaining : 1567.43s\n",
      "*[0197/5000] tr_loss : 35.03234, val_loss : 40.85215, elapsed : 0.33s, total : 64.30s, remaining : 1575.91s\n",
      "*[0198/5000] tr_loss : 35.01643, val_loss : 40.80972, elapsed : 0.33s, total : 64.64s, remaining : 1604.57s\n",
      "*[0199/5000] tr_loss : 35.00018, val_loss : 40.77255, elapsed : 0.33s, total : 64.97s, remaining : 1596.02s\n",
      "*[0200/5000] tr_loss : 34.98997, val_loss : 40.75683, elapsed : 0.32s, total : 65.29s, remaining : 1552.85s\n",
      "*[0201/5000] tr_loss : 34.95716, val_loss : 40.72908, elapsed : 0.32s, total : 65.62s, remaining : 1550.43s\n",
      "*[0202/5000] tr_loss : 34.95416, val_loss : 40.70011, elapsed : 0.32s, total : 65.94s, remaining : 1553.57s\n",
      "*[0203/5000] tr_loss : 34.94530, val_loss : 40.68284, elapsed : 0.33s, total : 66.27s, remaining : 1573.85s\n",
      "*[0204/5000] tr_loss : 34.93138, val_loss : 40.64359, elapsed : 0.33s, total : 66.60s, remaining : 1567.66s\n",
      "*[0205/5000] tr_loss : 34.90679, val_loss : 40.62076, elapsed : 0.32s, total : 66.92s, remaining : 1550.45s\n",
      "*[0206/5000] tr_loss : 34.89975, val_loss : 40.59759, elapsed : 0.33s, total : 67.25s, remaining : 1565.83s\n",
      "*[0207/5000] tr_loss : 34.89770, val_loss : 40.57026, elapsed : 0.33s, total : 67.58s, remaining : 1576.97s\n",
      "*[0208/5000] tr_loss : 34.87407, val_loss : 40.54418, elapsed : 0.33s, total : 67.90s, remaining : 1576.44s\n",
      "*[0209/5000] tr_loss : 34.87046, val_loss : 40.52644, elapsed : 0.33s, total : 68.23s, remaining : 1563.36s\n",
      "*[0210/5000] tr_loss : 34.84885, val_loss : 40.51533, elapsed : 0.33s, total : 68.56s, remaining : 1583.17s\n",
      "*[0211/5000] tr_loss : 34.83212, val_loss : 40.48035, elapsed : 0.33s, total : 68.89s, remaining : 1585.54s\n",
      "*[0212/5000] tr_loss : 34.81300, val_loss : 40.45804, elapsed : 0.33s, total : 69.22s, remaining : 1567.17s\n",
      "*[0213/5000] tr_loss : 34.79971, val_loss : 40.45026, elapsed : 0.33s, total : 69.55s, remaining : 1567.63s\n",
      "*[0214/5000] tr_loss : 34.79631, val_loss : 40.42687, elapsed : 0.33s, total : 69.88s, remaining : 1573.19s\n",
      "*[0215/5000] tr_loss : 34.78826, val_loss : 40.40414, elapsed : 0.33s, total : 70.21s, remaining : 1574.41s\n",
      "*[0216/5000] tr_loss : 34.79935, val_loss : 40.38317, elapsed : 0.32s, total : 70.53s, remaining : 1550.12s\n",
      "*[0217/5000] tr_loss : 34.77474, val_loss : 40.36928, elapsed : 0.32s, total : 70.85s, remaining : 1542.97s\n",
      "*[0218/5000] tr_loss : 34.75462, val_loss : 40.34227, elapsed : 0.33s, total : 71.18s, remaining : 1566.25s\n",
      "*[0219/5000] tr_loss : 34.74221, val_loss : 40.31986, elapsed : 0.33s, total : 71.51s, remaining : 1559.84s\n",
      "*[0220/5000] tr_loss : 34.74142, val_loss : 40.30117, elapsed : 0.33s, total : 71.83s, remaining : 1569.37s\n",
      "*[0221/5000] tr_loss : 34.71498, val_loss : 40.28157, elapsed : 0.33s, total : 72.16s, remaining : 1573.22s\n",
      "*[0222/5000] tr_loss : 34.70965, val_loss : 40.26051, elapsed : 0.33s, total : 72.49s, remaining : 1568.01s\n",
      "*[0223/5000] tr_loss : 34.69892, val_loss : 40.24668, elapsed : 0.33s, total : 72.82s, remaining : 1554.21s\n",
      "*[0224/5000] tr_loss : 34.68804, val_loss : 40.22409, elapsed : 0.33s, total : 73.14s, remaining : 1555.31s\n",
      "*[0225/5000] tr_loss : 34.68270, val_loss : 40.20929, elapsed : 0.33s, total : 73.47s, remaining : 1556.60s\n",
      "*[0226/5000] tr_loss : 34.66202, val_loss : 40.17654, elapsed : 0.36s, total : 73.83s, remaining : 1714.67s\n",
      "*[0227/5000] tr_loss : 34.66548, val_loss : 40.16521, elapsed : 0.33s, total : 74.15s, remaining : 1555.09s\n",
      "*[0228/5000] tr_loss : 34.65239, val_loss : 40.15005, elapsed : 0.33s, total : 74.48s, remaining : 1564.31s\n",
      "*[0229/5000] tr_loss : 34.64659, val_loss : 40.13408, elapsed : 0.33s, total : 74.81s, remaining : 1578.87s\n",
      "*[0230/5000] tr_loss : 34.64399, val_loss : 40.11118, elapsed : 0.33s, total : 75.14s, remaining : 1583.85s\n",
      "*[0231/5000] tr_loss : 34.62489, val_loss : 40.09516, elapsed : 0.33s, total : 75.48s, remaining : 1592.96s\n",
      "*[0232/5000] tr_loss : 34.61774, val_loss : 40.07168, elapsed : 0.33s, total : 75.81s, remaining : 1570.88s\n",
      "*[0233/5000] tr_loss : 34.61571, val_loss : 40.06271, elapsed : 0.33s, total : 76.14s, remaining : 1561.69s\n",
      "*[0234/5000] tr_loss : 34.59707, val_loss : 40.05221, elapsed : 0.34s, total : 76.47s, remaining : 1615.86s\n",
      "*[0235/5000] tr_loss : 34.58812, val_loss : 40.02641, elapsed : 0.33s, total : 76.80s, remaining : 1565.14s\n",
      "*[0236/5000] tr_loss : 34.58473, val_loss : 40.00995, elapsed : 0.33s, total : 77.13s, remaining : 1577.59s\n",
      "*[0237/5000] tr_loss : 34.58554, val_loss : 39.99717, elapsed : 0.33s, total : 77.47s, remaining : 1580.45s\n",
      "*[0238/5000] tr_loss : 34.57026, val_loss : 39.98189, elapsed : 0.33s, total : 77.79s, remaining : 1561.05s\n",
      "*[0239/5000] tr_loss : 34.56091, val_loss : 39.95918, elapsed : 0.33s, total : 78.12s, remaining : 1565.28s\n",
      "*[0240/5000] tr_loss : 34.55669, val_loss : 39.93542, elapsed : 0.33s, total : 78.45s, remaining : 1554.63s\n",
      "*[0241/5000] tr_loss : 34.55447, val_loss : 39.91803, elapsed : 0.33s, total : 78.78s, remaining : 1583.31s\n",
      "*[0242/5000] tr_loss : 34.53949, val_loss : 39.91023, elapsed : 0.33s, total : 79.11s, remaining : 1562.81s\n",
      "*[0243/5000] tr_loss : 34.52168, val_loss : 39.89129, elapsed : 0.33s, total : 79.44s, remaining : 1585.80s\n",
      "*[0244/5000] tr_loss : 34.52880, val_loss : 39.88740, elapsed : 0.33s, total : 79.78s, remaining : 1576.87s\n",
      "*[0245/5000] tr_loss : 34.51001, val_loss : 39.85928, elapsed : 0.33s, total : 80.11s, remaining : 1578.21s\n",
      "*[0246/5000] tr_loss : 34.51038, val_loss : 39.83697, elapsed : 0.33s, total : 80.44s, remaining : 1571.85s\n",
      "*[0247/5000] tr_loss : 34.50235, val_loss : 39.82972, elapsed : 0.33s, total : 80.77s, remaining : 1557.63s\n",
      "*[0248/5000] tr_loss : 34.49498, val_loss : 39.81242, elapsed : 0.33s, total : 81.10s, remaining : 1583.78s\n",
      "*[0249/5000] tr_loss : 34.49749, val_loss : 39.79098, elapsed : 0.33s, total : 81.43s, remaining : 1579.73s\n",
      "*[0250/5000] tr_loss : 34.47099, val_loss : 39.77426, elapsed : 0.33s, total : 81.76s, remaining : 1571.68s\n",
      "*[0251/5000] tr_loss : 34.47876, val_loss : 39.76324, elapsed : 0.33s, total : 82.09s, remaining : 1566.36s\n",
      "*[0252/5000] tr_loss : 34.45880, val_loss : 39.75616, elapsed : 0.33s, total : 82.42s, remaining : 1566.61s\n",
      "*[0253/5000] tr_loss : 34.45297, val_loss : 39.73146, elapsed : 0.33s, total : 82.75s, remaining : 1556.34s\n",
      "*[0254/5000] tr_loss : 34.45451, val_loss : 39.72610, elapsed : 0.33s, total : 83.08s, remaining : 1570.19s\n",
      "*[0255/5000] tr_loss : 34.44655, val_loss : 39.71422, elapsed : 0.36s, total : 83.44s, remaining : 1703.40s\n",
      "*[0256/5000] tr_loss : 34.44738, val_loss : 39.68869, elapsed : 0.34s, total : 83.78s, remaining : 1592.01s\n",
      "*[0257/5000] tr_loss : 34.43457, val_loss : 39.67881, elapsed : 0.33s, total : 84.11s, remaining : 1558.60s\n",
      "*[0258/5000] tr_loss : 34.42419, val_loss : 39.66450, elapsed : 0.33s, total : 84.43s, remaining : 1562.38s\n",
      "*[0259/5000] tr_loss : 34.42532, val_loss : 39.65513, elapsed : 0.33s, total : 84.76s, remaining : 1562.94s\n",
      "*[0260/5000] tr_loss : 34.42568, val_loss : 39.63790, elapsed : 0.33s, total : 85.10s, remaining : 1570.62s\n",
      "*[0261/5000] tr_loss : 34.40975, val_loss : 39.61656, elapsed : 0.33s, total : 85.42s, remaining : 1551.77s\n",
      "*[0262/5000] tr_loss : 34.39905, val_loss : 39.60639, elapsed : 0.33s, total : 85.75s, remaining : 1568.11s\n",
      "*[0263/5000] tr_loss : 34.40104, val_loss : 39.59113, elapsed : 0.33s, total : 86.09s, remaining : 1571.52s\n",
      "*[0264/5000] tr_loss : 34.39627, val_loss : 39.58248, elapsed : 0.33s, total : 86.42s, remaining : 1559.01s\n",
      "*[0265/5000] tr_loss : 34.39181, val_loss : 39.57595, elapsed : 0.33s, total : 86.74s, remaining : 1557.80s\n",
      "*[0266/5000] tr_loss : 34.38398, val_loss : 39.56209, elapsed : 0.33s, total : 87.08s, remaining : 1564.93s\n",
      "*[0267/5000] tr_loss : 34.38787, val_loss : 39.55147, elapsed : 0.33s, total : 87.41s, remaining : 1562.32s\n",
      "*[0268/5000] tr_loss : 34.36908, val_loss : 39.53229, elapsed : 0.33s, total : 87.74s, remaining : 1565.96s\n",
      "*[0269/5000] tr_loss : 34.36028, val_loss : 39.51740, elapsed : 0.33s, total : 88.07s, remaining : 1566.18s\n",
      "*[0270/5000] tr_loss : 34.36950, val_loss : 39.50141, elapsed : 0.33s, total : 88.39s, remaining : 1543.65s\n",
      "*[0271/5000] tr_loss : 34.35179, val_loss : 39.49324, elapsed : 0.33s, total : 88.72s, remaining : 1559.98s\n",
      "*[0272/5000] tr_loss : 34.34806, val_loss : 39.48923, elapsed : 0.33s, total : 89.05s, remaining : 1562.15s\n",
      "*[0273/5000] tr_loss : 34.34805, val_loss : 39.46189, elapsed : 0.33s, total : 89.38s, remaining : 1551.26s\n",
      "*[0274/5000] tr_loss : 34.33931, val_loss : 39.45731, elapsed : 0.33s, total : 89.71s, remaining : 1553.57s\n",
      "*[0275/5000] tr_loss : 34.33796, val_loss : 39.44514, elapsed : 0.33s, total : 90.04s, remaining : 1557.24s\n",
      "*[0276/5000] tr_loss : 34.32460, val_loss : 39.42780, elapsed : 0.33s, total : 90.37s, remaining : 1547.50s\n",
      "*[0277/5000] tr_loss : 34.33556, val_loss : 39.41812, elapsed : 0.33s, total : 90.70s, remaining : 1570.45s\n",
      "*[0278/5000] tr_loss : 34.31510, val_loss : 39.40794, elapsed : 0.33s, total : 91.03s, remaining : 1570.49s\n",
      "*[0279/5000] tr_loss : 34.32041, val_loss : 39.39737, elapsed : 0.33s, total : 91.37s, remaining : 1570.32s\n",
      "*[0280/5000] tr_loss : 34.31894, val_loss : 39.39441, elapsed : 0.33s, total : 91.70s, remaining : 1554.70s\n",
      "*[0281/5000] tr_loss : 34.31262, val_loss : 39.37508, elapsed : 0.33s, total : 92.03s, remaining : 1567.49s\n",
      "*[0282/5000] tr_loss : 34.30925, val_loss : 39.36122, elapsed : 0.36s, total : 92.39s, remaining : 1706.99s\n",
      "*[0283/5000] tr_loss : 34.30111, val_loss : 39.34556, elapsed : 0.33s, total : 92.72s, remaining : 1563.20s\n",
      "*[0284/5000] tr_loss : 34.30181, val_loss : 39.34144, elapsed : 0.33s, total : 93.05s, remaining : 1553.01s\n",
      "*[0285/5000] tr_loss : 34.28762, val_loss : 39.33034, elapsed : 0.33s, total : 93.38s, remaining : 1549.33s\n",
      "*[0286/5000] tr_loss : 34.28961, val_loss : 39.30968, elapsed : 0.33s, total : 93.71s, remaining : 1550.81s\n",
      "*[0287/5000] tr_loss : 34.28361, val_loss : 39.30313, elapsed : 0.33s, total : 94.04s, remaining : 1562.91s\n",
      "*[0288/5000] tr_loss : 34.28215, val_loss : 39.29711, elapsed : 0.33s, total : 94.37s, remaining : 1539.41s\n",
      "*[0289/5000] tr_loss : 34.28273, val_loss : 39.28587, elapsed : 0.33s, total : 94.70s, remaining : 1549.06s\n",
      "*[0290/5000] tr_loss : 34.27646, val_loss : 39.26656, elapsed : 0.33s, total : 95.02s, remaining : 1541.13s\n",
      "*[0291/5000] tr_loss : 34.26480, val_loss : 39.25969, elapsed : 0.33s, total : 95.35s, remaining : 1548.95s\n",
      "*[0292/5000] tr_loss : 34.26819, val_loss : 39.25052, elapsed : 0.33s, total : 95.68s, remaining : 1553.07s\n",
      "*[0293/5000] tr_loss : 34.26905, val_loss : 39.24222, elapsed : 0.33s, total : 96.01s, remaining : 1543.96s\n",
      "*[0294/5000] tr_loss : 34.25323, val_loss : 39.22924, elapsed : 0.33s, total : 96.34s, remaining : 1546.51s\n",
      "*[0295/5000] tr_loss : 34.24864, val_loss : 39.22162, elapsed : 0.33s, total : 96.67s, remaining : 1544.94s\n",
      "*[0296/5000] tr_loss : 34.24970, val_loss : 39.21620, elapsed : 0.33s, total : 97.00s, remaining : 1556.39s\n",
      "*[0297/5000] tr_loss : 34.23778, val_loss : 39.20126, elapsed : 0.33s, total : 97.33s, remaining : 1549.99s\n",
      "*[0298/5000] tr_loss : 34.24479, val_loss : 39.19145, elapsed : 0.33s, total : 97.66s, remaining : 1549.02s\n",
      "*[0299/5000] tr_loss : 34.24838, val_loss : 39.18936, elapsed : 0.33s, total : 97.99s, remaining : 1563.33s\n",
      "*[0300/5000] tr_loss : 34.23736, val_loss : 39.18119, elapsed : 0.33s, total : 98.32s, remaining : 1545.39s\n",
      "*[0301/5000] tr_loss : 34.23938, val_loss : 39.17050, elapsed : 0.33s, total : 98.65s, remaining : 1569.02s\n",
      "*[0302/5000] tr_loss : 34.24233, val_loss : 39.15474, elapsed : 0.33s, total : 98.98s, remaining : 1557.86s\n",
      "*[0303/5000] tr_loss : 34.23650, val_loss : 39.14691, elapsed : 0.33s, total : 99.31s, remaining : 1548.98s\n",
      "*[0304/5000] tr_loss : 34.22046, val_loss : 39.13716, elapsed : 0.35s, total : 99.67s, remaining : 1660.13s\n",
      "*[0305/5000] tr_loss : 34.22713, val_loss : 39.12481, elapsed : 0.33s, total : 100.00s, remaining : 1549.80s\n",
      "*[0306/5000] tr_loss : 34.20746, val_loss : 39.10999, elapsed : 0.33s, total : 100.33s, remaining : 1549.89s\n",
      "*[0307/5000] tr_loss : 34.21565, val_loss : 39.10136, elapsed : 0.33s, total : 100.66s, remaining : 1553.90s\n",
      " [0308/5000] tr_loss : 34.22923, val_loss : 39.10992, elapsed : 0.34s, total : 100.99s, remaining : 1572.16s\n",
      "*[0309/5000] tr_loss : 34.20623, val_loss : 39.09391, elapsed : 0.36s, total : 101.35s, remaining : 1678.08s\n",
      "*[0310/5000] tr_loss : 34.19980, val_loss : 39.08289, elapsed : 0.33s, total : 101.68s, remaining : 1562.90s\n",
      "*[0311/5000] tr_loss : 34.20782, val_loss : 39.08007, elapsed : 0.33s, total : 102.02s, remaining : 1555.67s\n",
      "*[0312/5000] tr_loss : 34.20077, val_loss : 39.06567, elapsed : 0.33s, total : 102.34s, remaining : 1532.25s\n",
      "*[0313/5000] tr_loss : 34.19617, val_loss : 39.06080, elapsed : 0.33s, total : 102.67s, remaining : 1544.43s\n",
      "*[0314/5000] tr_loss : 34.20525, val_loss : 39.04714, elapsed : 0.33s, total : 103.00s, remaining : 1542.12s\n",
      "*[0315/5000] tr_loss : 34.20164, val_loss : 39.03978, elapsed : 0.33s, total : 103.33s, remaining : 1539.21s\n",
      "*[0316/5000] tr_loss : 34.18927, val_loss : 39.03837, elapsed : 0.33s, total : 103.66s, remaining : 1535.22s\n",
      "*[0317/5000] tr_loss : 34.18708, val_loss : 39.01747, elapsed : 0.33s, total : 103.99s, remaining : 1544.70s\n",
      "*[0318/5000] tr_loss : 34.18584, val_loss : 39.00941, elapsed : 0.33s, total : 104.32s, remaining : 1532.93s\n",
      "*[0319/5000] tr_loss : 34.18702, val_loss : 39.00379, elapsed : 0.33s, total : 104.64s, remaining : 1532.23s\n",
      "*[0320/5000] tr_loss : 34.17990, val_loss : 38.99728, elapsed : 0.33s, total : 104.97s, remaining : 1538.94s\n",
      " [0321/5000] tr_loss : 34.18827, val_loss : 39.00016, elapsed : 0.33s, total : 105.30s, remaining : 1541.04s\n",
      "*[0322/5000] tr_loss : 34.17630, val_loss : 38.98771, elapsed : 0.33s, total : 105.63s, remaining : 1532.70s\n",
      "*[0323/5000] tr_loss : 34.17413, val_loss : 38.98016, elapsed : 0.33s, total : 105.96s, remaining : 1548.66s\n",
      "*[0324/5000] tr_loss : 34.16675, val_loss : 38.97317, elapsed : 0.33s, total : 106.29s, remaining : 1538.35s\n",
      "*[0325/5000] tr_loss : 34.16650, val_loss : 38.96010, elapsed : 0.33s, total : 106.62s, remaining : 1526.72s\n",
      "*[0326/5000] tr_loss : 34.18466, val_loss : 38.95320, elapsed : 0.33s, total : 106.95s, remaining : 1542.88s\n",
      "*[0327/5000] tr_loss : 34.18375, val_loss : 38.94235, elapsed : 0.33s, total : 107.28s, remaining : 1548.76s\n",
      "*[0328/5000] tr_loss : 34.17189, val_loss : 38.94122, elapsed : 0.33s, total : 107.61s, remaining : 1560.88s\n",
      "*[0329/5000] tr_loss : 34.16211, val_loss : 38.92918, elapsed : 0.33s, total : 107.94s, remaining : 1525.09s\n",
      "*[0330/5000] tr_loss : 34.15339, val_loss : 38.92014, elapsed : 0.33s, total : 108.27s, remaining : 1532.53s\n",
      "*[0331/5000] tr_loss : 34.15796, val_loss : 38.91805, elapsed : 0.33s, total : 108.60s, remaining : 1547.94s\n",
      "*[0332/5000] tr_loss : 34.14023, val_loss : 38.90546, elapsed : 0.33s, total : 108.93s, remaining : 1548.58s\n",
      " [0333/5000] tr_loss : 34.15464, val_loss : 38.90790, elapsed : 0.33s, total : 109.26s, remaining : 1532.34s\n",
      "*[0334/5000] tr_loss : 34.15147, val_loss : 38.90310, elapsed : 0.32s, total : 109.58s, remaining : 1514.76s\n",
      "*[0335/5000] tr_loss : 34.15295, val_loss : 38.88329, elapsed : 0.33s, total : 109.91s, remaining : 1537.16s\n",
      "*[0336/5000] tr_loss : 34.13671, val_loss : 38.87327, elapsed : 0.33s, total : 110.24s, remaining : 1539.30s\n",
      "*[0337/5000] tr_loss : 34.14054, val_loss : 38.87148, elapsed : 0.36s, total : 110.60s, remaining : 1679.63s\n",
      "*[0338/5000] tr_loss : 34.14417, val_loss : 38.86558, elapsed : 0.33s, total : 110.93s, remaining : 1545.08s\n",
      "*[0339/5000] tr_loss : 34.13469, val_loss : 38.85993, elapsed : 0.33s, total : 111.26s, remaining : 1537.61s\n",
      "*[0340/5000] tr_loss : 34.13854, val_loss : 38.85185, elapsed : 0.33s, total : 111.59s, remaining : 1536.52s\n",
      "*[0341/5000] tr_loss : 34.13024, val_loss : 38.84229, elapsed : 0.33s, total : 111.92s, remaining : 1539.03s\n",
      " [0342/5000] tr_loss : 34.13192, val_loss : 38.84321, elapsed : 0.33s, total : 112.25s, remaining : 1524.98s\n",
      "*[0343/5000] tr_loss : 34.13209, val_loss : 38.82815, elapsed : 0.33s, total : 112.58s, remaining : 1524.73s\n",
      "*[0344/5000] tr_loss : 34.12988, val_loss : 38.82689, elapsed : 0.33s, total : 112.91s, remaining : 1539.65s\n",
      "*[0345/5000] tr_loss : 34.12241, val_loss : 38.81886, elapsed : 0.33s, total : 113.24s, remaining : 1540.70s\n",
      "*[0346/5000] tr_loss : 34.12448, val_loss : 38.81113, elapsed : 0.33s, total : 113.57s, remaining : 1526.00s\n",
      "*[0347/5000] tr_loss : 34.12708, val_loss : 38.80305, elapsed : 0.33s, total : 113.90s, remaining : 1538.81s\n",
      "*[0348/5000] tr_loss : 34.11314, val_loss : 38.79365, elapsed : 0.33s, total : 114.23s, remaining : 1527.41s\n",
      "*[0349/5000] tr_loss : 34.11910, val_loss : 38.78848, elapsed : 0.33s, total : 114.56s, remaining : 1533.10s\n",
      "*[0350/5000] tr_loss : 34.11122, val_loss : 38.78172, elapsed : 0.33s, total : 114.89s, remaining : 1556.73s\n",
      "*[0351/5000] tr_loss : 34.10892, val_loss : 38.78017, elapsed : 0.33s, total : 115.23s, remaining : 1552.07s\n",
      "*[0352/5000] tr_loss : 34.11591, val_loss : 38.77180, elapsed : 0.33s, total : 115.56s, remaining : 1553.90s\n",
      "*[0353/5000] tr_loss : 34.10388, val_loss : 38.75125, elapsed : 0.33s, total : 115.89s, remaining : 1535.06s\n",
      " [0354/5000] tr_loss : 34.11883, val_loss : 38.75616, elapsed : 0.33s, total : 116.22s, remaining : 1518.07s\n",
      " [0355/5000] tr_loss : 34.10923, val_loss : 38.75233, elapsed : 0.33s, total : 116.54s, remaining : 1516.10s\n",
      "*[0356/5000] tr_loss : 34.10808, val_loss : 38.74843, elapsed : 0.33s, total : 116.87s, remaining : 1527.44s\n",
      "*[0357/5000] tr_loss : 34.10578, val_loss : 38.73587, elapsed : 0.33s, total : 117.20s, remaining : 1537.88s\n",
      "*[0358/5000] tr_loss : 34.10894, val_loss : 38.72775, elapsed : 0.33s, total : 117.54s, remaining : 1534.74s\n",
      "*[0359/5000] tr_loss : 34.10268, val_loss : 38.72378, elapsed : 0.33s, total : 117.87s, remaining : 1533.09s\n",
      "*[0360/5000] tr_loss : 34.10856, val_loss : 38.70982, elapsed : 0.34s, total : 118.20s, remaining : 1555.99s\n",
      "*[0361/5000] tr_loss : 34.10129, val_loss : 38.70715, elapsed : 0.33s, total : 118.53s, remaining : 1540.44s\n",
      "*[0362/5000] tr_loss : 34.09839, val_loss : 38.70135, elapsed : 0.33s, total : 118.87s, remaining : 1545.02s\n",
      "*[0363/5000] tr_loss : 34.09700, val_loss : 38.69308, elapsed : 0.33s, total : 119.20s, remaining : 1532.38s\n",
      "*[0364/5000] tr_loss : 34.09434, val_loss : 38.69103, elapsed : 0.34s, total : 119.53s, remaining : 1553.58s\n",
      "*[0365/5000] tr_loss : 34.09854, val_loss : 38.67659, elapsed : 0.33s, total : 119.86s, remaining : 1533.71s\n",
      "*[0366/5000] tr_loss : 34.09819, val_loss : 38.66654, elapsed : 0.33s, total : 120.19s, remaining : 1522.83s\n",
      "*[0367/5000] tr_loss : 34.10446, val_loss : 38.65644, elapsed : 0.36s, total : 120.55s, remaining : 1667.38s\n",
      "*[0368/5000] tr_loss : 34.09486, val_loss : 38.65591, elapsed : 0.33s, total : 120.88s, remaining : 1540.86s\n",
      "*[0369/5000] tr_loss : 34.08529, val_loss : 38.65332, elapsed : 0.34s, total : 121.22s, remaining : 1571.48s\n",
      "*[0370/5000] tr_loss : 34.09270, val_loss : 38.64699, elapsed : 0.33s, total : 121.55s, remaining : 1532.33s\n",
      "*[0371/5000] tr_loss : 34.08388, val_loss : 38.64026, elapsed : 0.33s, total : 121.89s, remaining : 1531.82s\n",
      "*[0372/5000] tr_loss : 34.07981, val_loss : 38.62993, elapsed : 0.33s, total : 122.21s, remaining : 1522.75s\n",
      "*[0373/5000] tr_loss : 34.07470, val_loss : 38.62244, elapsed : 0.33s, total : 122.54s, remaining : 1525.11s\n",
      " [0374/5000] tr_loss : 34.08915, val_loss : 38.62282, elapsed : 0.33s, total : 122.87s, remaining : 1524.32s\n",
      "*[0375/5000] tr_loss : 34.07621, val_loss : 38.61107, elapsed : 0.33s, total : 123.21s, remaining : 1533.04s\n",
      "*[0376/5000] tr_loss : 34.07567, val_loss : 38.60461, elapsed : 0.33s, total : 123.54s, remaining : 1534.69s\n",
      "*[0377/5000] tr_loss : 34.07688, val_loss : 38.60035, elapsed : 0.33s, total : 123.87s, remaining : 1530.54s\n",
      "*[0378/5000] tr_loss : 34.07621, val_loss : 38.58825, elapsed : 0.33s, total : 124.20s, remaining : 1522.14s\n",
      "*[0379/5000] tr_loss : 34.08105, val_loss : 38.58482, elapsed : 0.33s, total : 124.53s, remaining : 1531.27s\n",
      "*[0380/5000] tr_loss : 34.07605, val_loss : 38.57883, elapsed : 0.34s, total : 124.86s, remaining : 1550.55s\n",
      "*[0381/5000] tr_loss : 34.06405, val_loss : 38.56784, elapsed : 0.33s, total : 125.20s, remaining : 1541.28s\n",
      "*[0382/5000] tr_loss : 34.07499, val_loss : 38.56694, elapsed : 0.33s, total : 125.53s, remaining : 1523.88s\n",
      "*[0383/5000] tr_loss : 34.05875, val_loss : 38.56163, elapsed : 0.33s, total : 125.86s, remaining : 1526.56s\n",
      "*[0384/5000] tr_loss : 34.07381, val_loss : 38.55658, elapsed : 0.33s, total : 126.19s, remaining : 1523.97s\n",
      "*[0385/5000] tr_loss : 34.06789, val_loss : 38.54659, elapsed : 0.33s, total : 126.52s, remaining : 1528.61s\n",
      "*[0386/5000] tr_loss : 34.05802, val_loss : 38.54171, elapsed : 0.33s, total : 126.85s, remaining : 1523.93s\n",
      "*[0387/5000] tr_loss : 34.06689, val_loss : 38.53567, elapsed : 0.33s, total : 127.18s, remaining : 1526.90s\n",
      "*[0388/5000] tr_loss : 34.06448, val_loss : 38.52792, elapsed : 0.33s, total : 127.52s, remaining : 1538.19s\n",
      "*[0389/5000] tr_loss : 34.05854, val_loss : 38.52343, elapsed : 0.33s, total : 127.85s, remaining : 1521.38s\n",
      "*[0390/5000] tr_loss : 34.06937, val_loss : 38.50853, elapsed : 0.33s, total : 128.18s, remaining : 1521.73s\n",
      "*[0391/5000] tr_loss : 34.04790, val_loss : 38.50653, elapsed : 0.33s, total : 128.51s, remaining : 1519.81s\n",
      " [0392/5000] tr_loss : 34.05815, val_loss : 38.50722, elapsed : 0.36s, total : 128.86s, remaining : 1641.04s\n",
      "*[0393/5000] tr_loss : 34.06204, val_loss : 38.49910, elapsed : 0.33s, total : 129.20s, remaining : 1537.50s\n",
      "*[0394/5000] tr_loss : 34.05994, val_loss : 38.49177, elapsed : 0.33s, total : 129.52s, remaining : 1517.54s\n",
      " [0395/5000] tr_loss : 34.05112, val_loss : 38.49279, elapsed : 0.33s, total : 129.86s, remaining : 1526.49s\n",
      "*[0396/5000] tr_loss : 34.04829, val_loss : 38.47743, elapsed : 0.33s, total : 130.19s, remaining : 1518.34s\n",
      "*[0397/5000] tr_loss : 34.05406, val_loss : 38.46740, elapsed : 0.33s, total : 130.52s, remaining : 1525.01s\n",
      "*[0398/5000] tr_loss : 34.05067, val_loss : 38.46423, elapsed : 0.33s, total : 130.85s, remaining : 1512.82s\n",
      " [0399/5000] tr_loss : 34.04291, val_loss : 38.46757, elapsed : 0.33s, total : 131.18s, remaining : 1523.88s\n",
      "*[0400/5000] tr_loss : 34.04639, val_loss : 38.46247, elapsed : 0.33s, total : 131.50s, remaining : 1498.82s\n",
      "*[0401/5000] tr_loss : 34.04478, val_loss : 38.45863, elapsed : 0.33s, total : 131.84s, remaining : 1524.59s\n",
      "*[0402/5000] tr_loss : 34.04803, val_loss : 38.45212, elapsed : 0.33s, total : 132.16s, remaining : 1506.48s\n",
      "*[0403/5000] tr_loss : 34.05412, val_loss : 38.45184, elapsed : 0.33s, total : 132.49s, remaining : 1526.12s\n",
      "*[0404/5000] tr_loss : 34.03709, val_loss : 38.44905, elapsed : 0.33s, total : 132.82s, remaining : 1512.65s\n",
      "*[0405/5000] tr_loss : 34.05673, val_loss : 38.44477, elapsed : 0.33s, total : 133.15s, remaining : 1516.60s\n",
      "*[0406/5000] tr_loss : 34.04611, val_loss : 38.43923, elapsed : 0.33s, total : 133.49s, remaining : 1530.30s\n",
      " [0407/5000] tr_loss : 34.04917, val_loss : 38.44105, elapsed : 0.33s, total : 133.82s, remaining : 1512.21s\n",
      " [0408/5000] tr_loss : 34.04820, val_loss : 38.44073, elapsed : 0.33s, total : 134.14s, remaining : 1501.61s\n",
      " [0409/5000] tr_loss : 34.03895, val_loss : 38.44038, elapsed : 0.33s, total : 134.47s, remaining : 1507.04s\n",
      " [0410/5000] tr_loss : 34.03977, val_loss : 38.45100, elapsed : 0.33s, total : 134.80s, remaining : 1503.90s\n",
      " [0411/5000] tr_loss : 34.03834, val_loss : 38.44972, elapsed : 0.33s, total : 135.13s, remaining : 1532.90s\n",
      " [0412/5000] tr_loss : 34.04037, val_loss : 38.45359, elapsed : 0.33s, total : 135.46s, remaining : 1508.78s\n",
      " [0413/5000] tr_loss : 34.03974, val_loss : 38.45580, elapsed : 0.33s, total : 135.79s, remaining : 1501.88s\n",
      " [0414/5000] tr_loss : 34.04155, val_loss : 38.45755, elapsed : 0.33s, total : 136.12s, remaining : 1507.06s\n",
      " [0415/5000] tr_loss : 34.04250, val_loss : 38.46948, elapsed : 0.33s, total : 136.45s, remaining : 1507.06s\n",
      " [0416/5000] tr_loss : 34.05054, val_loss : 38.46791, elapsed : 0.33s, total : 136.78s, remaining : 1502.80s\n",
      " [0417/5000] tr_loss : 34.04272, val_loss : 38.47059, elapsed : 0.36s, total : 137.13s, remaining : 1637.77s\n",
      " [0418/5000] tr_loss : 34.04486, val_loss : 38.47815, elapsed : 0.33s, total : 137.46s, remaining : 1515.05s\n",
      " [0419/5000] tr_loss : 34.04101, val_loss : 38.47694, elapsed : 0.33s, total : 137.79s, remaining : 1505.61s\n",
      " [0420/5000] tr_loss : 34.03450, val_loss : 38.47806, elapsed : 0.33s, total : 138.12s, remaining : 1495.92s\n",
      " [0421/5000] tr_loss : 34.02877, val_loss : 38.47651, elapsed : 0.33s, total : 138.45s, remaining : 1502.30s\n",
      " [0422/5000] tr_loss : 34.03142, val_loss : 38.47970, elapsed : 0.33s, total : 138.78s, remaining : 1521.02s\n",
      " [0423/5000] tr_loss : 34.03342, val_loss : 38.48084, elapsed : 0.33s, total : 139.11s, remaining : 1523.70s\n",
      " [0424/5000] tr_loss : 34.03890, val_loss : 38.48726, elapsed : 0.33s, total : 139.44s, remaining : 1498.27s\n",
      " [0425/5000] tr_loss : 34.03998, val_loss : 38.48724, elapsed : 0.33s, total : 139.77s, remaining : 1505.17s\n",
      " [0426/5000] tr_loss : 34.03028, val_loss : 38.49320, elapsed : 0.33s, total : 140.09s, remaining : 1491.01s\n",
      " [0427/5000] tr_loss : 34.03890, val_loss : 38.49534, elapsed : 0.33s, total : 140.42s, remaining : 1501.65s\n",
      " [0428/5000] tr_loss : 34.03771, val_loss : 38.49702, elapsed : 0.33s, total : 140.75s, remaining : 1496.07s\n",
      " [0429/5000] tr_loss : 34.02820, val_loss : 38.49744, elapsed : 0.33s, total : 141.08s, remaining : 1493.45s\n",
      " [0430/5000] tr_loss : 34.02697, val_loss : 38.49531, elapsed : 0.33s, total : 141.41s, remaining : 1498.63s\n",
      " [0431/5000] tr_loss : 34.03696, val_loss : 38.50451, elapsed : 0.33s, total : 141.73s, remaining : 1496.00s\n",
      " [0432/5000] tr_loss : 34.03367, val_loss : 38.49973, elapsed : 0.33s, total : 142.06s, remaining : 1496.42s\n",
      " [0433/5000] tr_loss : 34.03749, val_loss : 38.49886, elapsed : 0.33s, total : 142.39s, remaining : 1500.74s\n",
      " [0434/5000] tr_loss : 34.03823, val_loss : 38.50686, elapsed : 0.33s, total : 142.72s, remaining : 1497.08s\n",
      " [0435/5000] tr_loss : 34.03465, val_loss : 38.49484, elapsed : 0.33s, total : 143.05s, remaining : 1505.39s\n",
      " [0436/5000] tr_loss : 34.03988, val_loss : 38.50636, elapsed : 0.33s, total : 143.38s, remaining : 1505.13s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(4/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*[0001/5000] tr_loss : 41.83071, val_loss : 66.87370, elapsed : 0.33s, total : 0.33s, remaining : 1660.11s\n",
      "*[0002/5000] tr_loss : 41.78091, val_loss : 66.82153, elapsed : 0.33s, total : 0.66s, remaining : 1656.21s\n",
      "*[0003/5000] tr_loss : 41.73671, val_loss : 66.77351, elapsed : 0.33s, total : 0.99s, remaining : 1651.45s\n",
      "*[0004/5000] tr_loss : 41.69864, val_loss : 66.72635, elapsed : 0.36s, total : 1.35s, remaining : 1801.43s\n",
      "*[0005/5000] tr_loss : 41.65810, val_loss : 66.68436, elapsed : 0.33s, total : 1.69s, remaining : 1656.04s\n",
      "*[0006/5000] tr_loss : 41.61950, val_loss : 66.64581, elapsed : 0.33s, total : 2.02s, remaining : 1652.00s\n",
      "*[0007/5000] tr_loss : 41.58291, val_loss : 66.61897, elapsed : 0.33s, total : 2.35s, remaining : 1649.59s\n",
      "*[0008/5000] tr_loss : 41.54877, val_loss : 66.58782, elapsed : 0.33s, total : 2.68s, remaining : 1657.89s\n",
      "*[0009/5000] tr_loss : 41.51570, val_loss : 66.55724, elapsed : 0.33s, total : 3.01s, remaining : 1641.38s\n",
      "*[0010/5000] tr_loss : 41.48087, val_loss : 66.53525, elapsed : 0.33s, total : 3.34s, remaining : 1651.82s\n",
      "*[0011/5000] tr_loss : 41.44727, val_loss : 66.50966, elapsed : 0.33s, total : 3.67s, remaining : 1624.10s\n",
      "*[0012/5000] tr_loss : 41.41761, val_loss : 66.48719, elapsed : 0.33s, total : 4.00s, remaining : 1646.40s\n",
      "*[0013/5000] tr_loss : 41.38040, val_loss : 66.46427, elapsed : 0.33s, total : 4.33s, remaining : 1651.29s\n",
      "*[0014/5000] tr_loss : 41.35070, val_loss : 66.44350, elapsed : 0.33s, total : 4.66s, remaining : 1647.32s\n",
      "*[0015/5000] tr_loss : 41.31729, val_loss : 66.41983, elapsed : 0.33s, total : 4.99s, remaining : 1659.15s\n",
      "*[0016/5000] tr_loss : 41.28281, val_loss : 66.40326, elapsed : 0.33s, total : 5.32s, remaining : 1646.58s\n",
      "*[0017/5000] tr_loss : 41.25193, val_loss : 66.38445, elapsed : 0.34s, total : 5.66s, remaining : 1670.99s\n",
      "*[0018/5000] tr_loss : 41.21600, val_loss : 66.36366, elapsed : 0.33s, total : 5.99s, remaining : 1647.74s\n",
      "*[0019/5000] tr_loss : 41.18546, val_loss : 66.33973, elapsed : 0.33s, total : 6.31s, remaining : 1627.39s\n",
      "*[0020/5000] tr_loss : 41.15275, val_loss : 66.32152, elapsed : 0.33s, total : 6.64s, remaining : 1621.54s\n",
      "*[0021/5000] tr_loss : 41.11808, val_loss : 66.29665, elapsed : 0.33s, total : 6.97s, remaining : 1628.81s\n",
      "*[0022/5000] tr_loss : 41.08865, val_loss : 66.28378, elapsed : 0.33s, total : 7.29s, remaining : 1623.94s\n",
      "*[0023/5000] tr_loss : 41.05396, val_loss : 66.26531, elapsed : 0.33s, total : 7.62s, remaining : 1637.28s\n",
      "*[0024/5000] tr_loss : 41.01765, val_loss : 66.24378, elapsed : 0.34s, total : 7.96s, remaining : 1672.11s\n",
      "*[0025/5000] tr_loss : 40.98316, val_loss : 66.23321, elapsed : 0.33s, total : 8.28s, remaining : 1626.57s\n",
      "*[0026/5000] tr_loss : 40.94706, val_loss : 66.21217, elapsed : 0.33s, total : 8.61s, remaining : 1627.55s\n",
      "*[0027/5000] tr_loss : 40.91291, val_loss : 66.19702, elapsed : 0.35s, total : 8.96s, remaining : 1752.50s\n",
      "*[0028/5000] tr_loss : 40.87453, val_loss : 66.17776, elapsed : 0.33s, total : 9.29s, remaining : 1644.31s\n",
      "*[0029/5000] tr_loss : 40.84268, val_loss : 66.15794, elapsed : 0.33s, total : 9.62s, remaining : 1635.55s\n",
      "*[0030/5000] tr_loss : 40.80415, val_loss : 66.14288, elapsed : 0.33s, total : 9.95s, remaining : 1628.25s\n",
      "*[0031/5000] tr_loss : 40.76923, val_loss : 66.12486, elapsed : 0.33s, total : 10.28s, remaining : 1623.44s\n",
      "*[0032/5000] tr_loss : 40.73028, val_loss : 66.10899, elapsed : 0.33s, total : 10.61s, remaining : 1626.65s\n",
      "*[0033/5000] tr_loss : 40.69468, val_loss : 66.08130, elapsed : 0.34s, total : 10.95s, remaining : 1704.04s\n",
      "*[0034/5000] tr_loss : 40.65513, val_loss : 66.05328, elapsed : 0.35s, total : 11.30s, remaining : 1737.50s\n",
      "*[0035/5000] tr_loss : 40.62102, val_loss : 66.02961, elapsed : 0.35s, total : 11.65s, remaining : 1725.69s\n",
      "*[0036/5000] tr_loss : 40.58201, val_loss : 65.99784, elapsed : 0.35s, total : 12.00s, remaining : 1739.40s\n",
      "*[0037/5000] tr_loss : 40.55003, val_loss : 65.98427, elapsed : 0.34s, total : 12.33s, remaining : 1669.24s\n",
      "*[0038/5000] tr_loss : 40.50823, val_loss : 65.96172, elapsed : 0.34s, total : 12.67s, remaining : 1693.83s\n",
      "*[0039/5000] tr_loss : 40.47629, val_loss : 65.92971, elapsed : 0.33s, total : 13.01s, remaining : 1654.09s\n",
      "*[0040/5000] tr_loss : 40.43714, val_loss : 65.89993, elapsed : 0.34s, total : 13.35s, remaining : 1690.22s\n",
      "*[0041/5000] tr_loss : 40.39855, val_loss : 65.87075, elapsed : 0.35s, total : 13.70s, remaining : 1724.25s\n",
      "*[0042/5000] tr_loss : 40.35983, val_loss : 65.83317, elapsed : 0.34s, total : 14.04s, remaining : 1692.62s\n",
      "*[0043/5000] tr_loss : 40.32106, val_loss : 65.80899, elapsed : 0.36s, total : 14.39s, remaining : 1766.53s\n",
      "*[0044/5000] tr_loss : 40.28757, val_loss : 65.77589, elapsed : 0.36s, total : 14.76s, remaining : 1800.86s\n",
      "*[0045/5000] tr_loss : 40.24736, val_loss : 65.73749, elapsed : 0.37s, total : 15.13s, remaining : 1822.72s\n",
      "*[0046/5000] tr_loss : 40.21237, val_loss : 65.70706, elapsed : 0.38s, total : 15.50s, remaining : 1863.93s\n",
      "*[0047/5000] tr_loss : 40.16884, val_loss : 65.67806, elapsed : 0.35s, total : 15.85s, remaining : 1747.48s\n",
      "*[0048/5000] tr_loss : 40.13459, val_loss : 65.64025, elapsed : 0.39s, total : 16.25s, remaining : 1955.40s\n",
      "*[0049/5000] tr_loss : 40.09376, val_loss : 65.59592, elapsed : 0.39s, total : 16.64s, remaining : 1926.74s\n",
      "*[0050/5000] tr_loss : 40.05480, val_loss : 65.56810, elapsed : 0.36s, total : 17.00s, remaining : 1776.77s\n",
      "*[0051/5000] tr_loss : 40.02572, val_loss : 65.53996, elapsed : 0.35s, total : 17.35s, remaining : 1729.74s\n",
      "*[0052/5000] tr_loss : 39.98653, val_loss : 65.50217, elapsed : 0.37s, total : 17.72s, remaining : 1825.38s\n",
      "*[0053/5000] tr_loss : 39.94562, val_loss : 65.46770, elapsed : 0.37s, total : 18.09s, remaining : 1850.64s\n",
      "*[0054/5000] tr_loss : 39.90974, val_loss : 65.41639, elapsed : 0.38s, total : 18.47s, remaining : 1873.54s\n",
      "*[0055/5000] tr_loss : 39.87272, val_loss : 65.38860, elapsed : 0.37s, total : 18.84s, remaining : 1824.50s\n",
      "*[0056/5000] tr_loss : 39.83560, val_loss : 65.35302, elapsed : 0.36s, total : 19.19s, remaining : 1758.87s\n",
      "*[0057/5000] tr_loss : 39.79199, val_loss : 65.30551, elapsed : 0.38s, total : 19.57s, remaining : 1860.77s\n",
      "*[0058/5000] tr_loss : 39.75350, val_loss : 65.27523, elapsed : 0.37s, total : 19.94s, remaining : 1808.93s\n",
      "*[0059/5000] tr_loss : 39.71691, val_loss : 65.24140, elapsed : 0.36s, total : 20.30s, remaining : 1775.98s\n",
      "*[0060/5000] tr_loss : 39.68309, val_loss : 65.20513, elapsed : 0.37s, total : 20.66s, remaining : 1812.12s\n",
      "*[0061/5000] tr_loss : 39.64541, val_loss : 65.16654, elapsed : 0.37s, total : 21.03s, remaining : 1813.43s\n",
      "*[0062/5000] tr_loss : 39.61435, val_loss : 65.13397, elapsed : 0.35s, total : 21.38s, remaining : 1722.41s\n",
      "*[0063/5000] tr_loss : 39.56752, val_loss : 65.08965, elapsed : 0.37s, total : 21.75s, remaining : 1848.44s\n",
      "*[0064/5000] tr_loss : 39.54277, val_loss : 65.05503, elapsed : 0.36s, total : 22.11s, remaining : 1762.49s\n",
      "*[0065/5000] tr_loss : 39.50569, val_loss : 65.01284, elapsed : 0.37s, total : 22.48s, remaining : 1818.12s\n",
      "*[0066/5000] tr_loss : 39.46952, val_loss : 64.98196, elapsed : 0.35s, total : 22.83s, remaining : 1740.01s\n",
      "*[0067/5000] tr_loss : 39.44489, val_loss : 64.94568, elapsed : 0.40s, total : 23.23s, remaining : 1967.69s\n",
      "*[0068/5000] tr_loss : 39.40398, val_loss : 64.91472, elapsed : 0.36s, total : 23.59s, remaining : 1767.52s\n",
      "*[0069/5000] tr_loss : 39.36139, val_loss : 64.86686, elapsed : 0.34s, total : 23.93s, remaining : 1657.71s\n",
      "*[0070/5000] tr_loss : 39.33420, val_loss : 64.83571, elapsed : 0.34s, total : 24.26s, remaining : 1662.86s\n",
      "*[0071/5000] tr_loss : 39.30074, val_loss : 64.79559, elapsed : 0.33s, total : 24.60s, remaining : 1649.35s\n",
      "*[0072/5000] tr_loss : 39.25925, val_loss : 64.75120, elapsed : 0.33s, total : 24.92s, remaining : 1610.37s\n",
      "*[0073/5000] tr_loss : 39.23116, val_loss : 64.71838, elapsed : 0.33s, total : 25.25s, remaining : 1611.41s\n",
      "*[0074/5000] tr_loss : 39.20309, val_loss : 64.68019, elapsed : 0.33s, total : 25.58s, remaining : 1620.81s\n",
      "*[0075/5000] tr_loss : 39.17358, val_loss : 64.64244, elapsed : 0.32s, total : 25.91s, remaining : 1599.73s\n",
      "*[0076/5000] tr_loss : 39.13620, val_loss : 64.59810, elapsed : 0.33s, total : 26.24s, remaining : 1636.06s\n",
      "*[0077/5000] tr_loss : 39.09958, val_loss : 64.55401, elapsed : 0.33s, total : 26.57s, remaining : 1648.58s\n",
      "*[0078/5000] tr_loss : 39.06801, val_loss : 64.51852, elapsed : 0.34s, total : 26.91s, remaining : 1663.15s\n",
      "*[0079/5000] tr_loss : 39.04308, val_loss : 64.47266, elapsed : 0.33s, total : 27.24s, remaining : 1613.35s\n",
      "*[0080/5000] tr_loss : 39.00761, val_loss : 64.43832, elapsed : 0.32s, total : 27.56s, remaining : 1585.30s\n",
      "*[0081/5000] tr_loss : 38.97502, val_loss : 64.38828, elapsed : 0.32s, total : 27.88s, remaining : 1592.58s\n",
      "*[0082/5000] tr_loss : 38.94087, val_loss : 64.34667, elapsed : 0.32s, total : 28.21s, remaining : 1595.58s\n",
      "*[0083/5000] tr_loss : 38.90586, val_loss : 64.31112, elapsed : 0.33s, total : 28.53s, remaining : 1599.31s\n",
      "*[0084/5000] tr_loss : 38.87068, val_loss : 64.27556, elapsed : 0.33s, total : 28.86s, remaining : 1604.81s\n",
      "*[0085/5000] tr_loss : 38.84039, val_loss : 64.22800, elapsed : 0.32s, total : 29.19s, remaining : 1595.82s\n",
      "*[0086/5000] tr_loss : 38.80282, val_loss : 64.19870, elapsed : 0.34s, total : 29.52s, remaining : 1649.09s\n",
      "*[0087/5000] tr_loss : 38.75731, val_loss : 64.15552, elapsed : 0.34s, total : 29.87s, remaining : 1692.70s\n",
      "*[0088/5000] tr_loss : 38.73368, val_loss : 64.11150, elapsed : 0.33s, total : 30.20s, remaining : 1643.06s\n",
      "*[0089/5000] tr_loss : 38.69570, val_loss : 64.07333, elapsed : 0.34s, total : 30.54s, remaining : 1649.37s\n",
      "*[0090/5000] tr_loss : 38.66719, val_loss : 64.02003, elapsed : 0.33s, total : 30.87s, remaining : 1622.28s\n",
      "*[0091/5000] tr_loss : 38.61895, val_loss : 63.99120, elapsed : 0.34s, total : 31.20s, remaining : 1660.11s\n",
      "*[0092/5000] tr_loss : 38.58592, val_loss : 63.94030, elapsed : 0.34s, total : 31.54s, remaining : 1644.48s\n",
      "*[0093/5000] tr_loss : 38.54517, val_loss : 63.92261, elapsed : 0.34s, total : 31.88s, remaining : 1655.95s\n",
      "*[0094/5000] tr_loss : 38.51211, val_loss : 63.87925, elapsed : 0.33s, total : 32.21s, remaining : 1625.34s\n",
      "*[0095/5000] tr_loss : 38.47185, val_loss : 63.84539, elapsed : 0.34s, total : 32.55s, remaining : 1665.62s\n",
      "*[0096/5000] tr_loss : 38.43773, val_loss : 63.79081, elapsed : 0.34s, total : 32.89s, remaining : 1651.31s\n",
      "*[0097/5000] tr_loss : 38.39509, val_loss : 63.76712, elapsed : 0.34s, total : 33.22s, remaining : 1653.65s\n",
      "*[0098/5000] tr_loss : 38.35964, val_loss : 63.73040, elapsed : 0.34s, total : 33.56s, remaining : 1675.18s\n",
      "*[0099/5000] tr_loss : 38.32610, val_loss : 63.69285, elapsed : 0.34s, total : 33.90s, remaining : 1645.77s\n",
      "*[0100/5000] tr_loss : 38.28712, val_loss : 63.66452, elapsed : 0.37s, total : 34.27s, remaining : 1794.85s\n",
      "*[0101/5000] tr_loss : 38.25627, val_loss : 63.60329, elapsed : 0.34s, total : 34.60s, remaining : 1656.54s\n",
      "*[0102/5000] tr_loss : 38.22458, val_loss : 63.57637, elapsed : 0.33s, total : 34.94s, remaining : 1619.16s\n",
      "*[0103/5000] tr_loss : 38.18356, val_loss : 63.54588, elapsed : 0.33s, total : 35.27s, remaining : 1638.10s\n",
      "*[0104/5000] tr_loss : 38.14084, val_loss : 63.51138, elapsed : 0.33s, total : 35.60s, remaining : 1635.10s\n",
      "*[0105/5000] tr_loss : 38.10677, val_loss : 63.48620, elapsed : 0.33s, total : 35.94s, remaining : 1624.16s\n",
      "*[0106/5000] tr_loss : 38.07236, val_loss : 63.45041, elapsed : 0.33s, total : 36.27s, remaining : 1620.51s\n",
      "*[0107/5000] tr_loss : 38.03716, val_loss : 63.40504, elapsed : 0.34s, total : 36.61s, remaining : 1655.54s\n",
      "*[0108/5000] tr_loss : 37.99380, val_loss : 63.36843, elapsed : 0.33s, total : 36.94s, remaining : 1637.33s\n",
      "*[0109/5000] tr_loss : 37.95370, val_loss : 63.33955, elapsed : 0.37s, total : 37.31s, remaining : 1793.83s\n",
      "*[0110/5000] tr_loss : 37.92230, val_loss : 63.30528, elapsed : 0.35s, total : 37.65s, remaining : 1692.29s\n",
      "*[0111/5000] tr_loss : 37.87206, val_loss : 63.25779, elapsed : 0.37s, total : 38.02s, remaining : 1791.83s\n",
      "*[0112/5000] tr_loss : 37.82363, val_loss : 63.23444, elapsed : 0.33s, total : 38.35s, remaining : 1612.31s\n",
      "*[0113/5000] tr_loss : 37.78615, val_loss : 63.20219, elapsed : 0.37s, total : 38.72s, remaining : 1790.62s\n",
      "*[0114/5000] tr_loss : 37.72762, val_loss : 63.15085, elapsed : 0.33s, total : 39.05s, remaining : 1627.90s\n",
      "*[0115/5000] tr_loss : 37.70086, val_loss : 63.12409, elapsed : 0.33s, total : 39.38s, remaining : 1597.59s\n",
      "*[0116/5000] tr_loss : 37.65008, val_loss : 63.07487, elapsed : 0.34s, total : 39.72s, remaining : 1665.04s\n",
      "*[0117/5000] tr_loss : 37.61834, val_loss : 63.04451, elapsed : 0.34s, total : 40.05s, remaining : 1647.77s\n",
      "*[0118/5000] tr_loss : 37.58386, val_loss : 63.01170, elapsed : 0.33s, total : 40.39s, remaining : 1613.01s\n",
      "*[0119/5000] tr_loss : 37.53563, val_loss : 62.98856, elapsed : 0.33s, total : 40.71s, remaining : 1604.11s\n",
      "*[0120/5000] tr_loss : 37.49013, val_loss : 62.96219, elapsed : 0.33s, total : 41.05s, remaining : 1625.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x129a19dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/khj/.pyenv/versions/3.8.10/lib/python3.8/site-packages/tqdm/std.py\", line 1161, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = [str(x) if x>=10 else '0'+str(x) for x in tr_idx]\n",
    "    va_case_num = [str(x) if x>=10 else '0'+str(x) for x in va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)]\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)]\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (2) custom dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "    valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False)\n",
    "    valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "    \n",
    "    # [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "    # [y for x,y in iter(train_loader)]\n",
    "    # sum([y.size(0) for x,y in iter(train_loader)])\n",
    "\n",
    "    # len([x for x,y in iter(train_loader)])\n",
    "\n",
    "    # [(x[0].size(),x[1].size()) for x in train_loader]\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (3) modeling\n",
    "    #------------------------------------------------------------------------------------\n",
    "    seed_everything(CFG['SEED'])\n",
    "\n",
    "    input_size = [x.size(2) for x,y in train_loader][0]\n",
    "\n",
    "    # hidden  = [40]*4      # 40\n",
    "    # dropout = [0.5]*4     # 0.5\n",
    "    # num_layers = [1]*4    # 1\n",
    "    # bidirectional = False # False\n",
    "    # model = LSTM_Model(\n",
    "    #     input_size=input_size,\n",
    "    #     hidden=hidden,\n",
    "    #     dropout=dropout,\n",
    "    #     num_layers=num_layers,\n",
    "    #     bidirectional=bidirectional\n",
    "    # )\n",
    "    \n",
    "    model = NLinear_Model(seq_len=24,pred_len=1,input_size=input_size)\n",
    "    \n",
    "    # model = SCINet_Model(input_size=input_size)\n",
    "\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "    CFG['ES_PATIENCE'] = 30\n",
    "    CFG['ES_VERBOSE']  = 0\n",
    "    best_model = train(\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=True,\n",
    "        metric_period=1,\n",
    "        epochs=5000,\n",
    "        best_model_only=True,\n",
    "        verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c0a2d-652e-4f34-8561-3c1b2f4bda3a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b84979-ff44-4995-acf5-763a3f17fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# path = f'./model/best_model_tmp.pt'\n",
    "\n",
    "# path = f'./model/best_model_004.pt'\n",
    "path = f'./model/best_model.pt'\n",
    "\n",
    "# torch.save(best_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39f276-a23c-4e40-807a-2675e28f026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SCINet_Model(input_size = input_size)\n",
    "best_model.load_state_dict(torch.load(path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ff08a-41c8-443e-9473-207bccbf4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "true_list = []\n",
    "with torch.no_grad():\n",
    "    for X,y in iter(valid_loader): # train_loader, valid_loader\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X).cpu().numpy().reshape(-1).tolist()\n",
    "        # model_pred = np.exp(model_pred).tolist()\n",
    "        \n",
    "        y = y.cpu().numpy().reshape(-1).tolist()\n",
    "        # y = np.exp(y).tolist()\n",
    "        \n",
    "        pred_list += model_pred\n",
    "        true_list += y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53ce6a-ec95-4682-9ede-0432f84ecfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x.shape for x,y in iter(train_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f4dc0-e13d-48c3-b3a7-f6ba3962ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in np.array(pred_list)]),len([x for x in np.array(true_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d562a03-a59e-4cc1-8b72-410092615335",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len([x for x in np.array(pred_list)])/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb142c4e-c254-4ae8-9837-4d9e22afd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(size): # 3,25\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        'pred' : pred_list,\n",
    "        'true' : true_list,\n",
    "    })[i*28:(i+1)*28].reset_index(drop=True)\n",
    "    # plot_df = np.exp(plot_df)\n",
    "\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.true,color='black')\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.pred,color='red')\n",
    "    # plt.title(random_num[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed2f61-fe04-41f1-90de-72511ff90013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(plot_df.pred,plot_df.true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07627f-f599-4605-9d4c-2a1f269a2eea",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8067c42-4927-4c3e-a179-0cfbaa9cf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.shape[0]/28, test_input_df.shape[0]/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404749b-c4c7-4b18-b4de-f911eb036eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True, seq_length=seq_length)\n",
    "test_loader  = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dd1cf-0ea5-4785-a279-cadcc4a12ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X in iter(test_loader):\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X).cpu().numpy().reshape(-1).tolist()\n",
    "        # model_pred = np.exp(model_pred).tolist()\n",
    "        # print(np.array(model_pred))\n",
    "\n",
    "        pred_list += model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a2165-8949-45a7-894a-b780b5d3aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccbe14-ddfb-47b8-9179-e8a4d7ecc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test_label_df.sort_values(['case_num','DAT'])\n",
    "sub['predicted_weight_g'] = pred_list\n",
    "\n",
    "i=0\n",
    "for case_num in sub.case_num.unique():\n",
    "    i+=1\n",
    "    s = sub[sub.case_num==case_num].drop('case_num',axis=1)\n",
    "    s.DAT = s.DAT-1\n",
    "    print(i,s.isnull().sum())\n",
    "    s.to_csv(f'./out/scinet/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c2a59-8169-4608-a69f-05aae02464e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/home/studio-lab-user/Dacon/6_상추생육환경생성')\n",
    "os.chdir(\"./out/scinet/\")\n",
    "submission = zipfile.ZipFile(\"../scinet.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/home/studio-lab-user/Dacon/6_상추생육환경생성')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
