{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c5108c-b14d-4b09-8b1b-9a3c23c971d1",
   "metadata": {},
   "source": [
    "- gungu, dong 으로 segment -> test에만 있는 값이 있는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a48ac3-7a48-4762-a5a8-a8a9aba4399c",
   "metadata": {},
   "source": [
    "# Library setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bdb59bb-80d1-4ba7-836a-6bb4ca56f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Volumes/KHJ/Github/hyuckjinkim/lib-python')\n",
    "\n",
    "from base import gc_collect_all, setdiff\n",
    "from filesystem_utils import mkdir\n",
    "from environment import LocalFontSetting, ColabInstallFont, ColabFontSetting\n",
    "from graph import abline, actual_prediction_scatterplot\n",
    "from data_prepare import (\n",
    "    get_holiday, reduce_mem_usage, delete_unique_columns,\n",
    "    TypeController, CategoricalQuantileCalculator,\n",
    "    GroupScaler, OneHotEncoder, InteractionTerm, TargetTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a70cac-bbfe-465a-b8af-329f4b2e0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_collect_all()\n",
    "LocalFontSetting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb18713-a4a8-4bf6-8a96-a343b827b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.reset_option('display')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7240b4d-9ee9-4667-8bf2-a9d433360f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_columns(data):\n",
    "    unique_info = data.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    return unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30a8eb3-0c50-4b15-80a7-2dda14093e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    SUBSET_DEPTH = 3\n",
    "    N_SPLITS = 5\n",
    "    TARGET = ['사망자수','중상자수','경상자수','부상자수'] # 'ECLO'\n",
    "    TARGET_TRANSFORMATION = 'log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840762e1-cec5-4f48-9b69-57a1084dce66",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c89e16-810b-453a-a5a3-622cadd4434e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255fdf1-8cad-4aa8-a6a7-d4e6c7b506e4",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1b8906-0cee-4382-b2e9-3db0b4661672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# cctv_df = pd.read_csv('./data/external_open/대구 CCTV 정보.csv',encoding='cp949')\n",
    "# security_light_df = pd.read_csv('./data/external_open/대구 보안등 정보.csv',encoding='cp949',low_memory=False)\n",
    "# child_area_df = pd.read_csv('./data/external_open/대구 어린이 보호 구역 정보.csv',encoding='cp949')\n",
    "# parking_df = pd.read_csv('./data/external_open/대구 주차장 정보.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ead0f9-6e35-4c76-91af-683faf828f85",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20d5cd-eb46-40dd-a964-30c78ddb663e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bed0f1b-5c2a-4b66-82fd-0348644a3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    d = data.copy()\n",
    "    \n",
    "    # (1) test data에 없는 컬럼 제거\n",
    "    no_columns_in_test = ['사고유형 - 세부분류','법규위반','가해운전자 차종','가해운전자 성별','가해운전자 연령',\n",
    "                          '가해운전자 상해정도','피해운전자 차종','피해운전자 성별','피해운전자 연령','피해운전자 상해정도']\n",
    "    no_columns_in_test = list(set(d.columns)&set(no_columns_in_test))\n",
    "    d.drop(no_columns_in_test,axis=1,inplace=True)\n",
    "    \n",
    "    # (1) 시군구: 도시 / 구 / 동\n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+)'\n",
    "    d[['도시','구','동']] = d['시군구'].str.extract(location_pattern)\n",
    "    d.drop(['시군구','도시'],axis=1,inplace=True)\n",
    "    \n",
    "    # (2) 도로형태\n",
    "    d[['도로구분','도로형태상세']] = np.stack(d['도로형태'].str.split(' - '))\n",
    "    d['주차장여부'] = d['도로구분'].map({'단일로':0,'교차로':0,'기타':np.nan,'주차장':1,'미분류':np.nan})\n",
    "    d['도로여부']    = d['도로구분'].map({'단일로':1,'교차로':1,'기타':np.nan,'주차장':np.nan,'미분류':np.nan})\n",
    "    d.drop('도로형태',axis=1,inplace=True)\n",
    "    for col in ['주차장여부','도로여부']:\n",
    "        d[col] = d[col].fillna(0)\n",
    "        \n",
    "    # (3) 기상상태\n",
    "    d['기상상태맑음여부'] = np.where(d['기상상태']=='맑음',1,0)\n",
    "    d['노면상태건조여부'] = np.where(d['노면상태']=='건조',1,0)\n",
    "\n",
    "    return d\n",
    "\n",
    "def derived_features(data):\n",
    "    d = data.copy()\n",
    "    date = d['사고일시'].apply(lambda x: datetime.datetime.strptime(str(x),'%Y-%m-%d %H'))\n",
    "    \n",
    "    # (1) date columns\n",
    "    d['year']       = date.dt.year\n",
    "    d['month']      = date.dt.month\n",
    "    d['day']        = date.dt.day\n",
    "    d['hour']       = date.dt.hour\n",
    "    d['dayofweek']  = date.dt.dayofweek\n",
    "    d['weekend']    = date.dt.dayofweek.isin([5,6]).astype(int)\n",
    "    d['week']       = [t.isocalendar()[1] for t in date]\n",
    "    d['season']     = d['month'].map({1:0,2:0,3:1,4:1,5:1,6:2,7:2,8:2,9:3,10:3,11:3,12:3})\n",
    "    \n",
    "    # 저어어어어엉님 코드 (https://dacon.io/competitions/official/236176/codeshare/9381?page=1&dtype=recent)\n",
    "    # 주차 누적값\n",
    "    week_list=[]\n",
    "    for i in range(len(d)) :\n",
    "        if d['year'][i] == 2019 :\n",
    "            week_list.append(int(d['week'][i]))\n",
    "        elif d['year'][i] == 2020 :\n",
    "            week_list.append(int(d['week'][i])+52)\n",
    "        elif d['year'][i] == 2021 :\n",
    "            week_list.append(int(d['week'][i])+52+53)\n",
    "        elif d['year'][i] == 2022 :\n",
    "            week_list.append(int(d['week'][i])+52+53+53)\n",
    "        else:\n",
    "            raise ValueError('Unknown year value')\n",
    "    d['week_num']= week_list\n",
    "    \n",
    "    # datetime 패키지에서 19년 12월 마지막주가 첫째주로 들어가는거 발견하여 수정\n",
    "    d.loc[(d['year']==2019) & (d['month']==12) & (d['day']==30), 'week_num'] = 52\n",
    "    d.loc[(d['year']==2019) & (d['month']==12) & (d['day']==31), 'week_num'] = 52\n",
    "\n",
    "    # (2) is holiday & is dayoff\n",
    "    holiday_list = get_holiday(d['year'].unique())\n",
    "    d['is_holiday'] = date.isin(holiday_list).astype(int)\n",
    "    d['is_dayoff']  = ((d.is_holiday==1) | (d.weekend==1)).astype(int)\n",
    "    \n",
    "    # (3) unuse features\n",
    "    unuse_features = ['ID','사고일시','요일'] # '사망자수','중상자수','경상자수','부상자수'\n",
    "    unuse_features = list(set(d.columns)&set(unuse_features))\n",
    "    d.drop(columns=unuse_features,inplace=True)\n",
    "    \n",
    "    num_to_cat = ['year','month','day','hour','dayofweek','week','season']\n",
    "    d[num_to_cat] = d[num_to_cat].astype(str)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e6edf9-7420-47f8-95af-0ba79d38fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessing(train_df)\n",
    "train_df = derived_features(train_df)\n",
    "\n",
    "test_df = preprocessing(test_df)\n",
    "test_df = derived_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8311aa8e-3375-4379-836f-c9a64cdd275b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>도로구분</th>\n",
       "      <th>도로형태상세</th>\n",
       "      <th>주차장여부</th>\n",
       "      <th>도로여부</th>\n",
       "      <th>기상상태맑음여부</th>\n",
       "      <th>노면상태건조여부</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>week_num</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_dayoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>중구</td>\n",
       "      <td>대신동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흐림</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>달서구</td>\n",
       "      <td>감삼동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>수성구</td>\n",
       "      <td>두산동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>북구</td>\n",
       "      <td>복현동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>동구</td>\n",
       "      <td>신암동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  기상상태 노면상태  사고유형  사망자수  중상자수  경상자수  부상자수  ECLO    구    동 도로구분 도로형태상세  주차장여부  \\\n",
       "0   맑음   건조  차대사람     0     1     0     0     5   중구  대신동  단일로     기타    0.0   \n",
       "1   흐림   건조  차대사람     0     0     1     0     3  달서구  감삼동  단일로     기타    0.0   \n",
       "2   맑음   건조  차대사람     0     0     1     0     3  수성구  두산동  단일로     기타    0.0   \n",
       "3   맑음   건조   차대차     0     1     0     0     5   북구  복현동  단일로     기타    0.0   \n",
       "4   맑음   건조   차대차     0     0     1     0     3   동구  신암동  단일로     기타    0.0   \n",
       "\n",
       "   도로여부  기상상태맑음여부  노면상태건조여부  year month day hour dayofweek  weekend week  \\\n",
       "0   1.0         1         1  2019     1   1    0         1        0    1   \n",
       "1   1.0         0         1  2019     1   1    0         1        0    1   \n",
       "2   1.0         1         1  2019     1   1    1         1        0    1   \n",
       "3   1.0         1         1  2019     1   1    2         1        0    1   \n",
       "4   1.0         1         1  2019     1   1    4         1        0    1   \n",
       "\n",
       "  season  week_num  is_holiday  is_dayoff  \n",
       "0      0         1           1          1  \n",
       "1      0         1           1          1  \n",
       "2      0         1           0          0  \n",
       "3      0         1           0          0  \n",
       "4      0         1           0          0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef834de-0b2f-4993-ab69-dcef4f6cac3b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb299a8-b2b3-4d14-a576-a156cc367525",
   "metadata": {},
   "source": [
    "## Merge addtional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6857c09-3fce-4798-a896-7383f6abe1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_merge(data,info_data,group,target,fillna_method):\n",
    "    assert fillna_method in ['zero','min','max','avg'], \\\n",
    "        \"fillna_method must be one of ['zero','min','max','avg']\"\n",
    "    prefix = f'{target}_'\n",
    "    \n",
    "    d = data.copy()\n",
    "    info_d = info_data.copy()\n",
    "    \n",
    "    freq_data = info_data\\\n",
    "        .groupby(group)[target]\\\n",
    "        .apply(lambda x: x.value_counts())\\\n",
    "        .reset_index()\\\n",
    "        .pivot_table(index=group,columns=f'level_{len(group)}',values=target)\\\n",
    "        .add_prefix(prefix)\\\n",
    "        .reset_index()\n",
    "    d = pd.merge(d,freq_data,how='left',on=group)\n",
    "    \n",
    "    cols = info_data[target].dropna().unique()\n",
    "    cols = [f'{target}_{col}' for col in cols]\n",
    "    cols = list(set(cols)&set(d.columns))\n",
    "    for col in cols:\n",
    "        if fillna_method=='zero':\n",
    "            fillna_value = 0\n",
    "        elif fillna_method=='min':\n",
    "            fillna_value = d[col].min()\n",
    "        elif fillna_method=='max':\n",
    "            fillna_value = d[col].max()\n",
    "        elif fillna_method=='avg':\n",
    "            fillna_value = d[col].mean()\n",
    "        d[col].fillna(fillna_value,inplace=True)\n",
    "        \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae0177e-1dde-45ac-9c0c-8e9d905bf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_merge(data,info_data,group,target,agg,fillna_method=None):\n",
    "    assert fillna_method in [None,'zero','min','max','avg'], \\\n",
    "        \"fillna_method must be one of ['zero','min','max','avg']\"\n",
    "    assert agg in ['min','max','avg','sum'], \\\n",
    "        \"agg must be one of ['min','max','avg','sum']\"\n",
    "    \n",
    "    if agg=='min':\n",
    "        aggfunc = np.nanmin\n",
    "    elif agg=='max':\n",
    "        aggfunc = np.nanmax\n",
    "    elif agg=='avg':\n",
    "        aggfunc = np.nanmean\n",
    "    elif agg=='sum':\n",
    "        aggfunc = np.nansum\n",
    "        \n",
    "    d = data.copy()\n",
    "    info_d = info_data.copy()\n",
    "        \n",
    "    agg_df = info_d\\\n",
    "        .groupby(group)[target]\\\n",
    "        .apply(lambda x: aggfunc(x))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={target:f'{target}_{agg}'})\n",
    "    d = pd.merge(d,agg_df,how='left',on=group)\n",
    "    \n",
    "    if fillna_method is not None:\n",
    "        if fillna_method=='zero':\n",
    "            fillna_value = 0\n",
    "        elif fillna_method=='min':\n",
    "            fillna_value = d[f'{target}_{agg}'].min()\n",
    "        elif fillna_method=='max':\n",
    "            fillna_value = d[f'{target}_{agg}'].max()\n",
    "        elif fillna_method=='avg':\n",
    "            fillna_value = d[f'{target}_{agg}'].mean()\n",
    "        d[f'{target}_{agg}'].fillna(fillna_value,inplace=True)\n",
    "    \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01d750-5edd-447e-a597-ea6fa6ec508b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e520695-45a1-43f8-a316-8d56f495776a",
   "metadata": {},
   "source": [
    "### CCTV information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da82a469-07ae-4506-9d94-068b8be89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cctv_info(data):\n",
    "    d = data.copy()\n",
    "    cctv_df = pd.read_csv('./data/external_open/대구 CCTV 정보.csv',encoding='cp949')\n",
    "\n",
    "    # 소재지지번주소 -> 도시 / 구 / 동 / 지번\n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "    cctv_df[['도시','구','동','지번']] = cctv_df['소재지지번주소'].str.extract(location_pattern)\n",
    "    cctv_df.drop(['소재지지번주소','도시','지번'],axis=1,inplace=True)\n",
    "    \n",
    "    # 단속구분별 cctv 수\n",
    "    tmp = cctv_df[cctv_df['단속구분']!=99]\n",
    "    d = frequency_merge(d,tmp,group=['구','동'],target='단속구분',fillna_method='zero')\n",
    "\n",
    "    # 도로노선방향별 cctv 수\n",
    "    d = frequency_merge(d,cctv_df,group=['구','동'],target='도로노선방향',fillna_method='zero')\n",
    "\n",
    "    # 제한속도 평균\n",
    "    tmp = cctv_df[cctv_df['제한속도']!=0]\n",
    "    d = agg_merge(d,tmp,group=['구','동'],target='제한속도',agg='avg',fillna_method='max')\n",
    "\n",
    "    # 설치연도 평균\n",
    "    d = agg_merge(d,cctv_df,group=['구','동'],target='설치연도',agg='avg',fillna_method='min')\n",
    "\n",
    "    # 보호구역구분: 1,2인 경우 보호구역으로 보고, 나머지(99,nan)는 보호구역이 아닌 것으로 봄\n",
    "    cctv_df['보호구역여부'] = np.where(cctv_df['보호구역구분'].isin([1,2]),1,0)\n",
    "    d = agg_merge(d,cctv_df,group=['구','동'],target='보호구역여부',agg='sum',fillna_method='zero')\n",
    "    \n",
    "    # 위도,경도\n",
    "    d = agg_merge(d,cctv_df,group=['구','동'],target='위도',agg='avg',fillna_method=None)\n",
    "    d = agg_merge(d,cctv_df,group=['구','동'],target='경도',agg='avg',fillna_method=None)\n",
    "    \n",
    "    use_cols = setdiff(d.columns,data.columns)\n",
    "    rename_dict = {col:'CCTV_'+col for col in use_cols}\n",
    "    d = d.rename(columns=rename_dict)\n",
    "    \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd515ff0-faba-409e-ae54-24da77e80d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_cctv_info(train_df)\n",
    "test_df  = add_cctv_info(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4042854-bc61-47e2-9d47-5f8b27145365",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c7140-d183-4db5-8f0e-fdbfe9c143f0",
   "metadata": {},
   "source": [
    "### Light information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7adeee78-412b-4e9c-b01e-c6c5282604c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_light_info(data):\n",
    "    d = data.copy()\n",
    "    light_df = pd.read_csv('./data/external_open/대구 보안등 정보.csv',encoding='cp949',low_memory=False)\n",
    "    \n",
    "    # 소재지지번주소 -> 도시 / 구 / 동 / 지번\n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "    light_df[['도시','구','동','지번']] = light_df['소재지지번주소'].str.extract(location_pattern)\n",
    "    light_df.drop(['소재지지번주소','도시','지번'],axis=1,inplace=True)\n",
    "    \n",
    "    # 보안등 설치수\n",
    "    d = agg_merge(d,light_df,group=['구','동'],target='설치개수',agg='sum',fillna_method='zero')\n",
    "    \n",
    "    # 보안등 설치연도\n",
    "    d = agg_merge(d,light_df,group=['구','동'],target='설치연도',agg='avg',fillna_method='min')\n",
    "    \n",
    "    # 보안등 설치형태의 빈도\n",
    "    d = frequency_merge(d,light_df,group=['구','동'],target='설치형태',fillna_method='zero')\n",
    "    \n",
    "    # 위도,경도\n",
    "    d = agg_merge(d,light_df,group=['구','동'],target='위도',agg='avg',fillna_method=None)\n",
    "    d = agg_merge(d,light_df,group=['구','동'],target='경도',agg='avg',fillna_method=None)\n",
    "    \n",
    "    use_cols = setdiff(d.columns,data.columns)\n",
    "    rename_dict = {col:'보안등_'+col for col in use_cols}\n",
    "    d = d.rename(columns=rename_dict)\n",
    "    \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abf7f002-17b0-4494-be2a-37030eb44872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n"
     ]
    }
   ],
   "source": [
    "train_df = add_light_info(train_df)\n",
    "test_df  = add_light_info(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34046b95-389b-45c0-8c95-0f9fb229adf2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c8606-5dd2-49f8-bbe1-0b0264c95350",
   "metadata": {},
   "source": [
    "### Child area information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4e459d-c635-421c-a1dd-58c0439fb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_child_area_info(data):\n",
    "    d = data.copy()\n",
    "    child_area_df = pd.read_csv('./data/external_open/대구 어린이 보호 구역 정보.csv',encoding='cp949')\n",
    "\n",
    "    # 소재지지번주소 -> 도시 / 구 / 동 / 지번\n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "    child_area_df[['도시','구','동','지번']] = child_area_df['소재지지번주소'].str.extract(location_pattern)\n",
    "    child_area_df.drop(['소재지지번주소','도시','지번'],axis=1,inplace=True)\n",
    "\n",
    "    # 시설종류의 빈도\n",
    "    d = frequency_merge(d,child_area_df,group=['구','동'],target='시설종류',fillna_method='zero')\n",
    "\n",
    "    # 관할경찰서명의 빈도\n",
    "    d = frequency_merge(d,child_area_df,group=['구','동'],target='관할경찰서명',fillna_method='zero')\n",
    "\n",
    "    # CCTV설치여부의 빈도\n",
    "    d = frequency_merge(d,child_area_df,group=['구','동'],target='CCTV설치여부',fillna_method='zero')\n",
    "\n",
    "    # CCTV설치대수\n",
    "    d = agg_merge(d,child_area_df,group=['구','동'],target='CCTV설치대수',agg='sum',fillna_method='zero')\n",
    "\n",
    "    # 보호구역도로폭\n",
    "    child_area_df['보호구역도로폭'] = child_area_df['보호구역도로폭'].apply(lambda x: sum([float(s) for s in str(x).split('~')]))\n",
    "    d = agg_merge(d,child_area_df,group=['구','동'],target='보호구역도로폭',agg='avg',fillna_method='min')\n",
    "    \n",
    "    # 위도,경도\n",
    "    d = agg_merge(d,child_area_df,group=['구','동'],target='위도',agg='avg',fillna_method=None)\n",
    "    d = agg_merge(d,child_area_df,group=['구','동'],target='경도',agg='avg',fillna_method=None)\n",
    "    \n",
    "    use_cols = setdiff(d.columns,data.columns)\n",
    "    rename_dict = {col:'어린이보호구역_'+col for col in use_cols}\n",
    "    d = d.rename(columns=rename_dict)\n",
    "    \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f94f85-4815-4764-a9a7-e0caef08fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n"
     ]
    }
   ],
   "source": [
    "train_df = add_child_area_info(train_df)\n",
    "test_df  = add_child_area_info(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262368fc-3119-46ad-8429-28f8e2d82454",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56428f-563a-482a-afed-cba21386eb56",
   "metadata": {},
   "source": [
    "### Parking information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f8bd618-f5a5-431f-98f6-1b3e4aa1c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parking_info(data):\n",
    "    d = data.copy()\n",
    "    parking_df = pd.read_csv('./data/external_open/대구 주차장 정보.csv',encoding='cp949')\n",
    "\n",
    "    # 소재지지번주소 -> 도시 / 구 / 동 / 지번\n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "    parking_df[['도시','구','동','지번']] = parking_df['소재지지번주소'].str.extract(location_pattern)\n",
    "    parking_df.drop(['소재지지번주소','도시','지번'],axis=1,inplace=True)\n",
    "\n",
    "    # 주차장구분의 빈도\n",
    "    d = frequency_merge(d,parking_df,group=['구','동'],target='주차장구분',fillna_method='zero')\n",
    "\n",
    "    # 주차장유형의 빈도\n",
    "    d = frequency_merge(d,parking_df,group=['구','동'],target='주차장유형',fillna_method='zero')\n",
    "\n",
    "    # 급지구분의 빈도\n",
    "    d = frequency_merge(d,parking_df,group=['구','동'],target='급지구분',fillna_method='zero')\n",
    "\n",
    "    # 요금정보의 빈도\n",
    "    d = frequency_merge(d,parking_df,group=['구','동'],target='요금정보',fillna_method='zero')\n",
    "\n",
    "    # 주차구획수\n",
    "    d = agg_merge(d,parking_df,group=['구','동'],target='주차구획수',agg='avg',fillna_method='zero')\n",
    "\n",
    "    # 월정기권요금 (0제거)\n",
    "    parking_df['월정기권요금'] = parking_df['월정기권요금'].replace(0,np.nan)\n",
    "    d = agg_merge(d,parking_df,group=['구','동'],target='월정기권요금',agg='avg',fillna_method='zero')\n",
    "\n",
    "    # 평일/토요일/공휴일 운영시간\n",
    "    for daytype in ['평일','토요일','공휴일']:\n",
    "        new_col = f'{daytype}운영시간'\n",
    "        starttime_col = f'{daytype}운영시작시각'\n",
    "        endtime_col = f'{daytype}운영종료시각'\n",
    "\n",
    "        parking_df[new_col] = [round((datetime.datetime.strptime(b,'%H:%M')-datetime.datetime.strptime(a,'%H:%M')).seconds/3600,0)\n",
    "                               for a,b in parking_df[[starttime_col,endtime_col]].values]\n",
    "        parking_df[new_col] = [24 if t==0 else t for t in parking_df[new_col]]\n",
    "        parking_df[new_col] = [t if w.find(daytype)>=0 else 0 for t,w in parking_df[[new_col,'운영요일']].values]\n",
    "\n",
    "        d = agg_merge(d,parking_df,group=['구','동'],target=new_col,agg='avg',fillna_method='zero')\n",
    "        \n",
    "    use_cols = setdiff(d.columns,data.columns)\n",
    "    rename_dict = {col:'주차장_'+col for col in use_cols}\n",
    "    d = d.rename(columns=rename_dict)\n",
    "        \n",
    "    assert len(data)==len(d), \\\n",
    "        \"duplicated\"\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bc6104-85e3-44ae-a891-b8f7e926a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n",
      "/var/folders/4s/0m_928_d0z7c8r5nngpf8rtm0000gn/T/ipykernel_64348/3856018639.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .apply(lambda x: aggfunc(x))\\\n"
     ]
    }
   ],
   "source": [
    "train_df = add_parking_info(train_df)\n",
    "test_df  = add_parking_info(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce19734-f413-4758-b742-a9325dae2442",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1465e7f-bef2-4fb6-aa61-93a9da715cdf",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692a457-48d0-49e9-a9ea-b1b8cffb98d7",
   "metadata": {},
   "source": [
    "### fill N/A values by averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbe87301-d734-42a9-b1fa-e11ff94e7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_fillna(values):\n",
    "    x = np.array(values).flatten()\n",
    "    if sum(np.isnan(x)) == len(x):\n",
    "        return [np.nan]*len(x)\n",
    "    else:\n",
    "        fillna_value = np.mean(x[~np.isnan(x)])\n",
    "        return [fillna_value if np.isnan(v) else v for v in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbcc4e6e-6013-4962-862a-52118da3b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_cols  = train_df.columns[train_df.columns.str.contains('위도')].tolist()\n",
    "longitude_cols = train_df.columns[train_df.columns.str.contains('경도')].tolist()\n",
    "\n",
    "train_df[latitude_cols] = np.stack(train_df[latitude_cols].apply(coordinates_fillna,axis=1))\n",
    "test_df [latitude_cols] = np.stack(test_df [latitude_cols].apply(coordinates_fillna,axis=1))\n",
    "\n",
    "train_df[longitude_cols] = np.stack(train_df[longitude_cols].apply(coordinates_fillna,axis=1))\n",
    "test_df [longitude_cols] = np.stack(test_df [longitude_cols].apply(coordinates_fillna,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abfa60-1e6e-49c3-a668-5999d724882e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b45faf-864b-4309-9c45-bf568bdd06b5",
   "metadata": {},
   "source": [
    "### Get missing latitude and longitude via geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097cdd0c-c6b8-4983-a200-d1a3f1add941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (참조) https://medium.com/@hazallgultekin/convert-address-to-latitude-longitude-using-python-21844da3d032\n",
    "from geopy.geocoders import Nominatim\n",
    "def get_geocode(address):\n",
    "    # calling the Nominatim tool and create Nominatim class\n",
    "    loc = Nominatim(user_agent=\"Geopy Library\")\n",
    "\n",
    "    # entering the location name\n",
    "    getLoc = loc.geocode(address)\n",
    "\n",
    "    return getLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2159ff-cceb-41d5-9b96-150fc446ea9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0    1\n",
       "row_0            \n",
       "0      39286    0\n",
       "1          0  323"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = latitude_cols[0]\n",
    "pd.crosstab(\n",
    "    np.where(train_df[col].isnull(),1,0),\n",
    "    np.where(train_df[col].isnull(),1,0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e47b179-c28d-43d1-89f7-4ed89c55a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:06<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "구     0\n",
       "동     0\n",
       "위도    0\n",
       "경도    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 위도/경도가 Null인 구/동을 모두 모음\n",
    "gu_dong_data = pd.concat([\n",
    "    train_df[['구','동']][train_df[col].isnull()],\n",
    "    test_df [['구','동']][test_df [col].isnull()],\n",
    "],axis=0).drop_duplicates().values\n",
    "\n",
    "# 위의 구/동으로부터 위도/경도를 search (7개)\n",
    "coord_info = []\n",
    "for gu,dong in tqdm(gu_dong_data):\n",
    "    address = f'대구광역시 {gu} {dong}'\n",
    "    info = get_geocode(address)\n",
    "    if info is not None:\n",
    "        coord_info.append([gu,dong,info.latitude,info.longitude])\n",
    "    else:\n",
    "        coord_info.append([gu,dong,None,None])\n",
    "coord_info = pd.DataFrame(coord_info,columns=['구','동','위도','경도'])\n",
    "display(coord_info.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d445dd8-4f98-4df6-a150-deab7d1493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_fillna_with_geocode_values(values):\n",
    "    x = np.array(values).flatten()\n",
    "    if np.isnan(x[-1]):\n",
    "        return x[:-1]\n",
    "    else:\n",
    "        return [x[-1]]*(len(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11d231e9-a3b8-4628-8dfc-c8e091b88fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df,coord_info,how='left',on=['구','동'])\n",
    "train_df[latitude_cols]  = np.stack(train_df[latitude_cols +['위도']].apply(coordinates_fillna_with_geocode_values,axis=1))\n",
    "train_df[longitude_cols] = np.stack(train_df[longitude_cols+['위도']].apply(coordinates_fillna_with_geocode_values,axis=1))\n",
    "train_df.drop(['위도','경도'],axis=1,inplace=True)\n",
    "\n",
    "test_df = pd.merge(test_df,coord_info,how='left',on=['구','동'])\n",
    "test_df[latitude_cols]  = np.stack(test_df[latitude_cols +['위도']].apply(coordinates_fillna_with_geocode_values,axis=1))\n",
    "test_df[longitude_cols] = np.stack(test_df[longitude_cols+['위도']].apply(coordinates_fillna_with_geocode_values,axis=1))\n",
    "test_df .drop(['위도','경도'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8fe56ea-043b-4070-8855-e6f2d3aba4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum().sum(),test_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e6adce0-937d-4c32-95ab-f0a763aba1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39609, 78)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28958b61-d0d7-4d95-a7f7-20c9895d250d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c57eeb-31b5-4922-8cab-ea829b7377c2",
   "metadata": {},
   "source": [
    "## Target transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b7ccdd4-127e-4ba3-9de2-ebe97e92ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = TargetTransform(func=CFG.TARGET_TRANSFORMATION, offset=1)\n",
    "train_df[CFG.TARGET] = target_transform.fit_transform(train_df[CFG.TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a96c216-9764-4c5e-9a0b-76e2c8c8c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>도로구분</th>\n",
       "      <th>도로형태상세</th>\n",
       "      <th>주차장여부</th>\n",
       "      <th>도로여부</th>\n",
       "      <th>기상상태맑음여부</th>\n",
       "      <th>노면상태건조여부</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>week_num</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_dayoff</th>\n",
       "      <th>CCTV_단속구분_1</th>\n",
       "      <th>CCTV_단속구분_2</th>\n",
       "      <th>CCTV_단속구분_4</th>\n",
       "      <th>CCTV_도로노선방향_1</th>\n",
       "      <th>CCTV_도로노선방향_2</th>\n",
       "      <th>CCTV_도로노선방향_3</th>\n",
       "      <th>CCTV_제한속도_avg</th>\n",
       "      <th>CCTV_설치연도_avg</th>\n",
       "      <th>CCTV_보호구역여부_sum</th>\n",
       "      <th>CCTV_위도_avg</th>\n",
       "      <th>CCTV_경도_avg</th>\n",
       "      <th>보안등_설치개수_sum</th>\n",
       "      <th>보안등_설치연도_avg</th>\n",
       "      <th>보안등_설치형태_건축물</th>\n",
       "      <th>보안등_설치형태_전용주</th>\n",
       "      <th>보안등_설치형태_한전주</th>\n",
       "      <th>보안등_위도_avg</th>\n",
       "      <th>보안등_경도_avg</th>\n",
       "      <th>어린이보호구역_시설종류_어린이집</th>\n",
       "      <th>어린이보호구역_시설종류_유치원</th>\n",
       "      <th>어린이보호구역_시설종류_초등학교</th>\n",
       "      <th>어린이보호구역_시설종류_특수학교</th>\n",
       "      <th>어린이보호구역_시설종류_학원</th>\n",
       "      <th>어린이보호구역_관할경찰서명_강북경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_남부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_달성경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_대구광역시 중부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_북부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_서부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_수성경찰서</th>\n",
       "      <th>어린이보호구역_CCTV설치여부_N</th>\n",
       "      <th>어린이보호구역_CCTV설치여부_Y</th>\n",
       "      <th>어린이보호구역_CCTV설치대수_sum</th>\n",
       "      <th>어린이보호구역_보호구역도로폭_avg</th>\n",
       "      <th>어린이보호구역_위도_avg</th>\n",
       "      <th>어린이보호구역_경도_avg</th>\n",
       "      <th>주차장_주차장구분_공영</th>\n",
       "      <th>주차장_주차장구분_민영</th>\n",
       "      <th>주차장_주차장유형_노상</th>\n",
       "      <th>주차장_주차장유형_노외</th>\n",
       "      <th>주차장_급지구분_1</th>\n",
       "      <th>주차장_급지구분_2</th>\n",
       "      <th>주차장_급지구분_3</th>\n",
       "      <th>주차장_요금정보_무료</th>\n",
       "      <th>주차장_요금정보_유료</th>\n",
       "      <th>주차장_요금정보_혼합</th>\n",
       "      <th>주차장_주차구획수_avg</th>\n",
       "      <th>주차장_월정기권요금_avg</th>\n",
       "      <th>주차장_평일운영시간_avg</th>\n",
       "      <th>주차장_토요일운영시간_avg</th>\n",
       "      <th>주차장_공휴일운영시간_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>중구</td>\n",
       "      <td>대신동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.868237</td>\n",
       "      <td>128.580886</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2007.076046</td>\n",
       "      <td>177.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>35.867981</td>\n",
       "      <td>128.579156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.868541</td>\n",
       "      <td>128.581033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.636364</td>\n",
       "      <td>12.636364</td>\n",
       "      <td>2.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  기상상태 노면상태  사고유형  사망자수      중상자수  경상자수  부상자수  ECLO   구    동 도로구분 도로형태상세  \\\n",
       "0   맑음   건조  차대사람   0.0  0.693147   0.0   0.0     5  중구  대신동  단일로     기타   \n",
       "\n",
       "   주차장여부  도로여부  기상상태맑음여부  노면상태건조여부  year month day hour dayofweek  weekend  \\\n",
       "0    0.0   1.0         1         1  2019     1   1    0         1        0   \n",
       "\n",
       "  week season  week_num  is_holiday  is_dayoff  CCTV_단속구분_1  CCTV_단속구분_2  \\\n",
       "0    1      0         1           1          1          0.0          1.0   \n",
       "\n",
       "   CCTV_단속구분_4  CCTV_도로노선방향_1  CCTV_도로노선방향_2  CCTV_도로노선방향_3  CCTV_제한속도_avg  \\\n",
       "0          4.0            0.0            1.0            4.0           40.0   \n",
       "\n",
       "   CCTV_설치연도_avg  CCTV_보호구역여부_sum  CCTV_위도_avg  CCTV_경도_avg  보안등_설치개수_sum  \\\n",
       "0         2013.0              2.0    35.868237   128.580886         391.0   \n",
       "\n",
       "   보안등_설치연도_avg  보안등_설치형태_건축물  보안등_설치형태_전용주  보안등_설치형태_한전주  보안등_위도_avg  \\\n",
       "0   2007.076046         177.0          30.0         135.0   35.867981   \n",
       "\n",
       "   보안등_경도_avg  어린이보호구역_시설종류_어린이집  어린이보호구역_시설종류_유치원  어린이보호구역_시설종류_초등학교  \\\n",
       "0  128.579156                0.0               1.0                1.0   \n",
       "\n",
       "   어린이보호구역_시설종류_특수학교  어린이보호구역_시설종류_학원  어린이보호구역_관할경찰서명_강북경찰서  \\\n",
       "0                0.0              0.0                   0.0   \n",
       "\n",
       "   어린이보호구역_관할경찰서명_남부경찰서  어린이보호구역_관할경찰서명_달성경찰서  어린이보호구역_관할경찰서명_대구광역시 중부경찰서  \\\n",
       "0                   0.0                   0.0                         2.0   \n",
       "\n",
       "   어린이보호구역_관할경찰서명_북부경찰서  어린이보호구역_관할경찰서명_서부경찰서  어린이보호구역_관할경찰서명_수성경찰서  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "\n",
       "   어린이보호구역_CCTV설치여부_N  어린이보호구역_CCTV설치여부_Y  어린이보호구역_CCTV설치대수_sum  \\\n",
       "0                 0.0                 2.0                  13.0   \n",
       "\n",
       "   어린이보호구역_보호구역도로폭_avg  어린이보호구역_위도_avg  어린이보호구역_경도_avg  주차장_주차장구분_공영  \\\n",
       "0                 26.0       35.868541      128.581033           0.0   \n",
       "\n",
       "   주차장_주차장구분_민영  주차장_주차장유형_노상  주차장_주차장유형_노외  주차장_급지구분_1  주차장_급지구분_2  \\\n",
       "0          11.0           0.0          11.0        11.0         0.0   \n",
       "\n",
       "   주차장_급지구분_3  주차장_요금정보_무료  주차장_요금정보_유료  주차장_요금정보_혼합  주차장_주차구획수_avg  \\\n",
       "0         0.0          0.0         11.0          0.0      45.454545   \n",
       "\n",
       "   주차장_월정기권요금_avg  주차장_평일운영시간_avg  주차장_토요일운영시간_avg  주차장_공휴일운영시간_avg  \n",
       "0             0.0       12.636364        12.636364         2.181818  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68a6f8-88ec-429e-ab23-d4218a52f9a9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500d92-ceb7-4756-a01d-cf9f1fad12cd",
   "metadata": {},
   "source": [
    "## Quantile values of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b8985e-55be-4335-8f9a-ff5d8d8600c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = train_df.columns[train_df.dtypes=='object'].tolist()\n",
    "dummy_features = ['주차장여부','도로여부','기상상태맑음여부','노면상태건조여부','weekend','is_holiday','is_dayoff']\n",
    "num_features = setdiff(train_df.columns,cat_features+dummy_features+[CFG.TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cbe37b2-df19-4af1-bae5-f295cafd063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature engineering\n",
    "# calculator = CategoricalQuantileCalculator()\n",
    "# calculator.fit(\n",
    "#     data=train_df,\n",
    "#     test_data=test_df,\n",
    "#     target_feature=CFG.TARGET,\n",
    "#     cat_features=cat_features,\n",
    "#     subset_depth=CFG.SUBSET_DEPTH,\n",
    "# )\n",
    "# train_df = calculator.transform(train_df)\n",
    "# test_df  = calculator.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad15683c-50ca-4955-953d-3a57d7d661ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39609, 78)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc0550-eec0-4ea2-8bbd-203e60dcaa83",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b38f6-a17b-4c7d-af80-8d17c49bdb8a",
   "metadata": {},
   "source": [
    "## TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8fbb847-824f-4fef-9927-40144a75ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target값이 높은 category에 높은 숫자를 부여\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "te = TargetEncoder(cols=cat_features)\n",
    "train_df[cat_features] = te.fit_transform(train_df[cat_features],train_df['ECLO'])\n",
    "test_df [cat_features] = te.transform(test_df[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cf16a81-6083-4ff7-9a25-8dfbb12cbf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>도로구분</th>\n",
       "      <th>도로형태상세</th>\n",
       "      <th>주차장여부</th>\n",
       "      <th>도로여부</th>\n",
       "      <th>기상상태맑음여부</th>\n",
       "      <th>노면상태건조여부</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>week_num</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_dayoff</th>\n",
       "      <th>CCTV_단속구분_1</th>\n",
       "      <th>CCTV_단속구분_2</th>\n",
       "      <th>CCTV_단속구분_4</th>\n",
       "      <th>CCTV_도로노선방향_1</th>\n",
       "      <th>CCTV_도로노선방향_2</th>\n",
       "      <th>CCTV_도로노선방향_3</th>\n",
       "      <th>CCTV_제한속도_avg</th>\n",
       "      <th>CCTV_설치연도_avg</th>\n",
       "      <th>CCTV_보호구역여부_sum</th>\n",
       "      <th>CCTV_위도_avg</th>\n",
       "      <th>CCTV_경도_avg</th>\n",
       "      <th>보안등_설치개수_sum</th>\n",
       "      <th>보안등_설치연도_avg</th>\n",
       "      <th>보안등_설치형태_건축물</th>\n",
       "      <th>보안등_설치형태_전용주</th>\n",
       "      <th>보안등_설치형태_한전주</th>\n",
       "      <th>보안등_위도_avg</th>\n",
       "      <th>보안등_경도_avg</th>\n",
       "      <th>어린이보호구역_시설종류_어린이집</th>\n",
       "      <th>어린이보호구역_시설종류_유치원</th>\n",
       "      <th>어린이보호구역_시설종류_초등학교</th>\n",
       "      <th>어린이보호구역_시설종류_특수학교</th>\n",
       "      <th>어린이보호구역_시설종류_학원</th>\n",
       "      <th>어린이보호구역_관할경찰서명_강북경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_남부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_달성경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_대구광역시 중부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_북부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_서부경찰서</th>\n",
       "      <th>어린이보호구역_관할경찰서명_수성경찰서</th>\n",
       "      <th>어린이보호구역_CCTV설치여부_N</th>\n",
       "      <th>어린이보호구역_CCTV설치여부_Y</th>\n",
       "      <th>어린이보호구역_CCTV설치대수_sum</th>\n",
       "      <th>어린이보호구역_보호구역도로폭_avg</th>\n",
       "      <th>어린이보호구역_위도_avg</th>\n",
       "      <th>어린이보호구역_경도_avg</th>\n",
       "      <th>주차장_주차장구분_공영</th>\n",
       "      <th>주차장_주차장구분_민영</th>\n",
       "      <th>주차장_주차장유형_노상</th>\n",
       "      <th>주차장_주차장유형_노외</th>\n",
       "      <th>주차장_급지구분_1</th>\n",
       "      <th>주차장_급지구분_2</th>\n",
       "      <th>주차장_급지구분_3</th>\n",
       "      <th>주차장_요금정보_무료</th>\n",
       "      <th>주차장_요금정보_유료</th>\n",
       "      <th>주차장_요금정보_혼합</th>\n",
       "      <th>주차장_주차구획수_avg</th>\n",
       "      <th>주차장_월정기권요금_avg</th>\n",
       "      <th>주차장_평일운영시간_avg</th>\n",
       "      <th>주차장_토요일운영시간_avg</th>\n",
       "      <th>주차장_공휴일운영시간_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.541610</td>\n",
       "      <td>4.282449</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.842185</td>\n",
       "      <td>4.661716</td>\n",
       "      <td>4.814509</td>\n",
       "      <td>5.071256</td>\n",
       "      <td>4.627926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702492</td>\n",
       "      <td>4.723309</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.868237</td>\n",
       "      <td>128.580886</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2007.076046</td>\n",
       "      <td>177.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>35.867981</td>\n",
       "      <td>128.579156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>35.868541</td>\n",
       "      <td>128.581033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.636364</td>\n",
       "      <td>12.636364</td>\n",
       "      <td>2.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.779150</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.618441</td>\n",
       "      <td>4.738938</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.842185</td>\n",
       "      <td>4.661716</td>\n",
       "      <td>4.814509</td>\n",
       "      <td>5.071256</td>\n",
       "      <td>4.627926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702492</td>\n",
       "      <td>4.723309</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>2015.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.849099</td>\n",
       "      <td>128.540606</td>\n",
       "      <td>932.0</td>\n",
       "      <td>1999.889485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.849927</td>\n",
       "      <td>128.542160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>35.849513</td>\n",
       "      <td>128.541383</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>18.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.842715</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.842185</td>\n",
       "      <td>4.661716</td>\n",
       "      <td>4.814509</td>\n",
       "      <td>5.251121</td>\n",
       "      <td>4.627926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702492</td>\n",
       "      <td>4.723309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2018.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.834183</td>\n",
       "      <td>128.621395</td>\n",
       "      <td>473.0</td>\n",
       "      <td>2015.334395</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>35.834061</td>\n",
       "      <td>128.621224</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>35.833939</td>\n",
       "      <td>128.621053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.687669</td>\n",
       "      <td>4.208920</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.842185</td>\n",
       "      <td>4.661716</td>\n",
       "      <td>4.814509</td>\n",
       "      <td>5.407692</td>\n",
       "      <td>4.627926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702492</td>\n",
       "      <td>4.723309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2018.300000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.899975</td>\n",
       "      <td>128.619733</td>\n",
       "      <td>534.0</td>\n",
       "      <td>1990.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.895712</td>\n",
       "      <td>128.619904</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.636364</td>\n",
       "      <td>35.897687</td>\n",
       "      <td>128.622803</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.142857</td>\n",
       "      <td>23.142857</td>\n",
       "      <td>23.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.889534</td>\n",
       "      <td>4.549091</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.842185</td>\n",
       "      <td>4.661716</td>\n",
       "      <td>4.814509</td>\n",
       "      <td>5.128065</td>\n",
       "      <td>4.627926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702492</td>\n",
       "      <td>4.723309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2018.250000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.883077</td>\n",
       "      <td>128.620268</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>1990.285714</td>\n",
       "      <td>540.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>35.884415</td>\n",
       "      <td>128.623264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>35.883746</td>\n",
       "      <td>128.621766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       기상상태      노면상태      사고유형  사망자수      중상자수      경상자수  부상자수  ECLO  \\\n",
       "0  4.712888  4.712878  3.817650   0.0  0.693147  0.000000   0.0     5   \n",
       "1  4.779150  4.712878  3.817650   0.0  0.000000  0.693147   0.0     3   \n",
       "2  4.712888  4.712878  3.817650   0.0  0.000000  0.693147   0.0     3   \n",
       "3  4.712888  4.712878  4.944597   0.0  0.693147  0.000000   0.0     5   \n",
       "4  4.712888  4.712878  4.944597   0.0  0.000000  0.693147   0.0     3   \n",
       "\n",
       "          구         동      도로구분    도로형태상세  주차장여부  도로여부  기상상태맑음여부  노면상태건조여부  \\\n",
       "0  4.541610  4.282449  4.671841  4.599599    0.0   1.0         1         1   \n",
       "1  4.618441  4.738938  4.671841  4.599599    0.0   1.0         0         1   \n",
       "2  4.727300  4.842715  4.671841  4.599599    0.0   1.0         1         1   \n",
       "3  4.687669  4.208920  4.671841  4.599599    0.0   1.0         1         1   \n",
       "4  4.889534  4.549091  4.671841  4.599599    0.0   1.0         1         1   \n",
       "\n",
       "       year     month       day      hour  dayofweek  weekend      week  \\\n",
       "0  4.842185  4.661716  4.814509  5.071256   4.627926        0  4.702492   \n",
       "1  4.842185  4.661716  4.814509  5.071256   4.627926        0  4.702492   \n",
       "2  4.842185  4.661716  4.814509  5.251121   4.627926        0  4.702492   \n",
       "3  4.842185  4.661716  4.814509  5.407692   4.627926        0  4.702492   \n",
       "4  4.842185  4.661716  4.814509  5.128065   4.627926        0  4.702492   \n",
       "\n",
       "     season  week_num  is_holiday  is_dayoff  CCTV_단속구분_1  CCTV_단속구분_2  \\\n",
       "0  4.723309         1           1          1          0.0          1.0   \n",
       "1  4.723309         1           1          1          1.0          3.0   \n",
       "2  4.723309         1           0          0          2.0          2.0   \n",
       "3  4.723309         1           0          0          2.0          8.0   \n",
       "4  4.723309         1           0          0          2.0         10.0   \n",
       "\n",
       "   CCTV_단속구분_4  CCTV_도로노선방향_1  CCTV_도로노선방향_2  CCTV_도로노선방향_3  CCTV_제한속도_avg  \\\n",
       "0          4.0            0.0            1.0            4.0           40.0   \n",
       "1          8.0            3.0            1.0            8.0           52.5   \n",
       "2          0.0            3.0            1.0            0.0           55.0   \n",
       "3          0.0            7.0            3.0            0.0           38.0   \n",
       "4          0.0            9.0            3.0            0.0           40.0   \n",
       "\n",
       "   CCTV_설치연도_avg  CCTV_보호구역여부_sum  CCTV_위도_avg  CCTV_경도_avg  보안등_설치개수_sum  \\\n",
       "0    2013.000000              2.0    35.868237   128.580886         391.0   \n",
       "1    2015.333333              2.0    35.849099   128.540606         932.0   \n",
       "2    2018.250000              0.0    35.834183   128.621395         473.0   \n",
       "3    2018.300000              4.0    35.899975   128.619733         534.0   \n",
       "4    2018.250000              7.0    35.883077   128.620268        2057.0   \n",
       "\n",
       "   보안등_설치연도_avg  보안등_설치형태_건축물  보안등_설치형태_전용주  보안등_설치형태_한전주  보안등_위도_avg  \\\n",
       "0   2007.076046         177.0          30.0         135.0   35.867981   \n",
       "1   1999.889485           0.0           0.0           0.0   35.849927   \n",
       "2   2015.334395          14.0          31.0         425.0   35.834061   \n",
       "3   1990.285714           0.0           0.0           0.0   35.895712   \n",
       "4   1990.285714         540.0          57.0        1396.0   35.884415   \n",
       "\n",
       "   보안등_경도_avg  어린이보호구역_시설종류_어린이집  어린이보호구역_시설종류_유치원  어린이보호구역_시설종류_초등학교  \\\n",
       "0  128.579156                0.0               1.0                1.0   \n",
       "1  128.542160                0.0               0.0                0.0   \n",
       "2  128.621224                3.0               1.0                1.0   \n",
       "3  128.619904                3.0               4.0                3.0   \n",
       "4  128.623264                0.0               0.0                0.0   \n",
       "\n",
       "   어린이보호구역_시설종류_특수학교  어린이보호구역_시설종류_학원  어린이보호구역_관할경찰서명_강북경찰서  \\\n",
       "0                0.0              0.0                   0.0   \n",
       "1                0.0              0.0                   0.0   \n",
       "2                0.0              0.0                   0.0   \n",
       "3                1.0              0.0                   0.0   \n",
       "4                0.0              0.0                   0.0   \n",
       "\n",
       "   어린이보호구역_관할경찰서명_남부경찰서  어린이보호구역_관할경찰서명_달성경찰서  어린이보호구역_관할경찰서명_대구광역시 중부경찰서  \\\n",
       "0                   0.0                   0.0                         2.0   \n",
       "1                   0.0                   0.0                         0.0   \n",
       "2                   0.0                   0.0                         0.0   \n",
       "3                   0.0                   0.0                         0.0   \n",
       "4                   0.0                   0.0                         0.0   \n",
       "\n",
       "   어린이보호구역_관할경찰서명_북부경찰서  어린이보호구역_관할경찰서명_서부경찰서  어린이보호구역_관할경찰서명_수성경찰서  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   5.0   \n",
       "3                  11.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   어린이보호구역_CCTV설치여부_N  어린이보호구역_CCTV설치여부_Y  어린이보호구역_CCTV설치대수_sum  \\\n",
       "0                 0.0                 2.0                  13.0   \n",
       "1                 0.0                 0.0                   0.0   \n",
       "2                 0.0                 5.0                   0.0   \n",
       "3                 2.0                 9.0                  32.0   \n",
       "4                 0.0                 0.0                   0.0   \n",
       "\n",
       "   어린이보호구역_보호구역도로폭_avg  어린이보호구역_위도_avg  어린이보호구역_경도_avg  주차장_주차장구분_공영  \\\n",
       "0            26.000000       35.868541      128.581033           0.0   \n",
       "1             4.000000       35.849513      128.541383           4.0   \n",
       "2             4.000000       35.833939      128.621053           0.0   \n",
       "3             9.636364       35.897687      128.622803          14.0   \n",
       "4             4.000000       35.883746      128.621766           1.0   \n",
       "\n",
       "   주차장_주차장구분_민영  주차장_주차장유형_노상  주차장_주차장유형_노외  주차장_급지구분_1  주차장_급지구분_2  \\\n",
       "0          11.0           0.0          11.0        11.0         0.0   \n",
       "1           0.0           1.0           3.0         0.0         1.0   \n",
       "2           0.0           0.0           0.0         0.0         0.0   \n",
       "3           0.0          11.0           3.0         0.0         9.0   \n",
       "4           0.0           0.0           1.0         0.0         1.0   \n",
       "\n",
       "   주차장_급지구분_3  주차장_요금정보_무료  주차장_요금정보_유료  주차장_요금정보_혼합  주차장_주차구획수_avg  \\\n",
       "0         0.0          0.0         11.0          0.0      45.454545   \n",
       "1         3.0          2.0          2.0          0.0      28.500000   \n",
       "2         0.0          0.0          0.0          0.0       0.000000   \n",
       "3         5.0         13.0          1.0          0.0      26.714286   \n",
       "4         0.0          0.0          1.0          0.0      63.000000   \n",
       "\n",
       "   주차장_월정기권요금_avg  주차장_평일운영시간_avg  주차장_토요일운영시간_avg  주차장_공휴일운영시간_avg  \n",
       "0             0.0       12.636364        12.636364         2.181818  \n",
       "1         80000.0       18.500000        18.500000        18.500000  \n",
       "2             0.0        0.000000         0.000000         0.000000  \n",
       "3             0.0       23.142857        23.142857        23.142857  \n",
       "4         80000.0       10.000000        10.000000        10.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba06f0b-fb11-429d-84fd-e09ecab5513d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f9cc0-b95e-4cb2-8275-3eda3ca7c59e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Group scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dad1232-0adb-4d19-addc-8bd96195426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = train_df.drop(cat_features,axis=1)\n",
    "# num_features = setdiff(num_features,[CFG.TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0c73b13-5b9c-4176-8b98-25c0f8d8731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = GroupScaler(scaler=MinMaxScaler())\n",
    "# scaler.fit(\n",
    "#     data=train_df,\n",
    "#     group='gungu',\n",
    "#     num_features=num_features,\n",
    "# )\n",
    "# train_df = scaler.transform(train_df)\n",
    "# test_df  = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5501-d743-4b61-b08f-bea7ba25fb4c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a416f8-6400-4a57-8154-e8af0288a10e",
   "metadata": {},
   "source": [
    "## Memory reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa734b45-57ef-49e9-8044-451f63501599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, _ = reduce_mem_usage(train_df,verbose=False)\n",
    "test_df , _ = reduce_mem_usage(test_df ,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fd91881-ba37-4a77-9283-b1bbe94ef3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet('./out/train_data_log_target4.parquet')\n",
    "test_df .to_parquet('./out/test_data_log_target4.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cee81-c0c5-4166-ab41-62d72643f678",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d4a05-c75c-4830-8fcf-1955cc56b717",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "707b48ad-924c-4672-861b-cb7b63b7083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = train_df.copy()\n",
    "\n",
    "# for i,col in enumerate(num_features):\n",
    "#     str_i = str(i+1).zfill(len(str(len(num_features))))\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=d[col],y=d[CFG.TARGET])\n",
    "#     plt.grid()\n",
    "#     plt.title('[{}/{}] {}'.format(str_i,len(num_features),col))\n",
    "#     plt.show()\n",
    "\n",
    "# categorical_features = cat_features+dummy_features\n",
    "# for i,col in enumerate(categorical_features):\n",
    "#     str_i = str(i+1).zfill(len(str(len(categorical_features))))\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=d[col],y=d[CFG.TARGET])\n",
    "#     plt.grid()\n",
    "#     plt.title('[{}/{}] {}'.format(str_i,len(categorical_features),col))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1c25f-558f-4f67-81b8-c315658e56a4",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2710990-e8f8-4bd9-812d-05ce84939319",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d87be0e-2e01-41de-bdb0-cfda55e5080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/codeshare/6499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41254e64-c614-4cab-81bc-0bc4430d5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, MultiTaskElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return mean_squared_error(y_true=y_true,y_pred=y_pred)**0.5\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    log_true = np.log1p(y_true)\n",
    "    log_pred = np.log1p(y_pred)\n",
    "    squared_error = (log_true-log_pred)**2\n",
    "    return np.sqrt(np.mean(squared_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09b10a50-1e02-460e-a736-29677c33d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 업데이트버전\n",
    "#  (1) stacking 추가\n",
    "#  (2) LGBM에 sample_weight 추가\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 hyperparameters,\n",
    "                 weight='balanced',\n",
    "                 inverse_transform=None,\n",
    "                 eval_metric=None,\n",
    "                 method='ensemble',\n",
    "                 use_weightedsum_in_stacking=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        assert method in ['ensemble','stacking'], \\\n",
    "            \"method must be one of ['ensemble','stacking']\"\n",
    "        \n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.weight = weight\n",
    "        self.inverse_transform = inverse_transform\n",
    "        self.eval_metric = RMSE if eval_metric is None else eval_metric\n",
    "        self.method = method\n",
    "        self.use_weightedsum_in_stacking = use_weightedsum_in_stacking\n",
    "        \n",
    "        self._get_regressors()\n",
    "        self._get_regressors_name()\n",
    "        \n",
    "        if use_weightedsum_in_stacking:\n",
    "            self.stacking_feature = 'pred'\n",
    "        else:\n",
    "            self.stacking_feature = [f'pred{i+1}' for i in range(len(self.regressors))]\n",
    "            \n",
    "    def _get_regressors(self):\n",
    "        max_depth = 9\n",
    "        n_jobs = -1\n",
    "        cat_loss_function = 'RMSE' # 'RMSE','MAE','MultiRMSE'\n",
    "        cat_eval_metric = 'RMSE'   # 'RMSE','MAE','MultiRMSE'\n",
    "        lgb_metric = 'rmse' # 'rmse','mean_absolute_error'\n",
    "        cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=self.hyperparameters['random_state'])\n",
    "        \n",
    "        params_ridge = {\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : cv,\n",
    "        }\n",
    "        \n",
    "        params_lasso = {\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : cv,\n",
    "            'n_jobs' : n_jobs,\n",
    "            #'max_iter' : 30000,\n",
    "            'tol' : 0.001,\n",
    "        }\n",
    "        \n",
    "        params_elasticnet = {\n",
    "            'l1_ratio' : np.arange(0.1, 1, 0.1),\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : cv,\n",
    "            'n_jobs' : n_jobs,\n",
    "            #'max_iter' : 30000,\n",
    "            'tol' : 0.001,\n",
    "        }\n",
    "        \n",
    "        params_catboost1 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "            #'l2_leaf_reg' : 1,\n",
    "        }\n",
    "        \n",
    "        params_catboost2 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            #'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 3,\n",
    "        }\n",
    "        \n",
    "        params_catboost3 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'SymmetricTree', # 'Lossguide','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            #'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 1,\n",
    "        }\n",
    "        \n",
    "        params_catboost4 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Depthwise', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 3,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm1 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm2 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm3 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm4 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm5 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm6 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm7 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm8 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_xgboost = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['xgb_iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['xgb_early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['xgb_learning_rate'],\n",
    "            'objective' : 'reg:squarederror',#'reg:absoluteerror',\n",
    "            'verbosity' : 0,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        # params_extratrees = {\n",
    "        #     'random_state' : self.hyperparameters['random_state'],\n",
    "        #     'n_estimators' : self.hyperparameters['extratrees_iterations'],\n",
    "        #     'criterion' : 'absolute_error',\n",
    "        #     'verbose' : 0,\n",
    "        #     'max_depth' : self.hyperparameters['max_depth'],\n",
    "        #     'n_jobs' : n_jobs,\n",
    "        # }\n",
    "        \n",
    "        params_hgb = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'max_iter' : self.hyperparameters['hgb_iterations'],\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "        }\n",
    "        \n",
    "        params_rf = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['rf_iterations'],\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "        }\n",
    "        \n",
    "        self.regressors = [\n",
    "            # LinearRegression(),\n",
    "            # RidgeCV(**params_ridge),\n",
    "            # LassoCV(**params_lasso),\n",
    "            # ElasticNetCV(**params_elasticnet),\n",
    "            CatBoostRegressor(**params_catboost4),\n",
    "            XGBRegressor(**params_xgboost),\n",
    "            LGBMRegressor(**params_lightgbm8),\n",
    "            #RandomForestRegressor(**params_rf),\n",
    "            #ExtraTreesRegressor(**params_extratrees),\n",
    "            #HistGradientBoostingRegressor(**params_hgb),\n",
    "        ]\n",
    "        \n",
    "        self.stacking_regressors = [\n",
    "            # LinearRegression(),\n",
    "            # RidgeCV(**params_ridge),\n",
    "            # LassoCV(**params_lasso),\n",
    "            # ElasticNetCV(**params_elasticnet),\n",
    "            CatBoostRegressor(**params_catboost4),\n",
    "            XGBRegressor(**params_xgboost),\n",
    "            LGBMRegressor(**params_lightgbm8),\n",
    "            #RandomForestRegressor(**params_rf),\n",
    "            #ExtraTreesRegressor(**params_extratrees),\n",
    "            #HistGradientBoostingRegressor(**params_hgb),\n",
    "        ]\n",
    "        \n",
    "    def _get_regressors_name(self):\n",
    "        self.regressors_name = [type(r).__name__ for r in self.regressors]\n",
    "        self.stacking_regressors_name = [type(r).__name__ for r in self.stacking_regressors]\n",
    "        \n",
    "    def _get_ohe(self,X,cat_features):\n",
    "        ohe = OneHotEncoder()\n",
    "        ohe.fit(X,cat_features,remove_first=False)\n",
    "        return ohe\n",
    "        \n",
    "    def _set_zero_to_minimum(self,pred,minimum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred<0,1,0).sum()>0:\n",
    "            pred = [x if x>0 else minimum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def _set_inf_to_maximum(self,pred,maximum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred==np.inf,1,0).sum()>0:\n",
    "            pred = [x if x!=np.inf else maximum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def _preprocess(self,pred):\n",
    "        # pred = self._set_zero_to_minimum(pred,self.minimum_value)\n",
    "        # pred = self._set_inf_to_maximum(pred,self.maximum_value)\n",
    "        return pred\n",
    "    \n",
    "    def _fit_regressor(self,\n",
    "                       regressor,regressor_name,\n",
    "                       features,oh_features,\n",
    "                       X,X_oh,X_val,X_val_oh,y,y_val,cat_features,\n",
    "                       sample_weight,eval_sample_weight):\n",
    "        X = X[features]\n",
    "        X_val = X_val[features]\n",
    "        X_oh = X_oh[oh_features]\n",
    "        X_val_oh = X_val_oh[oh_features]\n",
    "        \n",
    "        if (regressor_name in ['LinearRegression','RidgeCV','LassoCV']) or\\\n",
    "            (regressor_name.find('ExtraTreesRegressor')>=0) or\\\n",
    "            (regressor_name.find('RandomForestRegressor')>=0) or\\\n",
    "            (regressor_name.find('HistGradientBoostingRegressor')>=0) or\\\n",
    "            (regressor_name.find('ElasticNetCV')>=0):\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # fitting\n",
    "            regressor.fit(X_oh,y)\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "\n",
    "        elif regressor_name.find('XGBRegressor')>=0:\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                X_oh,y,\n",
    "                eval_set=[(X_val_oh,y_val)],\n",
    "                verbose=0,\n",
    "            )\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "\n",
    "        elif regressor_name.find('CatBoostRegressor')>=0:\n",
    "            # dataset\n",
    "            train_dataset = Pool(X    ,y    ,cat_features=cat_features)\n",
    "            val_dataset   = Pool(X_val,y_val,cat_features=cat_features)\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                train_dataset,\n",
    "                eval_set=val_dataset,\n",
    "                #metric_period=self.hyperparameters['iterations']//50, verbose=True,\n",
    "                verbose=False,\n",
    "            )\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(train_dataset))\n",
    "            va_pred = self._preprocess(regressor.predict(val_dataset))\n",
    "\n",
    "        elif regressor_name.find('LGBMRegressor')>=0:\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # astype category\n",
    "            for col in cat_features:\n",
    "                X[col]     = X[col]    .astype('category')\n",
    "                X_val[col] = X_val[col].astype('category')\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                X,y,\n",
    "                eval_set=[(X_val,y_val)],\n",
    "                sample_weight=sample_weight,\n",
    "                eval_sample_weight=eval_sample_weight,\n",
    "                categorical_feature=cat_features,\n",
    "                verbose=-1,\n",
    "            )\n",
    "            tr_pred = self._preprocess(regressor.predict(X))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val))\n",
    "            # # fitting\n",
    "            # regressor.fit(\n",
    "            #     X_oh,y,\n",
    "            #     eval_set=[(X_val_oh,y_val)],\n",
    "            #     sample_weight=sample_weight,\n",
    "            #     eval_sample_weight=eval_sample_weight,\n",
    "            #     #categorical_feature=cat_features,\n",
    "            #     verbose=-1,\n",
    "            # )\n",
    "            # tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            # va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "        return regressor, tr_pred, va_pred\n",
    "            \n",
    "    def _get_prediction_values(self,X,X_oh,method,regressors_name,regressors,weights,return_weighted):\n",
    "        if method=='ensemble':\n",
    "            features    = self.features\n",
    "            oh_features = self.oh_features\n",
    "        elif method=='stacking':\n",
    "            stacking_feature = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            features    = self.features + stacking_feature\n",
    "            oh_features = self.oh_features + stacking_feature\n",
    "        \n",
    "        # (1) 예측값생성\n",
    "        pred_list = []\n",
    "        for regressor_name,regressor in zip(regressors_name,regressors):\n",
    "            if (regressor_name in ['LinearRegression','RidgeCV','LassoCV','ElasticNetCV','RandomForestRegressor']) or\\\n",
    "                (regressor_name.find('ExtraTreesRegressor')>=0) or\\\n",
    "                (regressor_name.find('RandomForestRegressor')>=0) or\\\n",
    "                (regressor_name.find('XGBRegressor')>=0) or\\\n",
    "                (regressor_name.find('HistGradientBoostingRegressor')>=0) or\\\n",
    "                (regressor_name.find('ElasticNetCV')>=0):\n",
    "                dataset = X_oh[oh_features]\n",
    "            elif regressor_name.find('CatBoostRegressor')>=0:\n",
    "                dataset = Pool(X[features],cat_features=self.cat_features)\n",
    "            elif regressor_name.find('LGBMRegressor')>=0:\n",
    "                # dataset = X_oh[oh_features]\n",
    "                dataset = X[features].copy()\n",
    "                for col in self.cat_features:\n",
    "                    dataset[col] = dataset[col].astype('category')\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "            y_pred = self._preprocess(regressor.predict(dataset))\n",
    "            pred_list.append(y_pred)\n",
    "        \n",
    "        # (2) return weighted or original value\n",
    "        if return_weighted:\n",
    "            final_pred = []\n",
    "            for pred,weight in zip(pred_list,weights):\n",
    "                p = np.array(pred)*weight\n",
    "                final_pred.append(p)\n",
    "            final_pred = np.sum(final_pred,axis=0)\n",
    "            if self.inverse_transform is not None:\n",
    "                final_pred = self.inverse_transform(np.array(final_pred))\n",
    "                final_pred = self._preprocess(final_pred)\n",
    "        else:\n",
    "            final_pred = np.array(pred_list).T\n",
    "            \n",
    "        return final_pred\n",
    "        \n",
    "    def _predict(self,X,method,return_weighted=True):\n",
    "        if len(self.cat_features)>0:\n",
    "            X_oh = self.ohe.transform(X)\n",
    "        else:\n",
    "            X_oh = X.copy()\n",
    "        assert len(X)==len(X_oh), \\\n",
    "            \"X and X_oh must be same length\"\n",
    "        \n",
    "        # (1) ensemble\n",
    "        pred_list = self._get_prediction_values(\n",
    "            X,X_oh,\n",
    "            'ensemble',\n",
    "            self.regressors_name,self.regressors,\n",
    "            self.ensemble_weights,return_weighted,\n",
    "        )\n",
    "        \n",
    "        if method=='ensemble':\n",
    "            return pred_list\n",
    "        \n",
    "        elif method=='stacking':\n",
    "            # (2) stacking\n",
    "            columns = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            pred_df = pd.DataFrame(pred_list,columns=columns,index=X.index)\n",
    "            \n",
    "            X    = pd.concat([X   ,pred_df],axis=1)\n",
    "            X_oh = pd.concat([X_oh,pred_df],axis=1)\n",
    "\n",
    "            pred_list = self._get_prediction_values(\n",
    "                X,X_oh,\n",
    "                'stacking',\n",
    "                self.stacking_regressors_name,self.stacking_regressors,\n",
    "                self.stacking_weights,return_weighted,\n",
    "            )\n",
    "            return pred_list\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        # message_print = warnings.warn\n",
    "        message_print = print\n",
    "        supported_models = self.regressors_name\n",
    "        \n",
    "        # # feature_importances_를 지원하는 모델들\n",
    "        # supported_models = ['CatBoostRegressor','XGBRegressor','LGBMRegressor','ExtraTreesRegressor',\n",
    "        #                     'ExtraTreesRegressor', 'RandomForestRegressor', 'HistGradientBoostingRegressor']\n",
    "\n",
    "        # # 지원하지않는 모델이 있는 경우 warning message\n",
    "        # not_supported_1 = len([name for name in self.regressors_name if name not in supported_models])\n",
    "        # not_supported_2 = 0\n",
    "        # for name in self.regressors_name:\n",
    "        #     for m in supported_models:\n",
    "        #         k = 0 if name.find(m)>=0 else 1\n",
    "        #         not_supported_2 += k\n",
    "\n",
    "        # if not_supported_1 + not_supported_2 > 0:\n",
    "        #     message_print(\"not support model\")\n",
    "\n",
    "        # get weighted feature importance by using ensemble_weights\n",
    "        feature_importance_df = pd.DataFrame(self.features,columns=['feature'])\n",
    "        for i,(regressor,regressor_name,weight) in enumerate(zip(self.regressors,self.regressors_name,self.ensemble_weights)):\n",
    "            if regressor_name in supported_models:\n",
    "                feature_importance = regressor.feature_importances_\n",
    "                fi_list = []\n",
    "                for feature in self.features:\n",
    "                    fi = feature_importance[np.where(np.array(self.features)==feature)[0]]\n",
    "                    fi_list.append([feature,sum(fi)])\n",
    "\n",
    "                imp_col = f'importance{i}'\n",
    "                fi_df = pd.DataFrame(fi_list,columns=['feature',imp_col]).sort_values(imp_col,ascending=False)\n",
    "                fi_df[imp_col] = 100 * fi_df[imp_col] / fi_df[imp_col].sum()\n",
    "                fi_df[imp_col] *= weight\n",
    "\n",
    "                feature_importance_df = pd.merge(feature_importance_df,fi_df,how='left',on='feature')\n",
    "\n",
    "        feature_importance_df = feature_importance_df.fillna(0)\n",
    "        feature_importance_df['importance'] = feature_importance_df.drop('feature',axis=1).sum(axis=1)\n",
    "        feature_importance_df = feature_importance_df[['feature','importance']]\n",
    "        \n",
    "        return feature_importance_df\n",
    "        \n",
    "    def plot_feature_importance(self):\n",
    "        feature_importance_df = self.get_feature_importance()\n",
    "        feature_importance_df.sort_values('importance',ascending=True,inplace=True)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.barh(feature_importance_df['feature'],feature_importance_df['importance'])\n",
    "        plt.show()\n",
    "            \n",
    "    def fit(self,\n",
    "            X,y,eval_set,cat_features=None,\n",
    "            sample_weight=None,eval_sample_weight=None,verbose=1):\n",
    "        assert len(eval_set)==1, \\\n",
    "            \"eval_set length must be 1. len(eval_set)={}\".format(len(eval_set))\n",
    "        \n",
    "        if len(self.regressors)!=len(self.regressors_name):\n",
    "            self._get_regressors_name()\n",
    "        \n",
    "        self.sample_weight = sample_weight\n",
    "        self.eval_sample_weight = eval_sample_weight\n",
    "        self.cat_features = [] if cat_features is None else cat_features\n",
    "        if cat_features is None:\n",
    "            self.enable_categorical = [False]*X.shape[1]\n",
    "        else:\n",
    "            self.enable_categorical = [True if col in cat_features else False for col in X.columns]\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # prepare dataset\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        X_val, y_val = eval_set[0]\n",
    "        \n",
    "        del_cols = return_unique_columns(X)\n",
    "        X.drop(del_cols,axis=1,inplace=True)\n",
    "        X_val.drop(del_cols,axis=1,inplace=True)\n",
    "        self.cat_features = list(set(self.cat_features)-set(del_cols))\n",
    "        \n",
    "        if len(self.cat_features)>0:\n",
    "            self.ohe = self._get_ohe(X,cat_features)\n",
    "            X_oh = self.ohe.transform(X)\n",
    "            X_val_oh = self.ohe.transform(X_val)\n",
    "        else:\n",
    "            X_oh = X.copy()\n",
    "            X_val_oh = X_val.copy()\n",
    "        \n",
    "        del_oh_cols = return_unique_columns(X_oh)\n",
    "        X_oh.drop(del_oh_cols,axis=1,inplace=True)\n",
    "        X_val_oh.drop(del_oh_cols,axis=1,inplace=True)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # save feature names\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        self.features    = X   .columns.tolist()\n",
    "        self.oh_features = X_oh.columns.tolist()\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # true value\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        tr_true = np.array(y)\n",
    "        va_true = np.array(y_val)\n",
    "        if self.inverse_transform is not None:\n",
    "            tr_true = self.inverse_transform(tr_true)\n",
    "            va_true = self.inverse_transform(va_true)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # set min,max value\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        self.minimum_value = min(np.nanmin(y),np.nanmin(y_val))\n",
    "        self.maximum_value = max(np.nanmax(y),np.nanmax(y_val))\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # (1) ensemble fitting\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # prepare ensemble fitting\n",
    "        self.ensemble_scores = []\n",
    "        self.ensemble_weights = []\n",
    "        self.ensemble_fitting_elapsed = []\n",
    "        ensemble_pbar = zip(self.regressors_name,self.regressors)\n",
    "\n",
    "        # fitting\n",
    "        if self.method=='stacking':\n",
    "            if verbose:\n",
    "                print('\\n########  <Step1> Ensemble  ########')\n",
    "        for fit_iter,(regressor_name,regressor) in enumerate(ensemble_pbar):\n",
    "            s = time.time()\n",
    "                \n",
    "            # fit\n",
    "            regressor, tr_pred, va_pred = self._fit_regressor(\n",
    "                regressor, regressor_name,\n",
    "                self.features,self.oh_features,\n",
    "                X, X_oh, X_val, X_val_oh, y, y_val, self.cat_features,\n",
    "                sample_weight, eval_sample_weight,\n",
    "            )\n",
    "            self.regressors[fit_iter] = regressor\n",
    "            \n",
    "            # progress\n",
    "            if self.inverse_transform is not None:\n",
    "                tr_pred = self.inverse_transform(tr_pred)\n",
    "                tr_pred = self._preprocess(tr_pred)\n",
    "                va_pred = self.inverse_transform(va_pred)\n",
    "                va_pred = self._preprocess(va_pred)\n",
    "            \n",
    "            tr_score = self.eval_metric(y_pred=tr_pred,y_true=tr_true)\n",
    "            va_score = self.eval_metric(y_pred=va_pred,y_true=va_true)\n",
    "            \n",
    "            e = time.time()\n",
    "            self.ensemble_scores.append(va_score)\n",
    "            self.ensemble_weights.append(1/va_score)\n",
    "            self.ensemble_fitting_elapsed.append(e-s)\n",
    "            \n",
    "            if verbose:\n",
    "                blank = ' '*(11-len(regressor_name))\n",
    "                fit_progress = '[{}/{}] {}{}: loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s'\\\n",
    "                    .format(fit_iter+1,len(self.regressors),regressor_name,blank,tr_score,va_score,e-s)\n",
    "                print(fit_progress)\n",
    "            \n",
    "        # get weighted prediction & score\n",
    "        if self.weight=='equal':\n",
    "            self.ensemble_weights = np.array([1.0 for _ in self.regressors])\n",
    "        self.ensemble_weights /= sum(self.ensemble_weights)\n",
    "        \n",
    "        tr_pred = self._predict(X,method='ensemble',return_weighted=True)\n",
    "        va_pred = self._predict(X_val,method='ensemble',return_weighted=True)\n",
    "        \n",
    "        ## -> self.predict에서 inverse_transform 해줌\n",
    "        # if self.inverse_transform is not None:\n",
    "        #     tr_pred = self.inverse_transform(tr_pred)\n",
    "        #     va_pred = self.inverse_transform(va_pred)\n",
    "        \n",
    "        ens_tr_score = self.eval_metric(y_true=tr_true,y_pred=tr_pred)\n",
    "        ens_va_score = self.eval_metric(y_true=va_true,y_pred=va_pred)\n",
    "        \n",
    "        if verbose:\n",
    "            ens_fit_progress = \"<Weighted Ensemble(weight='{}')> loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s\"\\\n",
    "                .format(self.weight,ens_tr_score,ens_va_score,sum(self.ensemble_fitting_elapsed))\n",
    "            print(ens_fit_progress)\n",
    "        \n",
    "        if self.method=='ensemble':\n",
    "            self.total_score = ens_va_score\n",
    "            \n",
    "        elif self.method=='stacking':\n",
    "            #----------------------------------------------------------------------------------------#\n",
    "            # (2) stacking fitting\n",
    "            #----------------------------------------------------------------------------------------#\n",
    "            tr_pred = self._predict(X,method='ensemble',return_weighted=self.use_weightedsum_in_stacking)\n",
    "            va_pred = self._predict(X_val,method='ensemble',return_weighted=self.use_weightedsum_in_stacking)\n",
    "\n",
    "            stacking_columns = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            tr_pred_df = pd.DataFrame(tr_pred,columns=stacking_columns,index=X.index)\n",
    "            va_pred_df = pd.DataFrame(va_pred,columns=stacking_columns,index=X_val.index)\n",
    "            \n",
    "            X        = pd.concat([X       ,tr_pred_df],axis=1)\n",
    "            X_oh     = pd.concat([X_oh    ,tr_pred_df],axis=1)\n",
    "            X_val    = pd.concat([X_val   ,va_pred_df],axis=1)\n",
    "            X_val_oh = pd.concat([X_val_oh,va_pred_df],axis=1)\n",
    "\n",
    "            # prepare stacking fitting\n",
    "            self.stacking_scores = []\n",
    "            self.stacking_weights = []\n",
    "            self.stacking_fitting_elapsed = []\n",
    "\n",
    "            stacking_regressors = deepcopy(self.stacking_regressors)\n",
    "            stacking_pbar = zip(self.stacking_regressors_name,stacking_regressors)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n########  <Step2> Stacking  ########')\n",
    "            self.stacking_regressors = []\n",
    "            for fit_iter,(regressor_name,regressor) in enumerate(stacking_pbar):\n",
    "                s = time.time()\n",
    "\n",
    "                # fitting\n",
    "                stacking_regressor, tr_pred, va_pred = self._fit_regressor(\n",
    "                    regressor, regressor_name,\n",
    "                    self.features+stacking_columns,self.oh_features+stacking_columns,\n",
    "                    X, X_oh, X_val, X_val_oh, y, y_val, self.cat_features,\n",
    "                    sample_weight, eval_sample_weight,\n",
    "                )\n",
    "                self.stacking_regressors.append(stacking_regressor)\n",
    "\n",
    "                # progress\n",
    "                if self.inverse_transform is not None:\n",
    "                    tr_pred = self.inverse_transform(tr_pred)\n",
    "                    tr_pred = self._preprocess(tr_pred)\n",
    "                    va_pred = self.inverse_transform(va_pred)\n",
    "                    va_pred = self._preprocess(va_pred)\n",
    "\n",
    "                tr_score = self.eval_metric(y_pred=tr_pred,y_true=tr_true)\n",
    "                va_score = self.eval_metric(y_pred=va_pred,y_true=va_true)\n",
    "\n",
    "                e = time.time()\n",
    "                self.stacking_scores.append(va_score)\n",
    "                self.stacking_weights.append(1/va_score)\n",
    "                self.stacking_fitting_elapsed.append(e-s)\n",
    "\n",
    "                if verbose:\n",
    "                    blank = ' '*(11-len(regressor_name))\n",
    "                    iter_str = str(fit_iter+1).zfill(len(str(len(stacking_regressors))))\n",
    "                    fit_progress = '[{}/{}] {}{}: loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s'\\\n",
    "                        .format(iter_str,len(stacking_regressors),regressor_name,blank,tr_score,va_score,e-s)\n",
    "                    print(fit_progress)\n",
    "\n",
    "            # get weighted prediction & score\n",
    "            if self.weight=='equal':\n",
    "                self.stacking_weights = np.array([1.0 for _ in self.stacking_regressors])\n",
    "            self.stacking_weights /= sum(self.stacking_weights)\n",
    "\n",
    "            tr_pred = self._predict(\n",
    "                X.drop(self.stacking_feature,axis=1),\n",
    "                method='stacking',\n",
    "                return_weighted=True,\n",
    "            )\n",
    "            va_pred = self._predict(\n",
    "                X_val.drop(self.stacking_feature,axis=1),\n",
    "                method='stacking',\n",
    "                return_weighted=True,\n",
    "            )\n",
    "\n",
    "            ## -> self.predict에서 inverse_transform 해줌\n",
    "            # if self.inverse_transform is not None:\n",
    "            #     tr_pred = self.inverse_transform(tr_pred)\n",
    "            #     va_pred = self.inverse_transform(va_pred)\n",
    "\n",
    "            stacking_tr_score = self.eval_metric(y_true=tr_true,y_pred=tr_pred)\n",
    "            stacking_va_score = self.eval_metric(y_true=va_true,y_pred=va_pred)\n",
    "\n",
    "            if verbose:\n",
    "                stacking_fit_progress = \"<Weighted Stacking(weight='{}')> loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s\"\\\n",
    "                    .format(self.weight,stacking_tr_score,stacking_va_score,sum(self.stacking_fitting_elapsed))\n",
    "                print(stacking_fit_progress)\n",
    "\n",
    "            self.total_score = stacking_va_score\n",
    "            \n",
    "        #self.feature_importances_ = self.get_feature_importance()['importance'].values.tolist()\n",
    "\n",
    "    def predict(self,X,method=None):\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        if (self.method=='ensemble') & (method=='stacking'):\n",
    "            raise ValueError(\"The training method is 'ensemble', so 'stacking' prediction is not possible\")\n",
    "        return self._predict(X,method=self.method,return_weighted=self.use_weightedsum_in_stacking)\n",
    "        \n",
    "    def save(self,path):\n",
    "        save_dict = {\n",
    "            'ohe' : self.ohe,\n",
    "            'cat_features' : self.cat_features,\n",
    "            'minimum_value' : self.minimum_value,\n",
    "            'maximum_value' : self.maximum_value,\n",
    "            'features' : self.features,\n",
    "            'oh_features' : self.oh_features,\n",
    "            'hyperparameters' : self.hyperparameters,\n",
    "            'inverse_transform' : self.inverse_transform,\n",
    "            'sample_weight' : self.sample_weight,\n",
    "            'eval_sample_weight' : self.eval_sample_weight,\n",
    "            'regressors' : self.regressors,\n",
    "            'ensemble_weights' : self.ensemble_weights,\n",
    "            'ensemble_fitting_elapsed' : self.ensemble_fitting_elapsed,\n",
    "            'ensemble_scores' : self.ensemble_scores,\n",
    "            'total_score' : self.total_score,\n",
    "            #'feature_importances_' : self.feature_importances_,\n",
    "        }\n",
    "        if self.method=='stacking':\n",
    "            additional_save_dict = {\n",
    "                'stacking_regressors' : self.stacking_regressors,\n",
    "                'stacking_weights' : self.stacking_weights,\n",
    "                'stacking_fitting_elapsed' : self.stacking_fitting_elapsed,\n",
    "                'stacking_scores' : self.stacking_scores,\n",
    "            }\n",
    "            save_dict = {**save_dict,**additional_save_dict}\n",
    "        with open(path, 'wb') as f:\n",
    "            #pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            dill.dump(save_dict, f)\n",
    "            \n",
    "    def load(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            #save_dict = pickle.load(f)\n",
    "            save_dict = dill.load(f)\n",
    "            self.ohe = save_dict['ohe']\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.minimum_value = save_dict['minimum_value']\n",
    "            self.maximum_value = save_dict['maximum_value']\n",
    "            self.features = save_dict['features']\n",
    "            self.oh_features = save_dict['oh_features']\n",
    "            self.hyperparameters = save_dict['hyperparameters']\n",
    "            self.inverse_transform = save_dict['inverse_transform']\n",
    "            self.sample_weight = save_dict['sample_weight']\n",
    "            self.eval_sample_weight = save_dict['eval_sample_weight']\n",
    "            self.regressors = save_dict['regressors']\n",
    "            self.ensemble_weights = save_dict['ensemble_weights']\n",
    "            self.ensemble_fitting_elapsed = save_dict['ensemble_fitting_elapsed']\n",
    "            self.ensemble_scores = save_dict['ensemble_scores']\n",
    "            self.total_score = save_dict['total_score']\n",
    "            #self.feature_importances_ = save_dict['feature_importances_']\n",
    "            \n",
    "            if self.method=='stacking':\n",
    "                self.stacking_regressors = save_dict['stacking_regressors']\n",
    "                self.stacking_weights = save_dict['stacking_weights']\n",
    "                self.stacking_fitting_elapsed = save_dict['stacking_fitting_elapsed']\n",
    "                self.stacking_scores = save_dict['stacking_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fa801a5-6f70-4659-b9f6-6d73c4c9c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KfoldWeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 hyperparameters,\n",
    "                 method='ensemble',\n",
    "                 weight='balanced',\n",
    "                 use_weightedsum_in_stacking=True,\n",
    "                 inverse_transform=None,\n",
    "                 eval_metric=None,\n",
    "                 n_splits=5,\n",
    "                 random_state=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        \n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.method = method\n",
    "        self.weight = weight\n",
    "        self.use_weightedsum_in_stacking = use_weightedsum_in_stacking\n",
    "        self.inverse_transform = inverse_transform\n",
    "        self.eval_metric = RMSE if eval_metric is None else eval_metric\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        fs = [m.features for m in self.base_models]\n",
    "        fs = list(set(item for sublist in fs for item in sublist))\n",
    "        feature_importance_df = pd.DataFrame(fs,columns=['feature'])\n",
    "\n",
    "        for i,(base_model,base_feature_importance) in enumerate(zip(self.base_models,self.base_feature_importances)):\n",
    "            imp_col = f'imp{i}'\n",
    "            imp_df = pd.DataFrame({\n",
    "                'feature' : base_model.features,\n",
    "                imp_col : base_feature_importance,\n",
    "            })\n",
    "            feature_importance_df = pd.merge(feature_importance_df,imp_df,how='left',on='feature')\n",
    "\n",
    "        feature_importance_df.fillna(0,inplace=True)\n",
    "        feature_importance_df['importance'] = feature_importance_df.drop('feature',axis=1).sum(axis=1)\n",
    "        feature_importance_df['importance'] = 100 * feature_importance_df['importance'] / feature_importance_df['importance'].sum()\n",
    "        \n",
    "        return feature_importance_df\n",
    "        \n",
    "    def plot_feature_importance(self):\n",
    "        feature_importance_df = self.get_feature_importance()\n",
    "        feature_importance_df.sort_values('importance',ascending=True,inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.barh(feature_importance_df.feature,feature_importance_df.importance)\n",
    "        plt.show()\n",
    "        \n",
    "    def fit(self,X,y,cat_features=None,sample_weight=None,verbose=True):\n",
    "        self.cat_features = [] if cat_features is None else cat_features\n",
    "        self.sample_weight = sample_weight\n",
    "        self.features = X.columns.tolist()\n",
    "\n",
    "        self.base_models = []\n",
    "        self.base_scores = []\n",
    "        self.base_feature_importances = []\n",
    "        kf = KFold(n_splits=self.n_splits,random_state=self.random_state,shuffle=True)\n",
    "\n",
    "        progress_fmt = '> KFold: {}/{}'\n",
    "        for k, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            k_str = str(k+1).zfill(len(str(self.n_splits)))\n",
    "            print('')\n",
    "            print('-'*80)\n",
    "            print(progress_fmt.format(k_str,self.n_splits))\n",
    "            print('-'*80)\n",
    "            \n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "            \n",
    "            if self.sample_weight is None:\n",
    "                sample_weight = None\n",
    "                eval_sample_weight = None\n",
    "            else:\n",
    "                sample_weight = self.sample_weight[tr_idx]\n",
    "                eval_sample_weight = self.sample_weight[val_idx]\n",
    "\n",
    "            #------------------------------------------------------------------------------------#\n",
    "            # (1) base model\n",
    "            #------------------------------------------------------------------------------------#\n",
    "            # define the base model\n",
    "            base_model = WeightedEnsembleRegressor(\n",
    "                hyperparameters,\n",
    "                weight='balanced', # 'equal','balanced',\n",
    "                inverse_transform=self.inverse_transform,\n",
    "                eval_metric=self.eval_metric,\n",
    "                method=self.method, # 'ensemble','stacking'\n",
    "                use_weightedsum_in_stacking=self.use_weightedsum_in_stacking,\n",
    "            )\n",
    "            # fit the model\n",
    "            base_model.fit(\n",
    "                X_tr,y_tr,\n",
    "                eval_set=[(X_va,y_va)],\n",
    "                cat_features=cat_features,\n",
    "                sample_weight=sample_weight,\n",
    "                eval_sample_weight=[eval_sample_weight],\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            \n",
    "            # prediction\n",
    "            y_pred = base_model.predict(X_va)\n",
    "            if self.inverse_transform is not None:\n",
    "                y_true = self.inverse_transform(y_va.values)\n",
    "            else:\n",
    "                y_true = y_va.values\n",
    "            \n",
    "            # caculate score\n",
    "            score = mean_squared_error(y_true=y_true,y_pred=y_pred)**0.5\n",
    "\n",
    "            # append inner loop\n",
    "            self.base_models.append(base_model)\n",
    "            self.base_scores.append([k+1,len(X_tr),len(X_va),score])\n",
    "            \n",
    "            # # plot feature importance\n",
    "            # self.base_feature_importances.append(base_model.feature_importances_)\n",
    "            # base_model.plot_feature_importance()\n",
    "        \n",
    "        self.base_score = pd.DataFrame(self.base_scores,columns=['k','n_train','n_val','rmse'])\n",
    "        self.validation_score = self.base_score.rmse.mean()\n",
    "        \n",
    "        #self.plot_feature_importance()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        pred = [base_model.predict(X) for base_model in self.base_models]\n",
    "        pred = np.mean(pred,axis=0)\n",
    "        return pred\n",
    "    \n",
    "    def save(self,path):\n",
    "        save_dict = {\n",
    "            'hyperparameters' : self.hyperparameters,\n",
    "            'weight' : self.weight,\n",
    "            'n_splits' : self.n_splits,\n",
    "            'random_state' : self.random_state,\n",
    "            'inverse_transform' : self.inverse_transform,\n",
    "            'cat_features' : self.cat_features,\n",
    "            'sample_weight' : self.sample_weight,\n",
    "            'base_models' : self.base_models,\n",
    "            'base_scores' : self.base_scores,\n",
    "            'base_score' : self.base_score,\n",
    "            'validation_score' : self.validation_score,\n",
    "            'base_feature_importances' : self.base_feature_importances,\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            #pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            dill.dump(save_dict, f)\n",
    "            \n",
    "    def load(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            #save_dict = pickle.load(f)\n",
    "            save_dict = dill.load(f)\n",
    "            \n",
    "            self.hyperparameters = save_dict['hyperparameters']\n",
    "            self.weight = save_dict['weight']\n",
    "            self.n_splits = save_dict['n_splits']\n",
    "            self.random_state = save_dict['random_state']\n",
    "            self.inverse_transform = save_dict['inverse_transform']\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.sample_weight = save_dict['sample_weight']\n",
    "            self.base_models = save_dict['base_models']\n",
    "            self.base_scores = save_dict['base_scores']\n",
    "            self.base_score = save_dict['base_score']\n",
    "            self.validation_score = save_dict['validation_score']\n",
    "            self.base_feature_importances = save_dict['base_feature_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c0cb4df-68bf-4153-af7b-671d6cafa36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth' : 9,\n",
    "    'random_state' : CFG.SEED,\n",
    "    'learning_rate' : 0.05,\n",
    "    'iterations' : 30000,\n",
    "    'early_stopping_rounds' : 300,\n",
    "    'xgb_learning_rate' : 0.3,         # default=0.3\n",
    "    'xgb_iterations' : 3000,           # default=100\n",
    "    'xgb_early_stopping_rounds' : 100,\n",
    "    'rf_iterations' : 100,\n",
    "    'hgb_iterations' : 100,            # default=100\n",
    "    #'extratrees_iterations' : 100,    # default=100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44682051-b1dd-4d0d-b17d-b648a1124968",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_path_fmt = './mc/weiens_model_log_target{}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ccfd6c9-a9f4-4a46-98ec-2ca375c057bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### [1/4] 사망자수\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.083, val_loss=0.090, elasped=7.5s\n",
      "[2/3] XGBRegressor: loss=0.061, val_loss=0.093, elasped=4.0s\n",
      "[3/3] LGBMRegressor: loss=0.089, val_loss=0.090, elasped=1.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.078, val_loss=0.090, elasped=12.9s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.088, val_loss=0.090, elasped=7.3s\n",
      "[2/3] XGBRegressor: loss=0.019, val_loss=0.126, elasped=2.1s\n",
      "[3/3] LGBMRegressor: loss=0.088, val_loss=0.090, elasped=0.9s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.072, val_loss=0.092, elasped=10.3s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.081, val_loss=0.100, elasped=7.2s\n",
      "[2/3] XGBRegressor: loss=0.060, val_loss=0.101, elasped=4.0s\n",
      "[3/3] LGBMRegressor: loss=0.086, val_loss=0.100, elasped=1.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.076, val_loss=0.099, elasped=12.7s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.081, val_loss=0.100, elasped=7.1s\n",
      "[2/3] XGBRegressor: loss=0.028, val_loss=0.130, elasped=2.7s\n",
      "[3/3] LGBMRegressor: loss=0.082, val_loss=0.100, elasped=1.0s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.066, val_loss=0.101, elasped=10.8s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.087, val_loss=0.085, elasped=7.1s\n",
      "[2/3] XGBRegressor: loss=0.066, val_loss=0.088, elasped=4.1s\n",
      "[3/3] LGBMRegressor: loss=0.090, val_loss=0.085, elasped=1.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.081, val_loss=0.085, elasped=12.6s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.090, val_loss=0.085, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.035, val_loss=0.115, elasped=3.5s\n",
      "[3/3] LGBMRegressor: loss=0.088, val_loss=0.085, elasped=1.3s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.074, val_loss=0.087, elasped=12.4s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.085, val_loss=0.087, elasped=7.0s\n",
      "[2/3] XGBRegressor: loss=0.065, val_loss=0.090, elasped=3.7s\n",
      "[3/3] LGBMRegressor: loss=0.089, val_loss=0.088, elasped=1.0s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.080, val_loss=0.087, elasped=11.7s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.087, val_loss=0.088, elasped=6.4s\n",
      "[2/3] XGBRegressor: loss=0.030, val_loss=0.118, elasped=2.8s\n",
      "[3/3] LGBMRegressor: loss=0.085, val_loss=0.088, elasped=0.9s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.071, val_loss=0.089, elasped=10.1s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.089, val_loss=0.086, elasped=5.8s\n",
      "[2/3] XGBRegressor: loss=0.065, val_loss=0.090, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.090, val_loss=0.086, elasped=1.0s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.082, val_loss=0.086, elasped=10.1s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.089, val_loss=0.086, elasped=6.4s\n",
      "[2/3] XGBRegressor: loss=0.029, val_loss=0.118, elasped=2.3s\n",
      "[3/3] LGBMRegressor: loss=0.089, val_loss=0.086, elasped=0.9s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.074, val_loss=0.087, elasped=9.6s\n",
      "################################################################################\n",
      "### [2/4] 중상자수\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.486, val_loss=0.501, elasped=7.9s\n",
      "[2/3] XGBRegressor: loss=0.460, val_loss=0.502, elasped=3.5s\n",
      "[3/3] LGBMRegressor: loss=0.490, val_loss=0.501, elasped=1.9s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.478, val_loss=0.500, elasped=13.2s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.486, val_loss=0.504, elasped=7.5s\n",
      "[2/3] XGBRegressor: loss=0.394, val_loss=0.522, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.482, val_loss=0.504, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.451, val_loss=0.502, elasped=12.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.488, val_loss=0.489, elasped=8.2s\n",
      "[2/3] XGBRegressor: loss=0.457, val_loss=0.494, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.496, val_loss=0.488, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.480, val_loss=0.489, elasped=13.3s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.491, val_loss=0.491, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.391, val_loss=0.517, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.490, val_loss=0.491, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.455, val_loss=0.492, elasped=12.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.485, val_loss=0.513, elasped=7.8s\n",
      "[2/3] XGBRegressor: loss=0.450, val_loss=0.514, elasped=3.9s\n",
      "[3/3] LGBMRegressor: loss=0.489, val_loss=0.513, elasped=1.8s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.474, val_loss=0.512, elasped=13.5s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.481, val_loss=0.514, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.396, val_loss=0.528, elasped=4.1s\n",
      "[3/3] LGBMRegressor: loss=0.481, val_loss=0.514, elasped=1.7s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.449, val_loss=0.511, elasped=13.4s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.474, val_loss=0.516, elasped=8.8s\n",
      "[2/3] XGBRegressor: loss=0.451, val_loss=0.518, elasped=5.1s\n",
      "[3/3] LGBMRegressor: loss=0.486, val_loss=0.516, elasped=2.0s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.470, val_loss=0.515, elasped=15.9s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.478, val_loss=0.519, elasped=7.7s\n",
      "[2/3] XGBRegressor: loss=0.376, val_loss=0.539, elasped=3.8s\n",
      "[3/3] LGBMRegressor: loss=0.474, val_loss=0.518, elasped=1.7s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.439, val_loss=0.517, elasped=13.2s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.492, val_loss=0.486, elasped=7.7s\n",
      "[2/3] XGBRegressor: loss=0.460, val_loss=0.489, elasped=4.1s\n",
      "[3/3] LGBMRegressor: loss=0.500, val_loss=0.486, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.483, val_loss=0.485, elasped=13.5s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.489, val_loss=0.488, elasped=7.5s\n",
      "[2/3] XGBRegressor: loss=0.396, val_loss=0.515, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.492, val_loss=0.489, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.456, val_loss=0.489, elasped=12.5s\n",
      "################################################################################\n",
      "### [3/4] 경상자수\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.952, val_loss=0.956, elasped=8.2s\n",
      "[2/3] XGBRegressor: loss=0.947, val_loss=0.967, elasped=3.5s\n",
      "[3/3] LGBMRegressor: loss=0.964, val_loss=0.956, elasped=1.6s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.953, val_loss=0.959, elasped=13.3s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.938, val_loss=0.967, elasped=7.7s\n",
      "[2/3] XGBRegressor: loss=0.951, val_loss=1.002, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.945, val_loss=0.969, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.942, val_loss=0.977, elasped=12.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.938, val_loss=0.984, elasped=8.6s\n",
      "[2/3] XGBRegressor: loss=0.930, val_loss=0.994, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.956, val_loss=0.984, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.940, val_loss=0.986, elasped=13.7s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.927, val_loss=0.998, elasped=7.7s\n",
      "[2/3] XGBRegressor: loss=0.932, val_loss=1.033, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.925, val_loss=0.998, elasped=1.7s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.925, val_loss=1.007, elasped=12.7s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.956, val_loss=0.978, elasped=8.0s\n",
      "[2/3] XGBRegressor: loss=0.950, val_loss=0.990, elasped=3.7s\n",
      "[3/3] LGBMRegressor: loss=0.958, val_loss=0.976, elasped=1.8s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.953, val_loss=0.980, elasped=13.6s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.930, val_loss=0.985, elasped=7.8s\n",
      "[2/3] XGBRegressor: loss=0.915, val_loss=1.011, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.934, val_loss=0.986, elasped=1.7s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.924, val_loss=0.992, elasped=12.7s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.954, val_loss=0.965, elasped=7.8s\n",
      "[2/3] XGBRegressor: loss=0.955, val_loss=0.978, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.964, val_loss=0.964, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.957, val_loss=0.967, elasped=12.9s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.943, val_loss=0.975, elasped=7.5s\n",
      "[2/3] XGBRegressor: loss=0.956, val_loss=1.008, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.945, val_loss=0.976, elasped=1.7s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.945, val_loss=0.984, elasped=12.5s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.949, val_loss=0.984, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.940, val_loss=0.996, elasped=3.5s\n",
      "[3/3] LGBMRegressor: loss=0.957, val_loss=0.983, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.948, val_loss=0.986, elasped=12.7s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.936, val_loss=0.998, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.944, val_loss=1.034, elasped=3.2s\n",
      "[3/3] LGBMRegressor: loss=0.941, val_loss=0.999, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.938, val_loss=1.008, elasped=12.4s\n",
      "################################################################################\n",
      "### [4/4] 부상자수\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.389, val_loss=0.370, elasped=7.6s\n",
      "[2/3] XGBRegressor: loss=0.363, val_loss=0.372, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.391, val_loss=0.370, elasped=1.9s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.381, val_loss=0.370, elasped=12.9s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.388, val_loss=0.372, elasped=6.9s\n",
      "[2/3] XGBRegressor: loss=0.275, val_loss=0.399, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.384, val_loss=0.372, elasped=1.6s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.351, val_loss=0.373, elasped=11.8s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.382, val_loss=0.407, elasped=7.1s\n",
      "[2/3] XGBRegressor: loss=0.356, val_loss=0.407, elasped=3.5s\n",
      "[3/3] LGBMRegressor: loss=0.387, val_loss=0.407, elasped=1.6s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.375, val_loss=0.406, elasped=12.2s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.381, val_loss=0.409, elasped=6.8s\n",
      "[2/3] XGBRegressor: loss=0.290, val_loss=0.429, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.384, val_loss=0.409, elasped=1.5s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.351, val_loss=0.409, elasped=11.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.386, val_loss=0.403, elasped=7.1s\n",
      "[2/3] XGBRegressor: loss=0.355, val_loss=0.406, elasped=3.7s\n",
      "[3/3] LGBMRegressor: loss=0.387, val_loss=0.402, elasped=1.7s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.376, val_loss=0.402, elasped=12.6s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.388, val_loss=0.405, elasped=7.0s\n",
      "[2/3] XGBRegressor: loss=0.288, val_loss=0.438, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.387, val_loss=0.405, elasped=1.5s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.355, val_loss=0.405, elasped=11.8s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.389, val_loss=0.381, elasped=7.3s\n",
      "[2/3] XGBRegressor: loss=0.358, val_loss=0.382, elasped=3.4s\n",
      "[3/3] LGBMRegressor: loss=0.395, val_loss=0.381, elasped=1.6s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.381, val_loss=0.380, elasped=12.3s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.390, val_loss=0.382, elasped=6.8s\n",
      "[2/3] XGBRegressor: loss=0.281, val_loss=0.413, elasped=3.6s\n",
      "[3/3] LGBMRegressor: loss=0.389, val_loss=0.382, elasped=1.5s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.355, val_loss=0.383, elasped=12.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "########  <Step1> Ensemble  ########\n",
      "[1/3] CatBoostRegressor: loss=0.383, val_loss=0.409, elasped=7.3s\n",
      "[2/3] XGBRegressor: loss=0.357, val_loss=0.410, elasped=4.0s\n",
      "[3/3] LGBMRegressor: loss=0.385, val_loss=0.409, elasped=1.6s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.375, val_loss=0.408, elasped=13.0s\n",
      "\n",
      "########  <Step2> Stacking  ########\n",
      "[1/3] CatBoostRegressor: loss=0.379, val_loss=0.410, elasped=7.1s\n",
      "[2/3] XGBRegressor: loss=0.294, val_loss=0.428, elasped=3.3s\n",
      "[3/3] LGBMRegressor: loss=0.381, val_loss=0.410, elasped=1.5s\n",
      "<Weighted Stacking(weight='balanced')> loss=0.351, val_loss=0.410, elasped=11.9s\n",
      "CPU times: user 35min 22s, sys: 12min 21s, total: 47min 43s\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = []\n",
    "for i,target in enumerate(CFG.TARGET):\n",
    "    print('#'*80)\n",
    "    print('### [{}/{}] {}'.format(i+1,len(CFG.TARGET),target))\n",
    "    print('#'*80)\n",
    "    \n",
    "    X = train_df.drop(CFG.TARGET,axis=1)\n",
    "    y = train_df[target]\n",
    "    X_test = test_df.copy()\n",
    "\n",
    "    unique_info = X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    unuse_cols = ['ECLO']\n",
    "\n",
    "    drop_cols = unique_cols+unuse_cols\n",
    "    X     .drop(list(set(X.columns)&set(drop_cols)),axis=1,inplace=True)\n",
    "    X_test.drop(list(set(X_test.columns)&set(drop_cols)),axis=1,inplace=True)\n",
    "    \n",
    "    model = KfoldWeightedEnsembleRegressor(\n",
    "        hyperparameters,\n",
    "        method='stacking', #'ensemble',\n",
    "        weight='balanced',\n",
    "        use_weightedsum_in_stacking=True,\n",
    "        inverse_transform=target_transform.inverse_transform,\n",
    "        eval_metric=RMSE,\n",
    "        n_splits=CFG.N_SPLITS,\n",
    "        random_state=CFG.SEED,\n",
    "    )\n",
    "    model.fit(\n",
    "        X,y,\n",
    "        cat_features=None,\n",
    "        sample_weight=None,\n",
    "        verbose=True,\n",
    "    )\n",
    "    mc_path = mc_path_fmt.format(i+1)\n",
    "    model.save(mc_path)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44313f35-47cf-4556-aacc-3069facbc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prediction = []\n",
    "te_prediction = []\n",
    "for i,target in enumerate(CFG.TARGET):\n",
    "    X = train_df.drop(CFG.TARGET,axis=1)\n",
    "    y = train_df[target]\n",
    "    X_test = test_df.copy()\n",
    "\n",
    "    unique_info = X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    unuse_cols = ['ECLO']\n",
    "\n",
    "    drop_cols = unique_cols+unuse_cols\n",
    "    X     .drop(list(set(X.columns)&set(drop_cols)),axis=1,inplace=True)\n",
    "    X_test.drop(list(set(X_test.columns)&set(drop_cols)),axis=1,inplace=True)\n",
    "    \n",
    "    model = models[i]\n",
    "    \n",
    "    tr_prediction.append(model.predict(X).flatten().tolist())\n",
    "    te_prediction.append(model.predict(X_test).flatten().tolist())\n",
    "    \n",
    "tr_prediction = np.array(tr_prediction).T\n",
    "te_prediction = np.array(te_prediction).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c23f153e-265c-4561-86d5-812ba117591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 사망자수, RMSE: 0.075\n",
      "Target: 중상자수, RMSE: 0.458\n",
      "Target: 경상자수, RMSE: 0.943\n",
      "Target: 부상자수, RMSE: 0.360\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(CFG.TARGET)):\n",
    "    pred = tr_prediction[:,i]\n",
    "    true = target_transform.inverse_transform(train_df[CFG.TARGET[i]])\n",
    "    rmse = mean_squared_error(pred,true)**0.5\n",
    "    print('Target: {}, RMSE: {:.3f}'.format(CFG.TARGET[i],rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "756dda0e-afe4-46d3-929b-849ddc2cb4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.121, RMSLE: 0.418\n"
     ]
    }
   ],
   "source": [
    "tr_pred = tr_prediction @ [10,5,3,1]\n",
    "tr_true = train_df['ECLO'].values\n",
    "print('RMSE: {:.3f}, RMSLE: {:.3f}'.format(RMSE(tr_true,tr_pred),RMSLE(tr_true,tr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec2e069e-a5f3-4625-ac53-f268b8ec2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred = te_prediction @ [10,5,3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d65cfe14-5860-4ab6-b6ef-412bc7c36a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ECLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCIDENT_39609</td>\n",
       "      <td>4.321914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCIDENT_39610</td>\n",
       "      <td>3.200378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCIDENT_39611</td>\n",
       "      <td>3.964339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCIDENT_39612</td>\n",
       "      <td>4.110498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCIDENT_39613</td>\n",
       "      <td>4.153589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      ECLO\n",
       "0  ACCIDENT_39609  4.321914\n",
       "1  ACCIDENT_39610  3.200378\n",
       "2  ACCIDENT_39611  3.964339\n",
       "3  ACCIDENT_39612  4.110498\n",
       "4  ACCIDENT_39613  4.153589"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.436056003\n",
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['ECLO'] = te_pred\n",
    "submit.to_csv('./out/15_weiens_stacking_merge_additional_data_target4.csv',index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_3.10.13",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
