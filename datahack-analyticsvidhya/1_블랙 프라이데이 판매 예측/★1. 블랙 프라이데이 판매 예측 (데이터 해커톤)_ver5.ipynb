{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161bd150",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "># **코드설명**\n",
    "\n",
    "---\n",
    "\n",
    "- 파 일 명 : 블랙 프라이데이 판매 예측 <br>\n",
    "- 시작날짜 : 2021.11.25 <br>\n",
    "- 수정날짜 : 2022.01.07 <br>\n",
    "- 작 성 자 : 김혁진 <br>\n",
    "- 작성주제 : DataHackaton / 블랙 프라이데이 판매 예측 <br>\n",
    "\n",
    "--- \n",
    "\n",
    "- **참조**\n",
    "\n",
    "  (1) 대회 홈페이지 : [DataHackaton](https://datahack.analyticsvidhya.com/contest/black-friday/#About) <br>\n",
    "  (2) 하이퍼 파리미터 설명 : [Naver Blog](https://blog.naver.com/wideeyed/221333529176) <br>\n",
    "  (3) Class문 설명 : [Github](https://zzsza.github.io/development/2020/07/05/python-class/) <br>\n",
    "  (4) GPU 설정 : [Medium](https://medium.com/@am.sharma/lgbm-on-colab-with-gpu-c1c09e83f2af) <br>\n",
    "  (5) RAM 모두사용으로 세션다운 : [Tistory](https://somjang.tistory.com/entry/Google-Colab-%EC%9E%90%EC%A3%BC%EB%81%8A%EA%B8%B0%EB%8A%94-%EB%9F%B0%ED%83%80%EC%9E%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0)\n",
    "\n",
    "---\n",
    "\n",
    "- **고려사항** <br>\n",
    "  (1) AutoEncoder로 파생변수 생성해보기 <br>\n",
    "  (2) 하이퍼파라미터 탐색 : grid-search, bayesian-optimization, [optuna](https://dacon.io/competitions/official/235713/codeshare/2704?page=1&dtype=recent) <br>\n",
    "  (3) RandomForest, XGBoost, Lightgbm, CatBoost 설명 [블로그](https://jhkim0759.tistory.com/12)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8eb9c1",
   "metadata": {},
   "source": [
    "># **기본설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59740e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Query Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d1e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.038275Z",
     "start_time": "2022-01-13T16:09:30.030269Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "query_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b430dee",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Github 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394db457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.054266Z",
     "start_time": "2022-01-13T16:09:30.039263Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 로그인\n",
    "# git config --global user.email \"hyuckjin12@naver.com\"\n",
    "# git config --global user.name  \"hyuckjinkim\"\n",
    "# git config -l\n",
    "\n",
    "# # 해당폴더로 들어가기\n",
    "# cd 'E:/USB포맷_210223/Python/★PSC/1. 블랙 프라이데이 판매 예측(데이터 해커톤)'\n",
    "# git init\n",
    "\n",
    "# # 상태확인\n",
    "# git status\n",
    "\n",
    "# # 모든 파일 추가 후, commit\n",
    "# git add .\n",
    "# git commit -m 'new ipynb file'\n",
    "\n",
    "# # 파일넣기\n",
    "# cd 'E:/USB포맷_210223/Python/★PSC/1. 블랙 프라이데이 판매 예측(데이터 해커톤)'\n",
    "# git remote rm origin\n",
    "# git remote add origin https://github.com/hyuckjinkim/DataHackaton.git\n",
    "\n",
    "# git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2214",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Markdown : Tabular Left Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624da7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.070258Z",
     "start_time": "2022-01-13T16:09:30.055256Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52129",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Jupyter Notebook Style : Theme, Display, TOC, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b639a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.086243Z",
     "start_time": "2022-01-13T16:09:30.071255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # theme 설치\n",
    "# !pip install jupyterthemes\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade notebook\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade jupyterthemes\n",
    "\n",
    "# 2.2.1. 테마바꾸기(customizing)\n",
    "# !jt -t onedork -fs 115 -nfs 125 -tfs 115 -dfs 115 -ofs 115 -cursc r -cellw 80% -lineh 115 -altmd  -kl -T -N\n",
    "\n",
    "# 2.2.2. 쥬피터 노트북 화면 넓게 사용\n",
    "# 출처: https://taehooh.tistory.com/entry/Jupyter-Notebook-주피터노트북-화면-넓게-쓰는방법\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# # 2.2.3. 좌측 TOC 만들기\n",
    "# # 출처 : https://gmnam.tistory.com/246\n",
    "# !pip install jupyter_nbextensions_configurator\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "\n",
    "# !jupyter nbextensions_configurator enable --user\n",
    "# !jupyter contrib nbextension install --user\n",
    "\n",
    "# !pip install nbconvert\n",
    "\n",
    "# !pip install jupytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecfd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.102246Z",
     "start_time": "2022-01-13T16:09:30.087242Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2.3.1 Google Drive Mount\n",
    "# # (Google Drive 사용 시 설정)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True) # 새로운 창에서 key 를 받아서 입력해야합니다. \n",
    "\n",
    "# # 2.3.2. 메모리 에러\n",
    "# https://growingsaja.tistory.com/477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8cbdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.118240Z",
     "start_time": "2022-01-13T16:09:30.103235Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2.3.3. GPU 사용 (6분)\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "# !mkdir build\n",
    "# %cd /content/LightGBM\n",
    "# !cmake -DUSE_GPU=1 #avoid ..\n",
    "# !make -j$(nproc)\n",
    "# !sudo apt-get -y install python-pip\n",
    "# !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "# %cd /content/LightGBM/python-package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8d53",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c6112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:30.134233Z",
     "start_time": "2022-01-13T16:09:30.119228Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall pandas -y\n",
    "# !pip uninstall numpy  -y\n",
    "# !pip uninstall lightgbm -y\n",
    "\n",
    "# !pip install pandas==1.1.0\n",
    "# !pip install numpy==1.21.2\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install lightgbm --install-option=--gpu\n",
    "\n",
    "# !pip install seaborn\n",
    "# !pip install plotnine\n",
    "# !pip install pandasql\n",
    "\n",
    "# lightgbm 에러떴는데, 콘다에서 실행하면 해결됨\n",
    "# conda install -c conda-forge lightgbm \n",
    "\n",
    "# bayesian optimization 설치\n",
    "# !pip install bayesian-optimization\n",
    "\n",
    "# xgboost 설치\n",
    "# !pip install xgboost\n",
    "\n",
    "# catboost 설치\n",
    "# !pip install catboost\n",
    "\n",
    "# !pip install dill\n",
    "\n",
    "# pycaret 에러떴는데, --user 붙이니깐 해결됨\n",
    "# !pip install --user pycaret\n",
    "\n",
    "# !pip install qgrid\n",
    "# !jupyter nbextension enable --py --sys-prefix qgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a02e69",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6ebbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:34.401181Z",
     "start_time": "2022-01-13T16:09:30.135220Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook 전용\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import qgrid\n",
    "\n",
    "# value_counts() 범용적인 버전\n",
    "from collections import Counter as cnt\n",
    "\n",
    "# save env.\n",
    "import dill\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [11.7, 8.27] # [15, 10] # [11.7,8.27] - A4 size\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# sqldf\n",
    "from pandasql import sqldf\n",
    "sql = lambda q: sqldf(q, globals())\n",
    "\n",
    "\n",
    "# modeling\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import f1_score,make_scorer,r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import lightgbm\n",
    "# !pip install lightgbm --install-option=--gpu --install-option=\"--opencl-include-dir=/usr/local/cuda/include/\" --install-option=\"--opencl-library=/usr/local/cuda/lib64/libOpenCL.so\"\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099b6ae",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47db36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:34.417179Z",
     "start_time": "2022-01-13T16:09:34.403169Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.5.1. Data Path\n",
    "# jupyter.notebook : 'os.getcwd() + '/DAT/블랙 프라이데이 판매 예측/''\n",
    "# google.colab     : '/content/drive/MyDrive/Python/4. 블랙프라이데이 판매예측/DAT/'\n",
    "BASE_PATH = 'E:/USB포맷_210223/Python'\n",
    "DATA_PATH = BASE_PATH + '/DAT/1. 블랙 프라이데이 판매 예측(데이터 해커톤)/'\n",
    "OUT_PATH  = BASE_PATH + '/OUT/1. 블랙 프라이데이 판매 예측(데이터 해커톤)/'\n",
    "\n",
    "# 2.5.2. set seed\n",
    "SEED = 777\n",
    "\n",
    "# 2.5.3. plot\n",
    "PLOT = True\n",
    "\n",
    "# 2.5.5. missing check\n",
    "MISSING_CHECK = True\n",
    "\n",
    "# 2.5.6. interaction\n",
    "INTERACTION_CHECK = True\n",
    "INTERACTION = True\n",
    "\n",
    "# 2.5.7. scaling\n",
    "SCALE_CHECK = True\n",
    "SCALE = True\n",
    "\n",
    "# 2.5.8. lightgbm parameter\n",
    "# 처음 (INIT_POINTS)회 랜덤 값으로 score 계산 후 (N_ITER)회 최적화\n",
    "INIT_POINTS = 15\n",
    "N_ITER = 15\n",
    "N_CV = 4\n",
    "EARLY_STOPPING_ROUNDS = 30\n",
    "N_ESTIMATORS = 2000\n",
    "OBJECTIVE = ['binary','regression'][1]\n",
    "METRIC = ['auc','binary_logloss','rmse'][2]\n",
    "\n",
    "# initial value save\n",
    "ini_var = [\n",
    "    'SEED','PLOT','SCALE','INTERACTION','MISSING_CHECK','INTERACTION_CHECK',\n",
    "    'INIT_POINTS','N_ITER','N_CV','EARLY_STOPPING_ROUNDS','N_ESTIMATORS','OBJECTIVE','METRIC'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd87120",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Set Off the Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3fc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:34.433156Z",
     "start_time": "2022-01-13T16:09:34.420163Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769937",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3e77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:34.499515Z",
     "start_time": "2022-01-13T16:09:34.435156Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--codefoling--#\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.1. Seed Fix\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def seed_everything(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    # torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    # torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.2. View all columns\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def View(data):\n",
    "\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    pd.set_option('display.max_rows', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    pd.set_option('display.width', 0)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.3. minmax function\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def minmax(x, digit=None):\n",
    "    if round is None:\n",
    "        return min(x), max(x)\n",
    "    else:\n",
    "        return round(min(x), digit), round(max(x), digit)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.4. 컬럼dict에서 target 제거\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - dict : 기준 dict\n",
    "# - key  : 삭제할 key\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def rmkey(dict, key):\n",
    "    tmp = dict.copy()\n",
    "    del tmp[key]\n",
    "    return tmp\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.5. 각 컬럼의 missing 개수를 파악하는 함수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - col_type : {column명 : type}로 이루어진 dictionary\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def missing_column_check(df, col_type):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    n_na = []\n",
    "    n_na_type = []\n",
    "    for col_nm in data.columns:\n",
    "        data[col_nm] = data[col_nm].astype(col_type[col_nm])\n",
    "\n",
    "        # str인 경우에는 blank(공백)도 있는지 확인\n",
    "        if col_type[col_nm] == str:\n",
    "\n",
    "            isnull_cnt = data[col_nm].str.strip().isnull().sum()\n",
    "            blank_cnt = sum(data[col_nm].str.strip() == '')\n",
    "            nan_cnt = sum(data[col_nm].str.strip() == 'nan')\n",
    "            null_cnt = sum(data[col_nm].str.strip() == 'null')\n",
    "\n",
    "            n_na_x = isnull_cnt+blank_cnt+nan_cnt+null_cnt\n",
    "            n_na.append(n_na_x)\n",
    "\n",
    "            if n_na_x > 0:\n",
    "                n_na_type_x = []\n",
    "                if isnull_cnt > 0:\n",
    "                    n_na_type_x.append('isnull')\n",
    "                if blank_cnt > 0:\n",
    "                    n_na_type_x.append('blank')\n",
    "                if nan_cnt > 0:\n",
    "                    n_na_type_x.append('nan')\n",
    "                if null_cnt > 0:\n",
    "                    n_na_type_x.append('null')\n",
    "                n_na_type_x = '+'.join(n_na_type_x)\n",
    "            else:\n",
    "                n_na_type_x = ''\n",
    "            n_na_type.append(n_na_type_x)\n",
    "\n",
    "        # numeric인 경우에는 null의 개수만 확인\n",
    "        else:\n",
    "            n_na_x = data[col_nm].isnull().sum()\n",
    "            n_na.append(n_na_x)\n",
    "\n",
    "            if n_na_x > 0:\n",
    "                n_na_type.append('isnull')\n",
    "            else:\n",
    "                n_na_type.append('')\n",
    "\n",
    "    res_df = pd.DataFrame({\n",
    "        'col': data.columns,\n",
    "        'n_na': n_na,\n",
    "        'n_n_ratio': [str(round(n/len(data)*100, 1))+'%' for n in n_na],\n",
    "        'na_type': n_na_type,\n",
    "        'col_type': [COL_TYPE[col].__name__ for col in data.columns]\n",
    "    })\n",
    "\n",
    "    res_df = res_df[res_df['n_na'] > 0]\n",
    "    if len(res_df) == 0:\n",
    "        return('Dataset does not have a null value')\n",
    "    else:\n",
    "        return(res_df)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.6. 교호작용항 추가\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - num_vari : 숫자형 변수 list\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def interaction_term(df, num_vari):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    num_var = list(set(num_vari) - set(['id']))\n",
    "\n",
    "    for i in range(0, len(num_var)):\n",
    "        for j in range(i, len(num_var)):\n",
    "            data[f'{num_var[i]}*{num_var[j]}'] = data[f'{num_var[i]}'] * \\\n",
    "                data[f'{num_var[j]}']\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.7. color when print\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.8. density plot : histogram + density plot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data : 기준 data\n",
    "# - vars : hist + kde를 그릴 숫자형 변수\n",
    "# - hue  : group화 변수\n",
    "# - binwidth_adj_ratio : binwidth 조정 비율\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def density_plot(df, vars,\n",
    "                 binwidths=None, hue=None,\n",
    "                 binwidth_adj_ratio=None,\n",
    "                 figsize=(15, 7)):\n",
    "\n",
    "    from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    # 1) vars가 1개뿐일 때 에러발생\n",
    "    #    -> 1개     : type = str\n",
    "    #    -> 2개이상 : type = ndarray, ...\n",
    "    if type(vars) == str:\n",
    "        vars = [vars]\n",
    "\n",
    "    # 2) plotting (nrow,ncol) 설정\n",
    "    nrow = math.ceil(len(vars)**(1/2))\n",
    "    ncol = nrow\n",
    "\n",
    "    # 3) binwidths가 없을 때, binwidth 설정\n",
    "    # 출처 : http://www.aistudy.co.kr/paper/pdf/histogram_jeon.pdf\n",
    "    if binwidths is None:\n",
    "        binwidths = []\n",
    "        for col in data[vars].columns:\n",
    "            n_bin = math.ceil(1 + 3.32*math.log10(len(data)))\n",
    "            binwidth = (data[col].max() - data[col].min()) / n_bin\n",
    "            binwidths.append(binwidth)\n",
    "            del binwidth\n",
    "\n",
    "    # 4) 설정한 binwidth를 조정하는 비율\n",
    "    if binwidth_adj_ratio is not None:\n",
    "        binwidths = [binwidth * binwidth_adj_ratio for binwidth in binwidths]\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # 5) vars 별로 plot 생성\n",
    "    for iter, var in enumerate(vars):\n",
    "\n",
    "        binwidth = binwidths[iter]\n",
    "\n",
    "        # (1) histogram\n",
    "        ax1 = fig.add_subplot(nrow, ncol, iter+1)\n",
    "        g1 = sns.histplot(data=data, x=var, hue=hue,\n",
    "                          kde=True, stat='probability',\n",
    "                          color='lightskyblue',\n",
    "                          binwidth=binwidth, ax=ax1)\n",
    "        ax2 = ax1.twinx()\n",
    "\n",
    "        # (2) density plot\n",
    "        g2 = sns.kdeplot(data=data, x=var, hue=hue,\n",
    "                         color='red', lw=2, ax=ax2)\n",
    "        # similir limits on the y-axis to align the plots\n",
    "        ax2.set_ylim(0, ax1.get_ylim()[1] / binwidth)\n",
    "        # ax2.yaxis.set_major_formatter(PercentFormatter(1 / binwidth))  # show axis such that 1/binwidth corresponds to 100%\n",
    "        ax2.grid(False)\n",
    "\n",
    "        # (3) density plot y축 없애기\n",
    "        g2.set(yticklabels=[])\n",
    "        g2.set(ylabel=None)\n",
    "        g2.tick_params(right=False)\n",
    "\n",
    "        a, b = divmod(iter, ncol)\n",
    "        if b != 0:\n",
    "            g1.set(ylabel=None)\n",
    "\n",
    "    # 안겹치도록 설정\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# example : density_plot(train, vars=num_vari)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.9. density plot : histogram + density plot\n",
    "#\n",
    "# (1) grp_var vs hue_var 막대그래프\n",
    "# (2) grp_var(x축), hue_var에 따른 각 num_var들의 barplot, violineplot, box+swarmplot + kdeplot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - grp_var : x축 구분할 그룹변수 (text)\n",
    "# - num_vari : 숫자형 변수 (list)\n",
    "# - data : 기준 data\n",
    "# - title_text : plot title (text)\n",
    "# - hue_var : hue 그루핑변수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def plot_num(grp_var, num_vari, df, title_text=None, hue_var=None,\n",
    "             figsize_1=(3, 3), figsize_2=(15, 15)):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    # (1)번 그래프 setting\n",
    "    fig0 = plt.figure(figsize=figsize_1)\n",
    "    ax0 = fig0.add_subplot(1, 1, 1)\n",
    "\n",
    "    if title_text is None:\n",
    "        title_text = grp_var\n",
    "\n",
    "    plt.title(title_text, loc='left', pad=20, fontdict={'fontsize': 30,\n",
    "                                                        'fontweight': 'bold',\n",
    "                                                        'color': 'c'})\n",
    "\n",
    "    # grp_var와 hue_var가 겹치는 경우, hue를 나누지 않음\n",
    "    if (grp_var != hue_var) and (hue_var is not None):\n",
    "\n",
    "        ct = pd.crosstab(data[grp_var], data[hue_var])\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "        ax.legend(title=hue_var, bbox_to_anchor=(1, 1.02), loc='upper left')\n",
    "\n",
    "    else:\n",
    "        ct = data[grp_var].value_counts()\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "\n",
    "    # show\n",
    "    plt.xlabel('')\n",
    "    plt.show()\n",
    "\n",
    "    # 숫자변수중에 [grp,id]변수가 있으면 제외\n",
    "    num_vari_x = list(set(num_vari) - set([grp_var, 'id']))\n",
    "\n",
    "    # plt 생성\n",
    "    fig = plt.figure(figsize=figsize_2)\n",
    "    plt.axis('off')  # 안끄면 x축에 0~1까지 축생김\n",
    "\n",
    "    for iter, var in enumerate(num_vari_x):\n",
    "\n",
    "        # hue랑 grp_var랑 같으면 hue를 넣지않음\n",
    "        hue_x = [None if grp_var == hue_var else hue_var][0]\n",
    "\n",
    "        # (n,4) plot\n",
    "        ax1 = fig.add_subplot(len(num_vari_x), 4, 4*iter+1)\n",
    "        ax2 = fig.add_subplot(len(num_vari_x), 4, 4*iter+2)\n",
    "        ax3 = fig.add_subplot(len(num_vari_x), 4, 4*iter+3)\n",
    "        ax4 = fig.add_subplot(len(num_vari_x), 4, 4*iter+4)\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        # (2-1) 3번째 : box + swarm plot (ylim가져오기위해서 제일 먼저 실행)\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        g11 = sns.swarmplot(x=grp_var, y=var, data=data,\n",
    "                            ax=ax3, color='crimson', marker='*', s=7)\n",
    "        g12 = sns.boxplot(x=grp_var, y=var, data=data, ax=ax3)\n",
    "        g12.set(ylabel=None)\n",
    "        g12.set(yticklabels=[])\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        # (2-2) 1번째 : barplot\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        ax1.set_ylim(ax3.get_ylim())\n",
    "        g21 = sns.barplot(x=grp_var, y=var, data=data, ax=ax1, hue=hue_x)\n",
    "        # g21.set(ylabel=None)\n",
    "        # g21.set(yticklabels=[])\n",
    "        # g21.axes.set_title(str(iter+1) + ':' + var, fontsize=20, weight='bold', ha='left', x=-.05)\n",
    "        g21.set_ylabel(var, fontsize=20)\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        # (2-3) 2번째 : violinplot\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        ax2.set_ylim(ax3.get_ylim())\n",
    "        g31 = sns.violinplot(x=grp_var, y=var, data=data,\n",
    "                             ax=ax2, legend=False, hue=hue_x)\n",
    "        g31.set(ylabel=None)\n",
    "        g31.set(yticklabels=[])\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        # (2-4) 4번째 : density plot\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        ax4.set_ylim(ax3.get_ylim())\n",
    "        g41 = sns.kdeplot(y=var, hue=grp_var, data=data, ax=ax4)\n",
    "        g41.set(ylabel=None)\n",
    "        g41.set(yticklabels=[])\n",
    "        g41.tick_params(right=False)\n",
    "        g41.set(xlabel=None)\n",
    "        g41.set(xticklabels=[])\n",
    "\n",
    "        # 맨 아래에만 x축이 생성되도록 setting\n",
    "        if (iter+1) != len(num_vari_x):\n",
    "\n",
    "            g12.set(xlabel=None)\n",
    "            g12.set(xticklabels=[])\n",
    "\n",
    "            g21.set(xlabel=None)\n",
    "            g21.set(xticklabels=[])\n",
    "\n",
    "            g31.set(xlabel=None)\n",
    "            g31.set(xticklabels=[])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # example\n",
    "# plot_num(grp_var = 'sex', num_vari = num_vari, hue_var = 'target',\n",
    "#          data = train, title_text = 'sex')\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.10. onehot encoding\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- data\n",
    "#- col_types\n",
    "#- ignore_features\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def onehot_encoding(df, col_types, ignore_features=['']):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    cols = setdiff(data.columns, ignore_features + ['target'])\n",
    "    cols = [col for col in cols if col_types[col] in [str]]\n",
    "\n",
    "    res_df = pd.get_dummies(data, columns=cols)\n",
    "\n",
    "    return(res_df)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.10. onehot encoding : str들 모두 int/category로 바꾸기\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- data\n",
    "#- col_types\n",
    "# - convert : int / category\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def str_convert(data, col_types, convert=[int, 'category']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col] == str:\n",
    "            data[col] = data[col].astype(convert)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.11. setdiff\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def setdiff(x, y):\n",
    "    return(list(set(x)-set(y)))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.12. automl_comp\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# missing value를 auto ml로 예측하기위함\n",
    "# target이 missing인 값들을 제외하고, kfold cv를 통해서 각 알고리즘들의 scoring을 계산\n",
    "\n",
    "# # cross_val_score에서 쓸 수 있는 scoring\n",
    "# list(sklearn.metrics.SCORERS.keys())\n",
    "def AUTOML_COMP(train,\n",
    "                target,\n",
    "                col_types,\n",
    "                objective,\n",
    "                scoring=None,\n",
    "                test=None,\n",
    "                ignore_features=None,\n",
    "                n_splits=10,\n",
    "                fit_model=False):\n",
    "\n",
    "    # copy\n",
    "    X_train = train.drop([target], axis=1)\n",
    "    y_train = train[target]\n",
    "    if test is not None:\n",
    "        X_test = test.drop([target], axis=1)\n",
    "        y_test = test[target]\n",
    "\n",
    "    # remove needless features\n",
    "    if ignore_features is not None:\n",
    "        features = list(set(X_train.columns)-set(ignore_features))\n",
    "    else:\n",
    "        features = list(X_train.columns)\n",
    "\n",
    "    # 모형적합\n",
    "    start_time = time.time()\n",
    "\n",
    "    # binary / regression\n",
    "    models = []\n",
    "    if objective == 'binary':\n",
    "\n",
    "        if scoring is None:\n",
    "            scoring = 'accuracy'\n",
    "\n",
    "        models.append(('LR', LogisticRegression(\n",
    "            solver='liblinear', multi_class='ovr', random_state=SEED)))\n",
    "        models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "        models.append(('KNN', KNeighborsClassifier()))\n",
    "        models.append(('CART', DecisionTreeClassifier(random_state=SEED)))\n",
    "        models.append(('NB', GaussianNB()))  # Gaussian Naive Bayes\n",
    "        models.append(('SVM', SVC(gamma='auto', random_state=SEED)))\n",
    "        models.append(('RFC', RandomForestClassifier(random_state=SEED)))\n",
    "        models.append(('XGBC', XGBClassifier(\n",
    "            iterations=10000, verbosity=0, random_state=SEED)))\n",
    "        models.append(('LGBMC', LGBMClassifier(random_state=SEED)))\n",
    "        models.append(('AdaC', AdaBoostClassifier(random_state=SEED)))\n",
    "        models.append(('Cat', CatBoostClassifier(\n",
    "            iterations=10000, silent=True, random_state=SEED)))\n",
    "\n",
    "    elif objective == 'regression':\n",
    "\n",
    "        if scoring is None:\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "\n",
    "        models.append(('LR', LinearRegression()))\n",
    "        models.append(('RIDGE', RidgeClassifier()))\n",
    "        models.append(('LASSO', Lasso(random_state=SEED)))\n",
    "        models.append(('KNN', KNeighborsRegressor()))\n",
    "        models.append(('CART', DecisionTreeRegressor(random_state=SEED)))\n",
    "        models.append(('EN', ElasticNet(random_state=SEED)))\n",
    "        models.append(('SVM', SVR()))\n",
    "        models.append(('RFR', RandomForestRegressor(random_state=SEED)))\n",
    "        models.append(('XGBR', XGBRegressor(\n",
    "            iterations=10000, verbosity=0, random_state=SEED)))\n",
    "        models.append(('LGBMR', LGBMRegressor(random_state=SEED)))\n",
    "        models.append(('AdaR', AdaBoostRegressor(random_state=SEED)))\n",
    "        models.append(('Cat', CatBoostRegressor(\n",
    "            iterations=10000, silent=True, random_state=SEED)))\n",
    "\n",
    "    # kfold cross validation\n",
    "    if fit_model:\n",
    "\n",
    "        results = []\n",
    "        names = []\n",
    "        msgs = []\n",
    "\n",
    "        pbar = tqdm(models)\n",
    "        for name, model in pbar:\n",
    "            pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "            kfold = KFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "            cv_results = cross_val_score(model,\n",
    "                                         X=X_train,\n",
    "                                         y=y_train,\n",
    "                                         cv=kfold,\n",
    "                                         scoring=scoring,\n",
    "                                         verbose=0)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msgs.append((name, cv_results.mean(), cv_results.std()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        running_time = (end_time - start_time)/60\n",
    "        running_time = f'{running_time:.1f} Mins'\n",
    "\n",
    "    ret = {}\n",
    "\n",
    "    ret['X_train'] = X_train\n",
    "    ret['y_train'] = y_train\n",
    "\n",
    "    if fit_model:\n",
    "        ret['run_time'] = running_time\n",
    "        ret['message'] = msgs\n",
    "        ret['model'] = models\n",
    "        ret['cv_result'] = results\n",
    "\n",
    "    if test is not None:\n",
    "        ret['X_test'] = X_test\n",
    "\n",
    "    return(ret)\n",
    "\n",
    "\n",
    "def AUTO_COMP_PLOT(object, title='Algorithm Comparision'):\n",
    "\n",
    "    cv_res_df = pd.DataFrame(\n",
    "        np.transpose(object['cv_result']),\n",
    "        columns=[name for name, model in object['model']]\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.add_subplot(111)\n",
    "    p = sns.boxplot(data=cv_res_df)\n",
    "    p = sns.swarmplot(data=cv_res_df, marker='*', s=7, color='crimson')\n",
    "    p.set_title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f20af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e1f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:34.514840Z",
     "start_time": "2022-01-13T16:09:34.500514Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f33a6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18474b8",
   "metadata": {},
   "source": [
    "데이터를 제공한 회사는 ABC Private Limited로 소매회사임. \\\n",
    "회사설명 : 작물 재배, 시장 원예, 원예(Growing of crops, market gardening, horticulture) [Link](https://www.zaubacorp.com/company/A-B-C-PRIVATE-LIMITED/U01110MH1950PTC008007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6df4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 변수정보 (변수명 참조 : [DataHackaton](https://datahack.analyticsvidhya.com/contest/black-friday/#ProblemStatement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28384f",
   "metadata": {},
   "source": [
    "|변수명 | 변수정보 |\n",
    "|:---:|:---|\n",
    "| User_ID | 사용자 ID |\n",
    "| Product_ID | 제품 ID |\n",
    "| Gender | 사용자의 성별 |\n",
    "| Age | 나이(구간) |\n",
    "| Occupation | 직업(마스킹됨) |\n",
    "| City_Category | 도시의 범주(A,B,C) |\n",
    "| Stay_In_Current_City_Years | 현재 도시에 체류한 기간 |\n",
    "| Marital_Status | 결혼 여부 |\n",
    "| Product_Category_1 | 제품 카테고리 (마스킹됨) |\n",
    "| Product_Category_2 | 제품 카테고리2(마스킹됨,다른 카테고리에도 속할 수 있음) |\n",
    "| Product_Category_3 | 제품 카테고리3(마스킹됨,다른 카테고리에도 속할 수 있음) |\n",
    "| Purchase | 구매금액(Target) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda586bc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c0dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:35.465473Z",
     "start_time": "2022-01-13T16:09:34.516030Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COL_TYPE = {\n",
    "    'User_ID'                     : str,\n",
    "    'Product_ID'                  : str,\n",
    "    'Gender'                      : str,\n",
    "    'Age'                         : str,\n",
    "    'Occupation'                  : str,\n",
    "    'City_Category'               : str,\n",
    "    'Stay_In_Current_City_Years'  : str,\n",
    "    'Marital_Status'              : str,\n",
    "    'Product_Category_1'          : str,\n",
    "    'Product_Category_2'          : str,\n",
    "    'Product_Category_3'          : str,\n",
    "    'Purchase'                    : float\n",
    "}\n",
    "\n",
    "# Train Data Load (550,068 rows, 12 columns)\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv', dtype = COL_TYPE)\n",
    "test  = pd.read_csv(DATA_PATH + 'test.csv' , dtype = COL_TYPE)\n",
    "sub   = pd.read_csv(DATA_PATH + 'sample_submission.csv', dtype = COL_TYPE)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144513a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Column 명 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a655303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:35.529459Z",
     "start_time": "2022-01-13T16:09:35.466484Z"
    }
   },
   "outputs": [],
   "source": [
    "COL_NAME = {\n",
    "    'User_ID'                     : 'user_id',\n",
    "    'Product_ID'                  : 'prod_id',\n",
    "    'Gender'                      : 'gender',\n",
    "    'Age'                         : 'age',\n",
    "    'Occupation'                  : 'occupation',\n",
    "    'City_Category'               : 'city_cat',\n",
    "    'Stay_In_Current_City_Years'  : 'stay_year',\n",
    "    'Marital_Status'              : 'marital',\n",
    "    'Product_Category_1'          : 'prod_cat_1',\n",
    "    'Product_Category_2'          : 'prod_cat_2',\n",
    "    'Product_Category_3'          : 'prod_cat_3',\n",
    "    'Purchase'                    : 'target',\n",
    "}\n",
    "\n",
    "def rename_fn(df,name_dict,type_dict):\n",
    "    data = df.copy()\n",
    "    for key in name_dict.keys():\n",
    "        if key in data.columns:\n",
    "            data.rename(columns={f'{key}': name_dict[key]}, inplace = True)\n",
    "    return(data)\n",
    "\n",
    "train2 = rename_fn(train, COL_NAME, COL_TYPE)\n",
    "test2  = rename_fn(test , COL_NAME, COL_TYPE)\n",
    "sub2   = rename_fn(sub  , COL_NAME, COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dc3af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### COL_TYPE 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b26333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:35.545452Z",
     "start_time": "2022-01-13T16:09:35.530444Z"
    }
   },
   "outputs": [],
   "source": [
    "for (old_key,old_value),(new_key,new_value) in zip(sorted(COL_TYPE.items()),sorted(COL_NAME.items())):\n",
    "    COL_TYPE[new_value] = COL_TYPE.pop(old_key)\n",
    "\n",
    "COL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4ebf0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Missing Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0e065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:47.122131Z",
     "start_time": "2022-01-13T16:09:35.546437Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(color.BOLD + color.BLUE + '> # of train Missing : \\n' + color.END, missing_column_check(train2, COL_TYPE), '\\n')\n",
    "print(color.BOLD + color.BLUE + '> # of test  Missing : \\n' + color.END, missing_column_check(test2 , COL_TYPE), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208a496",
   "metadata": {},
   "source": [
    "Product_Category_2, 3에서 많은 Null이 있음 \\\n",
    "정확한 Missing value 분석은 EDA에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f50fa",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ebd4e",
   "metadata": {},
   "source": [
    "## ID Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bb320",
   "metadata": {},
   "source": [
    "ID인 변수 user_id, prod_id에 대해서 특정한 패턴이 있는지 확인\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf42776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:47.280035Z",
     "start_time": "2022-01-13T16:09:47.123114Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Train : n_unique of user, prod : {train2.user_id.nunique()}, {train2.prod_id.nunique()} (NROW: {len(train2):,})')\n",
    "print(f'Test  : n_unique of user, prod : {test2 .user_id.nunique()}, {test2 .prod_id.nunique()} (NROW: {len(test2 ):,})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce95c7",
   "metadata": {},
   "source": [
    " 고객번호, 상품번호가 unique하지 않고 많이 겹침\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360d1e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:48.120549Z",
     "start_time": "2022-01-13T16:09:47.281035Z"
    }
   },
   "outputs": [],
   "source": [
    "df = train2[['user_id','prod_id']]\n",
    "df['user_prod_id'] = df['user_id'] +'_'+ df['prod_id']\n",
    "\n",
    "cnt(df.groupby(['user_prod_id'])['user_prod_id'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e9c72",
   "metadata": {},
   "source": [
    "user_id + prod_id는 unique key : 한 고객이 여러 개의 상품을 산 것은 고려되지 않음 (다 합쳐서 적재되어있는 것으로 보임)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ca419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:48.279472Z",
     "start_time": "2022-01-13T16:09:48.121549Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_overlay = list(set(train2['user_id'].unique()) & set(test2 ['user_id'].unique()))\n",
    "te_overlay = list(set(test2 ['user_id'].unique()) & set(train2['user_id'].unique()))\n",
    "\n",
    "tr_nunique  = train2['user_id'].nunique()\n",
    "te_nunique  = test2 ['user_id'].nunique()\n",
    "\n",
    "print(f'> Train - Total : {tr_nunique:,}, Overlay : {len(tr_overlay):,}, Else : {tr_nunique-len(tr_overlay):,}')\n",
    "print(f'> Test  - Total : {te_nunique:,}, Overlay : {len(te_overlay):,}, Else : {te_nunique-len(te_overlay):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c119737",
   "metadata": {},
   "source": [
    "고객번호가 모두 겹침\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da730660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:48.502371Z",
     "start_time": "2022-01-13T16:09:48.280470Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_overlay = list(set(train2['prod_id'].unique()) & set(test2 ['prod_id'].unique()))\n",
    "te_overlay = list(set(test2 ['prod_id'].unique()) & set(train2['prod_id'].unique()))\n",
    "\n",
    "tr_nunique  = train2['prod_id'].nunique()\n",
    "te_nunique  = test2 ['prod_id'].nunique()\n",
    "\n",
    "print(f'> Train - Total : {tr_nunique:,}, Overlay : {len(tr_overlay):,}, Else : {tr_nunique-len(tr_overlay):,}')\n",
    "print(f'> Test  - Total : {te_nunique:,}, Overlay : {len(te_overlay):,}, Else : {te_nunique-len(te_overlay):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a2667",
   "metadata": {},
   "source": [
    "제품번호는 겹치는건 3,445개이고, train에만 있는게 186개, test에만 있는게 46개\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:48.850218Z",
     "start_time": "2022-01-13T16:09:48.505370Z"
    }
   },
   "outputs": [],
   "source": [
    "# user_id는 모두 7자리\n",
    "cnt([len(x) for x in train2['user_id']]), cnt([len(x) for x in test2['user_id']])\n",
    "\n",
    "# prod_id는 8자리, 9자리\n",
    "cnt([len(x) for x in train2['prod_id']]), cnt([len(x) for x in test2['prod_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27359104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:49.423963Z",
     "start_time": "2022-01-13T16:09:48.852215Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.array([x for x in train2['prod_id'] if len(x)==8])[:20], np.array([x for x in train2['prod_id'] if len(x)==9])[:20]\n",
    "\n",
    "a = pd.DataFrame({'x':sorted(cnt([x[3:6] for x in train2['prod_id'] if len(x)==9]).keys()),'y':'T'})\n",
    "b = pd.DataFrame({'x':sorted(cnt([x[3:6] for x in train2['prod_id'] if len(x)==8]).keys()),'y':'T'})\n",
    "outer = pd.merge(a,b,how='outer',on='x')\n",
    "# View(outer.sort_values('x'))\n",
    "\n",
    "# 8자리인 경우 P다음에 0을 넣어주기\n",
    "prod_id = [x if len(x)==9 else \n",
    "           x[:1] + '0' + x[1:]\n",
    "           for x in train2['prod_id']]\n",
    "# cnt([len(x) for x in prod_id])\n",
    "\n",
    "a = pd.DataFrame({'x':sorted(cnt([x[3:6] for x in prod_id]).keys()),'y':'T'})\n",
    "# View(a.sort_values('x'))\n",
    "\n",
    "a.iloc[np.where(a.index == a.x.astype(int),False,True),:]\n",
    "# 맨끝에만 다르고 나머지는 순서가 같음\n",
    "\n",
    "# qgrid_widget = qgrid.show_grid(a,show_toolbar=True)\n",
    "# qgrid_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290434b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:49.948363Z",
     "start_time": "2022-01-13T16:09:49.424962Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_prod_id = [x if len(x) == 9 else\n",
    "              x[:1] + '0' + x[1:]\n",
    "              for x in train2['prod_id']]\n",
    "\n",
    "te_prod_id = [x if len(x) == 9 else\n",
    "              x[:1] + '0' + x[1:]\n",
    "              for x in test2['prod_id']]\n",
    "\n",
    "len(set(pd.unique(tr_prod_id)) & set(pd.unique(te_prod_id))), len(setdiff(pd.unique(tr_prod_id),pd.unique(te_prod_id))), len(setdiff(pd.unique(te_prod_id),pd.unique(tr_prod_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f7d6",
   "metadata": {},
   "source": [
    "수정해도 겹치지 않는 것이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5292e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c85bb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 고객별 구매 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae239eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:50.644430Z",
     "start_time": "2022-01-13T16:09:49.953346Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (1) 전체 구매자 수\n",
    "total_cnt = train2['user_id'].value_counts().count()\n",
    "print(f'{color.BLUE}> 전체 구매자 수 {color.END}: {total_cnt:,}명\\n')\n",
    "\n",
    "# (2) Q0 ~ Q4\n",
    "q0_q4 = train2['user_id'].value_counts().quantile([0,0.25,0.50,0.75,1]).astype(int).values\n",
    "q0_q4_text = '(' + ', '.join(q0_q4.astype(str)) + ')'\n",
    "print(f'{color.BLUE}> 고객별 구매 횟수 Q0-Q4 {color.END}: {q0_q4_text}')\n",
    "\n",
    "# (3) Upper Whisker\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(train2['user_id'].value_counts())[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "print(f'{color.BLUE}> 고객별 구매 횟수 Upper Whisker {color.END}: {whishi}\\n')\n",
    "\n",
    "stat = list(q0_q4)\n",
    "stat.append(whishi)\n",
    "stat.sort()\n",
    "\n",
    "cnt_x = train2['user_id'].value_counts()\n",
    "\n",
    "grp = ['0' if x<stat[1] else\n",
    "       '1' if x<stat[2] else\n",
    "       '2' if x<stat[3] else\n",
    "       '3' if x<stat[4] else\n",
    "       '4' if x<=stat[5] else\n",
    "       'nan' for x in cnt_x]\n",
    "\n",
    "# (4) 구간 별 구매자 수\n",
    "print(f'{color.BLUE}> 고객별 평균 구매 횟수(구간){color.END}')\n",
    "print(pd.Series(grp).value_counts().sort_index().rename(index={'0':'~Q1','1':'~Q2','2':'~Q3','3':'~Upper_Whisker','4':'~Q4'}))\n",
    "\n",
    "\n",
    "# (5) 구매횟수 boxplot\n",
    "def box_hist_plot(val,figsize,title='',title_size=20):\n",
    "    \n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, \n",
    "                                        sharex=True,\n",
    "                                        gridspec_kw={\"height_ratios\": (.15, .85)},\n",
    "                                        figsize=figsize)\n",
    "\n",
    "    sns.boxplot (val, ax=ax_box)\n",
    "    sns.distplot(val, ax=ax_hist)\n",
    "\n",
    "    ax_box.set(yticks=[])\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.despine(ax=ax_box, left=True)\n",
    "    \n",
    "    plt.axvline(x=stats['whishi'], color='r', linestyle=':')\n",
    "    \n",
    "    ax_box.set_xlabel('')\n",
    "    ax_box.title.set_text(title)\n",
    "    ax_box.title.set_size(title_size)\n",
    "    ax_box.title.set_color('aquamarine')\n",
    "    \n",
    "\n",
    "box_hist_plot(train2['user_id'].value_counts(),\n",
    "              figsize=(15*0.6,7*0.7),\n",
    "              title='고객별 구매 횟수\\n',\n",
    "              title_size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13d0f",
   "metadata": {},
   "source": [
    "전체 구매자 수는 5,891명, 최소 6번, 최대 1,026번 구매함 <br>\n",
    "한명의 사람이 많이 구매하는 경우가 있음 <br>\n",
    "구매 횟수는 포아송분포를 따르는 것으로 보임 <br></br>\n",
    "\n",
    "파생변수 : 구매 횟수에 대한 그룹 생성 (~Min / ~Q1 / ~Q2 / ~Q3 / ~Whishi / ~Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e179d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 고객별 구매 금액의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e677c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:50.834332Z",
     "start_time": "2022-01-13T16:09:50.646415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (1) 전체 평균 구매금액\n",
    "total_mean = train2['target'].mean()\n",
    "print(f'{color.BLUE}> 전체 평균 구매 금액 {color.END}: {total_mean:.1f}달러\\n')\n",
    "\n",
    "# (2) Q0-Q4\n",
    "sales_mean = train2.groupby(['user_id'])['target'].mean().reset_index()\n",
    "\n",
    "sales_mean_quantile = sales_mean['target'].quantile([0,0.25,0.5,0.75,1]).values.round(0).astype(int)\n",
    "sales_mean_quantile_text = ', '.join(sales_mean_quantile.astype(str))\n",
    "print(f'{color.BLUE}> 고객별 평균 구매 금액 Q0-Q4 {color.END}: ({sales_mean_quantile_text})달러')\n",
    "\n",
    "# (3) upper whisker보다 많이 구매한 사람\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(sales_mean['target'])[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "print(f'{color.BLUE}> 고객별 평균 구매 금액 Upper Whisker {color.END}: {whishi:.1f}달러\\n')\n",
    "\n",
    "stat = list(sales_mean_quantile)\n",
    "stat.append(whishi)\n",
    "stat.sort()\n",
    "\n",
    "grp = ['0' if x<stat[1] else\n",
    "       '1' if x<stat[2] else\n",
    "       '2' if x<stat[3] else\n",
    "       '3' if x<stat[4] else\n",
    "       '4' if x<=stat[5] else\n",
    "       'nan' for x in sales_mean['target']]\n",
    "\n",
    "# (4) 구간 별 구매자 수\n",
    "print(f'{color.BLUE}> 고객별 평균 구매 횟수(구간){color.END}')\n",
    "print(pd.Series(grp).value_counts().sort_index().rename(index={'0':'~Q1','1':'~Q2','2':'~Q3','3':'~Upper_Whisker','4':'~Q4'}),'\\n')\n",
    "\n",
    "# (5) 평균 구매금액 boxplot\n",
    "fig = plt.figure(figsize=(15*0.6,7*0.7))\n",
    "p = sns.boxplot(y=sales_mean['target'])\n",
    "p.set_title('고객별 평균 구매 금액\\n',fontsize=25,color='aquamarine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907eea8a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 고객별 구매 금액의 Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411361c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:51.742965Z",
     "start_time": "2022-01-13T16:09:50.835337Z"
    }
   },
   "outputs": [],
   "source": [
    "# (1) Q0 ~ Q4\n",
    "df = train2.copy()\n",
    "\n",
    "quantile_df = df.groupby(['user_id'])['target'].quantile(q=[0,0.25,0.50,0.75,1]).reset_index()\n",
    "quantile_df['level_1'] = 'sales_by_user_q_' + (quantile_df['level_1']*100).astype(int).astype(str)\n",
    "\n",
    "quantile_df = quantile_df.pivot_table(index='user_id',columns='level_1',values='target',fill_value=0,aggfunc=sum).reset_index()\n",
    "quantile_df.rename_axis(None,axis=1)\n",
    "\n",
    "# (2) mean\n",
    "\n",
    "mean_df = df.groupby(['user_id'])['target'].mean().reset_index()\n",
    "mean_df.rename(columns={'target':'sales_by_user_mean'},inplace=True)\n",
    "mean_df\n",
    "\n",
    "col = ['user_id']+['sales_by_user_' + x for x in ['q_0','q_25','q_50','mean','q_75','q_100']]\n",
    "summary_df = pd.merge(quantile_df, mean_df, how='left', on='user_id')[col]\n",
    "\n",
    "print(f'{color.BLUE}> 고객별 구매 금액의 Quantile {color.END}')\n",
    "display(summary_df.head(5))\n",
    "\n",
    "print(f'{color.BLUE}> 고객별 구매 금액 quantile의 median {color.END}\\n')\n",
    "print(summary_df.drop(['user_id'],axis=1).apply(lambda x:x.median()).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa75cbb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### prod_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc188587",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 제품별 구매 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420ce38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:52.345526Z",
     "start_time": "2022-01-13T16:09:51.743957Z"
    }
   },
   "outputs": [],
   "source": [
    "# (1) 전체 구매자 수\n",
    "total_cnt = train2['prod_id'].value_counts().count()\n",
    "print(f'{color.BLUE}> 전체 제품 개수 {color.END}: {total_cnt:,}개\\n')\n",
    "\n",
    "# (2) Q0 ~ Q4\n",
    "q0_q4 = train2['prod_id'].value_counts().quantile([0,0.25,0.50,0.75,1]).astype(int).values\n",
    "q0_q4_text = '(' + ', '.join(q0_q4.astype(str)) + ')'\n",
    "print(f'{color.BLUE}> 제품별 구매 횟수 Q0-Q4 {color.END}: {q0_q4_text}')\n",
    "\n",
    "# (3) Upper Whisker\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(train2['prod_id'].value_counts())[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "print(f'{color.BLUE}> 제품별 구매 횟수 Upper Whisker {color.END}: {whishi}\\n')\n",
    "\n",
    "stat = list(q0_q4)\n",
    "stat.append(whishi)\n",
    "stat.sort()\n",
    "\n",
    "cnt_x = train2['prod_id'].value_counts()\n",
    "\n",
    "grp = ['0' if x<stat[1] else\n",
    "       '1' if x<stat[2] else\n",
    "       '2' if x<stat[3] else\n",
    "       '3' if x<stat[4] else\n",
    "       '4' if x<=stat[5] else\n",
    "       'nan' for x in cnt_x]\n",
    "\n",
    "# (4) 구간 별 구매자 수\n",
    "print(f'{color.BLUE}> 제품별 구매 횟수(구간){color.END}')\n",
    "print(pd.Series(grp).value_counts().sort_index().rename(index={'0':'~Q1','1':'~Q2','2':'~Q3','3':'~Upper_Whisker','4':'~Q4'}))\n",
    "\n",
    "\n",
    "# (5) 구매횟수 boxplot\n",
    "def box_hist_plot(val,figsize,title='',title_size=20):\n",
    "    \n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, \n",
    "                                        sharex=True,\n",
    "                                        gridspec_kw={\"height_ratios\": (.15, .85)},\n",
    "                                        figsize=figsize)\n",
    "\n",
    "    sns.boxplot (val, ax=ax_box)\n",
    "    sns.distplot(val, ax=ax_hist)\n",
    "\n",
    "    ax_box.set(yticks=[])\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.despine(ax=ax_box, left=True)\n",
    "    \n",
    "    plt.axvline(x=stats['whishi'], color='r', linestyle=':')\n",
    "    \n",
    "    ax_box.set_xlabel('')\n",
    "    ax_box.title.set_text(title)\n",
    "    ax_box.title.set_size(title_size)\n",
    "    ax_box.title.set_color('aquamarine')\n",
    "    \n",
    "\n",
    "box_hist_plot(train2['prod_id'].value_counts(),\n",
    "              figsize=(15*0.6,7*0.7),\n",
    "              title='제품별 구매 횟수\\n',\n",
    "              title_size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62248a98",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 제품별 구매 금액의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c81cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:52.537443Z",
     "start_time": "2022-01-13T16:09:52.346527Z"
    }
   },
   "outputs": [],
   "source": [
    "# (1) 전체 평균 구매금액\n",
    "total_mean = train2['target'].mean()\n",
    "print(f'{color.BLUE}> 전체 평균 구매 금액 {color.END}: {total_mean:.1f}달러\\n')\n",
    "\n",
    "# (2) Q0-Q4\n",
    "sales_mean = train2.groupby(['prod_id'])['target'].mean().reset_index()\n",
    "\n",
    "sales_mean_quantile = sales_mean['target'].quantile([0,0.25,0.5,0.75,1]).values.round(0).astype(int)\n",
    "sales_mean_quantile_text = ', '.join(sales_mean_quantile.astype(str))\n",
    "print(f'{color.BLUE}> 제품별 평균 구매 금액 Q0-Q4 {color.END}: ({sales_mean_quantile_text})달러')\n",
    "\n",
    "# (3) upper whisker보다 많이 구매한 사람\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(sales_mean['target'])[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "print(f'{color.BLUE}> 제품별 평균 구매 금액 Upper Whisker {color.END}: {whishi:.1f}달러\\n')\n",
    "\n",
    "stat = list(sales_mean_quantile)\n",
    "stat.append(whishi)\n",
    "stat.sort()\n",
    "\n",
    "grp = ['0' if x<stat[1] else\n",
    "       '1' if x<stat[2] else\n",
    "       '2' if x<stat[3] else\n",
    "       '3' if x<stat[4] else\n",
    "       '4' if x<=stat[5] else\n",
    "       'nan' for x in sales_mean['target']]\n",
    "\n",
    "# (4) 구간 별 구매자 수\n",
    "print(f'{color.BLUE}> 제품별 평균 구매 횟수(구간){color.END}')\n",
    "print(pd.Series(grp).value_counts().sort_index().rename(index={'0':'~Q1','1':'~Q2','2':'~Q3','3':'~Upper_Whisker','4':'~Q4'}),'\\n')\n",
    "\n",
    "# (5) 평균 구매금액 boxplot\n",
    "fig = plt.figure(figsize=(15*0.6,7*0.7))\n",
    "p = sns.boxplot(y=sales_mean['target'])\n",
    "p.set_title('제품별 평균 구매 금액\\n',fontsize=25,color='aquamarine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f60f93",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 제품별 구매 금액의 Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6a1b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:54.176241Z",
     "start_time": "2022-01-13T16:09:52.538443Z"
    }
   },
   "outputs": [],
   "source": [
    "# (1) Q0 ~ Q4\n",
    "df = train2.copy()\n",
    "\n",
    "quantile_df = df.groupby(['prod_id'])['target'].quantile(q=[0,0.25,0.50,0.75,1]).reset_index()\n",
    "quantile_df['level_1'] = 'sales_by_prod_q_' + (quantile_df['level_1']*100).astype(int).astype(str)\n",
    "\n",
    "quantile_df = quantile_df.pivot_table(index='prod_id',columns='level_1',values='target',fill_value=0,aggfunc=sum).reset_index()\n",
    "quantile_df.rename_axis(None,axis=1)\n",
    "\n",
    "# (2) mean\n",
    "\n",
    "mean_df = df.groupby(['prod_id'])['target'].mean().reset_index()\n",
    "mean_df.rename(columns={'target':'sales_by_prod_mean'},inplace=True)\n",
    "mean_df\n",
    "\n",
    "col = ['prod_id']+['sales_by_prod_' + x for x in ['q_0','q_25','q_50','mean','q_75','q_100']]\n",
    "summary_df = pd.merge(quantile_df, mean_df, how='left', on='prod_id')[col]\n",
    "\n",
    "print(f'{color.BLUE}> 고객별 구매 금액의 Quantile {color.END}')\n",
    "display(summary_df.head(5))\n",
    "\n",
    "print(f'{color.BLUE}> 고객별 구매 금액 quantile의 median {color.END}\\n')\n",
    "print(summary_df.drop(['prod_id'],axis=1).apply(lambda x:x.median()).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5214e7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 제품별 구매 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867acdce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:54.573067Z",
     "start_time": "2022-01-13T16:09:54.178241Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df['prod_ratio'] = summary_df['sales_by_prod_q_100'] / summary_df['sales_by_prod_q_0']\n",
    "\n",
    "df = train2.copy()\n",
    "df = pd.merge(df,summary_df,how='left',on='prod_id')\n",
    "\n",
    "minmax_ratio = minmax(df['prod_ratio'])\n",
    "print(f'{color.BLUE}> 제품별 구매금액의 min,max ratio의 최소/최대값 :\\n{color.END} {minmax_ratio}\\n')\n",
    "\n",
    "lower_2 = cnt(cnt(df['prod_id'][df['prod_ratio']<2]).values())\n",
    "print(f'{color.BLUE}> 제품별 구매금액의 min,max ratio가 2보다 작은 prod_id의 건수의 Freq. :\\n{color.END} {lower_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbb538",
   "metadata": {},
   "source": [
    "제품별 구매금액이 5배까지 차이가 남 <br>\n",
    "→ 제품별 구매개수는 알 수 없음 <br>\n",
    "→ ratio(max/min)를 구매개수로 생각할 수 있음\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245b514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:58.360396Z",
     "start_time": "2022-01-13T16:09:54.574065Z"
    }
   },
   "outputs": [],
   "source": [
    "df['target'] = df['target']\n",
    "df['target_per_trade'] = df['target'] / df['prod_ratio']\n",
    "\n",
    "fig = plt.figure(figsize=(22,7))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "sns.violinplot(y=df['target'])\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "sns.violinplot(y=df['target_per_trade'])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d3895",
   "metadata": {},
   "source": [
    "분포가 mixed normal에서 log-normal으로 바뀜\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a10399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:58.663252Z",
     "start_time": "2022-01-13T16:09:58.362386Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=df[df['user_id']=='1000001']\n",
    "minmax(sub['prod_ratio'])\n",
    "sub[['user_id','prod_id','prod_ratio']]\n",
    "\n",
    "a = pd.DataFrame({\n",
    "    'min' : df.groupby(['user_id'])['prod_ratio'].min(),\n",
    "    'max' : df.groupby(['user_id'])['prod_ratio'].max(),\n",
    "    'max/min' : df.groupby(['user_id'])['prod_ratio'].max() / df.groupby(['user_id'])['prod_ratio'].min(),\n",
    "    'median' : df.groupby(['user_id'])['prod_ratio'].median(),\n",
    "})\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b49295",
   "metadata": {},
   "source": [
    "user_id는 동일하니까, test에 구매개수의 중앙값을 join으로 붙이기\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff8eda",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 유형별 변수 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e31a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:58.727225Z",
     "start_time": "2022-01-13T16:09:58.665252Z"
    }
   },
   "outputs": [],
   "source": [
    "# 숫자형변수, 문자형변수\n",
    "num_vari  = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [int,float]]\n",
    "char_vari = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [str      ]]\n",
    "\n",
    "num_df  = train2[num_vari ]\n",
    "char_df = train2[char_vari]\n",
    "\n",
    "print('전체 변수 :', len(train2.columns))\n",
    "print('숫자 변수 :', len(num_vari),  '\\t:', num_vari)\n",
    "print('문자 변수 :', len(char_vari), '\\t:', char_vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21bfe7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Characteristic Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed60241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:59.282147Z",
     "start_time": "2022-01-13T16:09:58.728224Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_unique = 5\n",
    "\n",
    "plot_char_vari = sorted(list(set(char_vari)-set(['id'])))\n",
    "\n",
    "max_char_len = max([len(var) for var in plot_char_vari])\n",
    "for xvar in plot_char_vari:\n",
    "    blank_var = ' '*(max_char_len-len(xvar))\n",
    "    \n",
    "    unique_val = sorted(train2[xvar].dropna().unique())\n",
    "    if len(unique_val)>=max_unique:\n",
    "        example = unique_val[:max_unique] + ['...']\n",
    "    else:\n",
    "        example = unique_val\n",
    "    \n",
    "    print(f'{xvar} {blank_var}: {example}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14505e5c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 문자형 변수 짧게 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a9b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:09:59.330124Z",
     "start_time": "2022-01-13T16:09:59.284145Z"
    }
   },
   "outputs": [],
   "source": [
    "def char_short_to_long(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "train3 = char_short_to_long(train2)\n",
    "test3  = char_short_to_long(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607aa0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 1-Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b688555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:04.264249Z",
     "start_time": "2022-01-13T16:09:59.331125Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_char_vari = setdiff(plot_char_vari,['user_id','prod_id'])\n",
    "\n",
    "fig = plt.figure(figsize=(15*1.2,7*4))\n",
    "ncol = 3\n",
    "nrow = np.ceil(len(plot_char_vari)/ncol)\n",
    "\n",
    "iter=0\n",
    "for var in sorted(plot_char_vari):\n",
    "    iter+=1\n",
    "    \n",
    "    # ordering\n",
    "    unique_x = train3[var].dropna().unique()\n",
    "    try:\n",
    "        order_x = [str(x) for x in sorted(unique_x.astype(int))]\n",
    "    except ValueError:\n",
    "        order_x = sorted(unique_x)\n",
    "        \n",
    "    fig.add_subplot(nrow,ncol,iter)\n",
    "    ax = sns.countplot(train3[var],\n",
    "                       order = order_x)\n",
    "    ax.set_xlabel(var, size=15)\n",
    "    ax.set_ylabel('Count',size=15)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        count_x = p.get_height()\n",
    "        sum_x   = sum([p.get_height() for p in ax.patches])\n",
    "        ratio_x = ( p.get_height() / sum_x )*100\n",
    "        ann_x   = f'{count_x:,}\\n({ratio_x:.1f}%)'\n",
    "        ax.annotate(ann_x, (p.get_x() + 0.0, p.get_height() +50), size = 12, color='white')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cb960",
   "metadata": {},
   "source": [
    "|변수명 | 정보 |\n",
    "|:---:|:---|\n",
    "| age | 미성년자의 비중이 적음, 26-35세의 비중이 제일 큼, 18-45세의 비중이 78%로 매우 높음 |\n",
    "| city_cat | B의 비중이 제일 높음. B>C>A의 순서 |\n",
    "| gender | 남자고객의 비중이 75%로 매우 높음 |\n",
    "| marital | 기혼이 41%, 미혼이 59% |\n",
    "| occupation | 빈도가 높은 0,4,7 등의 경우는 무직,학생,주부,회사원과 같은 일반적인 카테고리일 가능성이 높음 <br> 이런 카테고리는 일반 구매자일 가능성이 높고, count가 낮은 카테고리들은 법인일 가능성이 있음 <br> -> 카테고리 별 구매금액 확인 및 clustering을 해보는것도 좋을 것 같음 |\n",
    "| prod_cat_1 | 1,5,8번의 경우에는 잘팔리는 것들(major), 나머지는 minor한 것들 <br> prod_cat_2,3에 비해서 분명하게 나뉘는 것들이 많음 |\n",
    "| prod_cat_2 | ... |\n",
    "| prod_cat_3 | ... |\n",
    "| stay_year | 현재도시에 체류한 기간이 1년인게 1/3로 가장 많고, 나머지는 비슷한 수준 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933807",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 2-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6db50d",
   "metadata": {},
   "source": [
    "문자열 조합의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ec6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:04.279243Z",
     "start_time": "2022-01-13T16:10:04.267240Z"
    }
   },
   "outputs": [],
   "source": [
    "n_comb = math.comb(len(plot_char_vari),2)\n",
    "ncol = math.ceil(np.sqrt(n_comb))\n",
    "nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "print(f'n_comb : {n_comb}, (nrow,ncol) : ({nrow},{ncol})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66d433",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "그룹이 많은 prod_cat은 빼고 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66bf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:06.772040Z",
     "start_time": "2022-01-13T16:10:04.281243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_var = setdiff(plot_char_vari,['prod_cat_1','prod_cat_2','prod_cat_3'])\n",
    "n_comb = math.comb(len(check_var),2)\n",
    "\n",
    "ncol = math.ceil(np.sqrt(n_comb))\n",
    "nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "comb = list(itertools.product([True, False], repeat=len(check_var)))\n",
    "comb = [x for x in comb if sum(x)==2]\n",
    "\n",
    "for iter,c in enumerate(comb):\n",
    "    xvar,yvar = np.array(check_var)[[c]]\n",
    "    print(color.BOLD+color.BLUE+f'> ({iter+1}/{len(comb)}) ({xvar},{yvar})'+color.END)\n",
    "    \n",
    "    View(pd.crosstab(train3[xvar].fillna('nan'),\n",
    "                     train3[yvar].fillna('nan')))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60a43e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "prod_cat_2,3의 missing을 채우기 위해서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2e679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:08.219402Z",
     "start_time": "2022-01-13T16:10:06.773038Z"
    }
   },
   "outputs": [],
   "source": [
    "except_prod_1 = setdiff(plot_char_vari,['prod_cat_1'])\n",
    "\n",
    "for iter,var in enumerate(except_prod_1):\n",
    "    xvar = var\n",
    "    yvar = 'prod_cat_1'\n",
    "    print(color.BOLD+color.BLUE+f'> ({iter+1}/{len(except_prod_1)}) ({xvar})'+color.END)\n",
    "    \n",
    "    View(pd.crosstab(train3[xvar].fillna('nan'),\n",
    "                     train3[yvar].fillna('nan')))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5728c7e",
   "metadata": {},
   "source": [
    "별다른 큰 패턴이 보이지 않음.. → ML로 결측값을 채워넣거나, 삭제\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc055e9",
   "metadata": {},
   "source": [
    "모든 문자형변수 조합의 2D plot <br>\n",
    "각 변수의 그룹의 개수가 많아서 지저분해보임 → 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a439a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:08.234398Z",
     "start_time": "2022-01-13T16:10:08.221402Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     check_var = setdiff(plot_char_vari,['prod_cat_1','prod_cat_2','prod_cat_3'])\n",
    "    \n",
    "#     n_comb = math.comb(len(check_var),2)\n",
    "\n",
    "#     ncol = math.ceil(np.sqrt(n_comb))\n",
    "#     nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "#     comb = list(itertools.product([True, False], repeat=len(check_var)))\n",
    "#     comb = [x for x in comb if sum(x)==2]\n",
    "\n",
    "#     fig  = plt.figure(figsize=(15*3,7*4))\n",
    "\n",
    "#     tick_size      = 40\n",
    "#     label_size     = 40\n",
    "#     freq_font_size = 30\n",
    "#     title_size     = 50\n",
    "#     legend_size    = 30\n",
    "\n",
    "#     p1_iter = 0\n",
    "#     change_row = 0\n",
    "#     for iter,c in enumerate(comb):\n",
    "\n",
    "#         if change_row==(ncol):\n",
    "#             change_row  = 1 # go back\n",
    "#             p1_iter    += ncol+1\n",
    "#         else:\n",
    "#             change_row += 1\n",
    "#             p1_iter    += 1\n",
    "\n",
    "#         p2_iter = p1_iter+ncol\n",
    "\n",
    "#         # # check\n",
    "#         # print(f'change_row : {change_row}, p1_iter : {p1_iter}, p2_iter : {p2_iter}')\n",
    "\n",
    "#         xvar,yvar = np.array(check_var)[[c]]\n",
    "\n",
    "#         # (1) crosstab plot\n",
    "#         fig.add_subplot(2*nrow, ncol, p1_iter)\n",
    "#         ct = pd.crosstab(train3[xvar],train3[yvar]).sort_index(level=0, ascending=True).sort_index(level=1, ascending=True)\n",
    "#         p0 = sns.heatmap(ct.T, annot=True, fmt='.0f', cbar=False, annot_kws={\"size\": freq_font_size}, cmap='YlGnBu')\n",
    "#         p0.tick_params(axis = 'y', labelsize=tick_size)\n",
    "#         p0.tick_params(axis = 'x', labelsize=0)\n",
    "#         p0.set_title('-'*27 + f'{iter+1}' + '-'*27,fontsize=title_size)\n",
    "#         p0.set_xlabel('')\n",
    "#         p0.set_ylabel(f'{yvar}',fontsize=label_size)\n",
    "\n",
    "#         # (2) count plot\n",
    "#         fig.add_subplot(2*nrow, ncol, p2_iter)\n",
    "#         p1 = sns.countplot(train3[xvar], hue = train3[yvar], dodge=True, palette = 'Set1',\n",
    "#                            order     = sorted(train3[xvar].value_counts().index),\n",
    "#                            hue_order = sorted(train3[yvar].value_counts().index))\n",
    "#         p1.tick_params(labelsize=tick_size)\n",
    "#         p1.set_xlabel(f'{xvar}',fontsize=label_size)\n",
    "#         p1.set_ylabel('Count',fontsize=label_size)\n",
    "#         p1.get_legend().remove()\n",
    "#         #plt.legend(bbox_to_anchor=(1.02, 1), \n",
    "#         #           loc=2, borderaxespad=0, fontsize=legend_size)\n",
    "\n",
    "# #         # show freq. into plot\n",
    "# #         values=train3[xvar].value_counts().values\n",
    "# #         for j, g1 in enumerate(p1.patches):\n",
    "# #             p1.annotate(f'\\n{g1.get_height()}', (g1.get_x()+0.13, g1.get_height()+1.1), ha='center', va='top', color='white', size=freq_font_size)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d449a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101543a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:08.282390Z",
     "start_time": "2022-01-13T16:10:08.235395Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(train3.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece2e73",
   "metadata": {},
   "source": [
    "숫자형변수는 target 밖에 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d64b4d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 1-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6124b",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b1115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:09.714766Z",
     "start_time": "2022-01-13T16:10:08.285374Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15*0.5,7*0.7))\n",
    "\n",
    "# len(num_vari) : 6\n",
    "\n",
    "nrow = 1\n",
    "ncol = 1\n",
    "\n",
    "if PLOT:\n",
    "    \n",
    "    for iter,var in enumerate(num_vari,1):\n",
    "        fig.add_subplot(nrow,ncol,iter)\n",
    "        sns.violinplot(y=train3[var])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764bd50",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### hist + kde plot (hue=target)\n",
    "<br></br>\n",
    "각 숫자형변수별 분포 확인 & target에 따른 분포확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d559f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:15.287952Z",
     "start_time": "2022-01-13T16:10:09.716740Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    # No hue\n",
    "    print(color.BOLD + color.BLUE + '> No Group' + color.END)\n",
    "    density_plot(train3,\n",
    "                 vars = sorted(set(num_vari) - set(['id'])),\n",
    "                 binwidth_adj_ratio = 0.8,\n",
    "                 figsize = (15*0.5,7*0.5))\n",
    "    plt.show()\n",
    "\n",
    "#     # hue : target\n",
    "#     print(color.BOLD + color.BLUE + '> Group by Target' + color.END)\n",
    "#     density_plot(train,\n",
    "#                  vars = set(num_vari) - set(['id']),\n",
    "#                  hue = 'target',\n",
    "#                  binwidth_adj_ratio = 0.8)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de2f2d",
   "metadata": {},
   "source": [
    "몇가지 정규분포가 섞여 있는 것으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f9eae",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 2-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d4424",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b73b39",
   "metadata": {},
   "source": [
    "숫자형 변수가 target 하나라서 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51584c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:15.303954Z",
     "start_time": "2022-01-13T16:10:15.288951Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     pairplot_df = num_df.copy()\n",
    "\n",
    "#     sns.pairplot(pairplot_df, corner=True)#, hue = 'target')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba16495",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### Clustermap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fb979",
   "metadata": {},
   "source": [
    "숫자형 변수가 target 하나라서 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ac1c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:15.319938Z",
     "start_time": "2022-01-13T16:10:15.304960Z"
    }
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     pairplot_df = num_df.copy()\n",
    "    \n",
    "#     sns.clustermap(pairplot_df.corr(), annot=True, cmap = 'RdYlBu_r')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0b992",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable * Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a0edb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ca415",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "데이터 크기가 커서 너무 오래걸림 → 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea829f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:15.335940Z",
     "start_time": "2022-01-13T16:10:15.320937Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     for _iter,_col in enumerate(sorted(plot_char_vari)):\n",
    "#         plot_num(grp_var = _col, num_vari = num_vari, #hue_var='target',\n",
    "#                  df = train3,\n",
    "#                  title_text = str(_iter+1) + '. ' + _col,\n",
    "#                  figsize_1=(5,4),\n",
    "#                  figsize_2=(15*0.8,7*0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427be31c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "boxplot만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55412941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:28.390218Z",
     "start_time": "2022-01-13T16:10:15.336940Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yvar = 'target'\n",
    "fig = plt.figure(figsize=(22,15))\n",
    "\n",
    "nrow = np.ceil(np.sqrt(len(plot_char_vari)))\n",
    "ncol = np.ceil(len(plot_char_vari)/nrow)\n",
    "\n",
    "for iter,xvar in enumerate(sorted(plot_char_vari)):\n",
    "\n",
    "    fig.add_subplot(nrow,ncol,iter+1)\n",
    "    sns.violinplot(x=train3[xvar],y=train3[yvar])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f730b45e",
   "metadata": {},
   "source": [
    "|변수명|변수정보|\n",
    "|---|---|\n",
    "| stay_year, age, marital, occupation | 구매금액차이가 큰 차이가 없음 |\n",
    "| city_cat, gender | 매우 조금 차이가 있음 |\n",
    "| prod_cat | 큰 차이가 있음 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7b37b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24598a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:28.405222Z",
     "start_time": "2022-01-13T16:10:28.392218Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     for _iter,_col in enumerate(sorted(plot_char_vari)):\n",
    "#         plot_num(grp_var = _col, num_vari = list(set(num_vari)-set(['body_mass'])), #hue_var='target',\n",
    "#                  df = test3,\n",
    "#                  title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52a354",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Grouping**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a24b6",
   "metadata": {},
   "source": [
    "prod_cat_1,2,3의 경우 카테고리가 너무 많음. 이를 그룹핑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b36d3e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## prod_cat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fd584",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### prod_cat_1 그룹별 median 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d58395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:28.532972Z",
     "start_time": "2022-01-13T16:10:28.407211Z"
    }
   },
   "outputs": [],
   "source": [
    "q = train3['target'].quantile([0.25,0.50,0.75])\n",
    "    \n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(train3['target'])[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "\n",
    "stat = list(q.values) + [whishi]\n",
    "stat.sort()\n",
    "\n",
    "prod_cat_1_grp = [np.nan    if med is np.nan else\n",
    "                  '~q1'     if med<stat[0] else\n",
    "                  '~q2'     if med<stat[1] else\n",
    "                  '~q3'     if med<stat[2] else\n",
    "                  '~whishi' if med<stat[3] else\n",
    "                  '~q4'     for med in train3.groupby(['prod_cat_1'])['target'].median()]\n",
    "\n",
    "grp_df = pd.DataFrame({\n",
    "    'grp' : prod_cat_1_grp,\n",
    "    'prod_cat_1' : train3['prod_cat_1'].unique()\n",
    "}).sort_values(['grp','prod_cat_1'])\n",
    "\n",
    "max_char_len = max([len(x) for x in grp_df['grp'].unique()])\n",
    "for uniq in grp_df['grp'].unique():\n",
    "    n_blank = ' '*(max_char_len-len(uniq))\n",
    "    \n",
    "    value = grp_df['prod_cat_1'][grp_df['grp']==uniq]\n",
    "    value = ', '.join(value)\n",
    "    \n",
    "    print(f'{uniq}{n_blank} : ({value})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434dfdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:29.155714Z",
     "start_time": "2022-01-13T16:10:28.534971Z"
    }
   },
   "outputs": [],
   "source": [
    "grp_fn = pd.merge(train3,grp_df,how='left',on='prod_cat_1')\n",
    "pd.crosstab(grp_fn['prod_cat_1'],grp_fn['grp'])\n",
    "\n",
    "sns.boxplot(grp_fn['grp'],grp_fn['target'],\n",
    "            order = sorted(grp_fn['grp'].unique()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be539671",
   "metadata": {},
   "source": [
    "~q2를 제외하고는 괜찮은 것 같은데, ~q2가 너무 많은 범위를 차지하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c7689",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 제품별 구매 개수를 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a7972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:29.602100Z",
     "start_time": "2022-01-13T16:10:29.156698Z"
    }
   },
   "outputs": [],
   "source": [
    "df = train3.copy()\n",
    "df = pd.merge(df,summary_df,how='left',on='prod_id')\n",
    "\n",
    "df['target_per_trade'] = df['target'] / df['prod_ratio']\n",
    "\n",
    "\n",
    "q = df['target_per_trade'].quantile([0.25,0.50,0.75])\n",
    "\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "stats = boxplot_stats(df['target_per_trade'])[0]\n",
    "whislo, whishi = stats['whislo'], stats['whishi']\n",
    "\n",
    "stat = list(q.values) + [whishi]\n",
    "stat.sort()\n",
    "\n",
    "\n",
    "prod_cat_1_grp = [np.nan    if med is np.nan else\n",
    "                  '~q1'     if med<stat[0] else\n",
    "                  '~q2'     if med<stat[1] else\n",
    "                  '~q3'     if med<stat[2] else\n",
    "                  '~whishi' if med<stat[3] else\n",
    "                  '~q4'     for med in df.groupby(['prod_cat_1'])['target_per_trade'].median()]\n",
    "\n",
    "grp_df = pd.DataFrame({\n",
    "    'grp' : prod_cat_1_grp,\n",
    "    'prod_cat_1' : df['prod_cat_1'].unique()\n",
    "}).sort_values(['grp','prod_cat_1'])\n",
    "\n",
    "max_char_len = max([len(x) for x in grp_df['grp'].unique()])\n",
    "for uniq in grp_df['grp'].unique():\n",
    "    n_blank = ' '*(max_char_len-len(uniq))\n",
    "    \n",
    "    value = grp_df['prod_cat_1'][grp_df['grp']==uniq]\n",
    "    value = ', '.join(value)\n",
    "    \n",
    "    print(f'{uniq}{n_blank} : ({value})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92255d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:30.303696Z",
     "start_time": "2022-01-13T16:10:29.603100Z"
    }
   },
   "outputs": [],
   "source": [
    "grp_fn = pd.merge(df,grp_df,how='left',on='prod_cat_1')\n",
    "\n",
    "pd.crosstab(grp_fn['prod_cat_1'],grp_fn['grp'])\n",
    "\n",
    "sns.boxplot(grp_fn['grp'],grp_fn['target_per_trade'],\n",
    "            order = sorted(grp_fn['grp'].unique()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1ee12",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### EDA에 근거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660c1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:30.954290Z",
     "start_time": "2022-01-13T16:10:30.304681Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(grp_fn['prod_cat_1'],grp_fn['target'],\n",
    "            order = grp_fn.groupby('prod_cat_1')['target'].median().sort_values().index)\n",
    "\n",
    "# [str(x) for x in sorted(train4['prod_cat_1'].unique().astype(int))]\n",
    "\n",
    "plt.axvline(x=5.5 ,color='red',linestyle='--')\n",
    "plt.axvline(x=11.5,color='red',linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aaca54",
   "metadata": {},
   "source": [
    "\\> 저가 순서대로 <br>\n",
    "(1) 19,20,13,12,4,18 <br>\n",
    "(2) 11,5,8,17,3 <br>\n",
    "(3) 2,9,14,1,16,6,15,7,10 <br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abe6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:31.637878Z",
     "start_time": "2022-01-13T16:10:30.955279Z"
    }
   },
   "outputs": [],
   "source": [
    "grp_fn['grp_new'] = ['1' if p in (19,20,13,12,4,18)      else\n",
    "                     '2' if p in (11,5,8,17,3)           else\n",
    "                     '3' if p in (2,9,14,1,16,6,15,7,10) else\n",
    "                     'error' for p in grp_fn['prod_cat_1'].astype(int)]\n",
    "\n",
    "sns.boxplot(x=grp_fn['grp_new'],y=grp_fn['target'],\n",
    "            order = [str(x) for x in sorted(grp_fn['grp_new'].astype(int).unique())])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a17c64",
   "metadata": {},
   "source": [
    "가격순서대로 적당히 나뉘는 느낌임\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bf0d2",
   "metadata": {},
   "source": [
    "구매개수 고려해서 확인 → outlier가 많아서 의미없어보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e7283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:32.303382Z",
     "start_time": "2022-01-13T16:10:31.639877Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(grp_fn['prod_cat_1'],grp_fn['target_per_trade'],\n",
    "            order = grp_fn.groupby('prod_cat_1')['target_per_trade'].median().sort_values().index)\n",
    "\n",
    "# [str(x) for x in sorted(train4['prod_cat_1'].unique().astype(int))]\n",
    "\n",
    "plt.axvline(x=5.5 ,color='red',linestyle='--')\n",
    "plt.axvline(x=11.5,color='red',linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a2a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:32.703196Z",
     "start_time": "2022-01-13T16:10:32.304373Z"
    }
   },
   "outputs": [],
   "source": [
    "grp_1 = ['1' if p in (19,20,13,12,4,18)      else\n",
    "         '2' if p in (11,5,8,17,3)           else\n",
    "         '3' if p in (2,9,14,1,16,6,15,7,10) else\n",
    "         'error' for p in train3['prod_cat_1'].astype(int)]\n",
    "\n",
    "grp_2 = ['1' if p in (11,12,13,18,5,7,8)     else\n",
    "         '2' if p in (10,20)                 else\n",
    "         '3' if p in (15,16)                 else\n",
    "         '4' if p in (1,14,17,19,2,3,4,6,9)  else\n",
    "         'error' for p in train3['prod_cat_1'].astype(int)]\n",
    "\n",
    "# pd.crosstab(np.array(grp_1),np.array(grp_2))\n",
    "# pd.crosstab(np.array(grp_1),train3['prod_cat_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50956e3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## prod_cat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029527c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:10:34.280116Z",
     "start_time": "2022-01-13T16:10:32.704195Z"
    }
   },
   "outputs": [],
   "source": [
    "var_list = ['prod_cat_2','prod_cat_3']\n",
    "\n",
    "fig = plt.figure(figsize=(22,7))\n",
    "iter = 0\n",
    "for var in var_list:\n",
    "\n",
    "    iter += 1\n",
    "    fig.add_subplot(1,2,iter)\n",
    "    df = train3.copy()\n",
    "    df[var] = df[var].fillna('nan')\n",
    "\n",
    "    # df = df[df['prod_cat_1']=='2']\n",
    "    # print(df.shape)\n",
    "\n",
    "    sns.boxplot(x=df[var],\n",
    "                y=df['target'],\n",
    "                order=list(df.groupby(var)['target'].median().sort_values().index))\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22909ff2",
   "metadata": {},
   "source": [
    "구분은 있는데 나누기 힘듬 → nan을 0으로 넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a2757",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c07f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:20:58.574413Z",
     "start_time": "2022-01-13T16:20:58.544426Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(_train, _test):\n",
    "\n",
    "    from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "    tr_df = _train.copy()\n",
    "    te_df = _test .copy()\n",
    "\n",
    "    #------------------------------------------------------------#\n",
    "    # 1. prod_cat_2,3 : nan to 0\n",
    "    #------------------------------------------------------------#\n",
    "    tr_df['prod_cat_2'] = tr_df['prod_cat_2'].fillna('0')\n",
    "    tr_df['prod_cat_3'] = tr_df['prod_cat_3'].fillna('0')\n",
    "\n",
    "    te_df['prod_cat_2'] = te_df['prod_cat_2'].fillna('0')\n",
    "    te_df['prod_cat_3'] = te_df['prod_cat_3'].fillna('0')\n",
    "\n",
    "    #------------------------------------------------------------#\n",
    "    # 2. prod_id : 8자리인 것들 수정\n",
    "    #------------------------------------------------------------#\n",
    "    tr_df['prod_id'] = [x if len(x) == 9 else\n",
    "                        x[:1] + '0' + x[1:]\n",
    "                        for x in tr_df['prod_id']]\n",
    "    te_df['prod_id'] = [x if len(x) == 9 else\n",
    "                        x[:1] + '0' + x[1:]\n",
    "                        for x in te_df['prod_id']]\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 3. 고객별 구매건수 : user_id는 train,test 동일하므로 train에서 가져오기\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    tr_user_cnt_df = pd.DataFrame({\n",
    "        'user_id': tr_df['user_id'].value_counts().index,\n",
    "        'user_cnt': tr_df['user_id'].value_counts().values\n",
    "    })\n",
    "\n",
    "    tr_df = pd.merge(tr_df, tr_user_cnt_df, how='left', on='user_id')\n",
    "    te_df = pd.merge(te_df, tr_user_cnt_df, how='left', on='user_id')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 4. 제품별 구매건수 : prod_id는 안겹치는것도 있으니까,\n",
    "    #                      겹치는거는 가져오고, 안겹치는거는 test에 대해서 만듦\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # train의 prod_cnt 계산\n",
    "    tr_prod_cnt_df = pd.DataFrame({\n",
    "        'prod_id': tr_df['prod_id'].value_counts().index,\n",
    "        'prod_cnt': tr_df['prod_id'].value_counts().values\n",
    "    })\n",
    "\n",
    "    # test가 train에 겹치지 않는 부분 location 확인\n",
    "    tr_df = pd.merge(tr_df, tr_prod_cnt_df, how='left', on='prod_id')\n",
    "    te_df = pd.merge(te_df, tr_prod_cnt_df, how='left', on='prod_id')\n",
    "\n",
    "    # test가 train에 겹치는 부분은 train의 cnt를 가져오고,\n",
    "    #                겹치지 않는 부분은 test의 cnt를 가져옴\n",
    "    prod_cnt_null_loc = te_df['prod_cnt'].isnull()\n",
    "    te_prod_cnt_df = pd.DataFrame({\n",
    "        'prod_id': te_df['prod_id'][prod_cnt_null_loc].value_counts().index,\n",
    "        'prod_cnt': te_df['prod_id'][prod_cnt_null_loc].value_counts().values\n",
    "    })\n",
    "\n",
    "    tr_te_prod_cnt_df = pd.concat(\n",
    "        [tr_prod_cnt_df, te_prod_cnt_df], axis=0).reset_index(drop=True)\n",
    "    tr_te_prod_cnt_df\n",
    "\n",
    "    te_df = pd.merge(te_df.drop(['prod_cnt'], axis=1),\n",
    "                     tr_te_prod_cnt_df, how='left', on='prod_id')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 5. 고객별 구매건수 그룹 : user_id는 train,test 동일하므로 train에서 가져오기\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    stats = boxplot_stats(tr_df['user_id'].value_counts())[0]\n",
    "    whislo, whishi = stats['whislo'], stats['whishi']\n",
    "\n",
    "    stat = list(tr_df['user_id'].value_counts().quantile(\n",
    "        [0, 0.25, 0.50, 0.75, 1]).astype(int).values)\n",
    "    stat.append(whishi)\n",
    "    stat.sort()\n",
    "\n",
    "    grp = ['0' if x < stat[1] else\n",
    "           '1' if x < stat[2] else\n",
    "           '2' if x < stat[3] else\n",
    "           '3' if x < stat[4] else\n",
    "           '4' if x <= stat[5] else\n",
    "           'nan' for x in tr_df['user_id'].value_counts()]\n",
    "\n",
    "    grp_df = pd.DataFrame({\n",
    "        'user_id': tr_df['user_id'].value_counts().index,\n",
    "        'user_cnt_grp': grp\n",
    "    })\n",
    "\n",
    "    tr_df = pd.merge(tr_df, grp_df, how='left', on='user_id')\n",
    "    te_df = pd.merge(te_df, grp_df, how='left', on='user_id')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 6. 제품별 구매건수 그룹 : prod_id는 안겹치는것도 있으니까,\n",
    "    #                          겹치는거는 가져오고, 안겹치는거는 test에 대해서 만듦\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # train의 prod_cnt_grp 계산\n",
    "    stats = boxplot_stats(tr_df['prod_id'].value_counts())[0]\n",
    "    whislo, whishi = stats['whislo'], stats['whishi']\n",
    "\n",
    "    stat = list(tr_df['prod_id'].value_counts().quantile(\n",
    "        [0, 0.25, 0.50, 0.75, 1]).astype(int).values)\n",
    "    stat.append(whishi)\n",
    "    stat.sort()\n",
    "\n",
    "    grp = ['0' if x < stat[1] else\n",
    "           '1' if x < stat[2] else\n",
    "           '2' if x < stat[3] else\n",
    "           '3' if x < stat[4] else\n",
    "           '4' if x <= stat[5] else\n",
    "           'nan' for x in tr_df['prod_id'].value_counts()]\n",
    "\n",
    "    tr_grp_df = pd.DataFrame({\n",
    "        'prod_id': tr_df['prod_id'].value_counts().index,\n",
    "        'prod_cnt_grp': grp\n",
    "    })\n",
    "\n",
    "    tr_df = pd.merge(tr_df, tr_grp_df, how='left', on='prod_id')\n",
    "    te_df = pd.merge(te_df, tr_grp_df, how='left', on='prod_id')\n",
    "\n",
    "    # test가 train에 겹치지 않는 부분 location 확인\n",
    "    prod_cnt_null_loc = te_df['prod_cnt_grp'].isnull()\n",
    "    grp = ['0' if x < stat[1] else\n",
    "           '1' if x < stat[2] else\n",
    "           '2' if x < stat[3] else\n",
    "           '3' if x < stat[4] else\n",
    "           '4' if x <= stat[5] else\n",
    "           'nan' for x in te_df['prod_id'][te_df['prod_cnt_grp'].isnull()].value_counts()]\n",
    "\n",
    "    # 대부분 건수가 적어서 0으로 몰릴 것으로 확인됨\n",
    "    # grp\n",
    "\n",
    "    te_grp_df = pd.DataFrame({\n",
    "        'prod_id': te_df[prod_cnt_null_loc]['prod_id'].value_counts().index,\n",
    "        'prod_cnt_grp': grp\n",
    "    })\n",
    "\n",
    "    tr_te_grp_df = pd.concat([tr_grp_df, te_grp_df],\n",
    "                             axis=0).reset_index(drop=True)\n",
    "    tr_te_grp_df\n",
    "\n",
    "    te_df = pd.merge(te_df.drop(['prod_cnt_grp'], axis=1),\n",
    "                     tr_te_grp_df, how='left', on='prod_id')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 7. 제품별 구매횟수 : user_id는 train,test 동일하므로 train에서 가져오기\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # (1) quantile by prod_id\n",
    "    quantile_df = tr_df.groupby(['prod_id'])['target'].quantile(\n",
    "        q=[0, 0.25, 0.50, 0.75, 1]).reset_index()\n",
    "    quantile_df['level_1'] = 'sales_by_prod_q_' + \\\n",
    "        (quantile_df['level_1']*100).astype(int).astype(str)\n",
    "\n",
    "    quantile_df = quantile_df.pivot_table(\n",
    "        index='prod_id', columns='level_1', values='target', fill_value=0, aggfunc=sum).reset_index()\n",
    "    quantile_df.rename_axis(None, axis=1)\n",
    "\n",
    "    # (2) mean by prod_id\n",
    "    mean_df = df.groupby(['prod_id'])['target'].mean().reset_index()\n",
    "    mean_df.rename(columns={'target': 'sales_by_prod_mean'}, inplace=True)\n",
    "    mean_df\n",
    "\n",
    "    # (1) + (2)\n",
    "    col = ['sales_by_prod_' + x for x in ['q_0',\n",
    "                                          'q_25', 'q_50', 'mean', 'q_75', 'q_100']]\n",
    "    summary_df = pd.merge(quantile_df, mean_df, how='left',\n",
    "                          on='prod_id')[['prod_id']+col]\n",
    "    summary_df['prod_ratio'] = summary_df['sales_by_prod_q_100'] / \\\n",
    "        summary_df['sales_by_prod_q_0']\n",
    "\n",
    "    # (1) + (2) median by user_id\n",
    "    summary_by_user = pd.merge(tr_df, summary_df, how='left', on='prod_id')\n",
    "    summary_by_user = summary_by_user.groupby(\n",
    "        'user_id')[col+['prod_ratio']].median().reset_index()\n",
    "\n",
    "    rename_dict = {}\n",
    "    old = summary_by_user[col+['prod_ratio']].columns.values\n",
    "    for old_x in old:\n",
    "        rename_dict[old_x] = 'median_' + old_x\n",
    "\n",
    "    summary_by_user = summary_by_user.rename(columns=rename_dict)[\n",
    "        ['user_id']+['median_'+c for c in col+['prod_ratio']]]\n",
    "\n",
    "    # result\n",
    "    tr_df = pd.merge(tr_df, summary_by_user, how='left', on='user_id')\n",
    "    te_df = pd.merge(te_df, summary_by_user, how='left', on='user_id')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 8. user_id group\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    char_len = len(tr_df['user_id'][0])\n",
    "    for iter in range(char_len):\n",
    "        tr_df[f'user_id_{iter+1}'] = [user_id[iter:iter+1]\n",
    "                                      for user_id in tr_df['user_id']]\n",
    "        te_df[f'user_id_{iter+1}'] = [user_id[iter:iter+1]\n",
    "                                      for user_id in te_df['user_id']]\n",
    "\n",
    "        if (tr_df[f'user_id_{iter+1}'].nunique() == 1):\n",
    "            del tr_df[f'user_id_{iter+1}'], te_df[f'user_id_{iter+1}']\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # 9. prod_id group\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    char_len = len(tr_df['prod_id'][0])\n",
    "    for iter in range(char_len):\n",
    "        tr_df[f'prod_id_{iter+1}'] = [prod_id[iter:iter+1]\n",
    "                                      for prod_id in tr_df['prod_id']]\n",
    "        te_df[f'prod_id_{iter+1}'] = [prod_id[iter:iter+1]\n",
    "                                      for prod_id in te_df['prod_id']]\n",
    "\n",
    "        if (tr_df[f'prod_id_{iter+1}'].nunique() == 1):\n",
    "            del tr_df[f'prod_id_{iter+1}'], te_df[f'prod_id_{iter+1}']\n",
    "\n",
    "    return(tr_df, te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7f4f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:08.329696Z",
     "start_time": "2022-01-13T16:21:01.187510Z"
    }
   },
   "outputs": [],
   "source": [
    "train4, test4 = preprocessing(train3, test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa47e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:11.669680Z",
     "start_time": "2022-01-13T16:21:11.656678Z"
    }
   },
   "outputs": [],
   "source": [
    "new_var = sorted(setdiff(train4.columns, train3.columns))\n",
    "np.array(new_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717a022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:14.293541Z",
     "start_time": "2022-01-13T16:21:14.279558Z"
    }
   },
   "outputs": [],
   "source": [
    "for new_var_x in new_var:\n",
    "    if (new_var_x.find('_grp') + new_var_x.find('_id') >= 0):\n",
    "        COL_TYPE[new_var_x] = str\n",
    "    else:\n",
    "        COL_TYPE[new_var_x] = float\n",
    "        \n",
    "COL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b878b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033d512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:20.660875Z",
     "start_time": "2022-01-13T16:21:19.304478Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train5 = str_convert(train4, col_types = COL_TYPE, convert = 'category')\n",
    "test5  = str_convert(test4 , col_types = COL_TYPE, convert = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440262e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Segment** : segment를 구분하여 따로 모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1b75b",
   "metadata": {},
   "source": [
    "6에서 확인한 prod_cat_1로 segment 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb3651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:23.941292Z",
     "start_time": "2022-01-13T16:21:23.321348Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_segment(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    df['segment_1'] = ['1' if p in (19,20,13,12,4,18)      else\n",
    "                       '2' if p in (11,5,8,17,3)           else\n",
    "                       '3' if p in (2,9,14,1,16,6,15,7,10) else\n",
    "                       'error' for p in df['prod_cat_1'].astype(int)]\n",
    "\n",
    "    df['segment_2'] = ['1' if p in (11,12,13,18,5,7,8)     else\n",
    "                       '2' if p in (10,20)                 else\n",
    "                       '3' if p in (15,16)                 else\n",
    "                       '4' if p in (1,14,17,19,2,3,4,6,9)  else\n",
    "                       'error' for p in df['prod_cat_1'].astype(int)]\n",
    "    \n",
    "    for seg in ['segment_1','segment_2']:\n",
    "        df[seg] = df[seg].astype('category')\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "train6 = make_segment(train5)\n",
    "test6  = make_segment(test5)\n",
    "\n",
    "# col_type 추가\n",
    "for var in ['segment_1','segment_2']:\n",
    "    COL_TYPE[var]=str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7c88",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 각 조합별 건수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:37.821499Z",
     "start_time": "2022-01-12T12:27:37.806506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # island, sex, species\n",
    "\n",
    "# segment0 = pd.Series(['1' if x=='B' else '0' for x in train3['island']])\n",
    "# segment1 = train3['sex']\n",
    "# segment2 = train3['species']\n",
    "\n",
    "# def comb_seg(n_comb, head):\n",
    "\n",
    "#     # 각 조합에 대해 건수를 추출\n",
    "#     res = []\n",
    "#     combination = list(itertools.product([1, 0], repeat=3))\n",
    "#     for comb in combination:\n",
    "        \n",
    "#         # n_comb와 맞는 것들만 추출\n",
    "#         if sum(comb)==n_comb:\n",
    "#             comb_number = np.where(np.array(comb)==1)[0].tolist()\n",
    "#             comb_seg_ele = ['segment' + str(c) for c in comb_number]\n",
    "#             comb_seg = eval('+'.join(comb_seg_ele))\n",
    "\n",
    "#             ct = np.array(list(cnt(comb_seg).values()))\n",
    "#             ct = ct.flatten()\n",
    "#             ct = np.unique(ct)\n",
    "            \n",
    "#             res.append(ct)\n",
    "\n",
    "#     res = np.array(res)\n",
    "#     res_shape = res.shape\n",
    "    \n",
    "#     # level이 달라서, flatten이 안되는 경우\n",
    "#     # -> 1행의 list로 변환\n",
    "#     if len(res_shape)==1:\n",
    "#         ret = []\n",
    "#         for r in res:\n",
    "#             for ele in r:\n",
    "#                 ret.append(ele)\n",
    "        \n",
    "#         res = ret\n",
    "        \n",
    "#     else:\n",
    "#         res = res.flatten()\n",
    "        \n",
    "#     # unique, sort, head\n",
    "#     res = np.unique(sorted(res))[:head]\n",
    "\n",
    "#     return(res)\n",
    "\n",
    "# print('  iter : min(1st, 2nd, 3rd, ...)')\n",
    "# print('-'*40)\n",
    "# for iter in range(1,3+1):\n",
    "#     print(f'     {iter} : {comb_seg(iter,head=7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0c1a8",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Missing Prediction**\n",
    "\n",
    "7에서 missing들을 0으로 분류하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c796e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 'nan' or 'null' → np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059ca19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:26.979562Z",
     "start_time": "2022-01-13T16:21:26.695689Z"
    }
   },
   "outputs": [],
   "source": [
    "def str_nan_convert(df,col_types):\n",
    "    \n",
    "    _df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col_types[col]==str:\n",
    "            df[col][df[col]=='nan']  = np.nan\n",
    "            df[col][df[col]=='null'] = np.nan\n",
    "            \n",
    "    return(df)\n",
    "\n",
    "train7 = str_nan_convert(train6,COL_TYPE)\n",
    "test7  = str_nan_convert(test6 ,COL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d54258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:29.748666Z",
     "start_time": "2022-01-13T16:21:29.622737Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train7.info(),'\\n')\n",
    "print(test7 .info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d0890",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Category Level Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7925bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:34.066423Z",
     "start_time": "2022-01-13T16:21:33.952485Z"
    }
   },
   "outputs": [],
   "source": [
    "# 하나의 데이터셋에 대해서 level 개수를 search\n",
    "def check_category(data,col_types, ret=['dict','list']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['id','target']))\n",
    "\n",
    "    if ret=='dict':\n",
    "        len_cate = {}\n",
    "    elif ret=='list':\n",
    "        len_cate = []\n",
    "    else:\n",
    "        raise('error ret')\n",
    "        \n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            _len = len(data[col].value_counts().index)\n",
    "            \n",
    "            if ret=='dict':\n",
    "                len_cate[col] = _len\n",
    "            elif ret=='list':\n",
    "                len_cate.append(_len)\n",
    "            \n",
    "    return(len_cate)\n",
    "\n",
    "check_category(train7,COL_TYPE,ret='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621b1b",
   "metadata": {},
   "source": [
    "##### train에서 모두 2개 이상으로, 이상없음\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:37.729180Z",
     "start_time": "2022-01-13T16:21:37.545252Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 두개 데이터셋의 동일한 변수에 대해서, level이 같은지 확인\n",
    "def check_category2(data1,data2,col_types):\n",
    "\n",
    "    cols = list(set(data1.columns) - set(['id','target']))\n",
    "    max_char_len = max([len(x) if col_types[x]==str else 0 for x in col_types.keys()])\n",
    "    \n",
    "    # 없음\n",
    "    for col in cols:\n",
    "        \n",
    "        #print(col)\n",
    "        if col_types[col]==str:\n",
    "            data1_cate = [str(x) for x in sorted(data1[col].value_counts().index.values)]\n",
    "            data2_cate = [str(x) for x in sorted(data2[col].value_counts().index.values)]\n",
    "            \n",
    "            n_blank = (max_char_len-len(col))\n",
    "            if len(data1_cate)==len(data2_cate):\n",
    "                same_sum     = list(set(data1_cate) & set(data2_cate))\n",
    "                not_same_sum = len(setdiff(data1_cate,data2_cate)) + len(setdiff(data1_cate,data1_cate))\n",
    "                print(col, ' '*n_blank, ':', not_same_sum)\n",
    "            else:\n",
    "                print(col, ' '*n_blank, ': differ length')\n",
    "            \n",
    "print(color.BOLD + color.BLUE + '> 다른 카테고리의 개수' + color.END)\n",
    "check_category2(train7,test7,COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313e296",
   "metadata": {},
   "source": [
    "생각 외로, 구매자(user_id)는 일치하고, 제품명(prod_id)은 일치하지 않음\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde8e80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:21:46.422167Z",
     "start_time": "2022-01-13T16:21:46.399168Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(train7['prod_cat_1'].astype(int).unique()), sorted(test7 ['prod_cat_1'].astype(int).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e123c",
   "metadata": {},
   "source": [
    "일치하지 않는 prod_cat_1 확인 결과, test에서는 19,20이 없음. <br>\n",
    "onehot encoding 시, test에 prod_cat_1_19,20 = 0 추가해야함 <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975b562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:23:10.752065Z",
     "start_time": "2022-01-13T16:23:10.712079Z"
    }
   },
   "outputs": [],
   "source": [
    "print('prod_id_8 - Train :', sorted(train7['prod_id_8'].astype(int).unique()), ', Test :',sorted(test7['prod_id_8'].astype(int).unique()))\n",
    "print('prod_id_9 - Train :', sorted(train7['prod_id_9'].astype(int).unique()), ', Test :',sorted(test7['prod_id_9'].astype(int).unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683b439",
   "metadata": {},
   "source": [
    "test가 모두 train에 포함됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c19d7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **OneHot-Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfd4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:23:30.569609Z",
     "start_time": "2022-01-13T16:23:30.528637Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop_var = ['user_id','prod_id']\n",
    "\n",
    "# train8 = onehot_encoding(train7.drop(drop_var,axis=1),COL_TYPE)\n",
    "# test8  = onehot_encoding(test7 .drop(drop_var,axis=1),COL_TYPE)\n",
    "\n",
    "# # test는 prod_cat_1에 19,20이 없음\n",
    "# test8['prod_cat_1_19'] = 0\n",
    "# test8['prod_cat_1_20'] = 0\n",
    "\n",
    "train8 = train7.copy()\n",
    "test8  = test7 .copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9a97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **교호작용항**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cfa0e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "make_scorer을 통해서 rmse로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066550d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:23:36.907630Z",
     "start_time": "2022-01-13T16:23:36.896635Z"
    }
   },
   "outputs": [],
   "source": [
    "def mape_fn(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    return mape\n",
    "\n",
    "def rmse_fn(y_test, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return(rmse)\n",
    "\n",
    "def r2_fn(y_test, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a1f2e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## without Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb65e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.350090Z",
     "start_time": "2022-01-12T12:27:38.336097Z"
    }
   },
   "outputs": [],
   "source": [
    "# if INTERACTION_CHECK:\n",
    "#     automl_no_interaction = AUTOML_COMP(train = train8,\n",
    "#                                         test  = None,\n",
    "#                                         target = 'target',\n",
    "#                                         col_types = COL_TYPE,\n",
    "#                                         objective = 'regression',\n",
    "#                                         ignore_features = None,\n",
    "#                                         scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                                         fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d25285",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## with Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abac84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.366097Z",
     "start_time": "2022-01-12T12:27:38.359086Z"
    }
   },
   "outputs": [],
   "source": [
    "# if INTERACTION_CHECK:\n",
    "\n",
    "#     # interaction term 추가\n",
    "#     interaction_df = interaction_term(train10.copy(),list(set(num_vari)-set(['body_mass'])))\n",
    "\n",
    "#     # COL_TYPE에도 추가\n",
    "#     COL_TYPE2 = COL_TYPE.copy()\n",
    "#     for col in interaction_df.columns:\n",
    "#         if col.find('*')>0:\n",
    "#             COL_TYPE2[col] = int\n",
    "            \n",
    "#     automl_interaction = AUTOML_COMP(train = interaction_df,\n",
    "#                                      test = None,\n",
    "#                                      target = 'body_mass',\n",
    "#                                      col_types = COL_TYPE2,\n",
    "#                                      objective = 'regression',\n",
    "#                                      ignore_features = ['id'],\n",
    "#                                      missing_return = False,\n",
    "#                                      scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                                      fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010106f",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.381891Z",
     "start_time": "2022-01-12T12:27:38.367083Z"
    }
   },
   "outputs": [],
   "source": [
    "# if INTERACTION_CHECK:\n",
    "    \n",
    "#     comp_df = pd.DataFrame({\n",
    "#         'model'          : [name for name,model in automl_no_interaction['model']],\n",
    "#         'no_interaction' : [np.mean(res) for res in automl_no_interaction['cv_result']],\n",
    "#         'interaction'    : [np.mean(res) for res in automl_interaction   ['cv_result']]\n",
    "#     })\n",
    "#     comp_df['res'] = comp_df['interaction'] - comp_df['no_interaction']\n",
    "\n",
    "#     sort_var = ['interaction' if comp_df['interaction'].max() > comp_df['no_interaction'].max() else 'no_interaction']\n",
    "#     comp_df  = comp_df.sort_values(sort_var,ascending = False)\n",
    "\n",
    "#     print(round(comp_df,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00fab6",
   "metadata": {},
   "source": [
    "LASSO,LR,KNN의 경우에는 교호작용의 효과가 안 좋고, LGBM, XGB는 교호작용의 효과가 좋음. 나머지는 비슷함\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494aa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.397884Z",
     "start_time": "2022-01-12T12:27:38.383881Z"
    }
   },
   "outputs": [],
   "source": [
    "# if INTERACTION:\n",
    "#     train11 = interaction_term(train10.copy() ,list(set(num_vari)-set(['body_mass'])))\n",
    "#     test11  = interaction_term(test10 .copy() ,list(set(num_vari)-set(['body_mass'])))\n",
    "\n",
    "#     for int_var in train11.columns[[col.find('*')>0 for col in train11.columns]]:\n",
    "#         COL_TYPE[int_var] = int\n",
    "# else:\n",
    "#     train11 = train10.copy()\n",
    "#     test11  = test10 .copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354b1c7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "여기까지 대략 14분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7f70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.413168Z",
     "start_time": "2022-01-12T12:27:38.398875Z"
    }
   },
   "outputs": [],
   "source": [
    "f'{(time.time()-query_start_time)/60:.1f} Mins'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147d4c5",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "save/load session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd356ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.428773Z",
     "start_time": "2022-01-12T12:27:38.415158Z"
    }
   },
   "outputs": [],
   "source": [
    "# dill.dump_session('notebook_env.db')\n",
    "# dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04617ab",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf778b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.444425Z",
     "start_time": "2022-01-12T12:27:38.430772Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale_var = setdiff([col for col in COL_TYPE.keys() if COL_TYPE[col] in [int,float]],\n",
    "#                     ['id','body_mass'])\n",
    "# print(scale_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fa6d3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "scaling 대상 변수들의 boundary 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abda0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.460419Z",
     "start_time": "2022-01-12T12:27:38.446415Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE:\n",
    "\n",
    "#     # len(scale_var),len(np.unique(scale_var)[0])\n",
    "\n",
    "#     # 모두 0 이상의 값을 가짐\n",
    "#     # min, max가 test에서 bound를 벗어나는게 있긴함..\n",
    "#     max_char_len = max([len(x) for x in scale_var])\n",
    "#     for iter,var in enumerate(scale_var):\n",
    "#         n_blank = ' '*(max_char_len-len(var))\n",
    "#         print(color.BOLD+color.BLUE+f'({iter+1}) {var} {n_blank}- '+color.END+\n",
    "#               f'tr : {minmax(train11[var],1)}\\t, te : {minmax(test11[var],1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e287a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7f41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.476402Z",
     "start_time": "2022-01-12T12:27:38.462409Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE_CHECK:\n",
    "\n",
    "#     automl_base = AUTOML_COMP(train = train11,\n",
    "#                               test = None,\n",
    "#                               target = 'body_mass',\n",
    "#                               col_types = COL_TYPE,\n",
    "#                               objective = 'regression',\n",
    "#                               ignore_features = ['id'],\n",
    "#                               missing_return = False,\n",
    "#                               scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                               fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46465feb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Normalization (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7b422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.491786Z",
     "start_time": "2022-01-12T12:27:38.477792Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE_CHECK:\n",
    "\n",
    "#     scale_df = train11.copy()\n",
    "    \n",
    "#     # Normalization\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler.fit(scale_df[scale_var])\n",
    "\n",
    "#     scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "#     automl_norm = AUTOML_COMP(train = scale_df,\n",
    "#                               test = None,\n",
    "#                               target = 'body_mass',\n",
    "#                               col_types = COL_TYPE,\n",
    "#                               objective = 'regression',\n",
    "#                               ignore_features = ['id'],\n",
    "#                               missing_return = False,\n",
    "#                               scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                               fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ea603",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Standardization (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e1b4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.507875Z",
     "start_time": "2022-01-12T12:27:38.492786Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE_CHECK:\n",
    "\n",
    "#     scale_df = train11.copy()\n",
    "    \n",
    "#     # Normalization\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(scale_df[scale_var])\n",
    "\n",
    "#     scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "#     automl_std  = AUTOML_COMP(train = scale_df,\n",
    "#                               test = None,\n",
    "#                               target = 'body_mass',\n",
    "#                               col_types = COL_TYPE,\n",
    "#                               objective = 'regression',\n",
    "#                               ignore_features = ['id'],\n",
    "#                               missing_return = False,\n",
    "#                               scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                               fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b11d46",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Robust (RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.523066Z",
     "start_time": "2022-01-12T12:27:38.510072Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE_CHECK:\n",
    "\n",
    "#     scale_df = train11.copy()\n",
    "    \n",
    "#     # Normalization\n",
    "#     scaler = RobustScaler()\n",
    "#     scaler.fit(scale_df[scale_var])\n",
    "\n",
    "#     scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "#     automl_rob  = AUTOML_COMP(train = scale_df,\n",
    "#                               test = None,\n",
    "#                               target = 'body_mass',\n",
    "#                               col_types = COL_TYPE,\n",
    "#                               objective = 'regression',\n",
    "#                               ignore_features = ['id'],\n",
    "#                               missing_return = False,\n",
    "#                               scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "#                               fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a2b36",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d00f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.539073Z",
     "start_time": "2022-01-12T12:27:38.525065Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE_CHECK:\n",
    "    \n",
    "#     comp_df = pd.DataFrame({\n",
    "#         'model' : [name for name,model in automl_base['model']],\n",
    "#         'base'  : [np.mean(res) for res in automl_base['cv_result']],\n",
    "#         'norm'  : [np.mean(res) for res in automl_norm['cv_result']],\n",
    "#         'std'   : [np.mean(res) for res in automl_std ['cv_result']],\n",
    "#         'rob'   : [np.mean(res) for res in automl_rob ['cv_result']],\n",
    "#     })\n",
    "#     comp_df['res1'] = comp_df['norm'] - comp_df['base']\n",
    "#     comp_df['res2'] = comp_df['std' ] - comp_df['base']\n",
    "#     comp_df['res3'] = comp_df['rob' ] - comp_df['base']\n",
    "    \n",
    "#     comp_df['res_max'] = comp_df[['res1','res2','res3']].apply(lambda x: max(x), axis=1)\n",
    "#     comp_df['res_min'] = comp_df[['res1','res2','res3']].apply(lambda x: min(x), axis=1)\n",
    "#     comp_df = comp_df.drop(['res1','res2','res3'],axis=1)\n",
    "    \n",
    "#     comp_df = comp_df.sort_values(['base'],ascending = False).reset_index(drop=True)\n",
    "\n",
    "#     print(round(comp_df,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bf85a",
   "metadata": {},
   "source": [
    "KNN, SVM의 경우에는 Scaling의 효과가 좋고, EN, CART의 경우에는 Scaling의 효과가 안 좋음. 나머지는 비슷함.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.554053Z",
     "start_time": "2022-01-12T12:27:38.541058Z"
    }
   },
   "outputs": [],
   "source": [
    "# if SCALE:\n",
    "\n",
    "#     # Normalization\n",
    "#     scaler = RobustScaler()\n",
    "#     scaler.fit(train11[scale_var])\n",
    "#     # print(scaler.n_samples_seen_, scaler.data_min_, scaler.data_max_, scaler.feature_range)\n",
    "\n",
    "#     train12 = train11.copy()\n",
    "#     test12  = test11 .copy()\n",
    "\n",
    "#     train12[scale_var] = scaler.transform(train12[scale_var])\n",
    "#     test12 [scale_var] = scaler.transform(test12 [scale_var])\n",
    "    \n",
    "#     minmax_df = test12[scale_var].apply(lambda x: minmax(x))\n",
    "#     minmax_df.index = ['min','max']\n",
    "    \n",
    "#     print(color.BOLD + color.BLUE + '> Robust Scaling' + color.END)\n",
    "#     print(minmax_df)\n",
    "    \n",
    "# else:\n",
    "#     train12 = train11.copy()\n",
    "#     test12  = test11. copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456318",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233309b6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Initial Value 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c101cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:23:48.459072Z",
     "start_time": "2022-01-13T16:23:48.449078Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "print('   Initial Values')\n",
    "print('-'*50)\n",
    "max_char_len = max([len(var) for var in ini_var])\n",
    "for var in ini_var:\n",
    "    char_len = ' '*(max_char_len-len(var))\n",
    "    print(f'\\t{var} {char_len} : {eval(var)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5166e9f",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## AutoML Validation (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ad4d4",
   "metadata": {},
   "source": [
    "모형 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338a3e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:27:38.586047Z",
     "start_time": "2022-01-12T12:27:38.572059Z"
    }
   },
   "outputs": [],
   "source": [
    "models_dict={\n",
    "    'LR'    : LinearRegression(),\n",
    "    'RIDGE' : Ridge(random_state=SEED),\n",
    "    'LASSO' : Lasso(random_state=SEED),\n",
    "    'KNN'   : KNeighborsRegressor(),\n",
    "    'CART'  : DecisionTreeRegressor(random_state=SEED),\n",
    "    'EN'    : ElasticNet(random_state=SEED),\n",
    "#     'SVM'   : SVR(),\n",
    "    'RFR'   : RandomForestRegressor(random_state=SEED),\n",
    "    'XGBR'  : XGBRegressor(iterations=10000,verbosity=0,random_state=SEED),\n",
    "    'LGBMR' : LGBMRegressor(random_state=SEED),\n",
    "    'AdaR'  : AdaBoostRegressor(random_state=SEED),\n",
    "    'Cat'   : CatBoostRegressor(iterations=10000,silent=True,random_state=SEED),\n",
    "}\n",
    "\n",
    "models=[]\n",
    "for name,model in models_dict.items():\n",
    "    model = (name,model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df3407",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4d4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T10:49:59.859667Z",
     "start_time": "2022-01-09T10:49:59.693657Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train8.drop(['user_id','prod_id'],axis=1)\n",
    "test_df  = test8 .drop(['user_id','prod_id'],axis=1)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df733b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T10:50:02.553821Z",
     "start_time": "2022-01-09T10:50:01.875782Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = onehot_encoding(train_df,COL_TYPE)\n",
    "X_te = onehot_encoding(test_df ,COL_TYPE)\n",
    "X_te['prod_cat_1_19'] = 0\n",
    "X_te['prod_cat_1_20'] = 0\n",
    "\n",
    "y_tr = X_tr['target']\n",
    "X_tr = X_tr.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d79f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T11:15:17.139450Z",
     "start_time": "2022-01-09T10:50:57.536966Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y_tr, test_size=0.3, random_state=SEED)\n",
    "\n",
    "X_train_raw = X_train.copy()\n",
    "X_test_raw  = X_test .copy()\n",
    "\n",
    "running_time = []\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # model = models[0][1]\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    X_train_raw[f'pred_{name}'] = model.predict(X_train)\n",
    "    X_test_raw [f'pred_{name}'] = model.predict(X_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_time.append(end_time-start_time)\n",
    "    \n",
    "    r = (end_time-start_time)/60\n",
    "    \n",
    "    max_char_len = max([len(name) for name,model in models])\n",
    "    n_blank = ' '*(max_char_len - len(name))\n",
    "    print(f'{name}{n_blank} : {r:.2f} Mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a82c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T11:18:27.644346Z",
     "start_time": "2022-01-09T11:18:27.581343Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_df = pd.concat([\n",
    "    X_test_raw,\n",
    "    y_test\n",
    "],axis=1)\n",
    "# tmp.to_csv('auto_ml_pred.csv',index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'model' : [name for name,model in models],\n",
    "    'rmse' : [rmse_fn(pred_df[pred_var],tmp['target']) for pred_var in [f'pred_{name}' for name,model in models]],\n",
    "    'running_time' : [r/60 for r in running_time]\n",
    "}).round(1).sort_values('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2adbaf",
   "metadata": {},
   "source": [
    "| model | rmse   | running_time(Min) |\n",
    "|-------|--------|-------------------|\n",
    "| LR    | 2873.8 | 0.15              |\n",
    "| RIDGE | 2873.8 | 0.03              |\n",
    "| LASSO | 2875.8 | 1.75              |\n",
    "| KNN   | 3544.8 | 1.43              |\n",
    "| CART  | 3737.3 | 0.18              |\n",
    "| EN    | 3715.5 | 0.05              |\n",
    "| SVM   | -      | -                 |\n",
    "| RFR   | 2719.4 | 10.05             |\n",
    "| XGBR  | 2636.6 | 1.09              |\n",
    "| LGBMR | 2722.1 | 0.08              |\n",
    "| AdaR  | 3094.0 | 1.13              |\n",
    "| Cat   | 2599.7 | 8.63              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c55f5e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Best : Segment as Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048cc2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T10:45:21.116723Z",
     "start_time": "2022-01-09T10:34:04.573027Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=10000,silent=True,random_state=SEED)\n",
    "model.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca600bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T10:45:24.280904Z",
     "start_time": "2022-01-09T10:45:23.387853Z"
    }
   },
   "outputs": [],
   "source": [
    "X_te['CAT_pred'] = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4fec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T10:45:27.575093Z",
     "start_time": "2022-01-09T10:45:26.908055Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_te[['CART_pred','user_id','prod_id']]\n",
    "sub_fn = pd.concat([\n",
    "    X_te['CAT_pred'],\n",
    "    test8[['user_id','prod_id']]\n",
    "],axis=1)\n",
    "\n",
    "sub_fn.rename(columns={'CAT_pred':'Purchase','user_id':'User_ID','prod_id':'Product_ID'},inplace=True)\n",
    "sub_fn.to_csv(OUT_PATH + 'submission_CAT.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70f83b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## AutoML Validation (RMSE) by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc69822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T12:39:09.087261Z",
     "start_time": "2022-01-09T12:39:09.073260Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train8.drop(['user_id','prod_id'],axis=1)\n",
    "test_df  = test8 .drop(['user_id','prod_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec80e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T12:39:10.741356Z",
     "start_time": "2022-01-09T12:39:10.193324Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = onehot_encoding(train_df,COL_TYPE,ignore_features=['segment_1'])\n",
    "X_te = onehot_encoding(test_df ,COL_TYPE,ignore_features=['segment_1'])\n",
    "X_te['prod_cat_1_19'] = 0\n",
    "X_te['prod_cat_1_20'] = 0\n",
    "\n",
    "y_tr = X_tr['target']\n",
    "X_tr = X_tr.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9ad7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T12:59:47.968121Z",
     "start_time": "2022-01-09T12:39:11.986427Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "_train = []\n",
    "_test  = []\n",
    "_runtime = []\n",
    "\n",
    "seg_dict = {}\n",
    "\n",
    "iter = 0\n",
    "for value in sorted(X_tr['segment_1'].unique()):\n",
    "    iter += 1\n",
    "    \n",
    "    n_seg = X_tr['segment_1'].nunique()\n",
    "    \n",
    "    X_tr_seg = X_tr[X_tr['segment_1']==value].drop(['segment_1'],axis=1)\n",
    "    y_tr_seg = y_tr[X_tr['segment_1']==value]\n",
    "    \n",
    "    X_tr_ratio = len(X_tr_seg)/len(X_tr)*100\n",
    "    \n",
    "    print(f'{color.BOLD}{color.BLUE}> ({iter}/{n_seg}) segment_1 = {value} / nrow = {len(X_tr_seg):,} ({X_tr_ratio:.1f}%) {color.END}')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tr_seg, y_tr_seg, test_size=0.3, random_state=SEED)\n",
    "\n",
    "    X_train_raw = X_train.copy()\n",
    "    X_test_raw  = X_test .copy()\n",
    "    \n",
    "    running_time = []\n",
    "    pbar = tqdm(models)\n",
    "    for name,model in pbar:\n",
    "        pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # model = models[0][1]\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        X_train_raw[f'pred_{name}'] = model.predict(X_train)\n",
    "        X_test_raw [f'pred_{name}'] = model.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        running_time.append(end_time-start_time)\n",
    "\n",
    "        r = (end_time-start_time)/60\n",
    "\n",
    "        max_char_len = max([len(name) for name,model in models])\n",
    "        n_blank = ' '*(max_char_len - len(name))\n",
    "        print(f'{name}{n_blank} : {r:.2f} Mins ({datetime.datetime.now()})')\n",
    "        \n",
    "    _train.append(pd.concat([X_train_raw,y_train],axis=1))\n",
    "    _test .append(pd.concat([X_test_raw ,y_test ],axis=1))\n",
    "    _runtime.append(running_time)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "seg_dict['train'] = _train\n",
    "seg_dict['test']  = _test\n",
    "seg_dict['runtime'] = _runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b92036",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_dict = {}\n",
    "seg_dict['train'] = _train\n",
    "seg_dict['test']  = _test\n",
    "seg_dict['runtime'] = _runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76327faf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T13:09:20.169849Z",
     "start_time": "2022-01-09T13:09:19.898833Z"
    }
   },
   "outputs": [],
   "source": [
    "# 20분\n",
    "sum([sum(r) for r in _runtime])/60\n",
    "\n",
    "train_fn = pd.concat(_train,axis=0).reset_index(drop=True)\n",
    "test_fn  = pd.concat(_test ,axis=0).reset_index(drop=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'model'   : [name for name,model in models],\n",
    "    'tr_rmse' : [rmse_fn(train_fn[pred],train_fn['target']) for pred in [f'pred_{name}' for name,model in models]],\n",
    "    'te_rmse' : [rmse_fn(test_fn [pred],test_fn ['target']) for pred in [f'pred_{name}' for name,model in models]],\n",
    "    'runtime' : np.sum(_runtime,axis=0)/60\n",
    "}).round(1).sort_values('te_rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3397ce",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Best : by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acc0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T13:12:42.475420Z",
     "start_time": "2022-01-09T13:12:42.459419Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train8.drop(['user_id','prod_id'],axis=1)\n",
    "test_df  = test8 .drop(['user_id','prod_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4230c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T13:31:55.977397Z",
     "start_time": "2022-01-09T13:31:55.404364Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = onehot_encoding(train_df,COL_TYPE,ignore_features=['segment_1'])\n",
    "X_te = onehot_encoding(test_df ,COL_TYPE,ignore_features=['segment_1'])\n",
    "X_te['prod_cat_1_19'] = 0\n",
    "X_te['prod_cat_1_20'] = 0\n",
    "\n",
    "y_tr = X_tr['target']\n",
    "X_tr = X_tr.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048e11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T13:43:36.529466Z",
     "start_time": "2022-01-09T13:31:56.924451Z"
    }
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(sorted(X_tr['segment_1'].unique()))\n",
    "\n",
    "_train2, _test2 = [],[]\n",
    "for value in pbar:\n",
    "    pbar.set_description(f'fitting... ({value})')\n",
    "    \n",
    "    X_tr_seg = X_tr[X_tr['segment_1']==value].drop(['segment_1'],axis=1)\n",
    "    y_tr_seg = y_tr[X_tr['segment_1']==value]\n",
    "\n",
    "    X_te_seg = X_te[X_te['segment_1']==value].drop(['segment_1'],axis=1)\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=10000,silent=True,random_state=SEED)\n",
    "    model.fit(X_tr_seg,y_tr_seg)\n",
    "\n",
    "    X_tr_seg['pred'] = model.predict(X_tr_seg)\n",
    "    X_te_seg['pred'] = model.predict(X_te_seg)\n",
    "    \n",
    "    _train2.append(X_tr_seg)\n",
    "    _test2 .append(X_te_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec71eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T13:45:42.872692Z",
     "start_time": "2022-01-09T13:45:42.180653Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fn = sub\n",
    "sub_fn['Purchase'] = pd.concat(_test2,axis=0).reset_index(drop=True)['pred']\n",
    "\n",
    "sub_fn.to_csv(OUT_PATH + 'submission_CAT_seg.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465aacd4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Ensemble with Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c031d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T11:24:50.189227Z",
     "start_time": "2022-01-09T11:24:50.173226Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_var = [f'pred_{name}' for name,model in models]\n",
    "\n",
    "pred_df['ensemble_pred_1'] = (pred_df['pred_Cat'] + pred_df['pred_XGBR'] + pred_df['pred_RFR'] + pred_df['pred_LGBMR']) / 4\n",
    "pred_df['ensemble_pred_2'] = (pred_df['pred_LR']  + pred_df['pred_XGBR'] + pred_df['pred_LGBMR']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb8a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T11:24:55.488530Z",
     "start_time": "2022-01-09T11:24:55.477529Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_fn(pred_df['ensemble_pred_1'],pred_df['target']), rmse_fn(pred_df['ensemble_pred_2'],pred_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aaa3e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM setting with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0bfa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:24:10.570798Z",
     "start_time": "2022-01-13T16:24:10.524896Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, _X, _y, objective = 'regression', n_split = 5, seed = SEED):\n",
    "    \n",
    "    # set reg options\n",
    "    if objective=='regression':\n",
    "        ML = lgb.LGBMRegressor\n",
    "    elif objective=='binary':\n",
    "        ML = lgb.LGBMClassifier\n",
    "    \n",
    "    # optuna hyper-parameter grid\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 800, step=5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30, step=2),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 0, 1000, step=50),\n",
    "        'lambda_l1': trial.suggest_int('lambda_l1', 0, 100, step=5),\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 0, 100, step=5),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 1, 30, step=3),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 0.99, step=0.1),\n",
    "        'bagging_freq': trial.suggest_categorical('bagging_freq',[1]),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 0.99, step=0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 3000, step=5),\n",
    "        \n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.01, 0.99, step=0.01),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.99 ,step=0.01),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 0.99 ,step=0.01),\n",
    "        \n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'seed': seed,\n",
    "        'feature_fraction_seed': seed,\n",
    "        'bagging_seed': seed,\n",
    "        'drop_seed': seed,\n",
    "        'data_random_seed': seed,\n",
    "        \n",
    "        'boosting': 'gbdt', \n",
    "        'verbose': -1,\n",
    "        #'verbose_eval': -1,\n",
    "        'boost_from_average': True,\n",
    "        \n",
    "        # -1 : all cpu\n",
    "        'n_jobs': -1,\n",
    "        \n",
    "        'objective' : OBJECTIVE,\n",
    "    }\n",
    "    \n",
    "    # cross validation\n",
    "    cv = StratifiedKFold(n_splits=n_split, shuffle=False)#, random_state=SEED)\n",
    "\n",
    "    # cross validation score\n",
    "    cv_scores = np.empty(n_split)\n",
    "    \n",
    "    # cv를 통해서 lgb적합해서 최적의 hyper-parameter 확인\n",
    "    for idx, (_train_idx, _test_idx) in enumerate(cv.split(_X, _y)):\n",
    "        _X_train, _X_test = _X.iloc[_train_idx], _X.iloc[_test_idx]\n",
    "        _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n",
    "\n",
    "        model = ML(**param_grid)\n",
    "        model.fit(\n",
    "            _X_train,\n",
    "            _y_train,\n",
    "            verbose = -1,\n",
    "            eval_set=[(_X_test, _y_test)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=30,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, 'rmse')\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        _preds = model.predict(_X_test)\n",
    "        cv_scores[idx] = rmse_fn(_y_test, _preds)\n",
    "\n",
    "    #return np.mean(f1_scores)\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b3e3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### segment and dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a523a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:24:07.594369Z",
     "start_time": "2022-01-13T16:24:07.500889Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ca==cp > exang > slope > sex\n",
    "# > ca는 2,3건수가 너무적음\n",
    "# > cp는 1,3건수가 너무 적음\n",
    "seg_var = ['segment_1','segment_2']\n",
    "\n",
    "# 각 세그별 최소 건수\n",
    "#[train12[seg_var_x].value_counts().min() for seg_var_x in seg_var]\n",
    "cnt(train8['segment_1']), cnt(train8['segment_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffdab6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d11f8",
   "metadata": {},
   "source": [
    "### seg_var as variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbfb7f",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b073cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:25:29.739402Z",
     "start_time": "2022-01-13T16:25:23.417203Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train8.copy()\n",
    "test_df  = test8 .copy()\n",
    "\n",
    "drop_var = ['user_id','prod_id','target']\n",
    "features = setdiff(train_df.columns,drop_var)\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['target']\n",
    "X_test  = test_df [features]\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "sub_df = test8.copy()\n",
    "sub_df['target'] = model.predict(X_test)\n",
    "\n",
    "# sub_df[['']]\n",
    "sub_df = sub_df[['target','user_id','prod_id']].rename(columns={'user_id':'User_ID','prod_id':'Product_ID','target':'Purchase'})\n",
    "sub_df.to_csv(OUT_PATH + 'submission_lgb_newfeature.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd37d51",
   "metadata": {},
   "source": [
    "#### feature_select + optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b4f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:46:40.831371Z",
     "start_time": "2022-01-13T17:46:40.704414Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train8.copy(), test8.copy()\n",
    "train_df['pred'], test_df['pred'] = np.nan, np.nan\n",
    "\n",
    "# set reg options\n",
    "if OBJECTIVE=='regression':\n",
    "    ML = lgb.LGBMRegressor\n",
    "elif OBJECTIVE=='binary':\n",
    "    ML = lgb.LGBMClassifier\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "# remove needless variable\n",
    "#---------------------------------------------------------------------------------------#\n",
    "drop_var = ['user_id','prod_id','target','pred'] # ,'segment_1','segment_2'\n",
    "X_train = train_df[list(set(train_df.columns)-set(drop_var))]\n",
    "X_test  = test_df [list(set(test_df .columns)-set(drop_var))]\n",
    "\n",
    "y_train = train_df['target'].astype(float).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62151ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:46:44.709768Z",
     "start_time": "2022-01-13T17:46:44.694783Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(sorted(X_train.columns)), y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935093a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:46:52.211194Z",
     "start_time": "2022-01-13T17:46:48.405590Z"
    }
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------#\n",
    "# feature importance\n",
    "#---------------------------------------------------------------------------------------#\n",
    "model = ML(random_state = SEED)\n",
    "model.fit(X_train, y_train, verbose=-1)\n",
    "# list(feature_imp.feature[feature_imp.imp<=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaf939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:46:56.302331Z",
     "start_time": "2022-01-13T17:46:55.844299Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns),reverse=True), columns=['importance','feature'])\n",
    "feature_imp['importance_ratio'] = feature_imp['importance'] / sum(feature_imp['importance'])\n",
    "feature_imp['importance_ratio_cumsum'] = feature_imp['importance_ratio'].cumsum()\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x='importance_ratio', y='feature', data=feature_imp.sort_values(by='importance_ratio', ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)', fontsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3da845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:47:15.867409Z",
     "start_time": "2022-01-13T17:47:15.824438Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_var = ['prod_cnt_grp','prod_id_9','gender','user_cnt_grp','prod_id_8','marital']\n",
    "\n",
    "X_train_new = X_train.drop(drop_var,axis=1)\n",
    "X_test_new  = X_test .drop(drop_var,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fcd23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:44:14.946244Z",
     "start_time": "2022-01-13T16:44:14.874277Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c9039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T16:45:20.096178Z",
     "start_time": "2022-01-13T16:45:15.522692Z"
    }
   },
   "outputs": [],
   "source": [
    "# save basic\n",
    "model = ML(random_state = SEED)\n",
    "model.fit(X_train_new, y_train, verbose=-1)\n",
    "\n",
    "sub_df = test8.copy()\n",
    "sub_df['target'] = model.predict(X_test_new)\n",
    "\n",
    "# sub_df[['']]\n",
    "sub_df = sub_df[['target','user_id','prod_id']].rename(columns={'user_id':'User_ID','prod_id':'Product_ID','target':'Purchase'})\n",
    "sub_df.to_csv(OUT_PATH + 'submission_lgb_newfeature_reduced.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905b10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:30:24.147796Z",
     "start_time": "2022-01-13T16:54:26.499079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# train_ls, test_ls = [],[]\n",
    "# for seed in range(1,5):\n",
    "\n",
    "train_df, test_df = train8.copy(), test8.copy()\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "# modelling\n",
    "#---------------------------------------------------------------------------------------#\n",
    "# larget dataset인 경우, n_split은 3이면 충분함\n",
    "study = optuna.create_study(direction='minimize', study_name='LGBM Regressor')\n",
    "func = lambda trial: objective(trial, X_train_new, y_train, objective='regression', n_split=3, seed = SEED)\n",
    "study.optimize(func, n_trials=10)\n",
    "\n",
    "model = ML(**study.best_params)\n",
    "model.fit(X_train_new,y_train)\n",
    "\n",
    "# seg별 predict값 넣기\n",
    "train_df['pred'] = model.predict(X_train_new)\n",
    "test_df ['pred'] = model.predict(X_test_new)\n",
    "\n",
    "# train_ls.append(train_df)\n",
    "# test_ls .append(test_df)\n",
    "\n",
    "end_time = time.time()\n",
    "running_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b888cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:35:51.713534Z",
     "start_time": "2022-01-13T17:35:51.695534Z"
    }
   },
   "outputs": [],
   "source": [
    "running_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e186284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:30:42.564516Z",
     "start_time": "2022-01-13T17:30:32.392370Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(OUT_PATH + 'train_df_lgb_no_seg_var_optuna_newfeature_featureselect_220114.csv',index=False)\n",
    "test_df .to_csv(OUT_PATH +  'test_df_lgb_no_seg_var_optuna_newfeature_featureselect_220114.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce8fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T17:30:47.694964Z",
     "start_time": "2022-01-13T17:30:46.933189Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fn = test_df[['pred','user_id','prod_id']].rename(columns={'pred':'Purchase','user_id':'User_ID','prod_id':'Product_ID'})\n",
    "sub_fn.to_csv(OUT_PATH + 'submission_lgb_no_seg_var_optuna_newfeature_featureselect_220114.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86b1a4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### seg_var as segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db340269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T14:28:42.935264Z",
     "start_time": "2022-01-09T14:08:08.162639Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train8.copy()\n",
    "test_df  = test8 .copy()\n",
    "\n",
    "train_df['pred'] = np.nan\n",
    "test_df ['pred'] = np.nan\n",
    "\n",
    "# set reg options\n",
    "if OBJECTIVE=='regression':\n",
    "    ML = lgb.LGBMRegressor\n",
    "elif OBJECTIVE=='binary':\n",
    "    ML = lgb.LGBMClassifier\n",
    "\n",
    "seg_x = 'segment_1'\n",
    "seg   = np.array([str(x) for x in sorted(train_df[seg_x].astype(int).unique())])\n",
    "\n",
    "train_ls, test_ls, model_ls = [],[],[]\n",
    "\n",
    "iter = 0\n",
    "pbar = tqdm(seg)\n",
    "for seg_value in pbar:\n",
    "    pbar.set_description(f'fitting... ({seg_value})')\n",
    "    \n",
    "    iter += 1\n",
    "    start_time_seg = time.time()\n",
    "    \n",
    "    tr_sub_df = train_df[train_df[seg_x]==seg_value]\n",
    "    te_sub_df = test_df [test_df [seg_x]==seg_value]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # remove needless variable\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    drop_var = ['user_id','prod_id','target','segment_1','segment_2','prod_cat_1']\n",
    "\n",
    "    X_train = tr_sub_df[list(set(tr_sub_df.columns)-set(drop_var))]\n",
    "    X_test  = te_sub_df[list(set(te_sub_df.columns)-set(drop_var))]\n",
    "\n",
    "    y_train = tr_sub_df['target'].astype(float).values\n",
    "\n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "#     # feature importance\n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "#     model = ML(random_state = SEED)\n",
    "#     model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "#     feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "#                                    model.feature_importances_.astype(float)), \n",
    "#                                columns=['feature','imp']).sort_values(by='imp')\n",
    "\n",
    "#     reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "    X_train_new = X_train#[reduced_var]\n",
    "    X_test_new  = X_test #[reduced_var]\n",
    "    \n",
    "#     del model\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # modelling\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    study = optuna.create_study(direction='minimize', study_name='LGBM Regressor')\n",
    "    func = lambda trial: objective(trial, X_train_new, y_train, objective='regression', n_split=3)\n",
    "    study.optimize(func, n_trials=5)\n",
    "\n",
    "    model = ML(**study.best_params)\n",
    "    model.fit(X_train_new,y_train)\n",
    "\n",
    "    # seg별 predict값 넣기\n",
    "    tr_sub_df['pred'] = model.predict(X_train_new)\n",
    "    te_sub_df['pred'] = model.predict(X_test_new)\n",
    "    \n",
    "    train_ls.append(tr_sub_df)\n",
    "    test_ls .append(te_sub_df)\n",
    "    model_ls.append(model)\n",
    "    \n",
    "    end_time_seg = time.time()\n",
    "    running_time = (end_time_seg - start_time_seg)/60\n",
    "    \n",
    "    #print(f'({iter}/{len(seg)}) {seg_value} : {running_time:.2f} {datetime.datetime.now()}')\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ade336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T14:41:54.218522Z",
     "start_time": "2022-01-09T14:41:53.538483Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_ls, test_ls, model_ls\n",
    "\n",
    "# model_ls[0].get_params()\n",
    "\n",
    "train_fn = pd.concat(train_ls,axis=0).reset_index(drop=True)\n",
    "test_fn  = pd.concat(test_ls ,axis=0).reset_index(drop=True)\n",
    "\n",
    "# rmse_fn(train_fn['pred'],train_fn['target'])\n",
    "\n",
    "sub_fn = test_fn[['pred','user_id','prod_id']].rename(columns={'pred':'Purchase','user_id':'User_ID','prod_id':'Product_ID'})\n",
    "sub_fn.to_csv(OUT_PATH + 'submission_lgb_optuna_segment_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed8afc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3647a0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T15:10:36.889053Z",
     "start_time": "2022-01-09T15:10:36.881053Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_for_study(X, y, model):\n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.30, random_state=SEED)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        early_stopping_rounds=30,\n",
    "        eval_set=[(X_valid, y_valid)], \n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    yhat = model.predict(X_valid)\n",
    "    return metrics.mean_squared_error(y_valid, yhat, squared=True)\n",
    "\n",
    "def objective_cat(trial,X,y,cat_features,GPU_ENABLED=False):\n",
    "    '''\n",
    "    Objective function to tune a `CatBoostRegressor` model.\n",
    "    '''\n",
    "\n",
    "    params = {\n",
    "        'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n",
    "        'od_wait':trial.suggest_int('od_wait', 500, 2300),\n",
    "        'learning_rate' : trial.suggest_uniform('learning_rate',0.01, 1),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "        'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "        'random_strength': trial.suggest_uniform('random_strength',10,50),\n",
    "        'depth': trial.suggest_int('depth',1, 15),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
    "    }\n",
    "\n",
    "    if GPU_ENABLED:\n",
    "        params['task_type'] = 'GPU'\n",
    "        params['bootstrap_type'] = \"Poisson\"\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function='RMSE',\n",
    "        random_state=SEED,\n",
    "        cat_features=cat_features,\n",
    "        **params,\n",
    "    )\n",
    "    \n",
    "    return train_model_for_study(X, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81affc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T15:32:42.569878Z",
     "start_time": "2022-01-09T15:12:29.739508Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train8.copy()\n",
    "test_df  = test8 .copy()\n",
    "\n",
    "train_df['pred'] = np.nan\n",
    "test_df ['pred'] = np.nan\n",
    "\n",
    "\n",
    "seg_x = 'segment_1'\n",
    "seg   = np.array([str(x) for x in sorted(train_df[seg_x].astype(int).unique())])\n",
    "\n",
    "train_ls, test_ls, model_ls = [],[],[]\n",
    "\n",
    "iter = 0\n",
    "pbar = tqdm(seg)\n",
    "for seg_value in pbar:\n",
    "    pbar.set_description(f'fitting... ({seg_value})')\n",
    "    \n",
    "    iter += 1\n",
    "    start_time_seg = time.time()\n",
    "    \n",
    "    tr_sub_df = train_df[train_df[seg_x]==seg_value]\n",
    "    te_sub_df = test_df [test_df [seg_x]==seg_value]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # remove needless variable\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    drop_var = ['user_id','prod_id','target','segment_1','segment_2','prod_cat_1','pred']\n",
    "\n",
    "    X_train = tr_sub_df[list(set(tr_sub_df.columns)-set(drop_var))]\n",
    "    X_test  = te_sub_df[list(set(te_sub_df.columns)-set(drop_var))]\n",
    "\n",
    "    y_train = tr_sub_df['target'].astype(float).values\n",
    "\n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "#     # feature importance\n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "#     model = CatBoostRegressor(random_state = SEED)\n",
    "#     model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "#     feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "#                                    model.feature_importances_.astype(float)), \n",
    "#                                columns=['feature','imp']).sort_values(by='imp')\n",
    "\n",
    "#     reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "    X_train_new = X_train#[reduced_var]\n",
    "    X_test_new  = X_test #[reduced_var]\n",
    "    \n",
    "#     del model\n",
    "\n",
    "    cat_features = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [str]]\n",
    "    cat_features = list(set(X_train_new.columns) & set(cat_features))\n",
    "\n",
    "    num_features = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [int,float]]\n",
    "    num_features = list(set(X_train_new.columns) & set(num_features))\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # modelling\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    study_cat = optuna.create_study(direction='minimize', study_name='CatBoost Regressor')\n",
    "    func = lambda trial: objective_cat(trial, \n",
    "                                       X = X_train_new, \n",
    "                                       y = y_train, \n",
    "                                       cat_features = cat_features)\n",
    "    study_cat.optimize(func, n_trials=5)\n",
    "    \n",
    "    model = CatBoostRegressor(**study_cat.best_params, cat_features = cat_features)\n",
    "    model.fit(X_train_new,y_train)\n",
    "\n",
    "    # seg별 predict값 넣기\n",
    "    tr_sub_df['pred'] = model.predict(X_train_new)\n",
    "    te_sub_df['pred'] = model.predict(X_test_new)\n",
    "    \n",
    "    train_ls.append(tr_sub_df)\n",
    "    test_ls .append(te_sub_df)\n",
    "    model_ls.append(model)\n",
    "    \n",
    "    end_time_seg = time.time()\n",
    "    running_time = (end_time_seg - start_time_seg)/60\n",
    "    \n",
    "    #print(f'({iter}/{len(seg)}) {seg_value} : {running_time:.2f} {datetime.datetime.now()}')\n",
    "\n",
    "end_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "847px",
    "left": "0px",
    "top": "91px",
    "width": "360px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
