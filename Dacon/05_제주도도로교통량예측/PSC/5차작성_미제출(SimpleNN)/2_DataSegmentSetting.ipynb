{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd8ab9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Setting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8ed78",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11f6549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7acdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytimekr\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd55c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d MyPython/2_Dacon_JEJU/DAT/\n",
    "# unzip open.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010b57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fe = fm.FontEntry(fname='../NanumFont/NanumGothic.ttf',name='NanumGothic')\n",
    "fm.fontManager.ttflist.insert(0, fe)  # or append is fine\n",
    "mpl.rcParams['font.family'] = fe.name # = 'your custom ttf font name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc76fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n",
    "os.environ['PYTHONWARNINGS']='ignore::FutureWarning'\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "from pytimekr import pytimekr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# import datatable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19e70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution\n",
    "\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28c7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--',color=color)\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + directory)\n",
    "        \n",
    "def cnt(x):\n",
    "    vc = x.value_counts().sort_index()\n",
    "    res = pd.DataFrame({\n",
    "        'index' : vc.index,\n",
    "        'freq'  : vc.values,\n",
    "    })\n",
    "    res['rate'] = 100 * res['freq'] / res['freq'].sum()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615c7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# verbose=0로 만들어주는 함수\n",
    "# (참조) https://stackoverflow.com/questions/11130156/suppress-stdout-stderr-print-from-python-functions\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "    Python, i.e. will suppress all print, even if the print originates in a\n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f6905",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "User Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f377e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.MyModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb9a5c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b50351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-12 08:09:31.269485\n"
     ]
    }
   ],
   "source": [
    "DAT_PATH = \"../DAT/\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabeac42",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3ee76",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Load\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189900f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6878b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 11 s, total: 23 s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df7 = pd.read_parquet('../OUT/train_df7.parquet.gz')\n",
    "test_df7  = pd.read_parquet('../OUT/test_df7.parquet.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717cbae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Use all Covid feature or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57754a9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1) if use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644bcbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid 컬럼을 가져오기 위한 gubun 값들\n",
    "gubuns = [\n",
    "    '강원','검역','경기','경남','경북','광주','대구','대전','부산','서울',\n",
    "    '세종','울산','인천','전남','전북','제주','충남','충북','합계'\n",
    "]\n",
    "\n",
    "# gubun 값들이 컬럼에 포함된 경우 Covid 컬럼으로 가져옴\n",
    "covid_features = []\n",
    "for gubun in gubuns:\n",
    "    col = [col for col in train_df7.columns if col.find(gubun)>=0]\n",
    "    covid_features+=col\n",
    "    \n",
    "not_jeju_col = [col for col in covid_features if col.find('제주')<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c669bc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deathCnt_제주</th>\n",
       "      <th>defCnt_제주</th>\n",
       "      <th>incDec_제주</th>\n",
       "      <th>isolClearCnt_제주</th>\n",
       "      <th>isolIngCnt_제주</th>\n",
       "      <th>localOccCnt_제주</th>\n",
       "      <th>overFlowCnt_제주</th>\n",
       "      <th>qurRate_제주</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deathCnt_제주</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.079869</td>\n",
       "      <td>-0.080310</td>\n",
       "      <td>-0.160987</td>\n",
       "      <td>0.078447</td>\n",
       "      <td>0.513374</td>\n",
       "      <td>0.991190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>defCnt_제주</th>\n",
       "      <td>0.991191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151195</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>-0.154311</td>\n",
       "      <td>0.149801</td>\n",
       "      <td>0.498721</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incDec_제주</th>\n",
       "      <td>0.079869</td>\n",
       "      <td>0.151195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762447</td>\n",
       "      <td>-0.106272</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-0.052035</td>\n",
       "      <td>0.151319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolClearCnt_제주</th>\n",
       "      <td>-0.080310</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.762447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.070244</td>\n",
       "      <td>0.762797</td>\n",
       "      <td>-0.167131</td>\n",
       "      <td>0.008390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolIngCnt_제주</th>\n",
       "      <td>-0.160987</td>\n",
       "      <td>-0.154311</td>\n",
       "      <td>-0.106272</td>\n",
       "      <td>-0.070244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.106046</td>\n",
       "      <td>-0.077065</td>\n",
       "      <td>-0.154325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>localOccCnt_제주</th>\n",
       "      <td>0.078447</td>\n",
       "      <td>0.149801</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.762797</td>\n",
       "      <td>-0.106046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.054802</td>\n",
       "      <td>0.149927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overFlowCnt_제주</th>\n",
       "      <td>0.513374</td>\n",
       "      <td>0.498721</td>\n",
       "      <td>-0.052035</td>\n",
       "      <td>-0.167131</td>\n",
       "      <td>-0.077065</td>\n",
       "      <td>-0.054802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qurRate_제주</th>\n",
       "      <td>0.991190</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.151319</td>\n",
       "      <td>0.008390</td>\n",
       "      <td>-0.154325</td>\n",
       "      <td>0.149927</td>\n",
       "      <td>0.498690</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 deathCnt_제주  defCnt_제주  incDec_제주  isolClearCnt_제주  \\\n",
       "deathCnt_제주         1.000000   0.991191   0.079869        -0.080310   \n",
       "defCnt_제주           0.991191   1.000000   0.151195         0.008329   \n",
       "incDec_제주           0.079869   0.151195   1.000000         0.762447   \n",
       "isolClearCnt_제주    -0.080310   0.008329   0.762447         1.000000   \n",
       "isolIngCnt_제주      -0.160987  -0.154311  -0.106272        -0.070244   \n",
       "localOccCnt_제주      0.078447   0.149801   0.999996         0.762797   \n",
       "overFlowCnt_제주      0.513374   0.498721  -0.052035        -0.167131   \n",
       "qurRate_제주          0.991190   0.999998   0.151319         0.008390   \n",
       "\n",
       "                 isolIngCnt_제주  localOccCnt_제주  overFlowCnt_제주  qurRate_제주  \n",
       "deathCnt_제주          -0.160987        0.078447        0.513374    0.991190  \n",
       "defCnt_제주            -0.154311        0.149801        0.498721    0.999998  \n",
       "incDec_제주            -0.106272        0.999996       -0.052035    0.151319  \n",
       "isolClearCnt_제주      -0.070244        0.762797       -0.167131    0.008390  \n",
       "isolIngCnt_제주         1.000000       -0.106046       -0.077065   -0.154325  \n",
       "localOccCnt_제주       -0.106046        1.000000       -0.054802    0.149927  \n",
       "overFlowCnt_제주       -0.077065       -0.054802        1.000000    0.498690  \n",
       "qurRate_제주           -0.154325        0.149927        0.498690    1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation이 높지만 따로 처리하지 않음\n",
    "train_df7[[col for col in train_df7.columns if col.find('_제주')>=0]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e0953",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(2) if not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b968db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_jeju_col=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29149e8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Categorical & Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61ee76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = train_df7\\\n",
    "    .drop(['id','target','base_date']+not_jeju_col,axis=1)\\\n",
    "    .select_dtypes(include=[object])\\\n",
    "    .columns.tolist()\n",
    "\n",
    "num_features = [x for x in train_df7.drop(['id','target','base_date']+not_jeju_col,axis=1).columns \n",
    "                if x not in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59bed83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_features = 'segment'\n",
    "\n",
    "del_features = ['day','start_node_name','end_node_name','road_name'] # 'wd','clfmAbbrCd'\n",
    "\n",
    "new_cat_features = [col for col in cat_features if col not in [segment_features]+del_features]\n",
    "new_num_features = [col for col in num_features if col not in [segment_features]+del_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d97222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical   features : 32\n",
      "categorical features : 15\n"
     ]
    }
   ],
   "source": [
    "print(f'numerical   features : {len(new_num_features)}')\n",
    "print(f'categorical features : {len(new_cat_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b8aab05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight_restricted         4\n",
       "road_type                 2\n",
       "start_turn_restricted     2\n",
       "end_turn_restricted       2\n",
       "weekday                   7\n",
       "weekend                   2\n",
       "holiday                   2\n",
       "address                   2\n",
       "stnId                     2\n",
       "rnQcflg                   2\n",
       "wd                       17\n",
       "ssQcflg                   3\n",
       "clfmAbbrCd               26\n",
       "tsQcflg                   2\n",
       "base_hour_3_interval      8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df7[new_cat_features].apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c89e96",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Make Modeling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71a052e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df7[[segment_features]+new_cat_features+new_num_features]\n",
    "y_train = train_df7.target\n",
    "\n",
    "X_test  = test_df7[[segment_features]+new_cat_features+new_num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea22a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_df7, test_df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621661f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b66c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:50<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train_oh = X_train.copy()\n",
    "X_test_oh  = X_test .copy()\n",
    "\n",
    "for col in tqdm(new_cat_features):\n",
    "    \n",
    "    # train, test의 모든 케이스에 대해서 onehot encoding을 위해, concat\n",
    "    tmp_data = pd.concat([\n",
    "        X_train_oh[[col]].assign(group='train'),\n",
    "        X_test_oh [[col]].assign(group='test'),\n",
    "    ],axis=0)\n",
    "    \n",
    "    # onehot encoding\n",
    "    oh_df = pd.get_dummies(tmp_data,columns=[col])\n",
    "    \n",
    "    # split train/test\n",
    "    tr_oh_df = oh_df[oh_df['group']=='train'].drop('group',axis=1)\n",
    "    te_oh_df = oh_df[oh_df['group']=='test' ].drop('group',axis=1)\n",
    "    \n",
    "    # 마지막 컬럼을 제거 -> multicollinearity 문제로 인해 컬럼 하나를 제거\n",
    "    tr_oh_df = tr_oh_df[tr_oh_df.columns[:-1]]\n",
    "    te_oh_df = te_oh_df[te_oh_df.columns[:-1]]\n",
    "    \n",
    "    # 기존컬럼 제거\n",
    "    X_train_oh.drop(col,axis=1,inplace=True)\n",
    "    X_test_oh .drop(col,axis=1,inplace=True)\n",
    "    \n",
    "    # one-hot encoding 컬럼 추가\n",
    "    X_train_oh = pd.concat([X_train_oh,tr_oh_df],axis=1)\n",
    "    X_test_oh  = pd.concat([X_test_oh ,te_oh_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18b9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4686836, 107), (291241, 107))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oh.shape, X_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b01a4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh.to_parquet('../OUT/X_train_oh.parquet.gz')\n",
    "X_test_oh .to_parquet('../OUT/X_test_oh.parquet.gz')\n",
    "y_train   .to_csv('../OUT/y_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c205f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kernel Dead로 인해, 실행 불가\n",
    "\n",
    "# print('(1) data concat')\n",
    "# tmp = pd.concat([\n",
    "#     X_train.assign(group='train'),\n",
    "#     X_test .assign(group='test'),\n",
    "# ],axis=0)\n",
    "\n",
    "# print('(2) get dummies')\n",
    "# tmp = pd.get_dummies(tmp, columns=new_cat_features)\n",
    "\n",
    "# print('(3) data split')\n",
    "# X_train_oh = tmp[tmp['group']=='train'].drop('group',axis=1)\n",
    "# X_test_oh  = tmp[tmp['group']=='test' ].drop('group',axis=1)\n",
    "\n",
    "# print('(4) save data')\n",
    "# X_train_oh.to_parquet('../OUT/X_train_oh.parquet.gz')\n",
    "# X_test_oh .to_parquet('../OUT/X_test_oh.parquet.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60433c0e",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08d1a0",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformation & Scaling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e2727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9ed0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def transformation(x,offset=1e-4,method=['log','sqrt','identity']):\n",
    "    #x = pd.Series(x)\n",
    "    \n",
    "    if method=='log':\n",
    "        res = np.log(x+offset)\n",
    "    elif method=='sqrt':\n",
    "        res = np.sqrt(x+offset)\n",
    "    elif method=='identity':\n",
    "        res = identity(x)\n",
    "    else:\n",
    "        raise('Unknown Transformation Method')\n",
    "    \n",
    "    return res\n",
    "\n",
    "def inverse_transformation(x,offset,method=['log','sqrt','identity']):\n",
    "    #x = pd.Series(x)\n",
    "    \n",
    "    if method=='log':\n",
    "        res = np.exp(x)-offset\n",
    "    elif method=='sqrt':\n",
    "        res = (x**2)-offset\n",
    "    elif method=='identity':\n",
    "        res = identity(x)\n",
    "    else:\n",
    "        raise('Unknown Transformation Method')\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140076f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Setting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4cf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Transformation method : identity(offset=0.0001)\n",
      "(2) Scaling method : MinMaxScaler\n"
     ]
    }
   ],
   "source": [
    "transform_method = ['identity','log','sqrt'][0]\n",
    "offset = 1e-4\n",
    "scaling_method = MinMaxScaler\n",
    "\n",
    "print(f'(1) Transformation method : {transform_method}(offset={offset})')\n",
    "print(f'(2) Scaling method : {scaling_method.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64894d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e330da",
   "metadata": {},
   "source": [
    "---\n",
    "# Save Segment Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh = pd.read_parquet('../OUT/X_train_oh.parquet.gz')\n",
    "X_test_oh  = pd.read_parquet('../OUT/X_test_oh.parquet.gz')\n",
    "y_train    = pd.read_csv('../OUT/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0654d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_features = 'segment'\n",
    "\n",
    "tr_segment = X_train_oh[segment_features].value_counts().sort_values(ascending=False).index\n",
    "te_segment = X_test_oh [segment_features].value_counts().index\n",
    "segment    = [seg for seg in tr_segment if seg in te_segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9ab23de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [06:31<00:00, 15.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for seg in tqdm(segment):\n",
    "\n",
    "    X_tr = X_train_oh[X_train_oh[segment_features]==seg].drop(segment_features,axis=1).astype(float)\n",
    "    X_te = X_test_oh [X_test_oh [segment_features]==seg].drop(segment_features,axis=1).astype(float)\n",
    "    \n",
    "    y_tr = y_train   [X_train_oh[segment_features]==seg].astype(float)\n",
    "    \n",
    "    # transformation\n",
    "    y_tr = transformation(y_tr,offset=offset,method=transform_method)\n",
    "    \n",
    "    # scaling\n",
    "    for col in X_tr.columns:\n",
    "        scaler = scaling_method()\n",
    "        scaler.fit(np.array(X_tr[col]).reshape(-1,1))\n",
    "\n",
    "        X_tr[col] = scaler.transform(np.array(X_tr[col]).reshape(-1,1))\n",
    "        X_te[col] = scaler.transform(np.array(X_te[col]).reshape(-1,1))\n",
    "        \n",
    "    X_tr = pd.concat([X_tr,y_tr],axis=1)\n",
    "        \n",
    "    createFolder(f'../OUT/segment/{seg}')\n",
    "    \n",
    "    X_tr.to_csv(f'../OUT/segment/{seg}/train_df.csv',index=False)\n",
    "    X_te.to_csv(f'../OUT/segment/{seg}/test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563d9a1",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcc6fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "> start time : 2022-11-12 08:09:31.269485\n",
      ">   end time : 2022-11-12 08:21:50.915741\n",
      ">   run time : 0:12:19.646256\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "run_time = end_time-start_time\n",
    "\n",
    "print('='*45)\n",
    "print(f'> start time : {start_time}')\n",
    "print(f'>   end time : {end_time}')\n",
    "print(f'>   run time : {run_time:}')\n",
    "print('='*45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
