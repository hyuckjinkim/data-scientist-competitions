{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c5108c-b14d-4b09-8b1b-9a3c23c971d1",
   "metadata": {},
   "source": [
    "- gungu, dong 으로 segment -> test에만 있는 값이 있는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a48ac3-7a48-4762-a5a8-a8a9aba4399c",
   "metadata": {},
   "source": [
    "# Library Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bdb59bb-80d1-4ba7-836a-6bb4ca56f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Volumes/KHJ/Github/hyuckjinkim/lib-python')\n",
    "\n",
    "from base import gc_collect_all, setdiff\n",
    "from filesystem_utils import mkdir\n",
    "from graph import abline\n",
    "from data_prepare import (\n",
    "    get_holiday, reduce_mem_usage, delete_unique_columns,\n",
    "    TypeController, CategoricalQuantileCalculator,\n",
    "    GroupScaler, OneHotEncoder, InteractionTerm, TargetTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a70cac-bbfe-465a-b8af-329f4b2e0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_collect_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb18713-a4a8-4bf6-8a96-a343b827b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7240b4d-9ee9-4667-8bf2-a9d433360f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_columns(data):\n",
    "    unique_info = data.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    return unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30a8eb3-0c50-4b15-80a7-2dda14093e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    TARGET = ['사망자수','중상자수','경상자수','부상자수']\n",
    "    TARGET_TRANSFORMATION = 'identity'\n",
    "    N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840762e1-cec5-4f48-9b69-57a1084dce66",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c89e16-810b-453a-a5a3-622cadd4434e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255fdf1-8cad-4aa8-a6a7-d4e6c7b506e4",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1b8906-0cee-4382-b2e9-3db0b4661672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# train_additional_df = pd.read_csv('./data/external_open/countrywide_accident.csv',encoding='utf-8')\n",
    "# cctv_df = pd.read_csv('./data/external_open/대구 CCTV 정보.csv',encoding='cp949')\n",
    "# security_light_df = pd.read_csv('./data/external_open/대구 보안등 정보.csv',encoding='cp949',low_memory=False)\n",
    "# child_zone_df = pd.read_csv('./data/external_open/대구 어린이 보호 구역 정보.csv',encoding='cp949')\n",
    "# parking_df = pd.read_csv('./data/external_open/대구 주차장 정보.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da27bef-b3e6-456b-85f9-bd16d00c3558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>사고일시</th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>시군구</th>\n",
       "      <th>도로형태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사고유형 - 세부분류</th>\n",
       "      <th>법규위반</th>\n",
       "      <th>가해운전자 차종</th>\n",
       "      <th>가해운전자 성별</th>\n",
       "      <th>가해운전자 연령</th>\n",
       "      <th>가해운전자 상해정도</th>\n",
       "      <th>피해운전자 차종</th>\n",
       "      <th>피해운전자 성별</th>\n",
       "      <th>피해운전자 연령</th>\n",
       "      <th>피해운전자 상해정도</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCIDENT_00000</td>\n",
       "      <td>2019-01-01 00</td>\n",
       "      <td>화요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 중구 대신동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>길가장자리구역통행중</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>여</td>\n",
       "      <td>51세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>보행자</td>\n",
       "      <td>여</td>\n",
       "      <td>70세</td>\n",
       "      <td>중상</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID           사고일시   요일 기상상태           시군구      도로형태 노면상태  사고유형  \\\n",
       "0  ACCIDENT_00000  2019-01-01 00  화요일   맑음  대구광역시 중구 대신동  단일로 - 기타   건조  차대사람   \n",
       "\n",
       "  사고유형 - 세부분류     법규위반 가해운전자 차종 가해운전자 성별 가해운전자 연령 가해운전자 상해정도 피해운전자 차종  \\\n",
       "0  길가장자리구역통행중  안전운전불이행       승용        여      51세       상해없음      보행자   \n",
       "\n",
       "  피해운전자 성별 피해운전자 연령 피해운전자 상해정도  사망자수  중상자수  경상자수  부상자수  ECLO  \n",
       "0        여      70세         중상     0     1     0     0     5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>사고일시</th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>시군구</th>\n",
       "      <th>도로형태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사고유형 - 세부분류</th>\n",
       "      <th>법규위반</th>\n",
       "      <th>가해운전자 차종</th>\n",
       "      <th>가해운전자 성별</th>\n",
       "      <th>가해운전자 연령</th>\n",
       "      <th>가해운전자 상해정도</th>\n",
       "      <th>피해운전자 차종</th>\n",
       "      <th>피해운전자 성별</th>\n",
       "      <th>피해운전자 연령</th>\n",
       "      <th>피해운전자 상해정도</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39608</th>\n",
       "      <td>ACCIDENT_39608</td>\n",
       "      <td>2021-12-31 23</td>\n",
       "      <td>금요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 서구 비산동</td>\n",
       "      <td>단일로 - 지하차도(도로)내</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>측면충돌</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>27세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>33세</td>\n",
       "      <td>경상</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID           사고일시   요일 기상상태           시군구             도로형태  \\\n",
       "39608  ACCIDENT_39608  2021-12-31 23  금요일   맑음  대구광역시 서구 비산동  단일로 - 지하차도(도로)내   \n",
       "\n",
       "      노면상태 사고유형 사고유형 - 세부분류     법규위반 가해운전자 차종 가해운전자 성별 가해운전자 연령 가해운전자 상해정도  \\\n",
       "39608   건조  차대차        측면충돌  안전운전불이행       승용        남      27세       상해없음   \n",
       "\n",
       "      피해운전자 차종 피해운전자 성별 피해운전자 연령 피해운전자 상해정도  사망자수  중상자수  경상자수  부상자수  ECLO  \n",
       "39608       승용        남      33세         경상     0     0     2     0     6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head(1))\n",
    "display(train_df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ead0f9-6e35-4c76-91af-683faf828f85",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20d5cd-eb46-40dd-a964-30c78ddb663e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bed0f1b-5c2a-4b66-82fd-0348644a3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week(weekend):\n",
    "    week = 1\n",
    "    week_list = []\n",
    "    for weekend in weekend:\n",
    "        if weekend==1:\n",
    "            week+=1\n",
    "        week_list.append(week)\n",
    "    return week_list\n",
    "\n",
    "def split_location(location):\n",
    "    error_return = [np.nan,np.nan]\n",
    "    if location is np.nan:\n",
    "        return error_return\n",
    "    sido, gungu, dong = location.split(' ')\n",
    "    dong = dong.split('동')[0] + '동'\n",
    "    if sido not in ['대구광역시','대구']:\n",
    "        print(f\"{sido}: sido is not '대구광역시'.\")\n",
    "        return error_return\n",
    "    if gungu[-1:] not in ['구','군']:\n",
    "        print(f\"{gungu}: gungu is not '~구' or '~군'\")\n",
    "        return error_return\n",
    "    return [gungu,dong]\n",
    "\n",
    "def preprocessing(data):\n",
    "    d = data.copy()\n",
    "    \n",
    "    # (1) test data에 없는 컬럼 제거\n",
    "    no_columns_in_test = ['사고유형 - 세부분류','법규위반','가해운전자 차종','가해운전자 성별','가해운전자 연령',\n",
    "                          '가해운전자 상해정도','피해운전자 차종','피해운전자 성별','피해운전자 연령','피해운전자 상해정도']\n",
    "    no_columns_in_test = list(set(d.columns)&set(no_columns_in_test))\n",
    "    d.drop(no_columns_in_test,axis=1,inplace=True)\n",
    "    \n",
    "    # (1) 시군구: 시/군구/동 -> 군구/동\n",
    "    d[['gungu','dong']] = np.stack(d['시군구'].apply(split_location))\n",
    "    d.drop('시군구',axis=1,inplace=True)\n",
    "    \n",
    "    # (2) 도로형태\n",
    "    d[['roadtype','roadtype_detail']] = np.stack(d['도로형태'].str.split(' - '))\n",
    "    d['is_parking'] = d['roadtype'].map({'단일로':0,'교차로':0,'기타':np.nan,'주차장':1,'미분류':np.nan})\n",
    "    d['is_road']    = d['roadtype'].map({'단일로':1,'교차로':1,'기타':np.nan,'주차장':np.nan,'미분류':np.nan})\n",
    "    d.drop('도로형태',axis=1,inplace=True)\n",
    "    for col in ['is_parking','is_road']:\n",
    "        d[col] = d[col].fillna(0)\n",
    "\n",
    "    return d\n",
    "\n",
    "def feature_engineering(data):\n",
    "    d = data.copy()\n",
    "    date = d['사고일시'].apply(lambda x: datetime.datetime.strptime(str(x),'%Y-%m-%d %H'))\n",
    "    \n",
    "    # (1) date columns\n",
    "    d['year']       = date.dt.year\n",
    "    d['month']      = date.dt.month\n",
    "    d['day']        = date.dt.day\n",
    "    d['hour']       = date.dt.hour\n",
    "    d['weekday']    = date.dt.weekday\n",
    "    d['weekend']    = date.dt.weekday.isin([5,6]).astype(int)\n",
    "    d['week']       = add_week(d['weekend'])\n",
    "\n",
    "    # (2) is holiday & is dayoff\n",
    "    holiday_list = get_holiday(d['year'].unique())\n",
    "    d['is_holiday'] = date.isin(holiday_list).astype(int)\n",
    "    d['is_dayoff']  = ((d.is_holiday==1) | (d.weekend==1)).astype(int)\n",
    "    \n",
    "    # (3) unuse features\n",
    "    unuse_features = ['ID','사고일시','요일','ECLO'] #'사망자수','중상자수','경상자수','부상자수'\n",
    "    unuse_features = list(set(d.columns)&set(unuse_features))\n",
    "    d.drop(columns=unuse_features,inplace=True)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e6edf9-7420-47f8-95af-0ba79d38fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessing(train_df)\n",
    "train_df = feature_engineering(train_df)\n",
    "\n",
    "test_df = preprocessing(test_df)\n",
    "test_df = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90ca321-1e3f-4a70-9078-d28f5066922b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>gungu</th>\n",
       "      <th>dong</th>\n",
       "      <th>roadtype</th>\n",
       "      <th>roadtype_detail</th>\n",
       "      <th>is_parking</th>\n",
       "      <th>is_road</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_dayoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>중구</td>\n",
       "      <td>대신동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흐림</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>달서구</td>\n",
       "      <td>감삼동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>수성구</td>\n",
       "      <td>두산동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>북구</td>\n",
       "      <td>복현동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>동구</td>\n",
       "      <td>신암동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  기상상태 노면상태  사고유형  사망자수  중상자수  경상자수  부상자수 gungu dong roadtype roadtype_detail  \\\n",
       "0   맑음   건조  차대사람     0     1     0     0    중구  대신동      단일로              기타   \n",
       "1   흐림   건조  차대사람     0     0     1     0   달서구  감삼동      단일로              기타   \n",
       "2   맑음   건조  차대사람     0     0     1     0   수성구  두산동      단일로              기타   \n",
       "3   맑음   건조   차대차     0     1     0     0    북구  복현동      단일로              기타   \n",
       "4   맑음   건조   차대차     0     0     1     0    동구  신암동      단일로              기타   \n",
       "\n",
       "   is_parking  is_road  year  month  day  hour  weekday  weekend  week  \\\n",
       "0         0.0      1.0  2019      1    1     0        1        0     1   \n",
       "1         0.0      1.0  2019      1    1     0        1        0     1   \n",
       "2         0.0      1.0  2019      1    1     1        1        0     1   \n",
       "3         0.0      1.0  2019      1    1     2        1        0     1   \n",
       "4         0.0      1.0  2019      1    1     4        1        0     1   \n",
       "\n",
       "   is_holiday  is_dayoff  \n",
       "0           1          1  \n",
       "1           1          1  \n",
       "2           0          0  \n",
       "3           0          0  \n",
       "4           0          0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef834de-0b2f-4993-ab69-dcef4f6cac3b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb299a8-b2b3-4d14-a576-a156cc367525",
   "metadata": {},
   "source": [
    "## Merge Addtional Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da82a469-07ae-4506-9d94-068b8be89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cctv_info(data):\n",
    "    d = data.copy()\n",
    "\n",
    "    cctv_df = pd.read_csv('./data/external_open/대구 CCTV 정보.csv',encoding='cp949')\n",
    "\n",
    "    # 다사읍 -> 다사읍동\n",
    "    cctv_df['소재지지번주소'] = cctv_df['소재지지번주소'].str.replace(' 다사읍 ',' 다사읍동 ')\n",
    "\n",
    "    # 수기로 수정 (1)\n",
    "    raw = ['대구광역시 다사읍동 세천리 1684-4','대구광역시 가창면 삼산리 산 327-9']\n",
    "    new = ['대구광역시 달성군 다사읍동 세천리 1684-4','대구광역시 달성군 가창면 삼산리 산 327-9']\n",
    "    for r,n in zip(raw,new):\n",
    "        loc = cctv_df['소재지지번주소'] == r\n",
    "        cctv_df.loc[loc, '소재지지번주소'] = n\n",
    "\n",
    "    # 수기로 수정 (2)\n",
    "    raw = ['대구광역시 중구 종로 17','대구광역시 중구 국채보상로 713']\n",
    "    new = ['대구 중구 남성로동 92','대구 중구 동인동4가 89-1']\n",
    "    for r,n in zip(raw,new):\n",
    "        loc = (cctv_df['소재지지번주소'].isnull()) & (cctv_df['소재지도로명주소']==r)\n",
    "        cctv_df.loc[loc, '소재지지번주소'] = n\n",
    "\n",
    "    cctv_df[['gungu','dong']] = [[np.nan,np.nan] if address is np.nan else\n",
    "                                 split_location(' '.join(address.split(' ')[:3]))\n",
    "                                 for address in cctv_df['소재지지번주소']]\n",
    "\n",
    "    # 단속구분별 cctv 수\n",
    "    for v in [1,2,4]:\n",
    "        colname = f'n_cctv_단속구분{v}'\n",
    "        sub = cctv_df[cctv_df['단속구분']==v]\n",
    "        n_cctv = sub.groupby(['gungu','dong']).size().reset_index().rename(columns={0:colname})\n",
    "        d = pd.merge(d,n_cctv,how='left',on=['gungu','dong'])\n",
    "        d[colname].fillna(0,inplace=True)\n",
    "\n",
    "    # 도로노선방향별 cctv 수\n",
    "    for v in [1,2,3]:\n",
    "        colname = f'n_cctv_도로노선방향{v}'\n",
    "        sub = cctv_df[cctv_df['도로노선방향']==v]\n",
    "        n_cctv = sub.groupby(['gungu','dong']).size().reset_index().rename(columns={0:colname})\n",
    "        d = pd.merge(d,n_cctv,how='left',on=['gungu','dong'])\n",
    "        d[colname].fillna(0,inplace=True)\n",
    "\n",
    "    # 'n_cctv_도로노선방향3', 'n_cctv_단속구분4'는 correlation이 100%임\n",
    "    d.drop('n_cctv_도로노선방향3',axis=1,inplace=True)\n",
    "\n",
    "    # 제한속도 최소/최대/평균\n",
    "    limit_speed = cctv_df\\\n",
    "        .groupby(['gungu','dong'])['제한속도']\\\n",
    "        .agg(min=np.nanmin,max=np.nanmax,avg=np.nanmean)\\\n",
    "        .add_prefix('limit_speed_')\\\n",
    "        .reset_index()\n",
    "    d = pd.merge(d,limit_speed,how='left',on=['gungu','dong'])\n",
    "\n",
    "    # 설치연도 최소/최대/평균\n",
    "    installation_year = cctv_df\\\n",
    "        .groupby(['gungu','dong'])['설치연도']\\\n",
    "        .agg(min=np.nanmin,max=np.nanmax,avg=np.nanmean)\\\n",
    "        .add_prefix('installation_year_')\\\n",
    "        .reset_index()\n",
    "    d = pd.merge(d,installation_year,how='left',on=['gungu','dong'])\n",
    "\n",
    "    # 보호구역구분: 1,2인 경우 보호구역으로 보고, 나머지(99,nan)는 보호구역이 아닌 것으로 봄\n",
    "    cctv_df['is_protectzone'] = np.where(cctv_df['보호구역구분'].isin([1,2]),1,0)\n",
    "    protectzone = cctv_df.groupby(['gungu','dong'])['is_protectzone'].agg(n_protectzone=np.nansum).reset_index()\n",
    "    d = pd.merge(d,protectzone,how='left',on=['gungu','dong'])\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd515ff0-faba-409e-ae54-24da77e80d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_cctv_info(train_df)\n",
    "test_df  = add_cctv_info(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28958b61-d0d7-4d95-a7f7-20c9895d250d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c57eeb-31b5-4922-8cab-ea829b7377c2",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7ccdd4-127e-4ba3-9de2-ebe97e92ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = TargetTransform(func=CFG.TARGET_TRANSFORMATION, offset=1)\n",
    "train_df[CFG.TARGET] = target_transform.fit_transform(\n",
    "    target=train_df[CFG.TARGET],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a96c216-9764-4c5e-9a0b-76e2c8c8c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>gungu</th>\n",
       "      <th>dong</th>\n",
       "      <th>roadtype</th>\n",
       "      <th>roadtype_detail</th>\n",
       "      <th>is_parking</th>\n",
       "      <th>is_road</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_dayoff</th>\n",
       "      <th>n_cctv_단속구분1</th>\n",
       "      <th>n_cctv_단속구분2</th>\n",
       "      <th>n_cctv_단속구분4</th>\n",
       "      <th>n_cctv_도로노선방향1</th>\n",
       "      <th>n_cctv_도로노선방향2</th>\n",
       "      <th>limit_speed_min</th>\n",
       "      <th>limit_speed_max</th>\n",
       "      <th>limit_speed_avg</th>\n",
       "      <th>installation_year_min</th>\n",
       "      <th>installation_year_max</th>\n",
       "      <th>installation_year_avg</th>\n",
       "      <th>n_protectzone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>중구</td>\n",
       "      <td>대신동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흐림</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>달서구</td>\n",
       "      <td>감삼동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2015.333333</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>수성구</td>\n",
       "      <td>두산동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2018.250000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>북구</td>\n",
       "      <td>복현동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2018.300000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맑음</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>동구</td>\n",
       "      <td>신암동</td>\n",
       "      <td>단일로</td>\n",
       "      <td>기타</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2018.250000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  기상상태 노면상태  사고유형  사망자수  중상자수  경상자수  부상자수 gungu dong roadtype roadtype_detail  \\\n",
       "0   맑음   건조  차대사람   0.0   1.0   0.0   0.0    중구  대신동      단일로              기타   \n",
       "1   흐림   건조  차대사람   0.0   0.0   1.0   0.0   달서구  감삼동      단일로              기타   \n",
       "2   맑음   건조  차대사람   0.0   0.0   1.0   0.0   수성구  두산동      단일로              기타   \n",
       "3   맑음   건조   차대차   0.0   1.0   0.0   0.0    북구  복현동      단일로              기타   \n",
       "4   맑음   건조   차대차   0.0   0.0   1.0   0.0    동구  신암동      단일로              기타   \n",
       "\n",
       "   is_parking  is_road  year  month  day  hour  weekday  weekend  week  \\\n",
       "0         0.0      1.0  2019      1    1     0        1        0     1   \n",
       "1         0.0      1.0  2019      1    1     0        1        0     1   \n",
       "2         0.0      1.0  2019      1    1     1        1        0     1   \n",
       "3         0.0      1.0  2019      1    1     2        1        0     1   \n",
       "4         0.0      1.0  2019      1    1     4        1        0     1   \n",
       "\n",
       "   is_holiday  is_dayoff  n_cctv_단속구분1  n_cctv_단속구분2  n_cctv_단속구분4  \\\n",
       "0           1          1           0.0           1.0           4.0   \n",
       "1           1          1           1.0           3.0           8.0   \n",
       "2           0          0           2.0           2.0           0.0   \n",
       "3           0          0           2.0           8.0           0.0   \n",
       "4           0          0           2.0          10.0           0.0   \n",
       "\n",
       "   n_cctv_도로노선방향1  n_cctv_도로노선방향2  limit_speed_min  limit_speed_max  \\\n",
       "0             0.0             1.0              0.0             40.0   \n",
       "1             3.0             1.0              0.0             60.0   \n",
       "2             3.0             1.0             50.0             60.0   \n",
       "3             7.0             3.0             30.0             50.0   \n",
       "4             9.0             3.0             30.0             50.0   \n",
       "\n",
       "   limit_speed_avg  installation_year_min  installation_year_max  \\\n",
       "0              8.0                 2006.0                 2021.0   \n",
       "1             17.5                 2008.0                 2021.0   \n",
       "2             55.0                 2014.0                 2021.0   \n",
       "3             38.0                 2014.0                 2021.0   \n",
       "4             40.0                 2016.0                 2020.0   \n",
       "\n",
       "   installation_year_avg  n_protectzone  \n",
       "0            2013.000000            2.0  \n",
       "1            2015.333333            2.0  \n",
       "2            2018.250000            0.0  \n",
       "3            2018.300000            4.0  \n",
       "4            2018.250000            7.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68a6f8-88ec-429e-ab23-d4218a52f9a9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500d92-ceb7-4756-a01d-cf9f1fad12cd",
   "metadata": {},
   "source": [
    "## Quantile values of Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42419a99-dbbc-4ce0-8e90-dee0c49304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['기상상태','노면상태','사고유형','gungu','dong','roadtype','roadtype_detail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbe37b2-df19-4af1-bae5-f295cafd063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Get quantiles of target by categorical features (depth=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset: dong + roadtype + roadtype_detail: 100%|█| 63/63 [00:08<00:00,  7.61it/s\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "calculator = CategoricalQuantileCalculator()\n",
    "calculator.fit(\n",
    "    data=train_df,\n",
    "    test_data=test_df,\n",
    "    target_feature=CFG.TARGET,\n",
    "    cat_features=cat_features,\n",
    "    subset_depth=3,#len(cat_features),\n",
    ")\n",
    "train_df = calculator.transform(train_df)\n",
    "test_df  = calculator.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba06f0b-fb11-429d-84fd-e09ecab5513d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f9cc0-b95e-4cb2-8275-3eda3ca7c59e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Group Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c73b13-5b9c-4176-8b98-25c0f8d8731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = GroupScaler(scaler=MinMaxScaler())\n",
    "# scaler.fit(\n",
    "#     data=train_df,\n",
    "#     group=CFG.KEY_FEATURES,\n",
    "#     num_features=num_features,\n",
    "# )\n",
    "# train_df = scaler.transform(train_df)\n",
    "# test_df  = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5501-d743-4b61-b08f-bea7ba25fb4c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a416f8-6400-4a57-8154-e8af0288a10e",
   "metadata": {},
   "source": [
    "## Memory Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa734b45-57ef-49e9-8044-451f63501599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, _ = reduce_mem_usage(train_df,verbose=False)\n",
    "test_df , _ = reduce_mem_usage(test_df ,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cee81-c0c5-4166-ab41-62d72643f678",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2710990-e8f8-4bd9-812d-05ce84939319",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d87be0e-2e01-41de-bdb0-cfda55e5080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/codeshare/6499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13227738-4a69-483a-8e5c-a21bd33524b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66785587/how-do-i-use-validation-sets-on-multioutputregressor-for-xgbregressor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.multioutput import _fit_estimator\n",
    "from sklearn.utils.validation import _check_fit_params\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.utils.fixes import delayed\n",
    "from joblib import Parallel\n",
    "\n",
    "class CustomMultiOutputRegressor(MultiOutputRegressor):\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\" Fit the model to data.\n",
    "        Fit a separate model for each output variable.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data.\n",
    "        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n",
    "            Multi-output targets. An indicator matrix turns on multilabel\n",
    "            estimation.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Only supported if the underlying regressor supports sample\n",
    "            weights.\n",
    "        **fit_params : dict of string -> object\n",
    "            Parameters passed to the ``estimator.fit`` method of each step.\n",
    "            .. versionadded:: 0.23\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self.estimator, \"fit\"):\n",
    "            raise ValueError(\"The base estimator should implement\"\n",
    "                             \" a fit method\")\n",
    "\n",
    "        X, y = self._validate_data(X, y,\n",
    "                                   force_all_finite=False,\n",
    "                                   multi_output=True, accept_sparse=True)\n",
    "\n",
    "        if is_classifier(self):\n",
    "            check_classification_targets(y)\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            raise ValueError(\"y must have at least two dimensions for \"\n",
    "                             \"multi-output regression but has only one.\")\n",
    "\n",
    "        if (sample_weight is not None and\n",
    "                not has_fit_parameter(self.estimator, 'sample_weight')):\n",
    "            raise ValueError(\"Underlying estimator does not support\"\n",
    "                             \" sample weights.\")\n",
    "\n",
    "        fit_params_validated = _check_fit_params(X, fit_params)\n",
    "            \n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_estimator)(\n",
    "                self.estimator, np.array(X), y[:, i], sample_weight,\n",
    "                **fit_params_validated,\n",
    "            ) \n",
    "            for i in range(y.shape[1])\n",
    "        )\n",
    "        return self\n",
    "\n",
    "class CustomEvalsetMultiOutputRegressor(MultiOutputRegressor):\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\" Fit the model to data.\n",
    "        Fit a separate model for each output variable.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data.\n",
    "        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n",
    "            Multi-output targets. An indicator matrix turns on multilabel\n",
    "            estimation.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Only supported if the underlying regressor supports sample\n",
    "            weights.\n",
    "        **fit_params : dict of string -> object\n",
    "            Parameters passed to the ``estimator.fit`` method of each step.\n",
    "            .. versionadded:: 0.23\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self.estimator, \"fit\"):\n",
    "            raise ValueError(\"The base estimator should implement\"\n",
    "                             \" a fit method\")\n",
    "\n",
    "        X, y = self._validate_data(X, y,\n",
    "                                   force_all_finite=False,\n",
    "                                   multi_output=True, accept_sparse=True)\n",
    "\n",
    "        if is_classifier(self):\n",
    "            check_classification_targets(y)\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            raise ValueError(\"y must have at least two dimensions for \"\n",
    "                             \"multi-output regression but has only one.\")\n",
    "\n",
    "        if (sample_weight is not None and\n",
    "                not has_fit_parameter(self.estimator, 'sample_weight')):\n",
    "            raise ValueError(\"Underlying estimator does not support\"\n",
    "                             \" sample weights.\")\n",
    "\n",
    "        fit_params_validated = _check_fit_params(X, fit_params)\n",
    "        [(X_test, Y_test)] = fit_params_validated.pop('eval_set')\n",
    "            \n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_estimator)(\n",
    "                self.estimator, np.array(X), y[:, i], sample_weight,\n",
    "                **fit_params_validated,\n",
    "                eval_set=[(np.array(X_test), np.array(Y_test)[:, i])] #.iloc로 수정\n",
    "            ) \n",
    "            for i in range(y.shape[1])\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "class MultiOutputLGBMRegressor(CustomEvalsetMultiOutputRegressor):\n",
    "    def __init__(self, estimator):\n",
    "        super().__init__(estimator)\n",
    "\n",
    "class MultiOutputXGBRegressor(CustomEvalsetMultiOutputRegressor):\n",
    "    def __init__(self, estimator):\n",
    "        super().__init__(estimator)\n",
    "        \n",
    "class MultiOutputRandomForestRegressor(CustomMultiOutputRegressor):\n",
    "    def __init__(self, estimator):\n",
    "        super().__init__(estimator)\n",
    "\n",
    "class MultiOutputHistGradientBoostingRegressor(CustomMultiOutputRegressor):\n",
    "    def __init__(self, estimator):\n",
    "        super().__init__(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41254e64-c614-4cab-81bc-0bc4430d5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return mean_squared_error(y_true=y_true,y_pred=y_pred)**0.5\n",
    "\n",
    "def WeightedMultiRMSE(y_true, y_pred, weight):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    weight = np.array(weight)\n",
    "    assert y_true.shape==y_pred.shape, \\\n",
    "        \"prediction shape is not correct. y_true: {}, y_pred: {}\".format(y_true.shape,y_pred.shape)\n",
    "    assert y_true.shape[1]==len(weight), \\\n",
    "        \"weight shape is not correct. y_true: {}, weight: {}\".format(y_true.shape,len(weight))\n",
    "    if sum(weight)!=1:\n",
    "        weight = weight / sum(weight)\n",
    "    \n",
    "    rmse_list = []\n",
    "    for k in range(y_true.shape[1]):\n",
    "        rmse = mean_squared_error(y_true[:,k],y_pred[:,k])**0.5\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    return rmse_list @ weight\n",
    "\n",
    "def WeightedMultiRMSLE(y_true, y_pred, weight):\n",
    "    return WeightedMultiRMSE(np.log1p(y_true),np.log1p(y_pred),weight)\n",
    "    \n",
    "def RMSLE(y_true, y_pred):\n",
    "    log_true = np.log1p(y_true)\n",
    "    log_pred = np.log1p(y_pred)\n",
    "    squared_error = (log_true-log_pred)**2\n",
    "    return np.sqrt(np.mean(squared_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09b10a50-1e02-460e-a736-29677c33d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 업데이트버전\n",
    "#  (1) stacking 추가\n",
    "#  (2) LGBM에 sample_weight 추가\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 hyperparameters,\n",
    "                 weight='balanced',\n",
    "                 inverse_transform=None,\n",
    "                 eval_metric=None,\n",
    "                 method='ensemble',\n",
    "                 use_weightedsum_in_stacking=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        \n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.weight = weight\n",
    "        self.inverse_transform = inverse_transform\n",
    "        self.eval_metric = RMSE if eval_metric is None else eval_metric\n",
    "        self.method = method\n",
    "        self.use_weightedsum_in_stacking = use_weightedsum_in_stacking\n",
    "        \n",
    "        self._get_regressors()\n",
    "        self._get_regressors_name()\n",
    "        \n",
    "        if use_weightedsum_in_stacking:\n",
    "            self.stacking_feature = 'pred'\n",
    "        else:\n",
    "            self.stacking_feature = [f'pred{i+1}' for i in range(len(self.regressors))]\n",
    "            \n",
    "    def _get_regressors(self):\n",
    "        max_depth = 9\n",
    "        n_jobs = -1\n",
    "        cat_loss_function = 'MultiRMSE' # 'RMSE','MAE'\n",
    "        cat_eval_metric = 'MultiRMSE'   # 'RMSE','MAE'\n",
    "        lgb_metric = 'rmse' # 'rmse','mean_absolute_error'\n",
    "        \n",
    "        params_ridge = {\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : RepeatedKFold(n_splits=5, n_repeats=3, \n",
    "                                 random_state=self.hyperparameters['random_state']),\n",
    "        }\n",
    "        \n",
    "        params_lasso = {\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : RepeatedKFold(n_splits=5, n_repeats=3,\n",
    "                                 random_state=self.hyperparameters['random_state']),\n",
    "            'n_jobs' : n_jobs,\n",
    "            #'max_iter' : 30000,\n",
    "            'tol' : 0.001,\n",
    "        }\n",
    "        \n",
    "        params_elasticnet = {\n",
    "            'l1_ratio' : np.arange(0.1, 1, 0.1),\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 1.0, 10.0, 100.0],\n",
    "            'cv' : RepeatedKFold(n_splits=5, n_repeats=3,\n",
    "                                 random_state=self.hyperparameters['random_state']),\n",
    "            'n_jobs' : n_jobs,\n",
    "            #'max_iter' : 30000,\n",
    "            'tol' : 0.001,\n",
    "        }\n",
    "        \n",
    "        params_catboost1 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "            #'l2_leaf_reg' : 1,\n",
    "        }\n",
    "        \n",
    "        params_catboost2 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            #'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 3,\n",
    "        }\n",
    "        \n",
    "        params_catboost3 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'SymmetricTree', # 'Lossguide','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            #'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 1,\n",
    "        }\n",
    "        \n",
    "        params_catboost4 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'iterations' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'loss_function' : cat_loss_function,\n",
    "            #'loss_function' : cat_loss_function, 'eval_metric' : cat_eval_metric,\n",
    "            'grow_policy' : 'Depthwise', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "            'l2_leaf_reg' : 1,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm1 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm2 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm3 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm4 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm5 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm6 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 1,\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm7 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'reg_lambda' : 1,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_lightgbm8 = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['iterations'],\n",
    "            'early_stopping_round' : self.hyperparameters['early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['learning_rate'],\n",
    "            'objective' : 'regression',\n",
    "            'metric' : lgb_metric,\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'reg_alpha' : 3,\n",
    "            'reg_lambda' : 3,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_xgboost = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'n_estimators' : self.hyperparameters['xgb_iterations'],\n",
    "            'early_stopping_rounds' : self.hyperparameters['xgb_early_stopping_rounds'],\n",
    "            'learning_rate' : self.hyperparameters['xgb_learning_rate'],\n",
    "            'objective' : 'reg:squarederror',#'reg:absoluteerror',\n",
    "            'verbosity' : 0,\n",
    "            'max_depth': self.hyperparameters['max_depth'],\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        # params_extratrees = {\n",
    "        #     'random_state' : self.hyperparameters['random_state'],\n",
    "        #     'n_estimators' : self.hyperparameters['extratrees_iterations'],\n",
    "        #     'criterion' : 'absolute_error',\n",
    "        #     'verbose' : 0,\n",
    "        #     'max_depth' : self.hyperparameters['max_depth'],\n",
    "        #     'n_jobs' : n_jobs,\n",
    "        # }\n",
    "        \n",
    "        params_hgb = {\n",
    "            'random_state' : self.hyperparameters['random_state'],\n",
    "            'max_iter' : self.hyperparameters['hgb_iterations'],\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : self.hyperparameters['max_depth'],\n",
    "        }\n",
    "        \n",
    "        self.regressors = [\n",
    "            # LinearRegression(),\n",
    "            # RidgeCV(**params_ridge),\n",
    "            # LassoCV(**params_lasso),\n",
    "            # ElasticNetCV(**params_elasticnet),\n",
    "            CatBoostRegressor(**params_catboost1),\n",
    "            # CatBoostRegressor(**params_catboost2),\n",
    "            # CatBoostRegressor(**params_catboost3),\n",
    "            # CatBoostRegressor(**params_catboost4),\n",
    "            MultiOutputXGBRegressor(XGBRegressor(**params_xgboost)),\n",
    "            MultiOutputLGBMRegressor(LGBMRegressor(**params_lightgbm1)),\n",
    "            # LGBMRegressor(**params_lightgbm2),\n",
    "            # LGBMRegressor(**params_lightgbm3),\n",
    "            # LGBMRegressor(**params_lightgbm4),\n",
    "            # LGBMRegressor(**params_lightgbm5),\n",
    "            # LGBMRegressor(**params_lightgbm6),\n",
    "            # LGBMRegressor(**params_lightgbm7),\n",
    "            # LGBMRegressor(**params_lightgbm8),\n",
    "            # MultiOutputExtraTreesRegressor(ExtraTreesRegressor(**params_extratrees)),\n",
    "            \n",
    "            MultiOutputHistGradientBoostingRegressor(HistGradientBoostingRegressor(**params_hgb)),\n",
    "        ]\n",
    "        \n",
    "        self.stacking_regressors = [\n",
    "            # LinearRegression(),\n",
    "            # RidgeCV(**params_ridge),\n",
    "            # LassoCV(**params_lasso),\n",
    "            # ElasticNetCV(**params_elasticnet),\n",
    "            # CatBoostRegressor(**params_catboost1),\n",
    "            # CatBoostRegressor(**params_catboost2),\n",
    "            # CatBoostRegressor(**params_catboost3),\n",
    "            # CatBoostRegressor(**params_catboost4),\n",
    "            # XGBRegressor(**params_xgboost),\n",
    "            LGBMRegressor(**params_lightgbm1),\n",
    "            LGBMRegressor(**params_lightgbm2),\n",
    "            LGBMRegressor(**params_lightgbm3),\n",
    "            LGBMRegressor(**params_lightgbm4),\n",
    "            LGBMRegressor(**params_lightgbm5),\n",
    "            LGBMRegressor(**params_lightgbm6),\n",
    "            LGBMRegressor(**params_lightgbm7),\n",
    "            LGBMRegressor(**params_lightgbm8),\n",
    "            # ExtraTreesRegressor(**params_extratrees),\n",
    "        ]\n",
    "        \n",
    "    def _get_regressors_name(self):\n",
    "        self.regressors_name = [type(r).__name__ for r in self.regressors]\n",
    "        self.stacking_regressors_name = [type(r).__name__ for r in self.stacking_regressors]\n",
    "        \n",
    "    def _get_ohe(self,X,cat_features):\n",
    "        ohe = OneHotEncoder()\n",
    "        ohe.fit(X,cat_features,remove_first=False)\n",
    "        return ohe\n",
    "        \n",
    "    def _set_zero_to_minimum(self,pred,minimum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred<0,1,0).sum()>0:\n",
    "            pred = [x if x>0 else minimum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def _set_inf_to_maximum(self,pred,maximum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred==np.inf,1,0).sum()>0:\n",
    "            pred = [x if x!=np.inf else maximum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def _preprocess(self,pred):\n",
    "        # pred = self._set_zero_to_minimum(pred,self.minimum_value)\n",
    "        # pred = self._set_inf_to_maximum(pred,self.maximum_value)\n",
    "        return pred\n",
    "    \n",
    "    def _fit_regressor(self,\n",
    "                       regressor,regressor_name,\n",
    "                       X,X_oh,X_val,X_val_oh,y,y_val,cat_features,\n",
    "                       sample_weight,eval_sample_weight):\n",
    "        X = X[self.features]\n",
    "        X_val = X_val[self.features]\n",
    "        X_oh = X_oh[self.oh_features]\n",
    "        X_val_oh = X_val_oh[self.oh_features]\n",
    "        \n",
    "        if (regressor_name in ['LinearRegression','RidgeCV','LassoCV','ElasticNetCV']) or\\\n",
    "            (regressor_name.find('ExtraTreesRegressor')>=0) or\\\n",
    "            (regressor_name.find('RandomForestRegressor')>=0) or\\\n",
    "            (regressor_name.find('HistGradientBoostingRegressor')>=0):\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # fitting\n",
    "            regressor.fit(X_oh,y)\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "\n",
    "        elif regressor_name.find('XGBRegressor')>=0:\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                X_oh,y,\n",
    "                eval_set=[(X_val_oh,y_val)],\n",
    "                verbose=0,\n",
    "            )\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "\n",
    "        elif regressor_name.find('CatBoostRegressor')>=0:\n",
    "            # dataset\n",
    "            train_dataset = Pool(X    ,y    ,cat_features=cat_features)\n",
    "            val_dataset   = Pool(X_val,y_val,cat_features=cat_features)\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                train_dataset,\n",
    "                eval_set=val_dataset,\n",
    "                #metric_period=self.hyperparameters['iterations']//50, verbose=True,\n",
    "                verbose=False,\n",
    "            )\n",
    "            # prediction\n",
    "            tr_pred = self._preprocess(regressor.predict(train_dataset))\n",
    "            va_pred = self._preprocess(regressor.predict(val_dataset))\n",
    "\n",
    "        elif regressor_name.find('LGBMRegressor')>=0:\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # # astype category\n",
    "            # for col in cat_features:\n",
    "            #     X[col]     = X[col]    .astype('category')\n",
    "            #     X_val[col] = X_val[col].astype('category')\n",
    "            # # fitting\n",
    "            # regressor.fit(\n",
    "            #     X,y,\n",
    "            #     eval_set=[(X_val,y_val)],\n",
    "            #     sample_weight=sample_weight,\n",
    "            #     eval_sample_weight=eval_sample_weight,\n",
    "            #     categorical_feature=cat_features,\n",
    "            #     verbose=-1,\n",
    "            # )\n",
    "            # fitting\n",
    "            regressor.fit(\n",
    "                X_oh,y,\n",
    "                eval_set=[(X_val_oh,y_val)],\n",
    "                sample_weight=sample_weight,\n",
    "                eval_sample_weight=eval_sample_weight,\n",
    "                #categorical_feature=cat_features,\n",
    "                verbose=-1,\n",
    "            )\n",
    "            tr_pred = self._preprocess(regressor.predict(X_oh))\n",
    "            va_pred = self._preprocess(regressor.predict(X_val_oh))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "        return regressor, tr_pred, va_pred\n",
    "            \n",
    "    def _get_prediction_values(self,X,X_oh,method,regressors_name,regressors,weights,return_weighted):\n",
    "        if method=='ensemble':\n",
    "            features    = self.features\n",
    "            oh_features = self.oh_features\n",
    "        elif method=='stacking':\n",
    "            stacking_feature = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            features    = self.features + stacking_feature\n",
    "            oh_features = self.oh_features + stacking_feature\n",
    "        \n",
    "        # (1) 예측값생성\n",
    "        pred_list = []\n",
    "        for regressor_name,regressor in zip(regressors_name,regressors):\n",
    "            if (regressor_name in ['LinearRegression','RidgeCV','LassoCV','ElasticNetCV','RandomForestRegressor']) or\\\n",
    "                (regressor_name.find('ExtraTreesRegressor')>=0) or\\\n",
    "                (regressor_name.find('RandomForestRegressor')>=0) or\\\n",
    "                (regressor_name.find('XGBRegressor')>=0) or\\\n",
    "                (regressor_name.find('HistGradientBoostingRegressor')>=0):\n",
    "                dataset = X_oh[oh_features]\n",
    "            elif regressor_name.find('CatBoostRegressor')>=0:\n",
    "                dataset = Pool(X[features],cat_features=self.cat_features)\n",
    "            elif regressor_name.find('LGBMRegressor')>=0:\n",
    "                dataset = X_oh[oh_features]\n",
    "                # dataset = X[features].copy()\n",
    "                # for col in self.cat_features:\n",
    "                #     dataset[col] = dataset[col].astype('category')\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "            y_pred = self._preprocess(regressor.predict(dataset))\n",
    "            pred_list.append(y_pred)\n",
    "        \n",
    "        # (2) return weighted or original value\n",
    "        if return_weighted:\n",
    "            final_pred = []\n",
    "            for pred,weight in zip(pred_list,weights):\n",
    "                p = np.array(pred)*weight\n",
    "                final_pred.append(p)\n",
    "            final_pred = np.sum(final_pred,axis=0)\n",
    "            if self.inverse_transform is not None:\n",
    "                final_pred = self.inverse_transform(np.array(final_pred))\n",
    "                final_pred = self._preprocess(final_pred)\n",
    "        else:\n",
    "            final_pred = np.array(pred_list).T\n",
    "            \n",
    "        return final_pred\n",
    "        \n",
    "    def _predict(self,X,method='stacking',return_weighted=True):\n",
    "        if len(self.cat_features)>0:\n",
    "            X_oh = self.ohe.transform(X)\n",
    "        else:\n",
    "            X_oh = X.copy()\n",
    "        assert len(X)==len(X_oh), \\\n",
    "            \"X and X_oh must be same length\"\n",
    "        assert method in ['ensemble','stacking'], \\\n",
    "            \"method must be one of ['ensemble','stacking']\"\n",
    "        \n",
    "        # (1) ensemble\n",
    "        pred_list = self._get_prediction_values(\n",
    "            X,X_oh,\n",
    "            'ensemble',\n",
    "            self.regressors_name,self.regressors,\n",
    "            self.ensemble_weights,return_weighted,\n",
    "        )\n",
    "        \n",
    "        if method=='ensemble':\n",
    "            return pred_list\n",
    "        \n",
    "        elif method=='stacking':\n",
    "            # (2) stacking\n",
    "            columns = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            pred_df = pd.DataFrame(pred_list,columns=columns,index=X.index)\n",
    "            \n",
    "            X    = pd.concat([X   ,pred_df],axis=1)\n",
    "            X_oh = pd.concat([X_oh,pred_df],axis=1)\n",
    "\n",
    "            pred_list = self._get_prediction_values(\n",
    "                X,X_oh,\n",
    "                'stacking',\n",
    "                self.stacking_regressors_name,self.stacking_regressors,\n",
    "                self.stacking_weights,return_weighted,\n",
    "            )\n",
    "            return pred_list\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        # message_print = warnings.warn\n",
    "        message_print = print\n",
    "        supported_models = self.regressors_name\n",
    "        \n",
    "#         # feature_importances_를 지원하는 모델들\n",
    "#         supported_models = ['CatBoostRegressor','XGBRegressor','LGBMRegressor','ExtraTreesRegressor',\n",
    "#                             'ExtraTreesRegressor', 'RandomForestRegressor', 'HistGradientBoostingRegressor']\n",
    "        \n",
    "#         # 지원하지않는 모델이 있는 경우 warning message\n",
    "#         not_supported_1 = len([name for name in self.regressors_name if name not in supported_models])\n",
    "#         not_supported_2 = 0\n",
    "#         for name in self.regressors_name:\n",
    "#             for m in supported_models:\n",
    "#                 k = 0 if name.find(m)>=0 else 1\n",
    "#                 not_supported_2 += k\n",
    "        \n",
    "#         if not_supported_1 + not_supported_2 > 0:\n",
    "#             message_print(\"not support model\")\n",
    "\n",
    "        # get weighted feature importance by using ensemble_weights\n",
    "        feature_importance_df = pd.DataFrame(self.features,columns=['feature'])\n",
    "        for i,(regressor,regressor_name,weight) in enumerate(zip(self.regressors,self.regressors_name,self.ensemble_weights)):\n",
    "            if regressor_name in supported_models:\n",
    "                feature_importance = regressor.feature_importances_\n",
    "                fi_list = []\n",
    "                for feature in self.features:\n",
    "                    fi = feature_importance[np.where(np.array(self.features)==feature)[0]]\n",
    "                    fi_list.append([feature,sum(fi)])\n",
    "\n",
    "                imp_col = f'importance{i}'\n",
    "                fi_df = pd.DataFrame(fi_list,columns=['feature',imp_col]).sort_values(imp_col,ascending=False)\n",
    "                fi_df[imp_col] = 100 * fi_df[imp_col] / fi_df[imp_col].sum()\n",
    "                fi_df[imp_col] *= weight\n",
    "\n",
    "                feature_importance_df = pd.merge(feature_importance_df,fi_df,how='left',on='feature')\n",
    "\n",
    "        feature_importance_df = feature_importance_df.fillna(0)\n",
    "        feature_importance_df['importance'] = feature_importance_df.drop('feature',axis=1).sum(axis=1)\n",
    "        feature_importance_df = feature_importance_df[['feature','importance']]\n",
    "        \n",
    "        return feature_importance_df\n",
    "        \n",
    "    def plot_feature_importance(self):\n",
    "        feature_importance_df = self.get_feature_importance()\n",
    "        feature_importance_df.sort_values('importance',ascending=True,inplace=True)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.barh(feature_importance_df['feature'],feature_importance_df['importance'])\n",
    "        plt.show()\n",
    "            \n",
    "    def fit(self,\n",
    "            X,y,eval_set,cat_features,\n",
    "            sample_weight=None,eval_sample_weight=None,verbose=1):\n",
    "        assert len(eval_set)==1, \\\n",
    "            \"eval_set length must be 1. len(eval_set)={}\".format(len(eval_set))\n",
    "        \n",
    "        if len(self.regressors)!=len(self.regressors_name):\n",
    "            self._get_regressors_name()\n",
    "        \n",
    "        self.sample_weight = sample_weight\n",
    "        self.eval_sample_weight = eval_sample_weight\n",
    "        self.cat_features = cat_features\n",
    "        self.enable_categorical = [True if col in cat_features else False for col in X.columns]\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # prepare dataset\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        X_val, y_val = eval_set[0]\n",
    "        \n",
    "        del_cols = return_unique_columns(X)\n",
    "        X.drop(del_cols,axis=1,inplace=True)\n",
    "        X_val.drop(del_cols,axis=1,inplace=True)\n",
    "        self.cat_features = list(set(cat_features)-set(del_cols))\n",
    "        \n",
    "        if len(self.cat_features)>0:\n",
    "            self.ohe = self._get_ohe(X,cat_features)\n",
    "            X_oh = self.ohe.transform(X)\n",
    "            X_val_oh = self.ohe.transform(X_val)\n",
    "        else:\n",
    "            X_oh = X.copy()\n",
    "            X_val_oh = X_val.copy()\n",
    "        \n",
    "        del_oh_cols = return_unique_columns(X_oh)\n",
    "        X_oh.drop(del_oh_cols,axis=1,inplace=True)\n",
    "        X_val_oh.drop(del_oh_cols,axis=1,inplace=True)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # save feature names\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        self.features    = X   .columns.tolist()\n",
    "        self.oh_features = X_oh.columns.tolist()\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # true value\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        tr_true = np.array(y)\n",
    "        va_true = np.array(y_val)\n",
    "        if self.inverse_transform is not None:\n",
    "            tr_true = self.inverse_transform(tr_true)\n",
    "            va_true = self.inverse_transform(va_true)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # set min,max value\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        self.minimum_value = min(np.nanmin(y),np.nanmin(y_val))\n",
    "        self.maximum_value = max(np.nanmax(y),np.nanmax(y_val))\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # (1) ensemble fitting\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # prepare ensemble fitting\n",
    "        self.ensemble_scores = []\n",
    "        self.ensemble_weights = []\n",
    "        self.ensemble_fitting_elapsed = []\n",
    "        ensemble_pbar = zip(self.regressors_name,self.regressors)\n",
    "\n",
    "        # fitting\n",
    "        if self.method=='stacking':\n",
    "            if verbose:\n",
    "                print('\\n########  <Step1> Ensemble  ########')\n",
    "        for fit_iter,(regressor_name,regressor) in enumerate(ensemble_pbar):\n",
    "            s = time.time()\n",
    "                \n",
    "            # fit\n",
    "            regressor, tr_pred, va_pred = self._fit_regressor(\n",
    "                regressor, regressor_name,\n",
    "                X, X_oh, X_val, X_val_oh, y, y_val, self.cat_features,\n",
    "                sample_weight, eval_sample_weight,\n",
    "            )\n",
    "            self.regressors[fit_iter] = regressor\n",
    "            \n",
    "            # progress\n",
    "            if self.inverse_transform is not None:\n",
    "                tr_pred = self.inverse_transform(tr_pred)\n",
    "                tr_pred = self._preprocess(tr_pred)\n",
    "                va_pred = self.inverse_transform(va_pred)\n",
    "                va_pred = self._preprocess(va_pred)\n",
    "            \n",
    "            tr_score = self.eval_metric(y_pred=tr_pred,y_true=tr_true)\n",
    "            va_score = self.eval_metric(y_pred=va_pred,y_true=va_true)\n",
    "            \n",
    "            e = time.time()\n",
    "            self.ensemble_scores.append(va_score)\n",
    "            self.ensemble_weights.append(1/va_score)\n",
    "            self.ensemble_fitting_elapsed.append(e-s)\n",
    "            \n",
    "            if verbose:\n",
    "                blank = ' '*(11-len(regressor_name))\n",
    "                fit_progress = '[{}/{}] {}{}: loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s'\\\n",
    "                    .format(fit_iter+1,len(self.regressors),regressor_name,blank,tr_score,va_score,e-s)\n",
    "                print(fit_progress)\n",
    "            \n",
    "        # get weighted prediction & score\n",
    "        if self.weight=='equal':\n",
    "            self.ensemble_weights = np.array([1.0 for _ in self.regressors])\n",
    "        self.ensemble_weights /= sum(self.ensemble_weights)\n",
    "        \n",
    "        tr_pred = self._predict(X,method='ensemble',return_weighted=True)\n",
    "        va_pred = self._predict(X_val,method='ensemble',return_weighted=True)\n",
    "        \n",
    "        ## -> self.predict에서 inverse_transform 해줌\n",
    "        # if self.inverse_transform is not None:\n",
    "        #     tr_pred = self.inverse_transform(tr_pred)\n",
    "        #     va_pred = self.inverse_transform(va_pred)\n",
    "        \n",
    "        ens_tr_score = self.eval_metric(y_true=tr_true,y_pred=tr_pred)\n",
    "        ens_va_score = self.eval_metric(y_true=va_true,y_pred=va_pred)\n",
    "        \n",
    "        if verbose:\n",
    "            ens_fit_progress = \"<Weighted Ensemble(weight='{}')> loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s\"\\\n",
    "                .format(self.weight,ens_tr_score,ens_va_score,sum(self.ensemble_fitting_elapsed))\n",
    "            print(ens_fit_progress)\n",
    "        \n",
    "        if self.method=='ensemble':\n",
    "            self.total_score = ens_va_score\n",
    "            \n",
    "        elif self.method=='stacking':\n",
    "            #----------------------------------------------------------------------------------------#\n",
    "            # (2) stacking fitting\n",
    "            #----------------------------------------------------------------------------------------#\n",
    "            tr_pred = self._predict(X,method='ensemble',return_weighted=self.use_weightedsum_in_stacking)\n",
    "            va_pred = self._predict(X_val,method='ensemble',return_weighted=self.use_weightedsum_in_stacking)\n",
    "\n",
    "            columns = [self.stacking_feature] if isinstance(self.stacking_feature,str) else self.stacking_feature\n",
    "            tr_pred_df = pd.DataFrame(tr_pred,columns=columns,index=X.index)\n",
    "            va_pred_df = pd.DataFrame(va_pred,columns=columns,index=X_val.index)\n",
    "            \n",
    "            X        = pd.concat([X       ,tr_pred_df],axis=1)\n",
    "            X_oh     = pd.concat([X_oh    ,tr_pred_df],axis=1)\n",
    "            X_val    = pd.concat([X_val   ,va_pred_df],axis=1)\n",
    "            X_val_oh = pd.concat([X_val_oh,va_pred_df],axis=1)\n",
    "\n",
    "            # prepare stacking fitting\n",
    "            self.stacking_scores = []\n",
    "            self.stacking_weights = []\n",
    "            self.stacking_fitting_elapsed = []\n",
    "\n",
    "            stacking_regressors = deepcopy(self.stacking_regressors)\n",
    "            stacking_pbar =  zip(self.stacking_regressors_name,stacking_regressors)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n########  <Step2> Stacking  ########')\n",
    "            self.stacking_regressors = []\n",
    "            for fit_iter,(regressor_name,regressor) in enumerate(stacking_pbar):\n",
    "                s = time.time()\n",
    "\n",
    "                # fitting\n",
    "                stacking_regressor, tr_pred, va_pred = self._fit_regressor(\n",
    "                    regressor, regressor_name,\n",
    "                    X, X_oh, X_val, X_val_oh, y, y_val, self.cat_features,\n",
    "                    sample_weight, eval_sample_weight,\n",
    "                )\n",
    "                self.stacking_regressors.append(stacking_regressor)\n",
    "\n",
    "                # progress\n",
    "                if self.inverse_transform is not None:\n",
    "                    tr_pred = self.inverse_transform(tr_pred)\n",
    "                    tr_pred = self._preprocess(tr_pred)\n",
    "                    va_pred = self.inverse_transform(va_pred)\n",
    "                    va_pred = self._preprocess(va_pred)\n",
    "\n",
    "                tr_score = self.eval_metric(y_pred=tr_pred,y_true=tr_true)\n",
    "                va_score = self.eval_metric(y_pred=va_pred,y_true=va_true)\n",
    "\n",
    "                e = time.time()\n",
    "                self.stacking_scores.append(va_score)\n",
    "                self.stacking_weights.append(1/va_score)\n",
    "                self.stacking_fitting_elapsed.append(e-s)\n",
    "\n",
    "                if verbose:\n",
    "                    blank = ' '*(11-len(regressor_name))\n",
    "                    iter_str = str(fit_iter+1).zfill(len(str(len(stacking_regressors))))\n",
    "                    fit_progress = '[{}/{}] {}{}: loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s'\\\n",
    "                        .format(iter_str,len(stacking_regressors),regressor_name,blank,tr_score,va_score,e-s)\n",
    "                    print(fit_progress)\n",
    "\n",
    "            # get weighted prediction & score\n",
    "            if self.weight=='equal':\n",
    "                self.stacking_weights = np.array([1.0 for _ in self.stacking_regressors])\n",
    "            self.stacking_weights /= sum(self.stacking_weights)\n",
    "\n",
    "            tr_pred = self._predict(\n",
    "                X   .drop(self.stacking_feature,axis=1),\n",
    "                X_oh.drop(self.stacking_feature,axis=1),\n",
    "                method='stacking',\n",
    "            )\n",
    "            va_pred = self._predict(\n",
    "                X_val   .drop(self.stacking_feature,axis=1),\n",
    "                X_val_oh.drop(self.stacking_feature,axis=1),\n",
    "                method='stacking',\n",
    "            )\n",
    "\n",
    "            ## -> self.predict에서 inverse_transform 해줌\n",
    "            # if self.inverse_transform is not None:\n",
    "            #     tr_pred = self.inverse_transform(tr_pred)\n",
    "            #     va_pred = self.inverse_transform(va_pred)\n",
    "\n",
    "            stacking_tr_score = self.eval_metric(y_true=tr_true,y_pred=tr_pred)\n",
    "            stacking_va_score = self.eval_metric(y_true=va_true,y_pred=va_pred)\n",
    "\n",
    "            if verbose:\n",
    "                stacking_fit_progress = \"<Weighted Stacking(weight='{}')> loss={:.3f}, val_loss={:.3f}, elasped={:.1f}s\"\\\n",
    "                    .format(self.weight,stacking_tr_score,stacking_va_score,sum(self.stacking_fitting_elapsed))\n",
    "                print(stacking_fit_progress)\n",
    "\n",
    "            self.total_score = stacking_va_score\n",
    "            \n",
    "        #self.feature_importances_ = self.get_feature_importance()['importance'].values.tolist()\n",
    "\n",
    "    def predict(self,X,method=None):\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        if (self.method=='ensemble') & (method=='stacking'):\n",
    "            raise ValueError(\"The training method is 'ensemble', so 'stacking' prediction is not possible\")\n",
    "        return self._predict(X,method=self.method,return_weighted=self.use_weightedsum_in_stacking)\n",
    "        \n",
    "    def save(self,path):\n",
    "        save_dict = {\n",
    "            'ohe' : self.ohe,\n",
    "            'cat_features' : self.cat_features,\n",
    "            'minimum_value' : self.minimum_value,\n",
    "            'maximum_value' : self.maximum_value,\n",
    "            'features' : self.features,\n",
    "            'oh_features' : self.oh_features,\n",
    "            'hyperparameters' : self.hyperparameters,\n",
    "            'inverse_transform' : self.inverse_transform,\n",
    "            'sample_weight' : self.sample_weight,\n",
    "            'eval_sample_weight' : self.eval_sample_weight,\n",
    "            'regressors' : self.regressors,\n",
    "            'ensemble_weights' : self.ensemble_weights,\n",
    "            'ensemble_fitting_elapsed' : self.ensemble_fitting_elapsed,\n",
    "            'ensemble_scores' : self.ensemble_scores,\n",
    "            'total_score' : self.total_score,\n",
    "            #'feature_importances_' : self.feature_importances_,\n",
    "        }\n",
    "        if self.method=='stacking':\n",
    "            additional_save_dict = {\n",
    "                'stacking_regressors' : self.stacking_regressors,\n",
    "                'stacking_weights' : self.stacking_weights,\n",
    "                'stacking_fitting_elapsed' : self.stacking_fitting_elapsed,\n",
    "                'stacking_scores' : self.stacking_scores,\n",
    "            }\n",
    "            save_dict = {**save_dict,**additional_save_dict}\n",
    "        with open(path, 'wb') as f:\n",
    "            #pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            dill.dump(save_dict, f)\n",
    "            \n",
    "    def load(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            #save_dict = pickle.load(f)\n",
    "            save_dict = dill.load(f)\n",
    "            self.ohe = save_dict['ohe']\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.minimum_value = save_dict['minimum_value']\n",
    "            self.maximum_value = save_dict['maximum_value']\n",
    "            self.features = save_dict['features']\n",
    "            self.oh_features = save_dict['oh_features']\n",
    "            self.hyperparameters = save_dict['hyperparameters']\n",
    "            self.inverse_transform = save_dict['inverse_transform']\n",
    "            self.sample_weight = save_dict['sample_weight']\n",
    "            self.eval_sample_weight = save_dict['eval_sample_weight']\n",
    "            self.regressors = save_dict['regressors']\n",
    "            self.ensemble_weights = save_dict['ensemble_weights']\n",
    "            self.ensemble_fitting_elapsed = save_dict['ensemble_fitting_elapsed']\n",
    "            self.ensemble_scores = save_dict['ensemble_scores']\n",
    "            self.total_score = save_dict['total_score']\n",
    "            #self.feature_importances_ = save_dict['feature_importances_']\n",
    "            \n",
    "            if self.method=='stacking':\n",
    "                self.stacking_regressors = save_dict['stacking_regressors']\n",
    "                self.stacking_weights = save_dict['stacking_weights']\n",
    "                self.stacking_fitting_elapsed = save_dict['stacking_fitting_elapsed']\n",
    "                self.stacking_scores = save_dict['stacking_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fa801a5-6f70-4659-b9f6-6d73c4c9c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KfoldWeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 hyperparameters,\n",
    "                 method='ensemble',\n",
    "                 weight='balanced',\n",
    "                 inverse_transform=None,\n",
    "                 eval_metric=None,\n",
    "                 use_ensemble=True,\n",
    "                 n_splits=5,\n",
    "                 random_state=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        \n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.method = method\n",
    "        self.weight = weight\n",
    "        self.inverse_transform = inverse_transform\n",
    "        self.eval_metric = RMSE if eval_metric is None else eval_metric\n",
    "        self.use_ensemble = use_ensemble\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        fs = [m.features for m in self.base_models]\n",
    "        fs = list(set(item for sublist in fs for item in sublist))\n",
    "        feature_importance_df = pd.DataFrame(fs,columns=['feature'])\n",
    "\n",
    "        for i,(base_model,base_feature_importance) in enumerate(zip(self.base_models,self.base_feature_importances)):\n",
    "            imp_col = f'imp{i}'\n",
    "            imp_df = pd.DataFrame({\n",
    "                'feature' : base_model.features,\n",
    "                imp_col : base_feature_importance,\n",
    "            })\n",
    "            feature_importance_df = pd.merge(feature_importance_df,imp_df,how='left',on='feature')\n",
    "\n",
    "        feature_importance_df.fillna(0,inplace=True)\n",
    "        feature_importance_df['importance'] = feature_importance_df.drop('feature',axis=1).sum(axis=1)\n",
    "        feature_importance_df['importance'] = 100 * feature_importance_df['importance'] / feature_importance_df['importance'].sum()\n",
    "        \n",
    "        return feature_importance_df\n",
    "        \n",
    "    def plot_feature_importance(self):\n",
    "        feature_importance_df = self.get_feature_importance()\n",
    "        feature_importance_df.sort_values('importance',ascending=True,inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.barh(feature_importance_df.feature,feature_importance_df.importance)\n",
    "        plt.show()\n",
    "        \n",
    "    def fit(self,X,y,cat_features,sample_weight=None,verbose=True):\n",
    "        self.cat_features = cat_features\n",
    "        self.sample_weight = sample_weight\n",
    "        self.features = X.columns.tolist()\n",
    "\n",
    "        self.base_models = []\n",
    "        self.base_scores = []\n",
    "        self.base_feature_importances = []\n",
    "        kf = KFold(n_splits=self.n_splits,random_state=self.random_state,shuffle=True)\n",
    "\n",
    "        progress_fmt = '> KFold: {}/{}'\n",
    "        for k, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            k_str = str(k+1).zfill(len(str(self.n_splits)))\n",
    "            print('')\n",
    "            print('-'*80)\n",
    "            print(progress_fmt.format(k_str,self.n_splits))\n",
    "            print('-'*80)\n",
    "            \n",
    "            X_tr   , X_va    = X   .iloc[tr_idx], X   .iloc[val_idx]\n",
    "            y_tr   , y_va    = y   .iloc[tr_idx], y   .iloc[val_idx]\n",
    "            \n",
    "            if self.sample_weight is None:\n",
    "                sample_weight = None\n",
    "                eval_sample_weight = None\n",
    "            else:\n",
    "                sample_weight = self.sample_weight[tr_idx]\n",
    "                eval_sample_weight = self.sample_weight[val_idx]\n",
    "\n",
    "            #------------------------------------------------------------------------------------#\n",
    "            # (1) base model\n",
    "            #------------------------------------------------------------------------------------#\n",
    "            # define the base model\n",
    "            base_model = WeightedEnsembleRegressor(\n",
    "                hyperparameters,\n",
    "                weight='balanced', # 'equal', 'balanced',\n",
    "                inverse_transform=self.inverse_transform,\n",
    "                eval_metric=self.eval_metric,\n",
    "                method=self.method, # 'ensemble', 'stacking'\n",
    "                use_weightedsum_in_stacking=True,\n",
    "            )\n",
    "            # fit the model\n",
    "            base_model.fit(\n",
    "                X_tr,y_tr,\n",
    "                eval_set=[(X_va,y_va)],\n",
    "                cat_features=cat_features,\n",
    "                sample_weight=sample_weight,\n",
    "                eval_sample_weight=[eval_sample_weight],\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            \n",
    "            # prediction\n",
    "            y_pred = base_model.predict(X_va)\n",
    "            if self.inverse_transform is not None:\n",
    "                y_true = self.inverse_transform(y_va.values)\n",
    "            else:\n",
    "                y_true = y_va.values\n",
    "            \n",
    "            # caculate score\n",
    "            score = mean_squared_error(y_true=y_true,y_pred=y_pred)**0.5\n",
    "\n",
    "            # append inner loop\n",
    "            self.base_models.append(base_model)\n",
    "            self.base_scores.append([k+1,len(X_tr),len(X_va),score])\n",
    "            \n",
    "            # # plot feature importance\n",
    "            # self.base_feature_importances.append(base_model.feature_importances_)\n",
    "            # base_model.plot_feature_importance()\n",
    "        \n",
    "        self.base_score = pd.DataFrame(self.base_scores,columns=['k','n_train','n_val','rmse'])\n",
    "        self.validation_score = self.base_score.rmse.mean()\n",
    "        \n",
    "        #self.plot_feature_importance()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        pred = [base_model.predict(X) for base_model in self.base_models]\n",
    "        pred = np.mean(pred,axis=0)\n",
    "        return pred\n",
    "    \n",
    "    def save(self,path):\n",
    "        save_dict = {\n",
    "            'hyperparameters' : self.hyperparameters,\n",
    "            'weight' : self.weight,\n",
    "            'n_splits' : self.n_splits,\n",
    "            'random_state' : self.random_state,\n",
    "            'inverse_transform' : self.inverse_transform,\n",
    "            'cat_features' : self.cat_features,\n",
    "            'sample_weight' : self.sample_weight,\n",
    "            'base_models' : self.base_models,\n",
    "            'base_scores' : self.base_scores,\n",
    "            'base_score' : self.base_score,\n",
    "            'validation_score' : self.validation_score,\n",
    "            'base_feature_importances' : self.base_feature_importances,\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            #pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            dill.dump(save_dict, f)\n",
    "            \n",
    "    def load(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            #save_dict = pickle.load(f)\n",
    "            save_dict = dill.load(f)\n",
    "            \n",
    "            self.hyperparameters = save_dict['hyperparameters']\n",
    "            self.weight = save_dict['weight']\n",
    "            self.n_splits = save_dict['n_splits']\n",
    "            self.random_state = save_dict['random_state']\n",
    "            self.inverse_transform = save_dict['inverse_transform']\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.sample_weight = save_dict['sample_weight']\n",
    "            self.base_models = save_dict['base_models']\n",
    "            self.base_scores = save_dict['base_scores']\n",
    "            self.base_score = save_dict['base_score']\n",
    "            self.validation_score = save_dict['validation_score']\n",
    "            self.base_feature_importances = save_dict['base_feature_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c0cb4df-68bf-4153-af7b-671d6cafa36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth' : 9,\n",
    "    'random_state' : CFG.SEED,\n",
    "    'learning_rate' : 0.01,\n",
    "    'iterations' : 30000,\n",
    "    'early_stopping_rounds' : 300,\n",
    "    'xgb_learning_rate' : 0.3,         # default=0.3\n",
    "    'xgb_iterations' : 300,           # default=100\n",
    "    'xgb_early_stopping_rounds' : 30,\n",
    "    # 'extratrees_iterations' : 5,#100,     # default=100\n",
    "    'hgb_iterations' : 3, # default=100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44682051-b1dd-4d0d-b17d-b648a1124968",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclo_weight = [10,5,3,1]\n",
    "eval_metric = lambda y_true,y_pred: WeightedMultiRMSE(y_true,y_pred,weight=eclo_weight)\n",
    "\n",
    "mc_path_fmt = './mc/weiens_multirmse_seg_gungu/{}.pkl'\n",
    "mkdir('/'.join(mc_path_fmt.split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37675b6-bcf3-4703-856a-a9a1b7606510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.350, val_loss=0.360, elasped=37.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.261, val_loss=0.378, elasped=56.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.326, val_loss=0.360, elasped=12.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.350, val_loss=0.363, elasped=9.8s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.318, val_loss=0.361, elasped=115.9s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.348, val_loss=0.346, elasped=42.7s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.266, val_loss=0.370, elasped=49.4s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.328, val_loss=0.346, elasped=12.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.354, val_loss=0.348, elasped=9.0s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.321, val_loss=0.347, elasped=113.1s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.353, val_loss=0.335, elasped=38.7s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.273, val_loss=0.360, elasped=55.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.342, val_loss=0.337, elasped=12.9s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.356, val_loss=0.339, elasped=9.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.328, val_loss=0.338, elasped=116.7s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.334, val_loss=0.361, elasped=81.3s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.231, val_loss=0.378, elasped=34.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.327, val_loss=0.362, elasped=6.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.349, val_loss=0.366, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.307, val_loss=0.361, elasped=122.1s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.346, val_loss=0.379, elasped=5.1s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.249, val_loss=0.395, elasped=0.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.320, val_loss=0.376, elasped=6.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.346, val_loss=0.382, elasped=0.2s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.311, val_loss=0.377, elasped=12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▌                                      | 1/8 [08:04<56:28, 484.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.314, val_loss=0.331, elasped=89.8s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.258, val_loss=0.341, elasped=27.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.306, val_loss=0.331, elasped=41.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.321, val_loss=0.335, elasped=5.2s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.297, val_loss=0.331, elasped=163.8s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.319, val_loss=0.318, elasped=189.2s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.265, val_loss=0.338, elasped=29.0s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.314, val_loss=0.319, elasped=32.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.324, val_loss=0.322, elasped=5.1s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.303, val_loss=0.320, elasped=255.3s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.316, val_loss=0.328, elasped=244.3s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.265, val_loss=0.340, elasped=53.5s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.307, val_loss=0.329, elasped=22.4s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.322, val_loss=0.332, elasped=4.8s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.300, val_loss=0.329, elasped=324.9s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.318, val_loss=0.316, elasped=233.6s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.269, val_loss=0.335, elasped=59.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.312, val_loss=0.316, elasped=23.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.324, val_loss=0.320, elasped=8.8s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.303, val_loss=0.317, elasped=324.9s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.319, val_loss=0.318, elasped=174.8s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.257, val_loss=0.332, elasped=52.4s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.312, val_loss=0.319, elasped=32.2s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.324, val_loss=0.323, elasped=4.2s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.301, val_loss=0.320, elasped=263.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▌                               | 2/8 [30:25<1:38:50, 988.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.323, val_loss=0.321, elasped=91.1s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.267, val_loss=0.337, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.319, val_loss=0.322, elasped=12.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.327, val_loss=0.326, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.306, val_loss=0.323, elasped=105.9s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.328, val_loss=0.300, elasped=18.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.254, val_loss=0.325, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.320, val_loss=0.301, elasped=14.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.332, val_loss=0.304, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.307, val_loss=0.303, elasped=34.5s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.324, val_loss=0.330, elasped=12.0s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.262, val_loss=0.345, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.318, val_loss=0.331, elasped=11.9s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.326, val_loss=0.333, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.304, val_loss=0.331, elasped=26.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.319, val_loss=0.343, elasped=14.6s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.260, val_loss=0.355, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.317, val_loss=0.344, elasped=11.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.322, val_loss=0.345, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.302, val_loss=0.344, elasped=28.2s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.318, val_loss=0.335, elasped=22.1s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.261, val_loss=0.347, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.309, val_loss=0.336, elasped=15.2s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.324, val_loss=0.339, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.300, val_loss=0.337, elasped=39.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████▌                           | 3/8 [34:21<53:44, 644.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.348, val_loss=0.369, elasped=17.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.281, val_loss=0.381, elasped=2.0s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.343, val_loss=0.370, elasped=12.8s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.352, val_loss=0.374, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.328, val_loss=0.370, elasped=32.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.345, val_loss=0.368, elasped=17.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.286, val_loss=0.387, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.341, val_loss=0.369, elasped=13.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.353, val_loss=0.372, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.328, val_loss=0.370, elasped=33.3s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.352, val_loss=0.356, elasped=10.6s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.295, val_loss=0.376, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.342, val_loss=0.357, elasped=13.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.356, val_loss=0.360, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.334, val_loss=0.358, elasped=26.5s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.350, val_loss=0.351, elasped=16.0s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.279, val_loss=0.366, elasped=1.8s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.345, val_loss=0.352, elasped=13.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.357, val_loss=0.356, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.329, val_loss=0.353, elasped=31.7s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.359, val_loss=0.337, elasped=9.0s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.289, val_loss=0.352, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.348, val_loss=0.336, elasped=14.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.360, val_loss=0.339, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.336, val_loss=0.337, elasped=25.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 4/8 [36:53<30:00, 450.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.358, val_loss=0.384, elasped=13.9s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.282, val_loss=0.397, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.346, val_loss=0.385, elasped=14.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.363, val_loss=0.388, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.334, val_loss=0.385, elasped=30.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.364, val_loss=0.343, elasped=17.2s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.287, val_loss=0.362, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.363, val_loss=0.345, elasped=12.1s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.372, val_loss=0.349, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.343, val_loss=0.345, elasped=31.4s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.368, val_loss=0.357, elasped=8.7s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.288, val_loss=0.373, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.355, val_loss=0.358, elasped=13.1s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.370, val_loss=0.361, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.342, val_loss=0.359, elasped=23.9s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.358, val_loss=0.369, elasped=16.5s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.293, val_loss=0.394, elasped=1.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.350, val_loss=0.370, elasped=13.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.367, val_loss=0.374, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.338, val_loss=0.371, elasped=31.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.356, val_loss=0.382, elasped=17.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.288, val_loss=0.389, elasped=1.7s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.329, val_loss=0.380, elasped=33.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.363, val_loss=0.387, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.330, val_loss=0.380, elasped=53.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████▌                | 5/8 [39:45<17:29, 349.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.351, val_loss=0.353, elasped=18.5s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.276, val_loss=0.376, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.346, val_loss=0.354, elasped=13.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.365, val_loss=0.358, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.331, val_loss=0.355, elasped=33.2s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.350, val_loss=0.352, elasped=20.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.276, val_loss=0.372, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.346, val_loss=0.354, elasped=12.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.365, val_loss=0.358, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.331, val_loss=0.354, elasped=34.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.349, val_loss=0.403, elasped=8.3s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.293, val_loss=0.417, elasped=1.0s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.345, val_loss=0.401, elasped=11.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.353, val_loss=0.404, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.331, val_loss=0.402, elasped=21.1s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.361, val_loss=0.350, elasped=10.2s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.292, val_loss=0.383, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.358, val_loss=0.354, elasped=10.9s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.365, val_loss=0.355, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.341, val_loss=0.355, elasped=22.5s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.353, val_loss=0.361, elasped=16.6s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.260, val_loss=0.378, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.352, val_loss=0.365, elasped=11.8s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.362, val_loss=0.368, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.328, val_loss=0.364, elasped=29.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████           | 6/8 [42:07<09:18, 279.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.390, val_loss=0.389, elasped=8.3s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.314, val_loss=0.418, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.385, val_loss=0.391, elasped=12.0s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.394, val_loss=0.392, elasped=0.5s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.367, val_loss=0.391, elasped=21.7s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.393, val_loss=0.391, elasped=5.8s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.318, val_loss=0.433, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.381, val_loss=0.392, elasped=11.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.392, val_loss=0.395, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.368, val_loss=0.395, elasped=18.5s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.399, val_loss=0.350, elasped=7.5s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.314, val_loss=0.381, elasped=1.0s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.385, val_loss=0.351, elasped=11.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.402, val_loss=0.354, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.372, val_loss=0.352, elasped=20.6s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.378, val_loss=0.421, elasped=10.2s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.298, val_loss=0.440, elasped=1.1s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.375, val_loss=0.423, elasped=11.2s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.384, val_loss=0.425, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.354, val_loss=0.422, elasped=23.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.368, val_loss=0.423, elasped=17.9s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.307, val_loss=0.438, elasped=1.3s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.359, val_loss=0.424, elasped=14.7s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.384, val_loss=0.428, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.350, val_loss=0.423, elasped=34.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████▌     | 7/8 [44:06<03:46, 226.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.315, val_loss=0.346, elasped=12.3s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.233, val_loss=0.362, elasped=0.9s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.306, val_loss=0.346, elasped=12.6s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.323, val_loss=0.351, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.290, val_loss=0.346, elasped=26.2s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.329, val_loss=0.318, elasped=6.1s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.275, val_loss=0.345, elasped=0.9s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.313, val_loss=0.318, elasped=16.5s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.330, val_loss=0.321, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.308, val_loss=0.319, elasped=23.8s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.325, val_loss=0.331, elasped=6.4s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.221, val_loss=0.348, elasped=0.8s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.314, val_loss=0.330, elasped=13.3s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.326, val_loss=0.333, elasped=0.4s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.293, val_loss=0.330, elasped=21.0s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.326, val_loss=0.326, elasped=7.0s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.257, val_loss=0.353, elasped=0.9s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.321, val_loss=0.330, elasped=10.4s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.328, val_loss=0.332, elasped=0.2s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.303, val_loss=0.330, elasped=18.4s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "> KFold: 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] CatBoostRegressor: loss=0.326, val_loss=0.326, elasped=24.6s\n",
      "[2/4] MultiOutputXGBRegressor: loss=0.246, val_loss=0.349, elasped=23.6s\n",
      "[3/4] MultiOutputLGBMRegressor: loss=0.310, val_loss=0.326, elasped=10.2s\n",
      "[4/4] MultiOutputHistGradientBoostingRegressor: loss=0.327, val_loss=0.329, elasped=3.6s\n",
      "<Weighted Ensemble(weight='balanced')> loss=0.300, val_loss=0.327, elasped=62.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [46:40<00:00, 350.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 29s, sys: 21min 12s, total: 1h 9min 41s\n",
      "Wall time: 46min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gungu_list = train_df['gungu'].unique()\n",
    "models = []\n",
    "for gungu in tqdm(gungu_list):\n",
    "    train_data = train_df[train_df.gungu==gungu].drop('gungu',axis=1)\n",
    "    test_data  = test_df [test_df .gungu==gungu].drop('gungu',axis=1)\n",
    "    \n",
    "    X = train_data.drop(CFG.TARGET,axis=1)\n",
    "    y = train_data[CFG.TARGET]\n",
    "    fixed_cat_features = setdiff(cat_features,['gungu'])\n",
    "    \n",
    "    model = KfoldWeightedEnsembleRegressor(\n",
    "        hyperparameters,\n",
    "        method='ensemble',\n",
    "        weight='balanced',\n",
    "        inverse_transform=target_transform.inverse_transform,\n",
    "        eval_metric=eval_metric,\n",
    "        use_ensemble=True,\n",
    "        n_splits=CFG.N_SPLITS,\n",
    "        random_state=CFG.SEED,\n",
    "    )\n",
    "    model.fit(\n",
    "        X,y,\n",
    "        cat_features=fixed_cat_features,\n",
    "        sample_weight=None,\n",
    "        verbose=True,\n",
    "    )\n",
    "    model.save(mc_path_fmt.format(gungu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e3e39ec-9381-4003-a234-a7de74d28f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_df = model.get_feature_importance()\n",
    "# feature_importance_df.sort_values('importance',ascending=True,inplace=True)\n",
    "# feature_importance_df = feature_importance_df.tail(10)\n",
    "\n",
    "# plt.figure(figsize=(15,7))\n",
    "# plt.barh(feature_importance_df.feature,feature_importance_df.importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cc8f646-b3a4-4b9f-999c-b37f611fdf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:08<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "tr_preds = []\n",
    "te_preds = []\n",
    "for gungu in tqdm(gungu_list):\n",
    "    train_data = train_df[train_df.gungu==gungu].drop('gungu',axis=1)\n",
    "    test_data  = test_df [test_df .gungu==gungu].drop('gungu',axis=1)\n",
    "    \n",
    "    X = train_data.drop(CFG.TARGET,axis=1)\n",
    "    y = train_data[CFG.TARGET]\n",
    "    fixed_cat_features = setdiff(cat_features,['gungu'])\n",
    "    \n",
    "    model = KfoldWeightedEnsembleRegressor(\n",
    "        hyperparameters,\n",
    "        method='ensemble',\n",
    "        weight='balanced',\n",
    "        inverse_transform=target_transform.inverse_transform,\n",
    "        eval_metric=eval_metric,\n",
    "        use_ensemble=True,\n",
    "        n_splits=CFG.N_SPLITS,\n",
    "        random_state=CFG.SEED,\n",
    "    )\n",
    "    model.load(mc_path_fmt.format(gungu))\n",
    "    \n",
    "    tr_p = model.predict(X)\n",
    "    te_p = model.predict(test_data)\n",
    "    \n",
    "    tr_pred = pd.DataFrame(y,index=train_data.index)\n",
    "    te_pred = pd.DataFrame(index=test_data.index)\n",
    "    \n",
    "    for i in range(len(CFG.TARGET)):\n",
    "        tr_pred[f'pred_{CFG.TARGET[i]}'] = tr_p[:,i]\n",
    "        te_pred[f'pred_{CFG.TARGET[i]}'] = te_p[:,i]\n",
    "    \n",
    "    tr_preds.append(tr_pred)\n",
    "    te_preds.append(te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33fc3126-c839-4e8a-8680-486fe2af848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prediction = pd.concat(tr_preds,axis=0).sort_index()\n",
    "te_prediction = pd.concat(te_preds,axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecce80e2-1a78-4375-b7c2-c0a4f8224c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32612910080214136"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeightedMultiRMSE(y_true=tr_prediction[CFG.TARGET],\n",
    "                  y_pred=tr_prediction[[f'pred_{t}' for t in CFG.TARGET]],\n",
    "                  weight=eclo_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a618df48-10ea-4877-af10-0e9582001d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4321323696883392"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = tr_prediction[CFG.TARGET] @ eclo_weight\n",
    "pred = tr_prediction[[f'pred_{t}' for t in CFG.TARGET]] @ eclo_weight\n",
    "\n",
    "RMSLE(y_true=true,y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d65cfe14-5860-4ab6-b6ef-412bc7c36a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ECLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCIDENT_39609</td>\n",
       "      <td>4.300596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCIDENT_39610</td>\n",
       "      <td>4.273892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCIDENT_39611</td>\n",
       "      <td>6.077025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCIDENT_39612</td>\n",
       "      <td>5.709065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCIDENT_39613</td>\n",
       "      <td>5.031157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      ECLO\n",
       "0  ACCIDENT_39609  4.300596\n",
       "1  ACCIDENT_39610  4.273892\n",
       "2  ACCIDENT_39611  6.077025\n",
       "3  ACCIDENT_39612  5.709065\n",
       "4  ACCIDENT_39613  5.031157"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['ECLO'] = te_prediction[[f'pred_{t}' for t in CFG.TARGET]] @ eclo_weight\n",
    "submit.to_csv('./out/5_weiens_multireg_weightedmultirmse_seg(gungu).csv',index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_3.10.13",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
