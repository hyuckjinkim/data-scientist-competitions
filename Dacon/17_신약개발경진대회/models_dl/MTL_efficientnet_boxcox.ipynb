{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448a1f81",
   "metadata": {
    "id": "pYzhJrEibIlq"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9380dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdkit\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561cc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ea0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4365ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 0\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 1024\n",
    "    LEARNING_RATE = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32664ba",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf1e6a",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fd0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c8fb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Num_H_Acceptors</th>\n",
       "      <th>Num_H_Donors</th>\n",
       "      <th>Num_RotatableBonds</th>\n",
       "      <th>LogD</th>\n",
       "      <th>Molecular_PolarSurfaceArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n",
       "      <td>26.010</td>\n",
       "      <td>50.680</td>\n",
       "      <td>3.259</td>\n",
       "      <td>400.495</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.259</td>\n",
       "      <td>117.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n",
       "      <td>29.270</td>\n",
       "      <td>50.590</td>\n",
       "      <td>2.169</td>\n",
       "      <td>301.407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.172</td>\n",
       "      <td>73.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n",
       "      <td>5.586</td>\n",
       "      <td>80.892</td>\n",
       "      <td>1.593</td>\n",
       "      <td>297.358</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.585</td>\n",
       "      <td>62.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n",
       "      <td>5.710</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.771</td>\n",
       "      <td>494.652</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.475</td>\n",
       "      <td>92.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n",
       "      <td>93.270</td>\n",
       "      <td>99.990</td>\n",
       "      <td>2.335</td>\n",
       "      <td>268.310</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.337</td>\n",
       "      <td>42.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             SMILES     MLM  \\\n",
       "0  TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n",
       "1  TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n",
       "2  TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n",
       "3  TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n",
       "4  TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n",
       "\n",
       "      HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
       "0  50.680  3.259           400.495                5             2   \n",
       "1  50.590  2.169           301.407                2             1   \n",
       "2  80.892  1.593           297.358                5             0   \n",
       "3   2.000  4.771           494.652                6             0   \n",
       "4  99.990  2.335           268.310                3             0   \n",
       "\n",
       "   Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  \n",
       "0                   8  3.259                      117.37  \n",
       "1                   2  2.172                       73.47  \n",
       "2                   3  1.585                       62.45  \n",
       "3                   5  3.475                       92.60  \n",
       "4                   1  2.337                       42.43  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eb475",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabed229",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93236fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e8456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = train_df.columns[train_df.dtypes!='object'].tolist()\n",
    "# for i,col in enumerate(num_features):\n",
    "\n",
    "#     fig = plt.figure(figsize=(15,7))\n",
    "#     fig.add_subplot(121)\n",
    "#     sns.histplot(train_df[col],bins=20)\n",
    "#     plt.grid()\n",
    "\n",
    "#     fig.add_subplot(122)\n",
    "#     sns.histplot(np.log(train_df[col]+1e-3),bins=20)\n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.suptitle('[{}/{}] {}'.format(i+1,len(num_features),col))\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # -> ['Molecular_Weight','Molecular_PolarSurfaceArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3be3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "# for col in cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=train_df[col],y=train_df['HLM'])\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435e2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds']\n",
    "# for col in cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=train_df[col],y=train_df.MLM)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5581593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260d8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists = sorted(train_df['Num_H_Acceptors'].unique())\n",
    "# for v in lists:\n",
    "#     print('########',v)\n",
    "#     d = train_df[train_df['Num_H_Acceptors']==v]\n",
    "    \n",
    "#     cols = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "#     for col in cols:\n",
    "#         print(col)\n",
    "#         plt.figure(figsize=(15,7))\n",
    "#         sns.scatterplot(x=d[col],y=d['HLM'])\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7048cd6",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c73ba4",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8295f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3aae5",
   "metadata": {},
   "source": [
    "## Set target range to [0,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "626c15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['MLM','HLM']\n",
    "for t in targets:\n",
    "    train_df[t] = [0 if x<0 else\n",
    "                   100 if x>100 else\n",
    "                   x for x in train_df[t]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128501ca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716604d",
   "metadata": {},
   "source": [
    "## Make molecule features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238ad3f0",
   "metadata": {
    "id": "oXOFfJVW22DL"
   },
   "outputs": [],
   "source": [
    "# Molecule to MorganFingerprint\n",
    "def mol2fp(mol):\n",
    "    fp = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n",
    "    ar = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5a978e9",
   "metadata": {
    "id": "7KbqRv6I19Rg"
   },
   "outputs": [],
   "source": [
    "# (1) SMILES를 통해 Molecule(분자구조) 생성\n",
    "PandasTools.AddMoleculeColumnToFrame(train_df,'SMILES','Molecule')\n",
    "PandasTools.AddMoleculeColumnToFrame(test_df ,'SMILES','Molecule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78adda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d232f56",
   "metadata": {
    "id": "1MTlg0wx22DM"
   },
   "outputs": [],
   "source": [
    "# (2) Morgan Fingerprint column 추가\n",
    "train_df[\"FPs\"] = train_df.Molecule.apply(mol2fp)\n",
    "test_df [\"FPs\"] = test_df .Molecule.apply(mol2fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0429d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498\n"
     ]
    }
   ],
   "source": [
    "# (3) Morgan Fingerprint 중, variance가 0.05보다 작은 컬럼들을 지우기\n",
    "feature_select = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "# 일부사용\n",
    "tr_fps_selected = feature_select.fit_transform(np.stack(train_df['FPs']))\n",
    "te_fps_selected = feature_select.transform(np.stack(test_df['FPs']))\n",
    "print(len(tr_fps_selected))\n",
    "\n",
    "# # 전체사용\n",
    "# tr_fps_selected = np.stack(train_df['FPs'])\n",
    "# te_fps_selected = np.stack(test_df ['FPs'])\n",
    "\n",
    "fps_names = ['fps'+str(i+1) for i in range(tr_fps_selected.shape[1])]\n",
    "\n",
    "train_df = pd.concat([\n",
    "    train_df.drop('FPs',axis=1),\n",
    "    pd.DataFrame(tr_fps_selected,columns=fps_names),\n",
    "],axis=1)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df.drop('FPs',axis=1),\n",
    "    pd.DataFrame(te_fps_selected,columns=fps_names),\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b006553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fps_raw_sum'] = np.sum(train_df[fps_names].values,axis=1)\n",
    "# test_df ['fps_raw_sum'] = np.sum(test_df [fps_names].values,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4337fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps_dummy_names = []\n",
    "# for col in tqdm(fps_names):\n",
    "#     train_df[f'{col}_dmy'] = np.where(train_df[col]==0,0,1)\n",
    "#     test_df [f'{col}_dmy'] = np.where(test_df [col]==0,0,1)\n",
    "#     fps_dummy_names.append(f'{col}_dmy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6a517c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fps_dummy_sum'] = np.sum(train_df[fps_names].values,axis=1)\n",
    "# test_df ['fps_dummy_sum'] = np.sum(test_df [fps_names].values,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15183f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 column만 추출\n",
    "features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors',\n",
    "            'Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "fps_features = [col for col in train_df.columns if col.find('fps')==0]\n",
    "smiles_feature = 'SMILES'\n",
    "targets  = ['MLM','HLM']\n",
    "\n",
    "train_df = train_df[features+fps_features+[smiles_feature]+targets]\n",
    "test_df  = test_df[features+fps_features+[smiles_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c960de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3498, 261)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0c066",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db556d55",
   "metadata": {},
   "source": [
    "## Imputaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4313f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a6c319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_check(data):\n",
    "    d = data.copy()\n",
    "    null_info = d.isnull().sum()\n",
    "    null_info = null_info[null_info!=0]\n",
    "    display(null_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84b91bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlogP    2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlogP    1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('> train')\n",
    "null_check(train_df)\n",
    "\n",
    "print('> test')\n",
    "null_check(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c95fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_features = ['AlogP']\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "train_df[null_features] = imputer.fit_transform(train_df[null_features])\n",
    "test_df [null_features] = imputer.transform(test_df[null_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "936e55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('> train')\n",
    "null_check(train_df)\n",
    "\n",
    "print('> test')\n",
    "null_check(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a127990",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d681c6d",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "694a0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80c687e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, va_df = train_test_split(train_df,test_size=0.1,shuffle=True,random_state=CFG.SEED)\n",
    "te_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2fd393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148, 350, 483)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_df), len(va_df), len(te_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cab225",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc8602",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48d5be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaling_features = features#+fps_features\n",
    "scalers = {}\n",
    "for f in scaling_features:\n",
    "    scaler = MinMaxScaler()\n",
    "    tr_df[f] = scaler.fit_transform(np.array(tr_df[f]).reshape(-1,1))\n",
    "    va_df[f] = scaler.transform(np.array(va_df[f]).reshape(-1,1))\n",
    "    te_df[f] = scaler.transform(np.array(te_df[f]).reshape(-1,1))\n",
    "    scalers[f] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310618d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febd26d",
   "metadata": {},
   "source": [
    "## Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76827e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import trange\n",
    "\n",
    "def get_abs_corr(x,y):\n",
    "    return np.abs(np.corrcoef(x,y))[0,1]\n",
    "\n",
    "class InteractionTerm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,num_features,corr_cutoff=0.7):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        \n",
    "        d = data.copy()\n",
    "        self.interaction_list = []\n",
    "        for i in trange(len(num_features),desc='fitting...'):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    col_i = num_features[i]\n",
    "                    col_j = num_features[j]\n",
    "                    \n",
    "                    # 상관계수가 cutoff보다 큰 경우에는 interaction을 생성하지 않음\n",
    "                    if (get_abs_corr(d[col_i]*d[col_j],d[col_i])>=corr_cutoff) | (get_abs_corr(d[col_i]*d[col_j],d[col_j])>=corr_cutoff):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.interaction_list.append(f'{col_i}*{col_j}')\n",
    "    \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        print('> the number of interaction term:',len(self.interaction_list))\n",
    "        for interaction in self.interaction_list:\n",
    "            col_i,col_j = interaction.split('*')\n",
    "            d[interaction] = d[col_i]*d[col_j]\n",
    "        return d\n",
    "    \n",
    "    def fit_transform(self,data,num_features,corr_cutoff=0.7):\n",
    "        self.fit(data,num_features,corr_cutoff)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d919346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = features + fps_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b98c3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_maker = InteractionTerm()\n",
    "# interaction_maker.fit(\n",
    "#     data=tr_df,\n",
    "#     num_features=num_features,\n",
    "#     corr_cutoff=0.15,\n",
    "# )\n",
    "# tr_df = interaction_maker.transform(tr_df)\n",
    "# va_df = interaction_maker.transform(va_df)\n",
    "# te_df = interaction_maker.transform(te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f50c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148, 261)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702cae5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62772f",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6119500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in targets:\n",
    "#     tr_df[t] = np.log(tr_df[t]+1e-3)\n",
    "#     va_df[t] = np.log(va_df[t]+1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7742f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inverse_transform(x):\n",
    "#     return torch.exp(x)-1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d6997ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "def boxcox_transform(x):\n",
    "    _lambda = 0.25\n",
    "    return boxcox(x,_lambda)\n",
    "\n",
    "def inverse_boxcox_transform(x):\n",
    "    _lambda = 0.25\n",
    "    return inv_boxcox(x,_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68a771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df[targets] = tr_df[targets].apply(boxcox_transform)\n",
    "va_df[targets] = va_df[targets].apply(boxcox_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6d516",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d13e8",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fa09511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, smiles, transforms=None, is_test=False):\n",
    "        self.data = data.copy()\n",
    "        self.targets = targets\n",
    "        self.smiles = smiles\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.smiles_features = []\n",
    "        for s in tqdm(data[smiles].values):\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            img = Draw.MolToImage(m)#, size=(224,224))\n",
    "            img = np.array(img)\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(image=img)['image']\n",
    "            self.smiles_features.append(img)\n",
    "            \n",
    "        if not self.is_test:\n",
    "            self.target_features = self.data[self.targets].values\n",
    "            self.num_features = self.data.drop(columns=targets+[smiles],axis=1).values\n",
    "        else:\n",
    "            self.num_features = self.data.drop(columns=[smiles],axis=1).values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            return (\n",
    "                torch.Tensor(self.num_features[index]),\n",
    "                torch.Tensor(self.smiles_features[index]),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.Tensor(self.num_features[index]),\n",
    "                torch.Tensor(self.smiles_features[index]),\n",
    "                torch.Tensor(self.target_features[index]),\n",
    "            )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f2ec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89c15dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  이미지 변환\n",
    "transform = A.Compose([\n",
    "    A.Resize(CFG.IMG_SIZE,CFG.IMG_SIZE),\n",
    "    A.ToGray(p=1),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19b6df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3148/3148 [00:29<00:00, 105.40it/s]\n",
      "100%|██████████| 350/350 [00:03<00:00, 104.80it/s]\n",
      "100%|██████████| 483/483 [00:04<00:00, 105.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(tr_df, ['MLM','HLM'], 'SMILES', transform, False)\n",
    "val_dataset   = CustomDataset(va_df, ['MLM','HLM'], 'SMILES', transform, False)\n",
    "test_dataset  = CustomDataset(te_df, ['MLM','HLM'], 'SMILES', transform, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d084922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset  , batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=CFG.BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37423d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.smiles_features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca77bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [feat for feat,img,target in train_loader][0]\n",
    "# [img for feat,img in test_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de3ec882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _img = [img for feat,img,target in train_loader][0][0].numpy()\n",
    "# plt.imshow(_img.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2a0e6",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577b7cd",
   "metadata": {
    "id": "8CHzFfvrbnOM"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e196b3",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f650458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e1659c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, feature_input_size, output_size, hidden_size, dropout_rate):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.image_output_size = 500\n",
    "        self.feature_output_size = 500\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # # efficientnet\n",
    "        # self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        # self.backbone.classifier = nn.Sequential(\n",
    "        #     nn.Dropout(p=0.2,inplace=True),\n",
    "        #     nn.Linear(self.backbone.classifier[-1].in_features,self.image_output_size),\n",
    "        # )\n",
    "        \n",
    "        # resnet\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features,self.image_output_size)\n",
    "        \n",
    "        self.image_layer = nn.Sequential(\n",
    "            self.backbone,\n",
    "            #nn.BatchNorm1d(self.image_output_size),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Dropout(self.dropout_rate),\n",
    "        )\n",
    "        \n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(feature_input_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_size,self.feature_output_size),\n",
    "        )\n",
    "        \n",
    "        fc_input_size = self.image_output_size+self.feature_output_size\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(fc_input_size,fc_input_size//2),\n",
    "            nn.BatchNorm1d(fc_input_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//2,fc_input_size//4),\n",
    "            nn.BatchNorm1d(fc_input_size//4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//4,fc_input_size//8),\n",
    "            nn.BatchNorm1d(fc_input_size//8),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//8,fc_input_size//16),\n",
    "        )\n",
    "        self.fc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_input_size,fc_input_size//2),\n",
    "            nn.BatchNorm1d(fc_input_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//2,fc_input_size//4),\n",
    "            nn.BatchNorm1d(fc_input_size//4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//4,fc_input_size//8),\n",
    "            nn.BatchNorm1d(fc_input_size//8),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//8,fc_input_size//16),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*(fc_input_size//16),fc_input_size//16),\n",
    "            nn.BatchNorm1d(fc_input_size//16),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//16,fc_input_size//32),\n",
    "            nn.BatchNorm1d(fc_input_size//32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(fc_input_size//32,output_size),\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, feature):\n",
    "        image_output = self.image_layer(image)\n",
    "        feature_output = self.feature_layer(feature)\n",
    "        combined = torch.cat((image_output,feature_output),dim=1)\n",
    "        output1 = self.fc_1(combined)\n",
    "        output2 = self.fc_2(combined)\n",
    "        output = torch.cat((output1,output2),dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65940dc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d1eba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a09772fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiRMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss1 = torch.sqrt(torch.mean((output[:,0]-target[:,0])**2))\n",
    "        loss2 = torch.sqrt(torch.mean((output[:,1]-target[:,1])**2))\n",
    "        loss = 0.5*loss1+0.5*loss2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0c4ec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        if self.path!='':\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        \n",
    "def train(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, epochs,\n",
    "    early_stopping, device='cpu', scheduler=None, metric_period=1, \n",
    "    verbose=True, save_model_path = './mc/best_model.pt',\n",
    "    use_best_model=True,\n",
    "    inverse_transform=None,\n",
    "):  \n",
    "    seed_everything(CFG.SEED)\n",
    "    model.to(device)\n",
    "\n",
    "    best_loss  = 999999999\n",
    "    best_epoch = 1\n",
    "    best_model = None\n",
    "    is_best    = np.nan\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        gc.collect()\n",
    "        \n",
    "        #model.train()\n",
    "        train_loss = []\n",
    "        for feat,img,target in train_loader:\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img,feat)#.float()\n",
    "            \n",
    "#             if inverse_transform is not None:\n",
    "#                 output = inverse_transform(output)\n",
    "#                 target = inverse_transform(target)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()  # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            valid_loss = validation(model, valid_loader, criterion, device, inverse_transform)\n",
    "        else:\n",
    "            valid_loss = loss\n",
    "            \n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        # update the best epoch & best loss\n",
    "        if (best_loss > valid_loss) | (epoch==1):\n",
    "            best_epoch = epoch\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            is_best = 1\n",
    "            torch.save(best_model.state_dict(), save_model_path)\n",
    "        else:\n",
    "            is_best = 0\n",
    "            if not use_best_model:\n",
    "                torch.save(best_model.state_dict(), save_model_path)\n",
    "            \n",
    "        # 결과물 printing\n",
    "        if (verbose) & (epoch % metric_period == 0):\n",
    "            mark = '*' if is_best else ' '\n",
    "            epoch_str = str(epoch).zfill(len(str(epochs)))\n",
    "            if valid_loader is not None:\n",
    "                progress = '{}[{}/{}] loss: {:.5f}, val_loss: {:.5f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                    .format(\n",
    "                        mark,\n",
    "                        epoch_str,\n",
    "                        epochs,\n",
    "                        np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        best_epoch,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                    )\n",
    "            else:\n",
    "                progress = '{}[{}/{}] loss: {:.5f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                    .format(\n",
    "                        mark,\n",
    "                        epoch_str,\n",
    "                        epochs,\n",
    "                        np.mean(train_loss),\n",
    "                        best_epoch,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                    )\n",
    "            epoch_s = time.time()\n",
    "            print(progress)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation(model, valid_loader, criterion, device, inverse_transform):\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for feat,img,target in valid_loader:\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(img,feat)#.float()\n",
    "            \n",
    "            if inverse_transform is not None:\n",
    "                output = torch.tensor(inverse_transform(output.cpu().numpy()),device=device)\n",
    "                target = torch.tensor(inverse_transform(target.cpu().numpy()),device=device)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)\n",
    "\n",
    "def predict(best_model,loader,device,inverse_transform):\n",
    "    best_model.to(device)\n",
    "\n",
    "    true_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for feat,img,target in iter(loader):\n",
    "            feat = feat.to(device)\n",
    "            img = img.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = best_model(img,feat)\n",
    "            \n",
    "            if inverse_transform is not None:\n",
    "                output = inverse_transform(output)\n",
    "                target = inverse_transform(target)\n",
    "\n",
    "            true_list.append(target)\n",
    "            pred_list.append(output)\n",
    "\n",
    "    trues = torch.cat(true_list,dim=0)\n",
    "    preds = torch.cat(pred_list,dim=0)\n",
    "\n",
    "    trues = trues.cpu().numpy()\n",
    "    preds = preds.cpu().numpy()\n",
    "\n",
    "    return trues, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0e2ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_input_size = [feat for feat,smiles,target in train_dataset][0].shape[0]\n",
    "output_size = 2\n",
    "hidden_size = 64\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "958f1fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8882e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskModel(feature_input_size,output_size,hidden_size,dropout_rate)\n",
    "criterion = MultiRMSELoss()\n",
    "# optimizer= torch.optim.Adam(model.parameters(), lr=CFG.LEARNING_RATE)#, weight_decay=5e-4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=CFG.LEARNING_RATE, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-7, verbose=False)\n",
    "# early_stopping = EarlyStopping(patience=10,verbose=False,path='')\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c4853314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3bd1d958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[0001/1024] loss: 5.56606, val_loss: 52.35971, best_epoch: 1, elapsed: 9.92s, total: 9.92s, remaining: 10144.85s\n",
      "*[0002/1024] loss: 3.86467, val_loss: 38.39719, best_epoch: 2, elapsed: 9.62s, total: 19.68s, remaining: 9832.43s\n",
      "*[0003/1024] loss: 2.94510, val_loss: 35.25707, best_epoch: 3, elapsed: 9.63s, total: 29.45s, remaining: 9829.87s\n",
      "*[0004/1024] loss: 2.85123, val_loss: 34.82481, best_epoch: 4, elapsed: 9.68s, total: 39.29s, remaining: 9873.88s\n",
      "*[0005/1024] loss: 2.72205, val_loss: 34.15099, best_epoch: 5, elapsed: 9.62s, total: 49.06s, remaining: 9804.42s\n",
      " [0006/1024] loss: 2.63933, val_loss: 34.18087, best_epoch: 5, elapsed: 9.62s, total: 58.82s, remaining: 9794.48s\n",
      " [0007/1024] loss: 2.55869, val_loss: 35.31099, best_epoch: 5, elapsed: 9.62s, total: 68.57s, remaining: 9779.19s\n",
      " [0008/1024] loss: 2.47819, val_loss: 34.44461, best_epoch: 5, elapsed: 9.62s, total: 78.33s, remaining: 9774.53s\n",
      " [0009/1024] loss: 2.30068, val_loss: 34.86433, best_epoch: 5, elapsed: 9.62s, total: 88.30s, remaining: 9764.97s\n",
      " [0010/1024] loss: 2.12081, val_loss: 34.78120, best_epoch: 5, elapsed: 9.62s, total: 99.36s, remaining: 9751.92s\n",
      " [0011/1024] loss: 1.96803, val_loss: 34.57998, best_epoch: 5, elapsed: 9.62s, total: 110.63s, remaining: 9745.40s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./mc/best_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse_boxcox_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, epochs, early_stopping, device, scheduler, metric_period, verbose, save_model_path, use_best_model, inverse_transform)\u001b[0m\n\u001b[1;32m     97\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Getting gradients\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Updating parameters\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validation(model, valid_loader, criterion, device, inverse_transform)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = train(\n",
    "    model, criterion, optimizer, train_loader, val_loader,\n",
    "    CFG.EPOCHS, early_stopping,\n",
    "    device=device, scheduler=scheduler,\n",
    "    metric_period=1, verbose=True,\n",
    "    save_model_path='./mc/best_model.pt',\n",
    "    use_best_model=False,\n",
    "    inverse_transform=inverse_boxcox_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a2953",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10826a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d92633",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = MultiTaskModel(feature_input_size,output_size,hidden_size,dropout_rate)\n",
    "best_model.load_state_dict(torch.load('./mc/best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_true, tr_pred = predict(best_model,train_loader,device,inverse_transform)\n",
    "va_true, va_pred = predict(best_model,val_loader,device,inverse_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2acbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(MultiRMSELoss()(torch.tensor(tr_true),torch.tensor(tr_pred)),\n",
    " MultiRMSELoss()(torch.tensor(va_true),torch.tensor(va_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be693b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_true[:10].round(1), tr_pred[:10].round(1)\n",
    "# va_true[:10].round(1), va_pred[:10].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(intercept,slope):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, linestyle='--', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1922d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.add_subplot(121)\n",
    "sns.scatterplot(x=tr_true[:,0],y=tr_pred[:,0])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "fig.add_subplot(122)\n",
    "sns.scatterplot(x=tr_true[:,1],y=tr_pred[:,1])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.suptitle('train',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.add_subplot(121)\n",
    "sns.scatterplot(x=va_true[:,0],y=va_pred[:,0])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "fig.add_subplot(122)\n",
    "sns.scatterplot(x=va_true[:,1],y=va_pred[:,1])\n",
    "abline(0,1)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.suptitle('validation',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0811d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "88a5da79f9030d36a713e3ceec9ed9a47a216907c035af9944c458137c4e5cb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
