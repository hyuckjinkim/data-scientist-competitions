{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2134b0-2c8d-4bec-9f7e-47bfce95a5d8",
   "metadata": {},
   "source": [
    "- kmeans labels 빼기\n",
    "- max_depth 줄이기\n",
    "- nunique 높은 cat 없애기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b987cd1-0680-4fb8-a482-f5567577e80f",
   "metadata": {},
   "source": [
    "# Library Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eeab870-95ed-41a6-b815-1bdf2dee38fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a79e989-ab48-437a-8d4d-bb841cd64ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_offset(x,verbose=True):\n",
    "    if min(x)>0:\n",
    "        offset = 0\n",
    "    elif min(x)==0:\n",
    "        offset = 1e-3\n",
    "    else:\n",
    "        offset = -min(x)+1e-3\n",
    "        if verbose:\n",
    "            print('minimum = {:.3f}'.format(min(x)))\n",
    "    return np.log(x+offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dd57aa-1f46-4164-8abb-ecf2c991b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeController:\n",
    "    def __init__(self,target_feature,cat_features=None,unuse_features=None,segment_feature=None):\n",
    "        assert type(target_feature).__name__ in ['str'], \\\n",
    "            \"target_feature must be 'str'\"\n",
    "        assert type(cat_features).__name__ in ['NoneType','list'], \\\n",
    "            \"cat_feature must be 'None' or 'list'\"\n",
    "        assert type(unuse_features).__name__ in ['NoneType','list'], \\\n",
    "            \"unuse_features must be 'None' or 'list'\"\n",
    "        assert type(segment_feature).__name__ in ['NoneType','str'], \\\n",
    "            \"unuse_features must be 'None' or 'str'\"\n",
    "        \n",
    "        self.target_feature     = target_feature\n",
    "        self.fixed_cat_features = [] if cat_features    is None else cat_features\n",
    "        self.unuse_features     = [] if unuse_features  is None else unuse_features\n",
    "        self.segment_feature    = [] if segment_feature is None else segment_feature\n",
    "    \n",
    "    def _check_dummy(self,data,col):\n",
    "        return (data[col].nunique()==2) & (sorted(data[col].unique()) == [0,1])\n",
    "    \n",
    "    def _check_str(self,data,col):\n",
    "        try:\n",
    "            data[col].astype(float)\n",
    "            dtype = 'float'\n",
    "        except:\n",
    "            dtype = 'nan'\n",
    "        return dtype=='nan'\n",
    "    \n",
    "    def _get_feature_type(self,verbose):\n",
    "        feature_list = ['target_feature','unuse_features','dummy_features','cat_features','num_features','segment_feature']\n",
    "        if verbose:\n",
    "            print('> gloval variable assignment')\n",
    "            \n",
    "        i=0\n",
    "        for feature in feature_list:\n",
    "            i+=1\n",
    "            if verbose:\n",
    "                print(\"[{}] complete: {}\".format(i,feature))\n",
    "            exec(\"globals()['{}'] = self.{}\".format(feature,feature))\n",
    "    \n",
    "    def fit(self,data,global_assignment=True,verbose=True) -> None:\n",
    "        self.cat_features   = []\n",
    "        self.dummy_features = []\n",
    "        self.num_features   = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            if col==self.target_feature:\n",
    "                pass\n",
    "            elif col in self.unuse_features:\n",
    "                pass\n",
    "            elif col==self.segment_feature:\n",
    "                pass\n",
    "            elif col in self.fixed_cat_features:\n",
    "                self.cat_features.append(col)\n",
    "            elif self._check_dummy(data,col):\n",
    "                self.dummy_features.append(col)\n",
    "            elif self._check_str(data,col):\n",
    "                self.cat_features.append(col)\n",
    "            else:\n",
    "                self.num_features.append(col)\n",
    "        \n",
    "        if global_assignment:\n",
    "            self._get_feature_type(verbose)\n",
    "\n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        \n",
    "        ## (1) unuse_features\n",
    "        #d.drop(columns=self.unuse_features,inplace=True)\n",
    "        \n",
    "        # (2) segment_feature\n",
    "        d[self.segment_feature] = d[self.segment_feature].astype(str)\n",
    "        \n",
    "        # (3) dummy_features\n",
    "        d[self.dummy_features] = d[self.dummy_features].astype(int)\n",
    "        \n",
    "        # (4) cat_features\n",
    "        d[self.cat_features] = d[self.cat_features].astype(str)\n",
    "        \n",
    "        # (5) num_features\n",
    "        d[self.num_features] = d[self.num_features].astype(float)\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b6f33-3b7d-401a-a710-ea0f089f95a7",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b54b8f-874d-49c7-95db-a535b984b01d",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe03a3b-5d80-448f-ad15-50e77c5128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    UNUSE_FEATURES = ['ID','판매도시']\n",
    "    \n",
    "    SEED = 0\n",
    "    \n",
    "    SUBSET_DEPTH = 2\n",
    "    INTERACTION = False\n",
    "    FS_ALPHA = 0.01\n",
    "    \n",
    "    N_SPLITS = 5\n",
    "    TARGET_TRANSFORMATION = ['log','sqrt'][0]\n",
    "    \n",
    "    LR = 0.01\n",
    "    EPOCHS = 30000\n",
    "    ES = 300\n",
    "    XGB_LR = 0.01     # default=0.3\n",
    "    XGB_EPOCHS = 3000 # default=100\n",
    "    XGB_ES = 100\n",
    "    XTRATREES_EPOCHS = 30 #default=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2cfe-7bf1-4cc3-b7e3-90b0886eab54",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbf58b-29ed-4ab7-9a2c-f51c6b08c00a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8ae59-c4c3-47f8-854a-6cfc780a2adc",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a824834-ad27-424a-8f55-7c4ca73eaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da88a595-b068-4b4b-97a9-4e6bcf863305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57920, 15), (14480, 14))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d6f038-af80-42c0-a33e-e62dcd8bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>생산년도</th>\n",
       "      <th>모델출시년도</th>\n",
       "      <th>브랜드</th>\n",
       "      <th>차량모델명</th>\n",
       "      <th>판매도시</th>\n",
       "      <th>판매구역</th>\n",
       "      <th>주행거리</th>\n",
       "      <th>배기량</th>\n",
       "      <th>압축천연가스(CNG)</th>\n",
       "      <th>경유</th>\n",
       "      <th>가솔린</th>\n",
       "      <th>하이브리드</th>\n",
       "      <th>액화석유가스(LPG)</th>\n",
       "      <th>가격</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2014</td>\n",
       "      <td>skoda</td>\n",
       "      <td>fabia</td>\n",
       "      <td>KAT</td>\n",
       "      <td>SLA</td>\n",
       "      <td>85231</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>2010</td>\n",
       "      <td>2006</td>\n",
       "      <td>toyota</td>\n",
       "      <td>auris</td>\n",
       "      <td>RKO</td>\n",
       "      <td>SWI</td>\n",
       "      <td>135000</td>\n",
       "      <td>1598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>clk-klasa</td>\n",
       "      <td>GNI</td>\n",
       "      <td>WIE</td>\n",
       "      <td>255223</td>\n",
       "      <td>1796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>2006</td>\n",
       "      <td>2001</td>\n",
       "      <td>nissan</td>\n",
       "      <td>x-trail</td>\n",
       "      <td>EHX</td>\n",
       "      <td>WIE</td>\n",
       "      <td>238000</td>\n",
       "      <td>2184</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>fiat</td>\n",
       "      <td>bravo</td>\n",
       "      <td>OSW</td>\n",
       "      <td>MAL</td>\n",
       "      <td>251000</td>\n",
       "      <td>1910</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  생산년도  모델출시년도            브랜드      차량모델명 판매도시 판매구역    주행거리  \\\n",
       "0  TRAIN_00000  2018    2014          skoda      fabia  KAT  SLA   85231   \n",
       "1  TRAIN_00001  2010    2006         toyota      auris  RKO  SWI  135000   \n",
       "2  TRAIN_00002  2002    2002  mercedes-benz  clk-klasa  GNI  WIE  255223   \n",
       "3  TRAIN_00003  2006    2001         nissan    x-trail  EHX  WIE  238000   \n",
       "4  TRAIN_00004  2007    2007           fiat      bravo  OSW  MAL  251000   \n",
       "\n",
       "    배기량  압축천연가스(CNG)  경유  가솔린  하이브리드  액화석유가스(LPG)     가격  \n",
       "0   999            0   0    1      0            0  51.74  \n",
       "1  1598            0   0    1      0            0  41.47  \n",
       "2  1796            0   0    1      0            0  17.81  \n",
       "3  2184            0   1    0      0            0  18.20  \n",
       "4  1910            0   1    0      0            0  17.55  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e90ba7-c342-4e79-b4a1-da5c1131feb8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f612e65-e18e-4970-9a9c-942cb3d1d712",
   "metadata": {},
   "source": [
    "## Type Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65e9aa6-9c2b-4995-a0e5-ab0990c37419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gloval variable assignment\n",
      "[1] complete: target_feature\n",
      "[2] complete: unuse_features\n",
      "[3] complete: dummy_features\n",
      "[4] complete: cat_features\n",
      "[5] complete: num_features\n",
      "[6] complete: segment_feature\n"
     ]
    }
   ],
   "source": [
    "type_controller = TypeController(\n",
    "    target_feature='가격',\n",
    "    cat_features=None,\n",
    "    unuse_features=CFG.UNUSE_FEATURES,\n",
    "    segment_feature=None,\n",
    ")\n",
    "type_controller.fit(\n",
    "    data=train_df,\n",
    "    global_assignment=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_df2 = type_controller.transform(train_df)\n",
    "test_df2  = type_controller.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469aee08-7b2e-4cc2-8b6f-2254c8aa5927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['브랜드', '차량모델명', '판매구역']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd75703f-64bb-4048-a151-adcf9095bca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57920, 15), (57920, 15))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unuse_feature 제거\n",
    "train_df.shape, train_df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04590f97-2b7a-4039-aa51-7528b7f14a91",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b653065-ce2f-4186-89d0-eb71b4bd0986",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30b74aa-75d9-4c9e-a67d-71a97fe529b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.TARGET_TRANSFORMATION=='log':\n",
    "    train_df2['가격'] = np.log(train_df2['가격'])\n",
    "elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "    train_df2['가격'] = np.sqrt(train_df2['가격'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204816d7-af14-427d-bc50-32b876febd64",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5254fac-62ad-4ca8-98d9-0cc687d11af2",
   "metadata": {},
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae5c16b-cf03-4b32-894f-10ef35d8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([str(round(int(year)/100,1)) for year in train_df6['생산년도']]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6541725-7cf0-4e7d-960b-cb96da805e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def all_subsets(ss):\n",
    "    return list(chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1))))\n",
    "\n",
    "def has_test_only_value(train,test,group):\n",
    "    tr_uniques = ['_'.join(x) for x in train[group].drop_duplicates().values]\n",
    "    te_uniques = ['_'.join(x) for x in test [group].drop_duplicates().values]\n",
    "    test_only = list(set(te_uniques)-set(tr_uniques))\n",
    "    _has_test_only = False if len(test_only)==0 else True\n",
    "    return _has_test_only\n",
    "\n",
    "def add_fuel_type(data):\n",
    "    dummy_features = ['압축천연가스(CNG)','액화석유가스(LPG)','경유','가솔린','하이브리드']\n",
    "    d = data.copy()\n",
    "    d['fuel_type'] = d[dummy_features].apply(\n",
    "        lambda x: dummy_features[np.where(x==1)[0][0]],axis=1)\n",
    "    return d['fuel_type']\n",
    "\n",
    "def create_production_year_group(data,unknown_value=99999):\n",
    "    \n",
    "    ## 브랜드별 year criterion 생성\n",
    "    # >=기준\n",
    "    production_year_criterion = [\n",
    "        ['opel', [2000]],\n",
    "        ['ford', [2003]],\n",
    "        ['volkswagen', [1998]],\n",
    "        ['audi', [1996]],\n",
    "        ['bmw', [1998]],\n",
    "        ['renault', [2000]],\n",
    "        ['toyota', [2003]],\n",
    "        ['skoda', [None]],\n",
    "        ['mercedes-benz', [2001]],\n",
    "        ['nissan', [2003]],\n",
    "        ['kia', [None]],\n",
    "        ['hyundai', [2016,2012]],\n",
    "        ['seat', [2004]],\n",
    "        ['mazda', [None]],\n",
    "        ['honda', [None]],\n",
    "        ['volvo', [None]],\n",
    "        ['fiat', [None]],\n",
    "        ['citroen', [None]],\n",
    "        ['peugeot', [2017]],\n",
    "        ['mitsubishi', [2007]],\n",
    "    ]\n",
    "\n",
    "    ## 브랜드별 year criterion 기준으로 group 생성\n",
    "    final_data = []\n",
    "    final_data = []\n",
    "    for brand, year_criterion in production_year_criterion:\n",
    "        d = data[data['브랜드']==brand].copy()\n",
    "        d['production_year_grp'] = unknown_value\n",
    "\n",
    "        if year_criterion[0] is None:\n",
    "            d['production_year_grp'] = 0\n",
    "            final_data.append(d)\n",
    "        else:\n",
    "            n = len(year_criterion)\n",
    "            grp = 0\n",
    "            for year in year_criterion:\n",
    "                grp+=1\n",
    "                d.loc[(d['생산년도']>=year)&(d['production_year_grp']==unknown_value),'production_year_grp'] = grp\n",
    "                if n==grp:\n",
    "                    d.loc[(d['생산년도']<year),'production_year_grp'] = grp+1\n",
    "            final_data.append(d)\n",
    "\n",
    "    # final concat\n",
    "    final_data = pd.concat(final_data,axis=0)\n",
    "    final_data = final_data.sort_index()\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def segment_count(train,test=None,segment=[]):\n",
    "    tr_seg_cnt = train.groupby(segment).size().reset_index().rename(columns={0:'tr_cnt'})\n",
    "    if test is not None:\n",
    "        te_seg_cnt = test.groupby(segment).size().reset_index().rename(columns={0:'te_cnt'})\n",
    "        seg_cnt = pd.merge(tr_seg_cnt,te_seg_cnt,how='left',on=segment).fillna(0)\n",
    "        seg_cnt['cnt_diff'] = seg_cnt['tr_cnt'] - seg_cnt['te_cnt']\n",
    "        int_cols = ['tr_cnt','te_cnt','cnt_diff']\n",
    "    else:\n",
    "        seg_cnt = tr_seg_cnt\n",
    "        int_cols = ['tr_cnt']\n",
    "    seg_cnt[int_cols] = seg_cnt[int_cols].astype(int)\n",
    "    return seg_cnt\n",
    "\n",
    "# # example)\n",
    "# train_df3 = create_production_year_group(train_df2)\n",
    "# test_df3  = create_production_year_group(test_df2)\n",
    "# segment_count(train_df3,test_df3,['브랜드','production_year_grp'])\n",
    "# segment_count(train_df3,None,['브랜드','production_year_grp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad20425d-a0f1-4f11-8046-16b7b3e84089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_quantile(self,x,col):\n",
    "        x = np.array(x).flatten()\n",
    "        x = x[pd.notnull(x)]\n",
    "\n",
    "        agg_df = pd.DataFrame(index=[0])\n",
    "        agg_df[f'{col}_Avg'] = np.mean(x)\n",
    "        for q in [25,50,75]:\n",
    "            agg_df[f'{col}_Q{q}'] = np.quantile(x,q/100)\n",
    "\n",
    "        return agg_df\n",
    "    \n",
    "    def _derived_features(self,data):\n",
    "        d = data.copy()\n",
    "\n",
    "        # (1) 모델출시년도에 생산된 차량인지\n",
    "        d['출시년도생산여부'] = np.where(d['생산년도'].astype(int)==d['모델출시년도'].astype(int),1,0)\n",
    "\n",
    "        # (2) 모델출시 이후에 몇년 지나서 생산됬는지\n",
    "        d['출시이후생산년수'] = d['생산년도'].astype(int)-d['모델출시년도'].astype(int)\n",
    "\n",
    "        # (3) 출시 이전에 생산되었는지\n",
    "        d['출시이전생산여부'] = np.where(d['출시이후생산년수']<=0,1,0)\n",
    "\n",
    "        # (4) 브랜드의 국적 (구글링)\n",
    "        d['브랜드국적'] = ['체코' if brand=='skoda' else\n",
    "                        '일본' if brand in ['toyota','nissan','mazda','honda','mitsubishi'] else\n",
    "                        '독일' if brand in ['mercedes-benz','audi','volkswagen','bmw','opel'] else\n",
    "                        '이탈리아' if brand=='fiat' else\n",
    "                        '프랑스' if brand in ['renault','citroen','peugeot'] else\n",
    "                        '미국' if brand=='ford' else\n",
    "                        '한국' if brand in ['kia','hyundai'] else\n",
    "                        '스페인' if brand=='seat' else\n",
    "                        '스웨덴' if brand=='volvo' else\n",
    "                        np.nan for brand in d['브랜드']]\n",
    "\n",
    "        # (5) 브랜드 국적의 대륙명\n",
    "        d['브랜드대륙명'] = ['유럽' if country in ['체코','독일','이탈리아','프랑스','스페인','스웨덴'] else\n",
    "                          '아시아' if country in ['일본','한국'] else\n",
    "                          '아메리카' if country in ['미국'] else\n",
    "                          np.nan for country in d['브랜드국적']]\n",
    "        \n",
    "        # (6) CNG or LPG\n",
    "        d['가스'] = [np.nan if (cng==np.nan)|(lpg==np.nan) else\n",
    "                    1 if (cng==1)|(lpg==1) else \n",
    "                    0 for cng,lpg in d[['압축천연가스(CNG)','액화석유가스(LPG)']].values]\n",
    "        d['가스'] = d['가스'].astype('Int64')\n",
    "        \n",
    "        # (7) 연료타입\n",
    "        d['연료타입'] = add_fuel_type(d)\n",
    "        d.drop(columns=['압축천연가스(CNG)','액화석유가스(LPG)','경유','가솔린','하이브리드'],inplace=True)\n",
    "        \n",
    "        # (8) 판매도시구역 동일여부\n",
    "        d['판매도시구역동일여부'] = np.where(d['판매도시']==d['판매구역'],1,0)\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    # test_data는 test에만 있는 group을 판별 시 사용됨\n",
    "    # -> 함수 : has_test_only_value(data,test_data,subset)\n",
    "    def fit(self,data,test_data,target_feature,num_features=[],cat_features=[],dummy_features=[],subset_depth=1):\n",
    "        assert len(cat_features)>=subset_depth, \\\n",
    "            'len(cat_features) >= subset_depth'\n",
    "        \n",
    "        self.data           = data\n",
    "        self.test_data      = data\n",
    "        self.target_feature = target_feature\n",
    "        self.num_features   = num_features\n",
    "        self.cat_features   = cat_features\n",
    "        self.dummy_features = dummy_features\n",
    "        self.subset_depth   = subset_depth\n",
    "        \n",
    "        self.original_features = [target_feature]+num_features+cat_features+dummy_features\n",
    "        \n",
    "        # (1) 파생변수생성\n",
    "        data      = self._derived_features(data)\n",
    "        test_data = self._derived_features(test_data)\n",
    "        \n",
    "        # (2) 브랜드별 생산년도기준으로 group 생성\n",
    "        data      = create_production_year_group(data)\n",
    "        test_data = create_production_year_group(test_data)\n",
    "        # segment_count(data,test_data,segment=['브랜드','production_year_grp'])\n",
    "        \n",
    "        # (3) 카테고리 변수에 따른 가격의 Quantile값\n",
    "        all_subset_list = all_subsets(self.cat_features) #+self.new_cat_features\n",
    "        all_subset_list = [subset for subset in all_subset_list if (len(subset)<=subset_depth) & (len(subset)>=1)]\n",
    "        print(f'> Get quantiles of target by categorical features (depth={subset_depth})')\n",
    "        \n",
    "        self.agg_dict = {}\n",
    "        pbar = tqdm(all_subset_list)\n",
    "        for subset in pbar:\n",
    "            subset = list(subset)\n",
    "            if has_test_only_value(data,test_data,subset):\n",
    "                pass\n",
    "            else:\n",
    "                pbar.set_description('Subset: {}'.format(' + '.join(subset)))\n",
    "                subset_name = '_'.join(subset)\n",
    "                agg_fn = data\\\n",
    "                    .groupby(subset)[self.target_feature]\\\n",
    "                    .apply(lambda x: self._get_quantile(x,subset_name))\\\n",
    "                    .reset_index()\n",
    "                drop_cols = [col for col in agg_fn if col.find('level_')>=0]\n",
    "                agg_fn.drop(columns=drop_cols,inplace=True)\n",
    "                if agg_fn.isnull().sum().sum()>0:\n",
    "                    print('> Null Detectd: {} -> Passed'.format('+'.join(subset)))\n",
    "                else:\n",
    "                    self.agg_dict[subset_name] = agg_fn\n",
    "                    \n",
    "        self.deleted_dummy_features = ['압축천연가스(CNG)','액화석유가스(LPG)','경유','가솔린','하이브리드']\n",
    "        self.new_cat_features       = ['브랜드국적','브랜드대륙명','연료타입','production_year_grp']\n",
    "        self.new_dummy_features     = ['출시년도생산여부','출시이전생산여부','가스','판매도시구역동일여부']\n",
    "        \n",
    "    def transform(self,data):\n",
    "        # (1) 파생변수생성\n",
    "        data = self._derived_features(data)\n",
    "        \n",
    "        # (2) 브랜드별 생산년도기준으로 group 생성\n",
    "        data = create_production_year_group(data)\n",
    "        \n",
    "        # (3) 카테고리 변수에 따른 가격의 Quantile값\n",
    "        for key,agg_fn in self.agg_dict.items():\n",
    "            data = pd.merge(data,agg_fn,how='left',on=key.split('_'))\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self,data,test_data,target_feature,num_features=[],cat_features=[],dummy_features=[],subset_depth=1):\n",
    "        self.fit(data,test_data,target_feature,num_features,cat_features,dummy_features,subset_depth)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a085347-abaa-4e5f-9f9b-f71f226bf84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Get quantiles of target by categorical features (depth=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset: 브랜드 + 판매구역: 100%|██████████| 6/6 [00:00<00:00,  8.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineering()\n",
    "fe.fit(\n",
    "    data=train_df2,\n",
    "    test_data=test_df2,\n",
    "    target_feature=target_feature,\n",
    "    num_features=num_features,\n",
    "    cat_features=cat_features,\n",
    "    dummy_features=dummy_features,\n",
    "    subset_depth=CFG.SUBSET_DEPTH,\n",
    ")\n",
    "train_df3 = fe.transform(train_df2)\n",
    "test_df3  = fe.transform(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb5e6f39-f8e0-4041-8000-00555d6d149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57920, 15), (57920, 39))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.shape, train_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa09b13-62ca-425d-bab5-299828454fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gloval variable assignment\n",
      "[1] complete: target_feature\n",
      "[2] complete: unuse_features\n",
      "[3] complete: dummy_features\n",
      "[4] complete: cat_features\n",
      "[5] complete: num_features\n",
      "[6] complete: segment_feature\n"
     ]
    }
   ],
   "source": [
    "type_controller = TypeController(\n",
    "    target_feature='가격',\n",
    "    cat_features=None,\n",
    "    unuse_features=CFG.UNUSE_FEATURES,\n",
    "    segment_feature=None,\n",
    ")\n",
    "type_controller.fit(\n",
    "    data=train_df3,\n",
    "    global_assignment=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_df3 = type_controller.transform(train_df3)\n",
    "test_df3  = type_controller.transform(test_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc448890-ad68-4ad5-ab2a-dc25982aaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null이 있으면 안됨\n",
    "assert (train_df3.isnull().sum().sum()==0) & (test_df3.isnull().sum().sum()==0),\\\n",
    "    \"null detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af8e5c65-95f9-4a32-b63d-c074537853de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57920, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>생산년도</th>\n",
       "      <th>모델출시년도</th>\n",
       "      <th>브랜드</th>\n",
       "      <th>차량모델명</th>\n",
       "      <th>판매도시</th>\n",
       "      <th>판매구역</th>\n",
       "      <th>주행거리</th>\n",
       "      <th>배기량</th>\n",
       "      <th>가격</th>\n",
       "      <th>출시년도생산여부</th>\n",
       "      <th>출시이후생산년수</th>\n",
       "      <th>출시이전생산여부</th>\n",
       "      <th>브랜드국적</th>\n",
       "      <th>브랜드대륙명</th>\n",
       "      <th>가스</th>\n",
       "      <th>연료타입</th>\n",
       "      <th>판매도시구역동일여부</th>\n",
       "      <th>production_year_grp</th>\n",
       "      <th>브랜드_Avg</th>\n",
       "      <th>브랜드_Q25</th>\n",
       "      <th>브랜드_Q50</th>\n",
       "      <th>브랜드_Q75</th>\n",
       "      <th>차량모델명_Avg</th>\n",
       "      <th>차량모델명_Q25</th>\n",
       "      <th>차량모델명_Q50</th>\n",
       "      <th>차량모델명_Q75</th>\n",
       "      <th>판매구역_Avg</th>\n",
       "      <th>판매구역_Q25</th>\n",
       "      <th>판매구역_Q50</th>\n",
       "      <th>판매구역_Q75</th>\n",
       "      <th>브랜드_차량모델명_Avg</th>\n",
       "      <th>브랜드_차량모델명_Q25</th>\n",
       "      <th>브랜드_차량모델명_Q50</th>\n",
       "      <th>브랜드_차량모델명_Q75</th>\n",
       "      <th>브랜드_판매구역_Avg</th>\n",
       "      <th>브랜드_판매구역_Q25</th>\n",
       "      <th>브랜드_판매구역_Q50</th>\n",
       "      <th>브랜드_판매구역_Q75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>skoda</td>\n",
       "      <td>fabia</td>\n",
       "      <td>KAT</td>\n",
       "      <td>SLA</td>\n",
       "      <td>85231.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>체코</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.881733</td>\n",
       "      <td>3.393501</td>\n",
       "      <td>4.057853</td>\n",
       "      <td>4.509072</td>\n",
       "      <td>3.470557</td>\n",
       "      <td>2.933059</td>\n",
       "      <td>3.712352</td>\n",
       "      <td>4.065687</td>\n",
       "      <td>3.734607</td>\n",
       "      <td>3.206803</td>\n",
       "      <td>3.773910</td>\n",
       "      <td>4.355041</td>\n",
       "      <td>3.470557</td>\n",
       "      <td>2.933059</td>\n",
       "      <td>3.712352</td>\n",
       "      <td>4.065687</td>\n",
       "      <td>3.910907</td>\n",
       "      <td>3.477232</td>\n",
       "      <td>4.021236</td>\n",
       "      <td>4.525044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>toyota</td>\n",
       "      <td>auris</td>\n",
       "      <td>RKO</td>\n",
       "      <td>SWI</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>3.724970</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.887563</td>\n",
       "      <td>3.353407</td>\n",
       "      <td>4.000034</td>\n",
       "      <td>4.435212</td>\n",
       "      <td>3.979365</td>\n",
       "      <td>3.594569</td>\n",
       "      <td>4.112512</td>\n",
       "      <td>4.387075</td>\n",
       "      <td>3.561957</td>\n",
       "      <td>3.113071</td>\n",
       "      <td>3.520461</td>\n",
       "      <td>4.066802</td>\n",
       "      <td>3.979365</td>\n",
       "      <td>3.594569</td>\n",
       "      <td>4.112512</td>\n",
       "      <td>4.387075</td>\n",
       "      <td>3.823161</td>\n",
       "      <td>3.341912</td>\n",
       "      <td>3.843102</td>\n",
       "      <td>4.286479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>clk-klasa</td>\n",
       "      <td>GNI</td>\n",
       "      <td>WIE</td>\n",
       "      <td>255223.0</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>2.879760</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>독일</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.736959</td>\n",
       "      <td>3.152736</td>\n",
       "      <td>3.785779</td>\n",
       "      <td>4.354655</td>\n",
       "      <td>3.396003</td>\n",
       "      <td>3.147165</td>\n",
       "      <td>3.440418</td>\n",
       "      <td>3.627069</td>\n",
       "      <td>3.646686</td>\n",
       "      <td>3.089678</td>\n",
       "      <td>3.707577</td>\n",
       "      <td>4.296605</td>\n",
       "      <td>3.396003</td>\n",
       "      <td>3.147165</td>\n",
       "      <td>3.440418</td>\n",
       "      <td>3.627069</td>\n",
       "      <td>3.700911</td>\n",
       "      <td>3.130263</td>\n",
       "      <td>3.746677</td>\n",
       "      <td>4.334280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>nissan</td>\n",
       "      <td>x-trail</td>\n",
       "      <td>EHX</td>\n",
       "      <td>WIE</td>\n",
       "      <td>238000.0</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>2.901422</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.876830</td>\n",
       "      <td>3.558201</td>\n",
       "      <td>3.923359</td>\n",
       "      <td>4.405499</td>\n",
       "      <td>4.354330</td>\n",
       "      <td>4.368303</td>\n",
       "      <td>4.575844</td>\n",
       "      <td>4.761062</td>\n",
       "      <td>3.646686</td>\n",
       "      <td>3.089678</td>\n",
       "      <td>3.707577</td>\n",
       "      <td>4.296605</td>\n",
       "      <td>4.354330</td>\n",
       "      <td>4.368303</td>\n",
       "      <td>4.575844</td>\n",
       "      <td>4.761062</td>\n",
       "      <td>3.809279</td>\n",
       "      <td>3.477232</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>4.321082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>fiat</td>\n",
       "      <td>bravo</td>\n",
       "      <td>OSW</td>\n",
       "      <td>MAL</td>\n",
       "      <td>251000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이탈리아</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.509326</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>3.529985</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>3.057793</td>\n",
       "      <td>2.894253</td>\n",
       "      <td>3.065725</td>\n",
       "      <td>3.210720</td>\n",
       "      <td>3.715470</td>\n",
       "      <td>3.201526</td>\n",
       "      <td>3.755837</td>\n",
       "      <td>4.305416</td>\n",
       "      <td>3.057793</td>\n",
       "      <td>2.894253</td>\n",
       "      <td>3.065725</td>\n",
       "      <td>3.210720</td>\n",
       "      <td>3.563764</td>\n",
       "      <td>2.897563</td>\n",
       "      <td>3.558201</td>\n",
       "      <td>4.360024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID    생산년도  모델출시년도            브랜드      차량모델명 판매도시 판매구역      주행거리  \\\n",
       "0  TRAIN_00000  2018.0  2014.0          skoda      fabia  KAT  SLA   85231.0   \n",
       "1  TRAIN_00001  2010.0  2006.0         toyota      auris  RKO  SWI  135000.0   \n",
       "2  TRAIN_00002  2002.0  2002.0  mercedes-benz  clk-klasa  GNI  WIE  255223.0   \n",
       "3  TRAIN_00003  2006.0  2001.0         nissan    x-trail  EHX  WIE  238000.0   \n",
       "4  TRAIN_00004  2007.0  2007.0           fiat      bravo  OSW  MAL  251000.0   \n",
       "\n",
       "      배기량        가격  출시년도생산여부  출시이후생산년수  출시이전생산여부 브랜드국적 브랜드대륙명  가스 연료타입  \\\n",
       "0   999.0  3.946231         0       4.0         0    체코     유럽   0  가솔린   \n",
       "1  1598.0  3.724970         0       4.0         0    일본    아시아   0  가솔린   \n",
       "2  1796.0  2.879760         1       0.0         1    독일     유럽   0  가솔린   \n",
       "3  2184.0  2.901422         0       5.0         0    일본    아시아   0   경유   \n",
       "4  1910.0  2.865054         1       0.0         1  이탈리아     유럽   0   경유   \n",
       "\n",
       "   판매도시구역동일여부  production_year_grp   브랜드_Avg   브랜드_Q25   브랜드_Q50   브랜드_Q75  \\\n",
       "0           0                  0.0  3.881733  3.393501  4.057853  4.509072   \n",
       "1           0                  1.0  3.887563  3.353407  4.000034  4.435212   \n",
       "2           0                  1.0  3.736959  3.152736  3.785779  4.354655   \n",
       "3           0                  1.0  3.876830  3.558201  3.923359  4.405499   \n",
       "4           0                  0.0  3.509326  2.865054  3.529985  4.174387   \n",
       "\n",
       "   차량모델명_Avg  차량모델명_Q25  차량모델명_Q50  차량모델명_Q75  판매구역_Avg  판매구역_Q25  판매구역_Q50  \\\n",
       "0   3.470557   2.933059   3.712352   4.065687  3.734607  3.206803  3.773910   \n",
       "1   3.979365   3.594569   4.112512   4.387075  3.561957  3.113071  3.520461   \n",
       "2   3.396003   3.147165   3.440418   3.627069  3.646686  3.089678  3.707577   \n",
       "3   4.354330   4.368303   4.575844   4.761062  3.646686  3.089678  3.707577   \n",
       "4   3.057793   2.894253   3.065725   3.210720  3.715470  3.201526  3.755837   \n",
       "\n",
       "   판매구역_Q75  브랜드_차량모델명_Avg  브랜드_차량모델명_Q25  브랜드_차량모델명_Q50  브랜드_차량모델명_Q75  \\\n",
       "0  4.355041       3.470557       2.933059       3.712352       4.065687   \n",
       "1  4.066802       3.979365       3.594569       4.112512       4.387075   \n",
       "2  4.296605       3.396003       3.147165       3.440418       3.627069   \n",
       "3  4.296605       4.354330       4.368303       4.575844       4.761062   \n",
       "4  4.305416       3.057793       2.894253       3.065725       3.210720   \n",
       "\n",
       "   브랜드_판매구역_Avg  브랜드_판매구역_Q25  브랜드_판매구역_Q50  브랜드_판매구역_Q75  \n",
       "0      3.910907      3.477232      4.021236      4.525044  \n",
       "1      3.823161      3.341912      3.843102      4.286479  \n",
       "2      3.700911      3.130263      3.746677      4.334280  \n",
       "3      3.809279      3.477232      3.946231      4.321082  \n",
       "4      3.563764      2.897563      3.558201      4.360024  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df3.shape)\n",
    "train_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663fe09c-896b-48ba-9072-8454af73343b",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f1466-3f68-43ec-a104-181930321dce",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f527f3-73cf-4be0-a2ce-81c594e568af",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed2f2b-9e63-43c3-b9af-437636db16a8",
   "metadata": {},
   "source": [
    "## Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad975e6b-73ff-4b04-9558-52c3af3604d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# data = train_df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f78064-a2e4-431e-94e3-913322d4655f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffd1c6-84ca-4aa8-b92e-084af242f327",
   "metadata": {},
   "source": [
    "### Without group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d1e3686-4ea5-4147-a968-4dc93785c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (1) quantile, avg 변수를 제외한 numerical features\n",
    "# fixed_num_features = [col for col in num_features if (col.find('_Q')<0)&(col.find('_A')<0)]\n",
    "\n",
    "# i=0\n",
    "# for col in fixed_num_features:\n",
    "#     i+=1\n",
    "#     corr,pvalue = scipy.stats.pearsonr(data[col],data[target_feature])\n",
    "\n",
    "#     title = '\\n({}/{}) {} (corr={:.3f}, pvalue={:.3f})'.format(i,len(fixed_num_features),col,corr,pvalue)\n",
    "    \n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=data[col],y=data[target_feature])\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.grid()\n",
    "#     plt.title(title)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e07a12-b123-4089-abc3-c57ae8007d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (2) nunique<40인 numerical features\n",
    "# nunique_info = train_df3[fixed_num_features].nunique()\n",
    "# low_freq_num_features = nunique_info[nunique_info<40].index.tolist()\n",
    "\n",
    "# i=0\n",
    "# for col in low_freq_num_features:\n",
    "#     i+=1\n",
    "#     title = '\\n({}/{}) {}'.format(i,len(low_freq_num_features),col)\n",
    "    \n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=train_df3[col],y=train_df3[target_feature],order=sorted(train_df3[col].unique()))\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.grid()\n",
    "#     plt.title(title)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338fe0d1-ed9a-4dd6-8fc5-aeb66c56c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (1결과) 전체기준 grouping\n",
    "# grp = [0 if year>=1998 else\n",
    "#        1 if year>=1992 else\n",
    "#        2 for year in train_df3['생산년도']]\n",
    "\n",
    "# # grp = [0 if year>=1998 else\n",
    "# #        1 if year>=1992 else\n",
    "# #        2 for year in train_df3['모델출시년도']]\n",
    "\n",
    "# display(pd.Series(grp).value_counts())\n",
    "\n",
    "# plt.figure(figsize=(15,7))\n",
    "# sns.boxplot(x=grp,y=train_df3[target_feature])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125e7e3-3810-456f-b5ad-96fdefcc04fa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b041677-d832-44c9-a215-2e471f226292",
   "metadata": {},
   "source": [
    "### With group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f33298-a2dc-4e79-9068-259bc96cc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # str만 가능하고, list는 안됨\n",
    "# group_feature = '브랜드'\n",
    "# check_num_feature = ['생산년도','모델출시년도'][1]\n",
    "# tr_data = train_df3.copy()\n",
    "# te_data = test_df3 .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3aeb66e-467c-4f4b-833c-8e183c6e2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (1) 브랜드별 생산년도 최소/최대값 확인 : 학습/테스트셋 사이의 생산년도 min/max의 gap이 큰지 확인하기위해서\n",
    "# brand_list = tr_data[group_feature].value_counts()\\\n",
    "#     .sort_values(ascending=False).index.tolist()\n",
    "# brand_name_max = max([len(b) for b in brand_list])\n",
    "\n",
    "# if check_num_feature in ['생산년도','모델출시년도']:\n",
    "#     types = {check_num_feature:int}\n",
    "# else:\n",
    "#     types = {}\n",
    "\n",
    "# i=0\n",
    "# for brand in brand_list:\n",
    "#     i+=1\n",
    "#     tr_year = tr_data[tr_data[group_feature]==brand][check_num_feature].astype(types)\n",
    "#     te_year = te_data[te_data[group_feature]==brand][check_num_feature].astype(types)\n",
    "    \n",
    "#     blank = ' '*(brand_name_max-len(brand)+1)\n",
    "#     i_str = str(i).zfill(len(str(len(brand_list))))\n",
    "#     text = '[{}/{}] {}{}: train({}~{}), test({}~{})'\\\n",
    "#         .format(i_str,len(brand_list),brand,blank,tr_year.min(),tr_year.max(),te_year.min(),te_year.max())\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a2078cd-ef0f-48f7-a208-9acde7155c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (2) 브랜드 그룹별 -> 생산년도에 따른 가격의 boxplot\n",
    "# for brand in tr_data[group_feature].value_counts().sort_values(ascending=False).index.tolist():\n",
    "#     d = tr_data[tr_data[group_feature]==brand]\n",
    "#     mean_df = d.groupby(check_num_feature)[target_feature].mean().reset_index()\n",
    "#     mean_df[check_num_feature] = range(len(mean_df))\n",
    "#     ymin,ymax = d[target_feature].min(), d[target_feature].max()\n",
    "    \n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     ax = sns.boxplot(x=d[check_num_feature],y=d[target_feature])#,order=sorted(d['생산년도'].unique()))\n",
    "#     ax.plot(mean_df[check_num_feature],mean_df[target_feature],'o-',color='red',alpha=0.5)\n",
    "#     # for cnt,xtick in zip(d[check_num_feature].value_counts().sort_index(), ax.get_xticks()):\n",
    "#     #     plt.annotate(cnt,(xtick,1.1*ymax),rotation=90,ha='center')\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.ylim(ymin,1.15*ymax)\n",
    "#     plt.title('{}(건수={:,})'.format(brand,len(d)))\n",
    "#     plt.show()\n",
    "\n",
    "# # # (2-1) 생산년도 (>=기준)\n",
    "# # opel, [2000]\n",
    "# # ford, [2003]\n",
    "# # volkswagen, [1998]\n",
    "# # audi, [1996]\n",
    "# # bmw, [1998]\n",
    "# # renault, [2000]\n",
    "# # toyota, [2003]\n",
    "# # skoda, None\n",
    "# # mercedes-benz, [2001]\n",
    "# # nissan, [2003]\n",
    "# # kia, None\n",
    "# # hyundai, [2016,2012]\n",
    "# # seat, [2004]\n",
    "# # mazda, None\n",
    "# # honda, None\n",
    "# # volvo, None\n",
    "# # fiat, None\n",
    "# # citroen, None\n",
    "# # peugeot, 2017\n",
    "# # mitsubishi, 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337def8-6ecc-4b61-b94c-09c47a036063",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818d1c-02b7-430c-8154-3cfba7ca190e",
   "metadata": {},
   "source": [
    "# Make Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f52d46b-311e-458a-abae-8d7ee30362db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment(data,segment: list):\n",
    "    d = data.copy()\n",
    "    d[segment] = d[segment].astype(str)\n",
    "    d['segment'] = d[segment].apply(lambda x: '___'.join(x),axis=1)\n",
    "    d.drop(columns=segment,inplace=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b815100c-8acb-4320-9418-affec13c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_segment_notin_testdata(tr_data,te_data,verbose=True):\n",
    "    test_only = list(set(te_data.segment.unique())-set(tr_data.segment.unique()))\n",
    "    assert len(test_only)==0, \\\n",
    "        \"Segment exists only in the test set ({})\".format(len(test_only))\n",
    "\n",
    "    train_only = list(set(tr_data['segment'].unique())-set(te_data['segment'].unique()))\n",
    "\n",
    "    n_asis = len(tr_data)\n",
    "    n_tobe = len(tr_data[~tr_data.segment.isin(train_only)])\n",
    "    tobe_tr_data = tr_data[~tr_data.segment.isin(train_only)]\n",
    "    if verbose:\n",
    "        print('> Train에만 존재하는 Segment 제거')\n",
    "        print(' - 데이터수 : {:,} -> {:,}'.format(n_asis,n_tobe))\n",
    "        print(' - 세그먼트수 : {:,}'.format(tobe_tr_data['segment'].nunique()))\n",
    "\n",
    "    vc = tobe_tr_data['segment'].value_counts().sort_values()\n",
    "    if verbose:\n",
    "        print('\\n> 세그먼트별 건수')\n",
    "        display(vc.head())\n",
    "        print('...')\n",
    "        display(vc.tail())\n",
    "        \n",
    "    return tobe_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ae943f-6e67-435b-b290-fd5314ae152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment 추가\n",
    "segment = ['브랜드'] #,'production_year_grp']\n",
    "train_df4 = make_segment(train_df3,segment)\n",
    "test_df4  = make_segment(test_df3 ,segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c91e863c-067d-4268-98c9-3da7cafa3467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Train에만 존재하는 Segment 제거\n",
      " - 데이터수 : 57,920 -> 57,920\n",
      " - 세그먼트수 : 20\n",
      "\n",
      "> 세그먼트별 건수\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "segment\n",
       "mitsubishi     556\n",
       "peugeot        793\n",
       "citroen       1129\n",
       "fiat          1164\n",
       "volvo         1352\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "segment\n",
       "bmw           5262\n",
       "audi          5597\n",
       "volkswagen    5693\n",
       "ford          5819\n",
       "opel          6651\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df4 = delete_segment_notin_testdata(train_df4,test_df4,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca21ee2c-71d9-4c6b-97bc-86c6dbd7f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gloval variable assignment\n",
      "[1] complete: target_feature\n",
      "[2] complete: unuse_features\n",
      "[3] complete: dummy_features\n",
      "[4] complete: cat_features\n",
      "[5] complete: num_features\n",
      "[6] complete: segment_feature\n"
     ]
    }
   ],
   "source": [
    "type_controller = TypeController(\n",
    "    target_feature='가격',\n",
    "    cat_features=None,\n",
    "    unuse_features=CFG.UNUSE_FEATURES,\n",
    "    segment_feature='segment',\n",
    ")\n",
    "type_controller.fit(\n",
    "    data=train_df4,\n",
    "    global_assignment=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_df4 = type_controller.transform(train_df4)\n",
    "test_df4  = type_controller.transform(test_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e20c4e7-9553-4c1e-88a7-1d40b7b77711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57920, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>생산년도</th>\n",
       "      <th>모델출시년도</th>\n",
       "      <th>차량모델명</th>\n",
       "      <th>판매도시</th>\n",
       "      <th>판매구역</th>\n",
       "      <th>주행거리</th>\n",
       "      <th>배기량</th>\n",
       "      <th>가격</th>\n",
       "      <th>출시년도생산여부</th>\n",
       "      <th>출시이후생산년수</th>\n",
       "      <th>출시이전생산여부</th>\n",
       "      <th>브랜드국적</th>\n",
       "      <th>브랜드대륙명</th>\n",
       "      <th>가스</th>\n",
       "      <th>연료타입</th>\n",
       "      <th>판매도시구역동일여부</th>\n",
       "      <th>production_year_grp</th>\n",
       "      <th>브랜드_Avg</th>\n",
       "      <th>브랜드_Q25</th>\n",
       "      <th>브랜드_Q50</th>\n",
       "      <th>브랜드_Q75</th>\n",
       "      <th>차량모델명_Avg</th>\n",
       "      <th>차량모델명_Q25</th>\n",
       "      <th>차량모델명_Q50</th>\n",
       "      <th>차량모델명_Q75</th>\n",
       "      <th>판매구역_Avg</th>\n",
       "      <th>판매구역_Q25</th>\n",
       "      <th>판매구역_Q50</th>\n",
       "      <th>판매구역_Q75</th>\n",
       "      <th>브랜드_차량모델명_Avg</th>\n",
       "      <th>브랜드_차량모델명_Q25</th>\n",
       "      <th>브랜드_차량모델명_Q50</th>\n",
       "      <th>브랜드_차량모델명_Q75</th>\n",
       "      <th>브랜드_판매구역_Avg</th>\n",
       "      <th>브랜드_판매구역_Q25</th>\n",
       "      <th>브랜드_판매구역_Q50</th>\n",
       "      <th>브랜드_판매구역_Q75</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>fabia</td>\n",
       "      <td>KAT</td>\n",
       "      <td>SLA</td>\n",
       "      <td>85231.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>체코</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.881733</td>\n",
       "      <td>3.393501</td>\n",
       "      <td>4.057853</td>\n",
       "      <td>4.509072</td>\n",
       "      <td>3.470557</td>\n",
       "      <td>2.933059</td>\n",
       "      <td>3.712352</td>\n",
       "      <td>4.065687</td>\n",
       "      <td>3.734607</td>\n",
       "      <td>3.206803</td>\n",
       "      <td>3.773910</td>\n",
       "      <td>4.355041</td>\n",
       "      <td>3.470557</td>\n",
       "      <td>2.933059</td>\n",
       "      <td>3.712352</td>\n",
       "      <td>4.065687</td>\n",
       "      <td>3.910907</td>\n",
       "      <td>3.477232</td>\n",
       "      <td>4.021236</td>\n",
       "      <td>4.525044</td>\n",
       "      <td>skoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>auris</td>\n",
       "      <td>RKO</td>\n",
       "      <td>SWI</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>3.724970</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.887563</td>\n",
       "      <td>3.353407</td>\n",
       "      <td>4.000034</td>\n",
       "      <td>4.435212</td>\n",
       "      <td>3.979365</td>\n",
       "      <td>3.594569</td>\n",
       "      <td>4.112512</td>\n",
       "      <td>4.387075</td>\n",
       "      <td>3.561957</td>\n",
       "      <td>3.113071</td>\n",
       "      <td>3.520461</td>\n",
       "      <td>4.066802</td>\n",
       "      <td>3.979365</td>\n",
       "      <td>3.594569</td>\n",
       "      <td>4.112512</td>\n",
       "      <td>4.387075</td>\n",
       "      <td>3.823161</td>\n",
       "      <td>3.341912</td>\n",
       "      <td>3.843102</td>\n",
       "      <td>4.286479</td>\n",
       "      <td>toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>clk-klasa</td>\n",
       "      <td>GNI</td>\n",
       "      <td>WIE</td>\n",
       "      <td>255223.0</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>2.879760</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>독일</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.736959</td>\n",
       "      <td>3.152736</td>\n",
       "      <td>3.785779</td>\n",
       "      <td>4.354655</td>\n",
       "      <td>3.396003</td>\n",
       "      <td>3.147165</td>\n",
       "      <td>3.440418</td>\n",
       "      <td>3.627069</td>\n",
       "      <td>3.646686</td>\n",
       "      <td>3.089678</td>\n",
       "      <td>3.707577</td>\n",
       "      <td>4.296605</td>\n",
       "      <td>3.396003</td>\n",
       "      <td>3.147165</td>\n",
       "      <td>3.440418</td>\n",
       "      <td>3.627069</td>\n",
       "      <td>3.700911</td>\n",
       "      <td>3.130263</td>\n",
       "      <td>3.746677</td>\n",
       "      <td>4.334280</td>\n",
       "      <td>mercedes-benz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>x-trail</td>\n",
       "      <td>EHX</td>\n",
       "      <td>WIE</td>\n",
       "      <td>238000.0</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>2.901422</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.876830</td>\n",
       "      <td>3.558201</td>\n",
       "      <td>3.923359</td>\n",
       "      <td>4.405499</td>\n",
       "      <td>4.354330</td>\n",
       "      <td>4.368303</td>\n",
       "      <td>4.575844</td>\n",
       "      <td>4.761062</td>\n",
       "      <td>3.646686</td>\n",
       "      <td>3.089678</td>\n",
       "      <td>3.707577</td>\n",
       "      <td>4.296605</td>\n",
       "      <td>4.354330</td>\n",
       "      <td>4.368303</td>\n",
       "      <td>4.575844</td>\n",
       "      <td>4.761062</td>\n",
       "      <td>3.809279</td>\n",
       "      <td>3.477232</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>4.321082</td>\n",
       "      <td>nissan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>bravo</td>\n",
       "      <td>OSW</td>\n",
       "      <td>MAL</td>\n",
       "      <td>251000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이탈리아</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.509326</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>3.529985</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>3.057793</td>\n",
       "      <td>2.894253</td>\n",
       "      <td>3.065725</td>\n",
       "      <td>3.210720</td>\n",
       "      <td>3.715470</td>\n",
       "      <td>3.201526</td>\n",
       "      <td>3.755837</td>\n",
       "      <td>4.305416</td>\n",
       "      <td>3.057793</td>\n",
       "      <td>2.894253</td>\n",
       "      <td>3.065725</td>\n",
       "      <td>3.210720</td>\n",
       "      <td>3.563764</td>\n",
       "      <td>2.897563</td>\n",
       "      <td>3.558201</td>\n",
       "      <td>4.360024</td>\n",
       "      <td>fiat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID    생산년도  모델출시년도      차량모델명 판매도시 판매구역      주행거리     배기량  \\\n",
       "0  TRAIN_00000  2018.0  2014.0      fabia  KAT  SLA   85231.0   999.0   \n",
       "1  TRAIN_00001  2010.0  2006.0      auris  RKO  SWI  135000.0  1598.0   \n",
       "2  TRAIN_00002  2002.0  2002.0  clk-klasa  GNI  WIE  255223.0  1796.0   \n",
       "3  TRAIN_00003  2006.0  2001.0    x-trail  EHX  WIE  238000.0  2184.0   \n",
       "4  TRAIN_00004  2007.0  2007.0      bravo  OSW  MAL  251000.0  1910.0   \n",
       "\n",
       "         가격  출시년도생산여부  출시이후생산년수  출시이전생산여부 브랜드국적 브랜드대륙명  가스 연료타입  판매도시구역동일여부  \\\n",
       "0  3.946231         0       4.0         0    체코     유럽   0  가솔린           0   \n",
       "1  3.724970         0       4.0         0    일본    아시아   0  가솔린           0   \n",
       "2  2.879760         1       0.0         1    독일     유럽   0  가솔린           0   \n",
       "3  2.901422         0       5.0         0    일본    아시아   0   경유           0   \n",
       "4  2.865054         1       0.0         1  이탈리아     유럽   0   경유           0   \n",
       "\n",
       "   production_year_grp   브랜드_Avg   브랜드_Q25   브랜드_Q50   브랜드_Q75  차량모델명_Avg  \\\n",
       "0                  0.0  3.881733  3.393501  4.057853  4.509072   3.470557   \n",
       "1                  1.0  3.887563  3.353407  4.000034  4.435212   3.979365   \n",
       "2                  1.0  3.736959  3.152736  3.785779  4.354655   3.396003   \n",
       "3                  1.0  3.876830  3.558201  3.923359  4.405499   4.354330   \n",
       "4                  0.0  3.509326  2.865054  3.529985  4.174387   3.057793   \n",
       "\n",
       "   차량모델명_Q25  차량모델명_Q50  차량모델명_Q75  판매구역_Avg  판매구역_Q25  판매구역_Q50  판매구역_Q75  \\\n",
       "0   2.933059   3.712352   4.065687  3.734607  3.206803  3.773910  4.355041   \n",
       "1   3.594569   4.112512   4.387075  3.561957  3.113071  3.520461  4.066802   \n",
       "2   3.147165   3.440418   3.627069  3.646686  3.089678  3.707577  4.296605   \n",
       "3   4.368303   4.575844   4.761062  3.646686  3.089678  3.707577  4.296605   \n",
       "4   2.894253   3.065725   3.210720  3.715470  3.201526  3.755837  4.305416   \n",
       "\n",
       "   브랜드_차량모델명_Avg  브랜드_차량모델명_Q25  브랜드_차량모델명_Q50  브랜드_차량모델명_Q75  브랜드_판매구역_Avg  \\\n",
       "0       3.470557       2.933059       3.712352       4.065687      3.910907   \n",
       "1       3.979365       3.594569       4.112512       4.387075      3.823161   \n",
       "2       3.396003       3.147165       3.440418       3.627069      3.700911   \n",
       "3       4.354330       4.368303       4.575844       4.761062      3.809279   \n",
       "4       3.057793       2.894253       3.065725       3.210720      3.563764   \n",
       "\n",
       "   브랜드_판매구역_Q25  브랜드_판매구역_Q50  브랜드_판매구역_Q75        segment  \n",
       "0      3.477232      4.021236      4.525044          skoda  \n",
       "1      3.341912      3.843102      4.286479         toyota  \n",
       "2      3.130263      3.746677      4.334280  mercedes-benz  \n",
       "3      3.477232      3.946231      4.321082         nissan  \n",
       "4      2.897563      3.558201      4.360024           fiat  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df4.shape)\n",
    "train_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a61b4ce-2598-404b-8428-2730c299d332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['차량모델명', '판매구역', '브랜드국적', '브랜드대륙명', '연료타입'],\n",
       " ['출시년도생산여부', '출시이전생산여부', '가스', '판매도시구역동일여부'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features, dummy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19d5173e-0f2a-468b-82ae-97028c346681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train_df4.copy()\n",
    "# col = '출시이후생산년수'\n",
    "\n",
    "# # data[col].value_counts().sort_index()\n",
    "\n",
    "# segment_list = data.segment.unique()\n",
    "# i=0\n",
    "# for segment in segment_list:\n",
    "#     i+=1\n",
    "    \n",
    "#     print('[{}/{}] {}'.format(i,len(segment_list),segment))\n",
    "#     d = data[data.segment==segment]\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=d[col],y=d[target_feature])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1352144-608e-4ee4-bff4-baa4b3f52f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_num_features = [col for col in num_features if col.find('_')<0]\n",
    "\n",
    "# i=0\n",
    "# for col in check_num_features:\n",
    "#     i+=1\n",
    "#     print('[{}/{}] {}'.format(i,len(check_num_features),col))\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=data['segment'],y=data[col])\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7382f5b-fdd0-40e3-8d0f-5d63470c5b0d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6de36-42be-4b4b-bc8c-f32dc03442af",
   "metadata": {},
   "source": [
    "# Scaling (for each segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf304ff7-fa09-4644-be07-754ad684580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GroupScaler:\n",
    "    def __init__(self,scaler=StandardScaler()):\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def fit(self,data,segment_feature,num_features):\n",
    "        not_num_features = [dtype for dtype in data[num_features].dtypes if dtype not in [int,float]]\n",
    "        assert len(not_num_features)==0, \\\n",
    "            \"not numerical features: {}\".format(not_num_features)\n",
    "            \n",
    "        self.segment_feature = segment_feature\n",
    "        self.num_features = num_features\n",
    "        self.segment_list = data[segment_feature].unique()\n",
    "        \n",
    "        self.scalers = {}\n",
    "        pbar = tqdm(self.segment_list)\n",
    "        i=0\n",
    "        for seg in pbar:\n",
    "            i+=1\n",
    "            self.scalers[seg] = {}\n",
    "            for feature in self.num_features:\n",
    "                progress = '[Fit] Segment: {}({}/{})'\\\n",
    "                    .format(seg,i,len(self.segment_list))\n",
    "                pbar.set_description(progress)\n",
    "                d = data[data[segment_feature]==seg]\n",
    "                scaler = deepcopy(self.scaler)\n",
    "                scaler.fit(np.array(d[feature]).reshape(-1,1))\n",
    "                self.scalers[seg][feature] = scaler\n",
    "                \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        \n",
    "        pbar = tqdm(self.segment_list)\n",
    "        i=0\n",
    "        for seg in pbar:\n",
    "            i+=1\n",
    "            for feature in self.num_features:\n",
    "                progress = '[Transform] Segment: {}({}/{})'\\\n",
    "                    .format(seg,i,len(self.segment_list))\n",
    "                pbar.set_description(progress)\n",
    "                seg_data = d.loc[(d[self.segment_feature]==seg),feature]\n",
    "                seg_data = np.array(seg_data).reshape(-1,1)\n",
    "                d.loc[(d[self.segment_feature]==seg),feature] = self.scalers[seg][feature].transform(seg_data)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09d022b5-84dc-4737-b922-dc6c89a4b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fit] Segment: mitsubishi(20/20): 100%|██████████| 20/20 [00:03<00:00,  6.12it/s] \n",
      "[Transform] Segment: mitsubishi(20/20): 100%|██████████| 20/20 [00:04<00:00,  4.95it/s] \n",
      "[Transform] Segment: mitsubishi(20/20): 100%|██████████| 20/20 [00:01<00:00, 12.92it/s] \n"
     ]
    }
   ],
   "source": [
    "scaler = GroupScaler(scaler=StandardScaler())\n",
    "scaler.fit(\n",
    "    data=train_df4,\n",
    "    segment_feature='segment',\n",
    "    num_features=num_features,\n",
    ")\n",
    "train_df5 = scaler.transform(train_df4)\n",
    "test_df5  = scaler.transform(test_df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65939305-7703-474a-912d-3841f3eed25d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5e17c-e94a-468c-a865-fc4abfb07852",
   "metadata": {},
   "source": [
    "# Add kmeans labels (for each segment->수정해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51ff2bc0-f8c8-4763-9af4-68ddc6e6ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df6 = train_df5.copy()\n",
    "test_df6  = test_df5 .copy()\n",
    "kmeans_label_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc73f7b8-dbbd-4f64-8006-0a08d115d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kneed import KneeLocator\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# def kmeans_predict(kmeans,dataset):\n",
    "#     distances = kmeans.transform(dataset)\n",
    "#     prediction = np.argmin(distances, axis=1)\n",
    "#     return np.array(prediction,dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf6d3e83-f86d-43bb-af2f-77d5d19f6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_df6[num_features]\n",
    "\n",
    "# kmeans_models = []\n",
    "# kf = KFold(n_splits=CFG.N_SPLITS,shuffle=True,random_state=CFG.SEED)\n",
    "# for tr_idx,val_idx in tqdm(kf.split(X),total=CFG.N_SPLITS):\n",
    "#     X_train, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    \n",
    "#     sse = []\n",
    "#     k_list = range(1, 11)\n",
    "#     for k in k_list:\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "#         kmeans.fit(X_train)\n",
    "#         sse.append(kmeans.inertia_)\n",
    "        \n",
    "#     kl = KneeLocator(k_list, sse, curve=\"convex\", direction=\"decreasing\")\n",
    "    \n",
    "#     # plt.figure(figsize=(15,7))\n",
    "#     # plt.plot(k_list,sse,'o-')\n",
    "#     # plt.xticks(k_list)\n",
    "#     # plt.grid()\n",
    "#     # plt.axvline(kl.elbow,color='red',linestyle='--')\n",
    "#     # plt.show()\n",
    "    \n",
    "#     kmeans = KMeans(n_clusters=kl.elbow, random_state=0)\n",
    "#     kmeans.fit(X_train)\n",
    "#     kmeans_models.append(kmeans)\n",
    "    \n",
    "# for i,kmeans in enumerate(kmeans_models):\n",
    "#     train_df6[f'kmeans_label_{i+1}'] = kmeans_predict(kmeans,train_df6[num_features])\n",
    "#     test_df6 [f'kmeans_label_{i+1}'] = kmeans_predict(kmeans,test_df6 [num_features])\n",
    "    \n",
    "# kmeans_label_features = [f'kmeans_label_{i+1}' for i in range(len(kmeans_models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e4f32eb-1659-496f-a196-f80cf86c710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df5.shape, train_df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a11fb9ce-d842-465c-8b35-9e866e7dea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(kmeans_models)):\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.boxplot(x=train_df6[f'kmeans_label_{i+1}'],y=train_df6[target_feature])\n",
    "#     plt.title(i+1)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d1eabb6-584a-4c38-a63e-16d321bd2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gloval variable assignment\n",
      "[1] complete: target_feature\n",
      "[2] complete: unuse_features\n",
      "[3] complete: dummy_features\n",
      "[4] complete: cat_features\n",
      "[5] complete: num_features\n",
      "[6] complete: segment_feature\n"
     ]
    }
   ],
   "source": [
    "type_controller = TypeController(\n",
    "    target_feature='가격',\n",
    "    cat_features=kmeans_label_features,\n",
    "    unuse_features=CFG.UNUSE_FEATURES,\n",
    "    segment_feature='segment',\n",
    ")\n",
    "type_controller.fit(\n",
    "    data=train_df6,\n",
    "    global_assignment=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_df6 = type_controller.transform(train_df6)\n",
    "test_df6  = type_controller.transform(test_df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0a5378a-3f1a-46fd-9eac-ec328756df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57920, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>생산년도</th>\n",
       "      <th>모델출시년도</th>\n",
       "      <th>차량모델명</th>\n",
       "      <th>판매도시</th>\n",
       "      <th>판매구역</th>\n",
       "      <th>주행거리</th>\n",
       "      <th>배기량</th>\n",
       "      <th>가격</th>\n",
       "      <th>출시년도생산여부</th>\n",
       "      <th>출시이후생산년수</th>\n",
       "      <th>출시이전생산여부</th>\n",
       "      <th>브랜드국적</th>\n",
       "      <th>브랜드대륙명</th>\n",
       "      <th>가스</th>\n",
       "      <th>연료타입</th>\n",
       "      <th>판매도시구역동일여부</th>\n",
       "      <th>production_year_grp</th>\n",
       "      <th>브랜드_Avg</th>\n",
       "      <th>브랜드_Q25</th>\n",
       "      <th>브랜드_Q50</th>\n",
       "      <th>브랜드_Q75</th>\n",
       "      <th>차량모델명_Avg</th>\n",
       "      <th>차량모델명_Q25</th>\n",
       "      <th>차량모델명_Q50</th>\n",
       "      <th>차량모델명_Q75</th>\n",
       "      <th>판매구역_Avg</th>\n",
       "      <th>판매구역_Q25</th>\n",
       "      <th>판매구역_Q50</th>\n",
       "      <th>판매구역_Q75</th>\n",
       "      <th>브랜드_차량모델명_Avg</th>\n",
       "      <th>브랜드_차량모델명_Q25</th>\n",
       "      <th>브랜드_차량모델명_Q50</th>\n",
       "      <th>브랜드_차량모델명_Q75</th>\n",
       "      <th>브랜드_판매구역_Avg</th>\n",
       "      <th>브랜드_판매구역_Q25</th>\n",
       "      <th>브랜드_판매구역_Q50</th>\n",
       "      <th>브랜드_판매구역_Q75</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>0.834442</td>\n",
       "      <td>0.724582</td>\n",
       "      <td>fabia</td>\n",
       "      <td>KAT</td>\n",
       "      <td>SLA</td>\n",
       "      <td>-0.836065</td>\n",
       "      <td>-1.575697</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095535</td>\n",
       "      <td>0</td>\n",
       "      <td>체코</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.776357e-15</td>\n",
       "      <td>-1.289723</td>\n",
       "      <td>-1.257159</td>\n",
       "      <td>-1.352954</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>0.260894</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.219604</td>\n",
       "      <td>0.502768</td>\n",
       "      <td>-1.289723</td>\n",
       "      <td>-1.257159</td>\n",
       "      <td>-1.352954</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>0.294547</td>\n",
       "      <td>0.453850</td>\n",
       "      <td>-0.080343</td>\n",
       "      <td>0.409862</td>\n",
       "      <td>skoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>-0.613603</td>\n",
       "      <td>-0.752969</td>\n",
       "      <td>auris</td>\n",
       "      <td>RKO</td>\n",
       "      <td>SWI</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>-0.023767</td>\n",
       "      <td>3.724970</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166189</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101140</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>1.776357e-15</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>0.345921</td>\n",
       "      <td>0.281534</td>\n",
       "      <td>0.295228</td>\n",
       "      <td>0.040522</td>\n",
       "      <td>-2.006230</td>\n",
       "      <td>-1.017156</td>\n",
       "      <td>-2.241310</td>\n",
       "      <td>-2.735300</td>\n",
       "      <td>0.345921</td>\n",
       "      <td>0.281534</td>\n",
       "      <td>0.295228</td>\n",
       "      <td>0.040522</td>\n",
       "      <td>-0.704950</td>\n",
       "      <td>-0.303903</td>\n",
       "      <td>-1.040675</td>\n",
       "      <td>-1.631444</td>\n",
       "      <td>toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>-1.143023</td>\n",
       "      <td>-0.562400</td>\n",
       "      <td>clk-klasa</td>\n",
       "      <td>GNI</td>\n",
       "      <td>WIE</td>\n",
       "      <td>0.499560</td>\n",
       "      <td>-0.673600</td>\n",
       "      <td>2.879760</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.347041</td>\n",
       "      <td>1</td>\n",
       "      <td>독일</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.249587</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>1.332268e-15</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.065284</td>\n",
       "      <td>-0.238086</td>\n",
       "      <td>-0.677804</td>\n",
       "      <td>-2.852173</td>\n",
       "      <td>-0.696033</td>\n",
       "      <td>-1.079302</td>\n",
       "      <td>-0.252732</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>-1.065284</td>\n",
       "      <td>-0.238086</td>\n",
       "      <td>-0.677804</td>\n",
       "      <td>-2.852173</td>\n",
       "      <td>-0.417619</td>\n",
       "      <td>-0.508368</td>\n",
       "      <td>-0.113595</td>\n",
       "      <td>-0.136579</td>\n",
       "      <td>mercedes-benz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>-1.476101</td>\n",
       "      <td>-2.086687</td>\n",
       "      <td>x-trail</td>\n",
       "      <td>EHX</td>\n",
       "      <td>WIE</td>\n",
       "      <td>1.591089</td>\n",
       "      <td>2.038881</td>\n",
       "      <td>2.901422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739109</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.129284</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>0.977428</td>\n",
       "      <td>1.440628</td>\n",
       "      <td>1.526758</td>\n",
       "      <td>1.058689</td>\n",
       "      <td>-0.782043</td>\n",
       "      <td>-1.200983</td>\n",
       "      <td>-0.304308</td>\n",
       "      <td>-0.039559</td>\n",
       "      <td>0.977428</td>\n",
       "      <td>1.440628</td>\n",
       "      <td>1.526758</td>\n",
       "      <td>1.058689</td>\n",
       "      <td>-0.656350</td>\n",
       "      <td>-0.740974</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>-0.383612</td>\n",
       "      <td>nissan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>-1.195204</td>\n",
       "      <td>-0.462942</td>\n",
       "      <td>bravo</td>\n",
       "      <td>OSW</td>\n",
       "      <td>MAL</td>\n",
       "      <td>1.710821</td>\n",
       "      <td>2.776681</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.240315</td>\n",
       "      <td>1</td>\n",
       "      <td>이탈리아</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.689201</td>\n",
       "      <td>-0.382146</td>\n",
       "      <td>-0.612669</td>\n",
       "      <td>-1.108377</td>\n",
       "      <td>0.184164</td>\n",
       "      <td>0.174178</td>\n",
       "      <td>0.191462</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>-0.689201</td>\n",
       "      <td>-0.382146</td>\n",
       "      <td>-0.612669</td>\n",
       "      <td>-1.108377</td>\n",
       "      <td>0.321677</td>\n",
       "      <td>0.063393</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>fiat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      생산년도    모델출시년도      차량모델명 판매도시 판매구역      주행거리       배기량  \\\n",
       "0  TRAIN_00000  0.834442  0.724582      fabia  KAT  SLA -0.836065 -1.575697   \n",
       "1  TRAIN_00001 -0.613603 -0.752969      auris  RKO  SWI  0.024455 -0.023767   \n",
       "2  TRAIN_00002 -1.143023 -0.562400  clk-klasa  GNI  WIE  0.499560 -0.673600   \n",
       "3  TRAIN_00003 -1.476101 -2.086687    x-trail  EHX  WIE  1.591089  2.038881   \n",
       "4  TRAIN_00004 -1.195204 -0.462942      bravo  OSW  MAL  1.710821  2.776681   \n",
       "\n",
       "         가격  출시년도생산여부  출시이후생산년수  출시이전생산여부 브랜드국적 브랜드대륙명  가스 연료타입  판매도시구역동일여부  \\\n",
       "0  3.946231         0  0.095535         0    체코     유럽   0  가솔린           0   \n",
       "1  3.724970         0  0.166189         0    일본    아시아   0  가솔린           0   \n",
       "2  2.879760         1 -1.347041         1    독일     유럽   0  가솔린           0   \n",
       "3  2.901422         0  0.739109         0    일본    아시아   0   경유           0   \n",
       "4  2.865054         1 -1.240315         1  이탈리아     유럽   0   경유           0   \n",
       "\n",
       "   production_year_grp       브랜드_Avg       브랜드_Q25       브랜드_Q50  \\\n",
       "0             0.000000 -8.881784e-16 -4.440892e-16 -8.881784e-16   \n",
       "1            -0.101140  0.000000e+00  8.881784e-16  1.776357e-15   \n",
       "2            -0.249587  0.000000e+00  4.440892e-16  1.332268e-15   \n",
       "3            -0.129284  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "4             0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "        브랜드_Q75  차량모델명_Avg  차량모델명_Q25  차량모델명_Q50  차량모델명_Q75  판매구역_Avg  \\\n",
       "0  1.776357e-15  -1.289723  -1.257159  -1.352954  -1.324219  0.260894   \n",
       "1  8.881784e-16   0.345921   0.281534   0.295228   0.040522 -2.006230   \n",
       "2 -8.881784e-16  -1.065284  -0.238086  -0.677804  -2.852173 -0.696033   \n",
       "3  8.881784e-16   0.977428   1.440628   1.526758   1.058689 -0.782043   \n",
       "4  0.000000e+00  -0.689201  -0.382146  -0.612669  -1.108377  0.184164   \n",
       "\n",
       "   판매구역_Q25  판매구역_Q50  판매구역_Q75  브랜드_차량모델명_Avg  브랜드_차량모델명_Q25  브랜드_차량모델명_Q50  \\\n",
       "0  0.076129  0.219604  0.502768      -1.289723      -1.257159      -1.352954   \n",
       "1 -1.017156 -2.241310 -2.735300       0.345921       0.281534       0.295228   \n",
       "2 -1.079302 -0.252732 -0.006405      -1.065284      -0.238086      -0.677804   \n",
       "3 -1.200983 -0.304308 -0.039559       0.977428       1.440628       1.526758   \n",
       "4  0.174178  0.191462  0.003637      -0.689201      -0.382146      -0.612669   \n",
       "\n",
       "   브랜드_차량모델명_Q75  브랜드_판매구역_Avg  브랜드_판매구역_Q25  브랜드_판매구역_Q50  브랜드_판매구역_Q75  \\\n",
       "0      -1.324219      0.294547      0.453850     -0.080343      0.409862   \n",
       "1       0.040522     -0.704950     -0.303903     -1.040675     -1.631444   \n",
       "2      -2.852173     -0.417619     -0.508368     -0.113595     -0.136579   \n",
       "3       1.058689     -0.656350     -0.740974      0.127434     -0.383612   \n",
       "4      -1.108377      0.321677      0.063393     -0.024993      0.881700   \n",
       "\n",
       "         segment  \n",
       "0          skoda  \n",
       "1         toyota  \n",
       "2  mercedes-benz  \n",
       "3         nissan  \n",
       "4           fiat  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df6.shape)\n",
    "train_df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920fd41-52ab-4de1-82a4-d9a5b28a4346",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6171ab-e115-43a0-906e-ea644ee74b86",
   "metadata": {},
   "source": [
    "# Add the Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c35453f-0aae-4d1c-b034-f47a24ec1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import trange\n",
    "\n",
    "def get_abs_corr(x,y):\n",
    "    return np.abs(np.corrcoef(x,y))[0,1]\n",
    "\n",
    "class InteractionTerm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,num_features,corr_cutoff=0.7):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        \n",
    "        d = data.copy()\n",
    "        self.interaction_list = []\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    col_i = num_features[i]\n",
    "                    col_j = num_features[j]\n",
    "                    \n",
    "                    # 상관계수가 cutoff보다 큰 경우에는 interaction을 생성하지 않음\n",
    "                    if (get_abs_corr(d[col_i]*d[col_j],d[col_i])>=corr_cutoff) | (get_abs_corr(d[col_i]*d[col_j],d[col_j])>=corr_cutoff):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.interaction_list.append(f'{col_i}*{col_j}')\n",
    "    \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        for interaction in self.interaction_list:\n",
    "            col_i,col_j = interaction.split('*')\n",
    "            d[interaction] = d[col_i]*d[col_j]\n",
    "        return d\n",
    "    \n",
    "    def fit_transform(self,data,num_features,corr_cutoff=0.7):\n",
    "        self.fit(data,num_features,corr_cutoff)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e973e0f2-e229-493c-9ad0-a42c086a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df7 = train_df6.copy()\n",
    "test_df7  = test_df6.copy()\n",
    "\n",
    "if CFG.INTERACTION:\n",
    "    interaction_maker = InteractionTerm()\n",
    "    interaction_maker.fit(\n",
    "        data=train_df7,\n",
    "        num_features=num_features,\n",
    "        corr_cutoff=0.7,\n",
    "    )\n",
    "    train_df7 = interaction_maker.transform(train_df7)\n",
    "    test_df7  = interaction_maker.transform(test_df7)\n",
    "\n",
    "    type_controller = TypeController(\n",
    "        target_feature='가격',\n",
    "        cat_features=kmeans_label_features,\n",
    "        unuse_features=CFG.UNUSE_FEATURES,\n",
    "        segment_feature='segment',\n",
    "    )\n",
    "    type_controller.fit(\n",
    "        data=train_df7,\n",
    "        global_assignment=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    train_df7 = type_controller.transform(train_df7)\n",
    "    test_df7  = type_controller.transform(test_df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a1f24a9-43d7-47c3-bdf9-377529549c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>생산년도</th>\n",
       "      <th>모델출시년도</th>\n",
       "      <th>차량모델명</th>\n",
       "      <th>판매도시</th>\n",
       "      <th>판매구역</th>\n",
       "      <th>주행거리</th>\n",
       "      <th>배기량</th>\n",
       "      <th>가격</th>\n",
       "      <th>출시년도생산여부</th>\n",
       "      <th>출시이후생산년수</th>\n",
       "      <th>출시이전생산여부</th>\n",
       "      <th>브랜드국적</th>\n",
       "      <th>브랜드대륙명</th>\n",
       "      <th>가스</th>\n",
       "      <th>연료타입</th>\n",
       "      <th>판매도시구역동일여부</th>\n",
       "      <th>production_year_grp</th>\n",
       "      <th>브랜드_Avg</th>\n",
       "      <th>브랜드_Q25</th>\n",
       "      <th>브랜드_Q50</th>\n",
       "      <th>브랜드_Q75</th>\n",
       "      <th>차량모델명_Avg</th>\n",
       "      <th>차량모델명_Q25</th>\n",
       "      <th>차량모델명_Q50</th>\n",
       "      <th>차량모델명_Q75</th>\n",
       "      <th>판매구역_Avg</th>\n",
       "      <th>판매구역_Q25</th>\n",
       "      <th>판매구역_Q50</th>\n",
       "      <th>판매구역_Q75</th>\n",
       "      <th>브랜드_차량모델명_Avg</th>\n",
       "      <th>브랜드_차량모델명_Q25</th>\n",
       "      <th>브랜드_차량모델명_Q50</th>\n",
       "      <th>브랜드_차량모델명_Q75</th>\n",
       "      <th>브랜드_판매구역_Avg</th>\n",
       "      <th>브랜드_판매구역_Q25</th>\n",
       "      <th>브랜드_판매구역_Q50</th>\n",
       "      <th>브랜드_판매구역_Q75</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>0.834442</td>\n",
       "      <td>0.724582</td>\n",
       "      <td>fabia</td>\n",
       "      <td>KAT</td>\n",
       "      <td>SLA</td>\n",
       "      <td>-0.836065</td>\n",
       "      <td>-1.575697</td>\n",
       "      <td>3.946231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095535</td>\n",
       "      <td>0</td>\n",
       "      <td>체코</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.776357e-15</td>\n",
       "      <td>-1.289723</td>\n",
       "      <td>-1.257159</td>\n",
       "      <td>-1.352954</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>0.260894</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.219604</td>\n",
       "      <td>0.502768</td>\n",
       "      <td>-1.289723</td>\n",
       "      <td>-1.257159</td>\n",
       "      <td>-1.352954</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>0.294547</td>\n",
       "      <td>0.453850</td>\n",
       "      <td>-0.080343</td>\n",
       "      <td>0.409862</td>\n",
       "      <td>skoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>-0.613603</td>\n",
       "      <td>-0.752969</td>\n",
       "      <td>auris</td>\n",
       "      <td>RKO</td>\n",
       "      <td>SWI</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>-0.023767</td>\n",
       "      <td>3.724970</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166189</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101140</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>1.776357e-15</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>0.345921</td>\n",
       "      <td>0.281534</td>\n",
       "      <td>0.295228</td>\n",
       "      <td>0.040522</td>\n",
       "      <td>-2.006230</td>\n",
       "      <td>-1.017156</td>\n",
       "      <td>-2.241310</td>\n",
       "      <td>-2.735300</td>\n",
       "      <td>0.345921</td>\n",
       "      <td>0.281534</td>\n",
       "      <td>0.295228</td>\n",
       "      <td>0.040522</td>\n",
       "      <td>-0.704950</td>\n",
       "      <td>-0.303903</td>\n",
       "      <td>-1.040675</td>\n",
       "      <td>-1.631444</td>\n",
       "      <td>toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>-1.143023</td>\n",
       "      <td>-0.562400</td>\n",
       "      <td>clk-klasa</td>\n",
       "      <td>GNI</td>\n",
       "      <td>WIE</td>\n",
       "      <td>0.499560</td>\n",
       "      <td>-0.673600</td>\n",
       "      <td>2.879760</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.347041</td>\n",
       "      <td>1</td>\n",
       "      <td>독일</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>가솔린</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.249587</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>1.332268e-15</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.065284</td>\n",
       "      <td>-0.238086</td>\n",
       "      <td>-0.677804</td>\n",
       "      <td>-2.852173</td>\n",
       "      <td>-0.696033</td>\n",
       "      <td>-1.079302</td>\n",
       "      <td>-0.252732</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>-1.065284</td>\n",
       "      <td>-0.238086</td>\n",
       "      <td>-0.677804</td>\n",
       "      <td>-2.852173</td>\n",
       "      <td>-0.417619</td>\n",
       "      <td>-0.508368</td>\n",
       "      <td>-0.113595</td>\n",
       "      <td>-0.136579</td>\n",
       "      <td>mercedes-benz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>-1.476101</td>\n",
       "      <td>-2.086687</td>\n",
       "      <td>x-trail</td>\n",
       "      <td>EHX</td>\n",
       "      <td>WIE</td>\n",
       "      <td>1.591089</td>\n",
       "      <td>2.038881</td>\n",
       "      <td>2.901422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739109</td>\n",
       "      <td>0</td>\n",
       "      <td>일본</td>\n",
       "      <td>아시아</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.129284</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>0.977428</td>\n",
       "      <td>1.440628</td>\n",
       "      <td>1.526758</td>\n",
       "      <td>1.058689</td>\n",
       "      <td>-0.782043</td>\n",
       "      <td>-1.200983</td>\n",
       "      <td>-0.304308</td>\n",
       "      <td>-0.039559</td>\n",
       "      <td>0.977428</td>\n",
       "      <td>1.440628</td>\n",
       "      <td>1.526758</td>\n",
       "      <td>1.058689</td>\n",
       "      <td>-0.656350</td>\n",
       "      <td>-0.740974</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>-0.383612</td>\n",
       "      <td>nissan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>-1.195204</td>\n",
       "      <td>-0.462942</td>\n",
       "      <td>bravo</td>\n",
       "      <td>OSW</td>\n",
       "      <td>MAL</td>\n",
       "      <td>1.710821</td>\n",
       "      <td>2.776681</td>\n",
       "      <td>2.865054</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.240315</td>\n",
       "      <td>1</td>\n",
       "      <td>이탈리아</td>\n",
       "      <td>유럽</td>\n",
       "      <td>0</td>\n",
       "      <td>경유</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.689201</td>\n",
       "      <td>-0.382146</td>\n",
       "      <td>-0.612669</td>\n",
       "      <td>-1.108377</td>\n",
       "      <td>0.184164</td>\n",
       "      <td>0.174178</td>\n",
       "      <td>0.191462</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>-0.689201</td>\n",
       "      <td>-0.382146</td>\n",
       "      <td>-0.612669</td>\n",
       "      <td>-1.108377</td>\n",
       "      <td>0.321677</td>\n",
       "      <td>0.063393</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>fiat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      생산년도    모델출시년도      차량모델명 판매도시 판매구역      주행거리       배기량  \\\n",
       "0  TRAIN_00000  0.834442  0.724582      fabia  KAT  SLA -0.836065 -1.575697   \n",
       "1  TRAIN_00001 -0.613603 -0.752969      auris  RKO  SWI  0.024455 -0.023767   \n",
       "2  TRAIN_00002 -1.143023 -0.562400  clk-klasa  GNI  WIE  0.499560 -0.673600   \n",
       "3  TRAIN_00003 -1.476101 -2.086687    x-trail  EHX  WIE  1.591089  2.038881   \n",
       "4  TRAIN_00004 -1.195204 -0.462942      bravo  OSW  MAL  1.710821  2.776681   \n",
       "\n",
       "         가격  출시년도생산여부  출시이후생산년수  출시이전생산여부 브랜드국적 브랜드대륙명  가스 연료타입  판매도시구역동일여부  \\\n",
       "0  3.946231         0  0.095535         0    체코     유럽   0  가솔린           0   \n",
       "1  3.724970         0  0.166189         0    일본    아시아   0  가솔린           0   \n",
       "2  2.879760         1 -1.347041         1    독일     유럽   0  가솔린           0   \n",
       "3  2.901422         0  0.739109         0    일본    아시아   0   경유           0   \n",
       "4  2.865054         1 -1.240315         1  이탈리아     유럽   0   경유           0   \n",
       "\n",
       "   production_year_grp       브랜드_Avg       브랜드_Q25       브랜드_Q50  \\\n",
       "0             0.000000 -8.881784e-16 -4.440892e-16 -8.881784e-16   \n",
       "1            -0.101140  0.000000e+00  8.881784e-16  1.776357e-15   \n",
       "2            -0.249587  0.000000e+00  4.440892e-16  1.332268e-15   \n",
       "3            -0.129284  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "4             0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "        브랜드_Q75  차량모델명_Avg  차량모델명_Q25  차량모델명_Q50  차량모델명_Q75  판매구역_Avg  \\\n",
       "0  1.776357e-15  -1.289723  -1.257159  -1.352954  -1.324219  0.260894   \n",
       "1  8.881784e-16   0.345921   0.281534   0.295228   0.040522 -2.006230   \n",
       "2 -8.881784e-16  -1.065284  -0.238086  -0.677804  -2.852173 -0.696033   \n",
       "3  8.881784e-16   0.977428   1.440628   1.526758   1.058689 -0.782043   \n",
       "4  0.000000e+00  -0.689201  -0.382146  -0.612669  -1.108377  0.184164   \n",
       "\n",
       "   판매구역_Q25  판매구역_Q50  판매구역_Q75  브랜드_차량모델명_Avg  브랜드_차량모델명_Q25  브랜드_차량모델명_Q50  \\\n",
       "0  0.076129  0.219604  0.502768      -1.289723      -1.257159      -1.352954   \n",
       "1 -1.017156 -2.241310 -2.735300       0.345921       0.281534       0.295228   \n",
       "2 -1.079302 -0.252732 -0.006405      -1.065284      -0.238086      -0.677804   \n",
       "3 -1.200983 -0.304308 -0.039559       0.977428       1.440628       1.526758   \n",
       "4  0.174178  0.191462  0.003637      -0.689201      -0.382146      -0.612669   \n",
       "\n",
       "   브랜드_차량모델명_Q75  브랜드_판매구역_Avg  브랜드_판매구역_Q25  브랜드_판매구역_Q50  브랜드_판매구역_Q75  \\\n",
       "0      -1.324219      0.294547      0.453850     -0.080343      0.409862   \n",
       "1       0.040522     -0.704950     -0.303903     -1.040675     -1.631444   \n",
       "2      -2.852173     -0.417619     -0.508368     -0.113595     -0.136579   \n",
       "3       1.058689     -0.656350     -0.740974      0.127434     -0.383612   \n",
       "4      -1.108377      0.321677      0.063393     -0.024993      0.881700   \n",
       "\n",
       "         segment  \n",
       "0          skoda  \n",
       "1         toyota  \n",
       "2  mercedes-benz  \n",
       "3         nissan  \n",
       "4           fiat  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3732aa9-e9d7-41d5-8e8c-667699fe5a30",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec950f8-0d43-4b9a-97ed-6feac553dba3",
   "metadata": {},
   "source": [
    "# Feature Selection (segment dictionary로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a07b7-5116-410a-a6d0-0d2c50b151b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=0\n",
    "# for i in range(len(num_features)):\n",
    "#     for j in range(len(num_features)):\n",
    "#         if i>j:\n",
    "#             col_i = num_features[i]\n",
    "#             col_j = num_features[j]\n",
    "#             corr = np.corrcoef(train_df5[col_i],train_df5[col_j])[0,1]\n",
    "#             if corr>=0.7:\n",
    "#                 k+=1\n",
    "#                 print(k,col_i,col_j,corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1cc67-3282-47c3-ada7-8c7776443b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e9267-ff9b-4867-a9f6-4810fe4de36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterMethodFeatureSelection:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _feature_selection_categorical(self,dataset,cat_features,target_feature,alpha,verbose):\n",
    "        data = dataset.copy()\n",
    "        delete_features = []\n",
    "        \n",
    "        # (1) ANOVA를 해서 p-value가 0.05보다 높은 것들 확인\n",
    "        if verbose:\n",
    "            pbar = tqdm(cat_features,desc='[1/3] Categorical Feature Selection')\n",
    "        else:\n",
    "            pbar = cat_features\n",
    "        pvalue_list = []\n",
    "        for col in pbar:\n",
    "            if data[col].nunique()==1:\n",
    "                delete_features.append(col)\n",
    "            else:\n",
    "                d = data[[col,target_feature]].rename(columns={col:'feature',target_feature:'target'})\n",
    "                model = ols(f'target ~ C(feature)',data=d).fit()\n",
    "                pvalue = anova_lm(model).values[0][-1]\n",
    "                pvalue_list.append([col,pvalue])\n",
    "        pvalue_df = pd.DataFrame(pvalue_list,columns=['feature','pvalue'])\\\n",
    "            .sort_values('pvalue',ascending=False)\n",
    "        # pvalue_df[pvalue_df.pvalue>=alpha].round(4)\n",
    "        \n",
    "        # (3) log변환을 했음에도 pvalue>alpha인 변수들은 삭제\n",
    "        #     log변환을 해서 pvalue<=alpha인 변수들은 log변환을 해줌\n",
    "        delete_features += pvalue_df[pvalue_df.pvalue>alpha].feature.tolist()\n",
    "        if verbose:\n",
    "            print('> categorical: delete_features')\n",
    "            print('  - length : {}'.format(len(delete_features)))\n",
    "            print('  - feature_name : {}'.format(delete_features))\n",
    "        \n",
    "        return delete_features\n",
    "    \n",
    "    def _feature_selection_numerical(self,dataset,num_features,target_feature,alpha,verbose):\n",
    "        data = dataset.copy()\n",
    "        delete_features = []\n",
    "        \n",
    "        # (1) corr test를 해서 p-value가 0.05보다 높은 것들 확인\n",
    "        if verbose:\n",
    "            pbar = tqdm(num_features,desc='[2/3] Check all numerical features')\n",
    "        else:\n",
    "            pbar = num_features\n",
    "        pvalue_list = []\n",
    "        for col in pbar:\n",
    "            if data[col].nunique()==1:\n",
    "                delete_features.append(col)\n",
    "            else:\n",
    "                corr,pvalue = scipy.stats.pearsonr(data[target_feature],data[col])\n",
    "                pvalue_list.append([col,pvalue])\n",
    "        pvalue_df = pd.DataFrame(pvalue_list,columns=['feature','pvalue'])\\\n",
    "            .sort_values('pvalue',ascending=False)\n",
    "        # pvalue_df.round(4).head()\n",
    "        \n",
    "        # (2) (1)에서 유의하지않은 feature들은 log적용 후에도 유의하지 않으면 제외\n",
    "        insignificant_features = pvalue_df[pvalue_df.pvalue>alpha].feature.tolist()\n",
    "        if verbose:\n",
    "            pbar = tqdm(insignificant_features,desc='[3/3] Check insignificant numerical features')\n",
    "        else:\n",
    "            pbar = insignificant_features\n",
    "        pvalue_list2 = []\n",
    "        for col in pbar:\n",
    "            if data[col].nunique()==1:\n",
    "                delete_features.append(col)\n",
    "            else:\n",
    "                corr,pvalue = scipy.stats.pearsonr(data[target_feature],log_offset(data[col],verbose=False))\n",
    "                pvalue_list2.append([col,pvalue])\n",
    "        pvalue_df2 = pd.DataFrame(pvalue_list2,columns=['feature','pvalue'])\\\n",
    "            .sort_values('pvalue',ascending=False)\n",
    "        # pvalue_df2.round(4).head()\n",
    "        \n",
    "        delete_features += pvalue_df2[pvalue_df2.pvalue> alpha].feature.tolist()\n",
    "        log_features     = pvalue_df2[pvalue_df2.pvalue<=alpha].feature.tolist()\n",
    "        if verbose:\n",
    "            print('> numerical: delete_features')\n",
    "            print('  - length : {}'.format(len(delete_features)))\n",
    "            print('  - feature_name : {}'.format(delete_features))\n",
    "            print('> numerical: log_features')\n",
    "            print('  - length : {}'.format(len(log_features)))\n",
    "            print('  - feature_name : {}'.format(log_features))\n",
    "\n",
    "        return delete_features, log_features\n",
    "        \n",
    "    def fit(self,dataset,num_features,cat_features,target_feature,alpha=0.01,verbose=True):\n",
    "        self.cat_delete_features = self._feature_selection_categorical(\n",
    "            dataset,cat_features,target_feature,alpha,verbose)\n",
    "        self.num_delete_features, self.num_log_features = self._feature_selection_numerical(\n",
    "            dataset,num_features,target_feature,alpha,verbose)\n",
    "        \n",
    "        self.cat_delete_features = list(set(self.cat_delete_features)-set([target_feature]))\n",
    "        self.num_delete_features = list(set(self.num_delete_features)-set([target_feature]))\n",
    "        self.num_log_features    = list(set(self.num_log_features)   -set([target_feature]))\n",
    "        \n",
    "    def transform(self,dataset):\n",
    "        data = dataset.copy()\n",
    "        data.drop(columns=self.cat_delete_features+self.num_delete_features,inplace=True)\n",
    "        for col in self.num_log_features:\n",
    "            data[col] = log_offset(data[col],verbose=False)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023eff6e-03a6-4868-9711-0e5cea9b960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupFilterMethodFeatureSelection(\n",
    "    tr_data,te_data,\n",
    "    num_features,cat_features,target_feature,segment_feature,\n",
    "    alpha=0.01,verbose=True,\n",
    "):\n",
    "\n",
    "    segment_data_dict = {}\n",
    "    segment_data_dict['segment_list'] = tr_data[segment_feature].unique()\n",
    "    segment_data_dict['train'] = {}\n",
    "    segment_data_dict['test']  = {}\n",
    "\n",
    "    # segment로 나눠서 feature selection하기\n",
    "    for seg in tqdm(segment_data_dict['segment_list']):\n",
    "        tr_d = tr_data[tr_data[segment_feature]==seg]\n",
    "        te_d = te_data[te_data[segment_feature]==seg]\n",
    "\n",
    "        feature_selector = FilterMethodFeatureSelection()\n",
    "        feature_selector.fit(\n",
    "            dataset=tr_d,\n",
    "            num_features=num_features,\n",
    "            cat_features=[col for col in cat_features if tr_d[col].nunique()<=100],\n",
    "            target_feature=target_feature,\n",
    "            alpha=alpha,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        segment_data_dict['train'][seg] = feature_selector.transform(tr_d)\n",
    "        segment_data_dict['test'] [seg] = feature_selector.transform(te_d)\n",
    "        \n",
    "    return segment_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22f63f-2ad5-4aeb-815a-98a7786aedf9",
   "metadata": {},
   "source": [
    "(1) 전체기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612f8f7-b05b-4b01-b20b-29d74d397b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "# feature_selector = FilterMethodFeatureSelection()\n",
    "# feature_selector.fit(\n",
    "#     dataset=train_df7,\n",
    "#     num_features=num_features,\n",
    "#     cat_features=[col for col in cat_features if train_df7[col].nunique()<=100],\n",
    "#     target_feature=target_feature,\n",
    "#     alpha=CFG.FS_ALPHA,\n",
    "#     verbose=True,\n",
    "# )\n",
    "# train_df8 = feature_selector.transform(train_df7)\n",
    "# test_df8  = feature_selector.transform(test_df7)\n",
    "\n",
    "# # type control\n",
    "# type_controller = TypeController(\n",
    "#     target_feature='가격',\n",
    "#     cat_features=kmeans_label_features,\n",
    "#     unuse_features=['ID'],\n",
    "#     segment_feature='segment',\n",
    "# )\n",
    "# type_controller.fit(\n",
    "#     data=train_df8,\n",
    "#     global_assignment=True,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# train_df8 = type_controller.transform(train_df8)\n",
    "# test_df8  = type_controller.transform(test_df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56ed34-afce-454a-a4a1-c74d02e411d8",
   "metadata": {},
   "source": [
    "(2) segment기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44fb42-bcbb-4dc9-80e9-4894f1320ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data_dict = GroupFilterMethodFeatureSelection(\n",
    "    tr_data=train_df7,\n",
    "    te_data=test_df7,\n",
    "    num_features=num_features,\n",
    "    cat_features=[col for col in cat_features if train_df7[col].nunique()<=100],\n",
    "    target_feature=target_feature,\n",
    "    segment_feature='segment',\n",
    "    alpha=CFG.FS_ALPHA,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1397f-ea02-4276-924c-7c3227ac14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_list = [key for key,value in segment_data_dict['train'].items()]\n",
    "brand_name_max = max([len(b) for b in brand_list])\n",
    "\n",
    "i=0\n",
    "for key,value in segment_data_dict['train'].items():\n",
    "    i+=1\n",
    "    str_i = str(i).zfill(len(str(len(segment_data_dict['train']))))\n",
    "    blank = ' '*(brand_name_max-len(key)+1)\n",
    "    print('[{}/{}] {}{}: ({}/{})'.format(str_i,len(segment_data_dict['train']),key,blank,value.shape[1],train_df7.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176ea54-23ec-469f-b732-88b6afcac30e",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060985f-f86e-49e1-b7c8-ae05fff630a2",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc53047-976c-476b-a94c-1710d70fa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.boxplot(x=train_df7['segment'],y=train_df7['가격'],order=sorted(train_df7.segment.unique()))\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb184b6f-e7b2-41c4-8331-1de7587f8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1,Q3를 기준으로 1.5 IQR보다 멀리있는 값들을 Outlier로 판단하여 제거함\n",
    "class OutlierDetect:\n",
    "    def __init__(self,target_feature,group=None):\n",
    "        self.target_feature = target_feature\n",
    "        self.group = group\n",
    "    \n",
    "    def _get_outlier(self,data,target_feature,whis):\n",
    "        target = data[target_feature]\n",
    "        q1,q3 = target.quantile([0.25,0.75]).values\n",
    "        outlier_lower = q1 - whis*(q3-q1)\n",
    "        outlier_upper = q3 + whis*(q3-q1)\n",
    "        outlier_boundary = [\n",
    "            outlier_lower,\n",
    "            outlier_upper,\n",
    "            np.where((target<outlier_lower)|(target>outlier_upper),1,0).sum(),\n",
    "            np.where((target<outlier_lower)|(target>outlier_upper),1,0).sum() / len(target),\n",
    "            len(target),\n",
    "        ]\n",
    "        return outlier_boundary\n",
    "    \n",
    "    def fit(self,data,whis=1.5):\n",
    "        assert self.target_feature in data.columns, \\\n",
    "            \"No {} column in the data\".format(self.target_feature)\n",
    "        self.data = data\n",
    "        \n",
    "        if self.group is None:\n",
    "            outlier_boundary = self._get_outlier(self.data,self.target_feature,whis)\n",
    "            self.outlier_boundary = pd.DataFrame(\n",
    "                [outlier_boundary],\n",
    "                columns=['outlier_lower','outlier_upper','n_outlier','p_outlier','n_total'],\n",
    "            )\n",
    "        else:\n",
    "            self.group_list = self.data[self.group].unique()\n",
    "            outlier_boundary_list = []\n",
    "            for group in self.group_list:\n",
    "                d = self.data[self.data[self.group]==group]\n",
    "                outlier_boundary = [group]+self._get_outlier(d,self.target_feature,whis)\n",
    "                outlier_boundary_list.append(outlier_boundary)\n",
    "            self.outlier_boundary = pd.DataFrame(\n",
    "                outlier_boundary_list,\n",
    "                columns=['group','outlier_lower','outlier_upper','n_outlier','p_outlier','n_total'],\n",
    "            )\n",
    "    \n",
    "    def transform(self,data,max_example=4,verbose=True):\n",
    "        example = []\n",
    "        if self.group is None:\n",
    "            if len(self.outlier_boundary)!=1:\n",
    "                raise ValueError('length of self.outlier_boundary_df must be 1')\n",
    "            else:\n",
    "                d = data.copy()\n",
    "                outlier_lower = self.outlier_boundary.outlier_lower.values[0]\n",
    "                outlier_upper = self.outlier_boundary.outlier_upper.values[0]\n",
    "                outlier_in = (d[self.target_feature]>=outlier_lower)&(d[self.target_feature]<=outlier_upper)\n",
    "                new_data = d[outlier_in]\n",
    "                ex = d[self.target_feature][~outlier_in]\n",
    "                if len(ex)>0:\n",
    "                    ex = sorted([round(e,3) for e in ex])\n",
    "                    if len(ex)>max_example:\n",
    "                        ex = ex[:2]+['...']+ex[-2:]\n",
    "                else:\n",
    "                    ex = ''\n",
    "                example.append(ex)\n",
    "        else:\n",
    "            check_1 = list(set(self.group_list)-set(data[self.group].unique()))\n",
    "            check_2 = list(set(data[self.group].unique())-set(self.group_list))\n",
    "            if (len(check_1)==0) & (len(check_2)==0):\n",
    "                pass\n",
    "            elif (len(check_1)>0) & (len(check_2)==0):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError('Unknown group values')\n",
    "            \n",
    "            new_data = []\n",
    "            for group in self.group_list:\n",
    "                d = data[data[self.group]==group]\n",
    "                outlier_d = self.outlier_boundary[self.outlier_boundary['group']==group]\n",
    "                if len(outlier_d)!=1:\n",
    "                    raise ValueError('length of self.outlier_boundary_df must be 1')\n",
    "                else:\n",
    "                    outlier_lower = outlier_d.outlier_lower.values[0]\n",
    "                    outlier_upper = outlier_d.outlier_upper.values[0]\n",
    "                    outlier_in = (d[self.target_feature]>=outlier_lower)&(d[self.target_feature]<=outlier_upper)\n",
    "                    new_d = d[outlier_in]\n",
    "                    new_data.append(new_d)\n",
    "                    ex = d[self.target_feature][~outlier_in]\n",
    "                    if len(ex)>0:\n",
    "                        ex = sorted([round(e,3) for e in ex])\n",
    "                        if len(ex)>max_example:\n",
    "                            ex = ex[:2]+['...']+ex[-2:]\n",
    "                    else:\n",
    "                        ex = ''\n",
    "                    example.append(ex)\n",
    "            new_data = pd.concat(new_data,axis=0)\n",
    "        \n",
    "        self.outlier_boundary['example'] = example\n",
    "        self.example = example\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"> {:,}'s outliers deleted ({:,}->{:,})\".format(len(data)-len(new_data),len(data),len(new_data)))\n",
    "            \n",
    "        return new_data\n",
    "    \n",
    "    def fit_transform(self,data,whis=1.5,max_example=4,verbose=True):\n",
    "        self.fit(data,whis)\n",
    "        return self.transform(data=data,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0bfb9-88eb-4f60-a5b4-7e06af0d3146",
   "metadata": {},
   "source": [
    "(1) 전체기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56beb6-228f-4272-8963-bd0a0d168657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df9 = train_df8.copy()\n",
    "# test_df9  = test_df8 .copy()\n",
    "\n",
    "# check_num_features = [col for col in num_features if col.find('_')<0] + ['가격']\n",
    "\n",
    "# outlier_boundary = []\n",
    "# total_n_outlier = 0\n",
    "# i=0\n",
    "# for col in check_num_features:\n",
    "#     i+=1\n",
    "#     print('[{}/{}] {}'.format(i,len(check_num_features),col))\n",
    "#     outlier_detector = OutlierDetect(\n",
    "#         target_feature='가격',\n",
    "#         group='segment',\n",
    "#     )\n",
    "#     train_df9 = outlier_detector.fit_transform(\n",
    "#         data=train_df9,\n",
    "#         whis=1.5,\n",
    "#         max_example=4,\n",
    "#     )\n",
    "#     train_df9.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     _outlier_boundary = outlier_detector.outlier_boundary.copy()\n",
    "#     _outlier_boundary.insert(0,'target_feature',col)\n",
    "#     _outlier_boundary = _outlier_boundary.reset_index().rename(columns={'index':'no'})\n",
    "#     _outlier_boundary.no+=1\n",
    "#     outlier_boundary.append(_outlier_boundary)\n",
    "    \n",
    "#     total_n_outlier += _outlier_boundary.n_outlier.sum()\n",
    "    \n",
    "# print('\\n> Total n_outliers: {}({:.2f}%)'.format(total_n_outlier,100*total_n_outlier/len(train_df8)))\n",
    "\n",
    "# outlier_boundary_df = pd.concat(outlier_boundary,axis=0)\n",
    "# # outlier_boundary_df.round(3).sort_values('n_total').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b0ed7-2181-4b60-b434-85f13971cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.boxplot(x=train_df9['segment'],y=train_df9['가격'],order=sorted(train_df9.segment.unique()))\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77f431-3725-481a-8b75-d22eb11dbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df8.shape[0], train_df9.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df72596-4775-4f01-95be-778a9d0339d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df9.shape)\n",
    "# train_df9.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21063fd5-635f-452a-bc60-83934e0b6e45",
   "metadata": {},
   "source": [
    "(2) segment 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575c5fe-99c2-4066-a4a9-36fbd26467fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_boundary = []\n",
    "# del_outlier_segment_data_dict = deepcopy(segment_data_dict)\n",
    "# del_outlier_segment_data_dict['train'] = {}\n",
    "# del_outlier_segment_data_dict['test']  = {}\n",
    "# total_n_outlier = 0\n",
    "\n",
    "# for seg in tqdm(segment_data_dict['segment_list']):\n",
    "#     check_num_features = [col for col in num_features\n",
    "#                           if (col.find('_')<0) & (col in segment_data_dict['train'][seg].columns)]\n",
    "#     check_num_features += [target_feature]\n",
    "    \n",
    "#     i=0\n",
    "#     for col in check_num_features:\n",
    "#         i+=1\n",
    "#         outlier_detector = OutlierDetect(\n",
    "#             target_feature=col,\n",
    "#             #group='segment',\n",
    "#         )\n",
    "#         outlier_detector.fit(\n",
    "#             data=segment_data_dict['train'][seg],\n",
    "#             whis=3.0,\n",
    "#         )\n",
    "#         tr_d = outlier_detector.transform(data=segment_data_dict['train'][seg],max_example=4,verbose=False)\\\n",
    "#             .reset_index(drop=True,inplace=True)\n",
    "#         te_d = segment_data_dict['test'][seg].copy()\n",
    "\n",
    "#         _outlier_boundary = outlier_detector.outlier_boundary.copy()\n",
    "#         _outlier_boundary.insert(0,'target_feature',col)\n",
    "#         _outlier_boundary.insert(0,'segment',seg)\n",
    "        \n",
    "#         outlier_boundary.append(_outlier_boundary)\n",
    "#         total_n_outlier += _outlier_boundary.n_outlier.sum()\n",
    "#         del_outlier_segment_data_dict['train'][seg] = tr_d\n",
    "#         del_outlier_segment_data_dict['test'] [seg] = te_d\n",
    "        \n",
    "# outlier_boundary_df = pd.concat(outlier_boundary,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff60ba5-0835-48b3-8042-7c086a62ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(total_n_outlier, sum([len(value) for key,value in segment_data_dict['train'].items()]))\n",
    "\n",
    "# # outlier_boundary_df[outlier_boundary_df.n_outlier!=0].head()\n",
    "\n",
    "# n_outlier_df = pd.merge(\n",
    "#     outlier_boundary_df.groupby('segment')['n_outlier'].sum().reset_index(),\n",
    "#     outlier_boundary_df.groupby('segment')['n_total'].mean().reset_index(),\n",
    "#     how='left',on='segment',\n",
    "# )\n",
    "# n_outlier_df['p_outlier'] = n_outlier_df.n_outlier / n_outlier_df.n_total\n",
    "# n_outlier_df.sort_values('p_outlier',ascending=False).round(3).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a97397-0043-4cb7-b2a1-ecef7c1e1cd0",
   "metadata": {},
   "source": [
    "(3) outlier 처리 하지않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ae278-fb52-4b25-9fbd-e5e2fa3a883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_outlier_segment_data_dict = deepcopy(segment_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfa2cc-6635-4b8e-9b6b-9f843ae381eb",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a491f-e182-490a-96bd-f58326d47855",
   "metadata": {},
   "source": [
    "# Memory Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4911221-7b8f-4726-a64c-28a30bf1b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(props,verbose=False):\n",
    "    # Byte -> MB : 2^20\n",
    "    asis_mem_usg = props.memory_usage().sum() / (2**20)\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    \n",
    "    num_cols = props.columns[props.dtypes!=object]\n",
    "    i=0\n",
    "    total=len(num_cols)\n",
    "    for col in num_cols:\n",
    "        i+=1\n",
    "        asis_dtype = props[col].dtype\n",
    "\n",
    "        # make variables for Int, max and min\n",
    "        IsInt = False\n",
    "        mx = props[col].max()\n",
    "        mn = props[col].min()\n",
    "\n",
    "        # Integer does not support NA, therefore, NA needs to be filled\n",
    "        if not np.isfinite(props[col]).all(): \n",
    "            NAlist.append(col)\n",
    "            props[col].fillna(mn-1,inplace=True)  \n",
    "\n",
    "        # test if column can be converted to an integer\n",
    "        asint = props[col].fillna(0).astype(np.int64)\n",
    "        result = (props[col] - asint)\n",
    "        result = result.sum()\n",
    "        if result > -0.01 and result < 0.01:\n",
    "            IsInt = True\n",
    "\n",
    "        # Make Integer/unsigned Integer datatypes\n",
    "        if IsInt:\n",
    "            if mn >= 0:\n",
    "                if mx < 255:\n",
    "                    props[col] = props[col].astype(np.uint8)\n",
    "                elif mx < 65535:\n",
    "                    props[col] = props[col].astype(np.uint16)\n",
    "                elif mx < 4294967295:\n",
    "                    props[col] = props[col].astype(np.uint32)\n",
    "                else:\n",
    "                    props[col] = props[col].astype(np.uint64)\n",
    "            else:\n",
    "                if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                    props[col] = props[col].astype(np.int8)\n",
    "                elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                    props[col] = props[col].astype(np.int16)\n",
    "                elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                    props[col] = props[col].astype(np.int32)\n",
    "                elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                    props[col] = props[col].astype(np.int64)    \n",
    "\n",
    "        # Make float datatypes 32 bit\n",
    "        else:\n",
    "            props[col] = props[col].astype(np.float32)\n",
    "\n",
    "        tobe_dtype = props[col].dtype\n",
    "        if verbose:\n",
    "            text = '[{}/{}] {}: {} -> {}'.format(\n",
    "                str(i).zfill(len(str(total))),total,col,asis_dtype,tobe_dtype)\n",
    "            print(text)\n",
    "    \n",
    "    # na가 있으면 min-1로 넣었으므로, 이 값들을 다시 nan으로 변경\n",
    "    for col in NAlist:\n",
    "        props[col] = props[col].replace(props[col].min(),np.nan)\n",
    "        \n",
    "    tobe_mem_usg = props.memory_usage().sum() / (2**20)\n",
    "    reduced_mem  = 100*tobe_mem_usg/asis_mem_usg\n",
    "    \n",
    "    if verbose:\n",
    "        print('Memory reduced by {:.2f}% ({:.2f} MB → {:.2f} MB)'.format(reduced_mem,asis_mem_usg,tobe_mem_usg))\n",
    "        \n",
    "    return props, (asis_mem_usg, tobe_mem_usg, reduced_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222a1a4-7dd3-426b-885a-c0318c9c4b5c",
   "metadata": {},
   "source": [
    "(1) 전체 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea3ff3-2851-4dcf-8637-cdd029750c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df10, _ = reduce_mem_usage(train_df9,verbose=False)\n",
    "# test_df10 , _ = reduce_mem_usage(test_df9 ,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59b597-7989-4126-8fcf-952f08f0b5bd",
   "metadata": {},
   "source": [
    "(2) segment 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3b038-a0ef-47ab-8d78-4c3b508822e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_segment_data_dict = deepcopy(del_outlier_segment_data_dict)\n",
    "final_segment_data_dict['train'] = {}\n",
    "final_segment_data_dict['test']  = {}\n",
    "\n",
    "for seg in final_segment_data_dict['segment_list']:\n",
    "    final_segment_data_dict['train'][seg], _ = reduce_mem_usage(segment_data_dict['train'][seg],verbose=False)\n",
    "    final_segment_data_dict['test'] [seg], _ = reduce_mem_usage(segment_data_dict['test'] [seg],verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7132e-26f2-4129-926b-d51cb1ff573d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdebcae-a0db-4e33-8f6a-8d655989d04f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d67f2-4951-4092-aefd-1be42b8577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def mkdir(paths):\n",
    "    if type(paths)==str:\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            print('> Create Folder: {}'.format(path))\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0570e7-41dd-484d-9460-dbd3f06d757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(x,func=['log','sqrt']):\n",
    "    if func=='log':\n",
    "        y = np.exp(x)\n",
    "    elif func=='sqrt':\n",
    "        y = np.square(x)\n",
    "    else:\n",
    "        raise ValueError('Unknown func')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01609cc3-d341-4d8c-9295-45934f1e4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir('./model_checkpoints')\n",
    "mkdir('./model_checkpoints/segment_catboost')\n",
    "mkdir('./model_checkpoints/segment_weightedensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efa7a6-468f-476b-9040-1073dd30ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert train_df10.isnull().sum().sum()==0, \\\n",
    "#     \"Train: null detected\"\n",
    "\n",
    "# assert test_df10.isnull().sum().sum()==0, \\\n",
    "#     \"Train: null detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d430390-c11e-4018-a420-29b1e29850ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (sum([v.isnull().sum().sum() for k,v in final_segment_data_dict['train'].items()])==0), \\\n",
    "    \"Train: null detected\"\n",
    "\n",
    "assert (sum([v.isnull().sum().sum() for k,v in final_segment_data_dict['test'].items()])==0), \\\n",
    "    \"Test: null detected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688c32a-a170-48ca-ab00-0c2b1cbc908b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20ad67-940e-4f80-8978-855e0b257564",
   "metadata": {},
   "source": [
    "## Check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e902282-dd04-44d1-ace3-7890d3c6d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c996d8-ea52-4f76-a183-2b73c95ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fn = train_df10.copy()\n",
    "# test_fn  = test_df10 .copy()\n",
    "\n",
    "final_dict = deepcopy(final_segment_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af8b32-bb2c-4e09-a47c-a20170f0ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2분\n",
    "\n",
    "# X = train_fn.drop(target_feature,axis=1)\n",
    "# y = train_fn[target_feature]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X,y,test_size=0.2,random_state=CFG.SEED,stratify=X['segment'])\n",
    "\n",
    "# train_dataset = Pool(X_train,y_train,cat_features=cat_features+['segment'])\n",
    "# val_dataset   = Pool(X_val  ,y_val  ,cat_features=cat_features+['segment'])\n",
    "\n",
    "# model = CatBoostRegressor(iterations=5000)\n",
    "# model.fit(train_dataset,eval_set=val_dataset,metric_period=1000,early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ff785-101e-4f1e-98d6-613b3f4c8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(val_dataset).flatten()\n",
    "# y_true = y_val.values.flatten()\n",
    "\n",
    "# y_pred = inverse_transform(y_pred,CFG.TARGET_TRANSFORMATION)\n",
    "# y_true = inverse_transform(y_true,CFG.TARGET_TRANSFORMATION)\n",
    "\n",
    "# mean_absolute_error(y_true=y_true,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513dc8b-d8bb-479c-b058-99acacf37e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance = model.feature_importances_\n",
    "# sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "# plt.figure(figsize=(15,10))\n",
    "# plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "# plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
    "# plt.title('Feature Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4369ef-ddbf-42f5-8d3b-008573b4b6b9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f1b72-9651-42d2-8410-52fd85595f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CatBoost\n",
    "- public score : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2683aa-dd52-4402-a77b-7560b27b9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6bfc-9f23-4ad2-bca5-16a3d57c42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eceedea-c652-4a96-b165-e222b01b6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fn = train_df10.copy()\n",
    "# test_fn  = test_df10 .copy()\n",
    "\n",
    "final_dict = deepcopy(final_segment_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0364030-ccdf-40df-a8cc-bf6fad1107f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1시간\n",
    "\n",
    "segment_list = final_dict['segment_list']\n",
    "\n",
    "models = {}\n",
    "feature_info = {}\n",
    "scores = []\n",
    "pbar = tqdm(segment_list)\n",
    "\n",
    "s_i = 0\n",
    "for segment in pbar:\n",
    "    s_i+=1\n",
    "    gc.collect()\n",
    "    \n",
    "    tr_data = final_dict['train'][segment]\n",
    "    \n",
    "    type_controller = TypeController(\n",
    "        target_feature='가격',\n",
    "        cat_features=kmeans_label_features,\n",
    "        unuse_features=CFG.UNUSE_FEATURES,\n",
    "        segment_feature='segment',\n",
    "    )\n",
    "    type_controller.fit(\n",
    "        data=tr_data,\n",
    "        global_assignment=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    tr_data = type_controller.transform(tr_data)\n",
    "    \n",
    "    # # segment에 해당하는 데이터추출\n",
    "    # _X = X[X.segment==segment].drop('segment',axis=1)\n",
    "    # _y = y[X.segment==segment]\n",
    "    _X = tr_data.drop(columns=unuse_features+['segment',target_feature])\n",
    "    _y = tr_data[target_feature]\n",
    "    \n",
    "    # kfold\n",
    "    kf = KFold(n_splits=CFG.N_SPLITS,random_state=1000*s_i+CFG.SEED,shuffle=True)\n",
    "    \n",
    "    # unique인 컬럼 제외\n",
    "    unique_info = _X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X = _X.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # categorical feature에서 unique인 컬럼을 제외\n",
    "    fixed_cat_features = [col for col in cat_features if col in _X.columns]\n",
    "    \n",
    "    _models = []\n",
    "    _scores = []\n",
    "    k=0\n",
    "    for tr_idx,val_idx in kf.split(_X,_y):\n",
    "        k+=1\n",
    "        \n",
    "        # kfold dataset\n",
    "        X_tr, X_va = _X.iloc[tr_idx], _X.iloc[val_idx]\n",
    "        y_tr, y_va = _y.iloc[tr_idx], _y.iloc[val_idx]\n",
    "\n",
    "        # progress\n",
    "        progress = 'Segment: [{}], Size: [{:,}], KFold: [{}/{}]'\\\n",
    "            .format(segment,len(_X),k,CFG.N_SPLITS)\n",
    "        pbar.set_description(progress)\n",
    "\n",
    "        # dataset\n",
    "        train_dataset = Pool(X_tr,y_tr,cat_features=fixed_cat_features)\n",
    "        valid_dataset = Pool(X_va,y_va,cat_features=fixed_cat_features)\n",
    "\n",
    "        # define the model\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function='MAE',\n",
    "            random_state=CFG.SEED,\n",
    "            iterations=CFG.EPOCHS,\n",
    "            learning_rate=CFG.LR,\n",
    "            allow_writing_files=False,\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            eval_set=valid_dataset,\n",
    "            early_stopping_rounds=CFG.ES,\n",
    "            verbose=0,\n",
    "            #metric_period=CFG.EPOCHS//5,\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        model.save_model(f'./model_checkpoints/segment_catboost/{segment}_k{k}.cbm')\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model.predict(valid_dataset).flatten()\n",
    "        y_true = y_va.values\n",
    "        \n",
    "        # inverse transform\n",
    "        if CFG.TARGET_TRANSFORMATION=='log':\n",
    "            y_pred = np.exp(y_pred)\n",
    "            y_true = np.exp(y_true)\n",
    "        elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "            y_pred = np.square(y_pred)\n",
    "            y_true = np.square(y_true)\n",
    "            \n",
    "        # calculate score\n",
    "        score = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "        \n",
    "        # append inner loop\n",
    "        _models.append(model)\n",
    "        _scores.append([segment,k,len(X_tr),len(X_va),score])\n",
    "\n",
    "    # append outer loop\n",
    "    models[segment] = _models\n",
    "    scores.append(_scores)\n",
    "    feature_info[segment] = {'cat_features':fixed_cat_features,'features':_X.columns.tolist()}\n",
    "    \n",
    "    # score report\n",
    "    mean_score_report = pd.Series(np.array(_scores)[:,-1]).astype(float).mean()\n",
    "    print('Segment: {}'.format(segment))\n",
    "    print(\"MAE's for {}-Fold: [{}]\".format(CFG.N_SPLITS,np.array(pd.Series(np.array(_scores)[:,-1]).astype(float).values)))\n",
    "    print(\"Mean of MAE's for {}-Fold: [{:.4f}]\".format(CFG.N_SPLITS,mean_score_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33753abc-bf26-4ee8-b7a6-847686119b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(\n",
    "    np.array(scores).reshape(len(scores)*5,5),\n",
    "    columns=['segment','k','n_tr','n_val','score']\n",
    ")\n",
    "score_df[['k','n_tr','n_val','score']] = score_df[['k','n_tr','n_val','score']].astype(float)\n",
    "score_df.sort_values(['segment','k']).head(10)\n",
    "\n",
    "# score_df.groupby('segment')[['n_tr','n_val','score']].mean().sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89dbbe-b7cc-4107-b3bc-197c368d49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "X_test = test_fn.copy()\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "tr_pred_list = []\n",
    "te_pred_list = []\n",
    "for segment in tqdm(segment_list):\n",
    "    ## data load\n",
    "    # (1) train\n",
    "    train_data = X[X.segment==segment][feature_info[segment]['features']]\n",
    "    train_dataset = Pool(train_data,cat_features=feature_info[segment]['cat_features'])\n",
    "    # (2) test\n",
    "    test_data = X_test[X_test.segment==segment][feature_info[segment]['features']]\n",
    "    test_dataset = Pool(test_data,cat_features=feature_info[segment]['cat_features'])\n",
    "    \n",
    "    ## model\n",
    "    kfold_models = models[segment]\n",
    "    \n",
    "    ## prediction\n",
    "    # (1) train\n",
    "    tr_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'true':y[X.segment==segment].values.flatten(),\n",
    "        'pred':np.mean([model.predict(train_dataset) for model in kfold_models],axis=0),\n",
    "        #'pred':np.sum([weight*model.predict(train_dataset) for weight,model in zip(kfold_weights,kfold_models)],axis=0),\n",
    "    })\n",
    "    tr_pred_df.index = train_data.index\n",
    "    # (2) test\n",
    "    te_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'pred':np.mean([model.predict(test_dataset) for model in kfold_models],axis=0),\n",
    "        #'pred':np.sum([weight*model.predict(test_dataset) for weight,model in zip(kfold_weights,kfold_models)],axis=0),\n",
    "    })\n",
    "    te_pred_df.index = test_data.index\n",
    "    \n",
    "    ## Target Transformation\n",
    "    if CFG.TARGET_TRANSFORMATION=='log':\n",
    "        tr_pred_df['true'] = np.exp(tr_pred_df['true'])\n",
    "        tr_pred_df['pred'] = np.exp(tr_pred_df['pred'])\n",
    "        te_pred_df['pred'] = np.exp(te_pred_df['pred'])\n",
    "    elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "        tr_pred_df['true'] = np.square(tr_pred_df['true'])\n",
    "        tr_pred_df['pred'] = np.square(tr_pred_df['pred'])\n",
    "        te_pred_df['pred'] = np.square(te_pred_df['pred'])\n",
    "    \n",
    "    ## append\n",
    "    tr_pred_list.append(tr_pred_df)\n",
    "    te_pred_list.append(te_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad43d66-1e96-437e-8d88-43c6549e1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_pred_df = pd.concat(tr_pred_list,axis=0).sort_index()\n",
    "mean_absolute_error(y_true=tr_pred_df.true,y_pred=tr_pred_df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a46da9-6aa5-4fd2-95a2-339aa93a4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred_df = pd.concat(te_pred_list,axis=0).sort_index()\n",
    "te_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930b0b3-1356-476c-a964-678388cc725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_csv('./out/15_ensemble_stacking_segment_브랜드_kfold_logy_interaction_.csv')['가격']\n",
    "# b = te_pred_df.pred.values\n",
    "\n",
    "# mean_absolute_error(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d926b-6db7-4b8d-9e16-1ca1c0331872",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['가격'] = te_pred_df.pred.values\n",
    "submit.to_csv('./out/19_catboost_segment_브랜드국적_kfold_logy_outlierdetection.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7a7e9-1c4b-47c3-8959-0412cfc188f6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34001f77-6935-4c16-8d86-2ee1e7ab27ef",
   "metadata": {},
   "source": [
    "## Weighted Ensemble\n",
    "- public score : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d603f-56f8-4006-a62b-386cc14d51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,columns):\n",
    "        self.transform_list = []\n",
    "        for col in columns:\n",
    "            for i,value in enumerate(sorted(data[col].unique())):\n",
    "                if i>0:\n",
    "                    self.transform_list.append([col,value])\n",
    "        \n",
    "    def transform(self,data):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        new_data = data.copy()\n",
    "        for col,value in self.transform_list:\n",
    "            new_data[f'{col}_{value}'] = np.where(new_data[col]==value,1,0)\n",
    "        drop_columns = pd.unique(np.array(self.transform_list)[:,0])\n",
    "        new_data.drop(columns=drop_columns,inplace=True)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20d5c0-a506-418d-838c-65c44c753dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,weight=['equal','balanced'],inverse_transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        \n",
    "        self.weight = weight\n",
    "        self.inverse_transform = inverse_transform\n",
    "        self._get_regressors()\n",
    "    \n",
    "    def _get_regressors(self):\n",
    "        max_depth = 20\n",
    "        n_jobs = -1\n",
    "        \n",
    "        params_elasticnet = {\n",
    "            'l1_ratio' : np.arange(0.1, 1, 0.1),\n",
    "            'alphas' : [1e-5, 1e-3, 1e-1, 0.0, 1.0, 10.0, 100.0],\n",
    "            'cv' : RepeatedKFold(n_splits=CFG.N_SPLITS, n_repeats=5, random_state=CFG.SEED),\n",
    "            'n_jobs' : n_jobs,\n",
    "            'max_iter' : 30000,\n",
    "            'tol' : 0.001,\n",
    "        }\n",
    "        \n",
    "        params_catboost = {\n",
    "            'random_state' : CFG.SEED,\n",
    "            'iterations' : CFG.EPOCHS,\n",
    "            'early_stopping_rounds' : CFG.ES,\n",
    "            'learning_rate' : CFG.LR,\n",
    "            'loss_function' : 'MAE',\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : max_depth,\n",
    "            #'l2_leaf_reg' : 1,\n",
    "        }\n",
    "    \n",
    "        params_xgboost = {\n",
    "            'random_state' : CFG.SEED,\n",
    "            'n_estimators' : CFG.XGB_EPOCHS,\n",
    "            'early_stopping_rounds' : CFG.XGB_ES,\n",
    "            'learning_rate' : CFG.XGB_LR,\n",
    "            'objective' : 'reg:absoluteerror',\n",
    "            'verbosity' : 0,\n",
    "            'max_depth': max_depth,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "    \n",
    "        params_lightgbm = {\n",
    "            'random_state' : CFG.SEED,\n",
    "            'n_estimators' : CFG.EPOCHS,\n",
    "            'early_stopping_round' : CFG.ES,\n",
    "            'learning_rate' : CFG.LR,\n",
    "            'objective' : 'regression',\n",
    "            'metric' : 'mean_absolute_error',\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': max_depth,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        params_extratrees = {\n",
    "            'random_state' : CFG.SEED,\n",
    "            'n_estimators' : CFG.XTRATREES_EPOCHS,\n",
    "            'criterion' : 'absolute_error',\n",
    "            'verbose' : 0,\n",
    "            'max_depth' : max_depth,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        self.regressors = [\n",
    "            #LinearRegression(),\n",
    "            ElasticNetCV(**params_elasticnet),\n",
    "            CatBoostRegressor(**params_catboost),\n",
    "            XGBRegressor(**params_xgboost),\n",
    "            LGBMRegressor(**params_lightgbm),\n",
    "            ExtraTreesRegressor(**params_extratrees),\n",
    "        ]\n",
    "        self.regressors_name = [\n",
    "            #'OLS',\n",
    "            'ElasticNet',\n",
    "            'CatBoost',\n",
    "            'XGBoost',\n",
    "            'LightGBM',\n",
    "            'ExtraTrees',\n",
    "        ]\n",
    "        \n",
    "    def _set_zero_to_minimum(self,pred,minimum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred<0,1,0).sum()>0:\n",
    "            pred = [x if x>0 else minimum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def _set_inf_to_maximum(self,pred,maximum_value):\n",
    "        pred = np.array(pred).flatten()\n",
    "        if np.where(pred==np.inf,1,0).sum()>0:\n",
    "            pred = [x if x!=np.inf else maximum_value for x in pred]\n",
    "        pred = np.array(pred).flatten()\n",
    "        return pred\n",
    "    \n",
    "    def fit(self,X,y,eval_set,oh_set,cat_features,verbose=1):\n",
    "        assert len(eval_set)==1, \\\n",
    "            \"eval_set length must be 1. len(eval_set)={}\".format(len(eval_set))\n",
    "        assert len(oh_set)==1, \\\n",
    "            \"oh_set length must be 1. len(oh_set)={}\".format(len(oh_set))\n",
    "        X_val, y_val = eval_set[0]\n",
    "        X_oh, X_val_oh = oh_set[0]\n",
    "        \n",
    "        tr_true = np.array(y)    .flatten()\n",
    "        va_true = np.array(y_val).flatten()\n",
    "        if self.inverse_transform is not None:\n",
    "            tr_true = self.inverse_transform(tr_true)\n",
    "            va_true = self.inverse_transform(va_true)\n",
    "        \n",
    "        self.minimum_value = min(np.nanmin(y),np.nanmin(y_val))\n",
    "        self.maximum_value = max(np.nanmax(y),np.nanmax(y_val))\n",
    "        \n",
    "        self.cat_features = cat_features\n",
    "        self.weights = []\n",
    "        self.fitting_elapsed = []\n",
    "        if verbose:\n",
    "            pbar = tqdm(zip(self.regressors_name,self.regressors),total=len(self.regressors))\n",
    "        else:\n",
    "            pbar = zip(self.regressors_name,self.regressors)\n",
    "            \n",
    "        fit_iter = 0\n",
    "        for regressor_name,regressor in pbar:\n",
    "            fit_iter+=1\n",
    "            s = time.time()\n",
    "            \n",
    "            if verbose:\n",
    "                pbar.set_description(name)\n",
    "                \n",
    "            if regressor_name in ['OLS','ElasticNet']:\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                regressor.fit(X_oh,y)\n",
    "                tr_pred = regressor.predict(X_oh)\n",
    "                va_pred = regressor.predict(X_val_oh)\n",
    "            elif regressor_name=='CatBoost':\n",
    "                train_dataset = Pool(X,y,cat_features=cat_features)\n",
    "                val_dataset   = Pool(X_val,y_val,cat_features=cat_features)\n",
    "                regressor.fit(\n",
    "                    train_dataset,\n",
    "                    eval_set=val_dataset,\n",
    "                    #metric_period=CFG.EPOCHS//5,\n",
    "                )\n",
    "                tr_pred = regressor.predict(train_dataset)\n",
    "                va_pred = regressor.predict(val_dataset)\n",
    "            elif regressor_name=='XGBoost':\n",
    "                regressor.fit(\n",
    "                    X_oh,y,\n",
    "                    eval_set=[(X_val_oh,y_val)],\n",
    "                    verbose=0,\n",
    "                )\n",
    "                tr_pred = regressor.predict(X_oh)\n",
    "                va_pred = regressor.predict(X_val_oh)\n",
    "            elif regressor_name=='LightGBM':\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                X_tmp = X.copy()\n",
    "                X_val_tmp = X_val.copy()\n",
    "                for col in cat_features:\n",
    "                    X_tmp[col]     = X_tmp[col]    .astype('category')\n",
    "                    X_val_tmp[col] = X_val_tmp[col].astype('category')\n",
    "                regressor.fit(\n",
    "                    X_tmp,y,\n",
    "                    eval_set=[(X_val_tmp,y_val)],\n",
    "                    verbose=-1,\n",
    "                )\n",
    "                tr_pred = regressor.predict(X_tmp)\n",
    "                va_pred = regressor.predict(X_val_tmp)\n",
    "            elif regressor_name=='ExtraTrees':\n",
    "                regressor.fit(X_oh,y)\n",
    "                tr_pred = regressor.predict(X_oh)\n",
    "                va_pred = regressor.predict(X_val_oh)\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "            tr_pred = np.array(tr_pred).flatten()\n",
    "            va_pred = np.array(va_pred).flatten()\n",
    "            \n",
    "            tr_pred = self._set_zero_to_minimum(tr_pred,self.minimum_value)\n",
    "            va_pred = self._set_zero_to_minimum(va_pred,self.minimum_value)\n",
    "            \n",
    "            if self.inverse_transform is not None:\n",
    "                tr_pred = self.inverse_transform(tr_pred)\n",
    "                va_pred = self.inverse_transform(va_pred)\n",
    "                tr_pred = self._set_inf_to_maximum(tr_pred,self.maximum_value)\n",
    "                va_pred = self._set_inf_to_maximum(va_pred,self.maximum_value)\n",
    "            \n",
    "            tr_score = mean_absolute_error(y_pred=tr_pred,y_true=tr_true)\n",
    "            va_score = mean_absolute_error(y_pred=va_pred,y_true=va_true)\n",
    "            e = time.time()\n",
    "            self.weights.append(1/va_score)\n",
    "            self.fitting_elapsed.append(e-s)\n",
    "            \n",
    "            blank = ' '*(11-len(regressor_name))\n",
    "            fit_progress = '[{}/{}] {}{}: score={:.3f}, val_score={:.3f}, elasped={:.1f}s'\\\n",
    "                .format(fit_iter,len(self.regressors),regressor_name,blank,tr_score,va_score,e-s)\n",
    "            print(fit_progress)\n",
    "        \n",
    "        if self.weight=='equal':\n",
    "            self.weights = np.array([1.0 for _ in self.regressors])\n",
    "        self.weights /= sum(self.weights)\n",
    "        \n",
    "        tr_pred = self.predict(X,X_oh)\n",
    "        va_pred = self.predict(X_val,X_val_oh)\n",
    "        \n",
    "        ## -> self.predict에서 inverse_transform 해줌\n",
    "        # if self.inverse_transform is not None:\n",
    "        #     tr_pred = self.inverse_transform(tr_pred)\n",
    "        #     va_pred = self.inverse_transform(va_pred)\n",
    "        \n",
    "        ens_tr_score = mean_absolute_error(y_true=tr_true,y_pred=tr_pred)\n",
    "        ens_va_score = mean_absolute_error(y_true=va_true,y_pred=va_pred)\n",
    "        \n",
    "        total_fit_progress = 'Weighted Ensemble({}): score={:.3f}, val_score={:.3f}, elasped={:.1f}s'\\\n",
    "            .format(self.weight,ens_tr_score,ens_va_score,sum(self.fitting_elapsed))\n",
    "        print(total_fit_progress)\n",
    "        \n",
    "    def predict(self,X,X_oh):\n",
    "        assert len(X)==len(X_oh), \\\n",
    "            \"X and X_oh must be same length\"\n",
    "        \n",
    "        pred_list = []\n",
    "        for regressor_name,regressor in zip(self.regressors_name,self.regressors):\n",
    "            if regressor_name in ['OLS','ElasticNet','XGBoost','ExtraTrees']:\n",
    "                dataset = X_oh.copy()\n",
    "            elif regressor_name=='CatBoost':\n",
    "                dataset = Pool(X,cat_features=self.cat_features)\n",
    "            elif regressor_name=='LightGBM':\n",
    "                dataset = X.copy()\n",
    "                for col in self.cat_features:\n",
    "                    dataset[col] = dataset[col].astype('category')\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "            y_pred = regressor.predict(dataset)\n",
    "            y_pred = self._set_zero_to_minimum(y_pred,self.minimum_value)\n",
    "            \n",
    "            pred_list.append(y_pred)\n",
    "            \n",
    "        final_pred = np.zeros(len(X))\n",
    "        for pred,weight in zip(pred_list,self.weights):\n",
    "            final_pred += np.array(pred)*weight\n",
    "            \n",
    "        if self.inverse_transform is not None:\n",
    "            final_pred = self.inverse_transform(np.array(final_pred))\n",
    "            final_pred = self._set_inf_to_maximum(final_pred,self.maximum_value)\n",
    "            \n",
    "        return final_pred\n",
    "    \n",
    "    def save_model(self,path):\n",
    "        save_dict = {\n",
    "            'cat_features' : self.cat_features,\n",
    "            'weights' : self.weights,\n",
    "            'fitting_elapsed' : self.fitting_elapsed,\n",
    "            'regressors' : self.regressors,\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_model(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.weights = save_dict['weights']\n",
    "            self.fitting_elapsed = save_dict['fitting_elapsed']\n",
    "            self.regressors = save_dict['regressors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e22bc-b630-430c-a9f9-6f539e219c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac1666-19fe-4f0c-b651-8d6267292d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003ed17-9aae-4651-9020-35ae377b036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fn = train_df10.copy()\n",
    "# test_fn  = test_df10 .copy()\n",
    "\n",
    "final_dict = deepcopy(final_segment_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb213e-a076-45ba-b12d-94449e44d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2시간\n",
    "\n",
    "segment_list = final_dict['segment_list']\n",
    "\n",
    "models = {}\n",
    "feature_info = {}\n",
    "scores = []\n",
    "ohes = {}\n",
    "\n",
    "s_i = 0\n",
    "for segment in tqdm(segment_list[:1]):\n",
    "    s_i+=1\n",
    "    gc.collect()\n",
    "    \n",
    "    tr_data = final_dict['train'][segment]\n",
    "    \n",
    "    type_controller = TypeController(\n",
    "        target_feature='가격',\n",
    "        cat_features=kmeans_label_features,\n",
    "        unuse_features=CFG.UNUSE_FEATURES,\n",
    "        segment_feature='segment',\n",
    "    )\n",
    "    type_controller.fit(\n",
    "        data=tr_data,\n",
    "        global_assignment=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    tr_data = type_controller.transform(tr_data)\n",
    "    \n",
    "    # segment에 해당하는 데이터추출\n",
    "    _X = tr_data.drop(columns=unuse_features+['segment',target_feature])\n",
    "    _y = tr_data[target_feature]\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(_X,cat_features)\n",
    "    _X_oh = ohe.transform(_X)\n",
    "    \n",
    "    # kfold\n",
    "    kf = KFold(n_splits=CFG.N_SPLITS,random_state=1000*s_i+CFG.SEED,shuffle=True)\n",
    "    \n",
    "    # unique인 컬럼 제외\n",
    "    # (1) X\n",
    "    unique_info = _X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X = _X.drop(unique_cols,axis=1)\n",
    "    # (2) X_oh\n",
    "    unique_info = _X_oh.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X_oh = _X_oh.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # categorical feature에서 unique인 컬럼을 제외\n",
    "    fixed_cat_features = [col for col in cat_features if col in _X.columns]\n",
    "    \n",
    "    # progress\n",
    "    progress = '> [{}/{}] Segment: {}, Shape: Dataset={}, OH-Dataset{}'.format(s_i,len(segment_list),segment,_X.shape,_X_oh.shape)\n",
    "    print('-'*80)\n",
    "    print(progress)\n",
    "    print('-'*80)\n",
    "    \n",
    "    _models = []\n",
    "    _scores = []\n",
    "    k=0\n",
    "    for tr_idx,val_idx in kf.split(_X,_y):\n",
    "        k+=1\n",
    "        print('> [K-Fold] {}/{}'.format(k,CFG.N_SPLITS))\n",
    "        \n",
    "        # kfold dataset\n",
    "        X_tr   , X_va    = _X   .iloc[tr_idx], _X   .iloc[val_idx]\n",
    "        X_tr_oh, X_va_oh = _X_oh.iloc[tr_idx], _X_oh.iloc[val_idx]\n",
    "        y_tr   , y_va    = _y   .iloc[tr_idx], _y   .iloc[val_idx]\n",
    "\n",
    "        # define the model\n",
    "        ensemble_model = WeightedEnsembleRegressor(\n",
    "            weight='balanced',\n",
    "            inverse_transform=lambda x: inverse_transform(x,func=CFG.TARGET_TRANSFORMATION),\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        ensemble_model.fit(\n",
    "            X_tr,y_tr,\n",
    "            eval_set=[(X_va,y_va)],\n",
    "            oh_set=[(X_tr_oh,X_va_oh)],\n",
    "            cat_features=fixed_cat_features,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        ensemble_model.save_model(f'./model_checkpoints/segment_weightedensemble/{segment}_k{k}.pickle')\n",
    "\n",
    "        # prediction\n",
    "        y_pred = ensemble_model.predict(X_va,X_va_oh).flatten()\n",
    "        y_true = y_va.values\n",
    "        \n",
    "        # caculate score\n",
    "        score = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "        \n",
    "        # append inner loop\n",
    "        _models.append(ensemble_model)\n",
    "        _scores.append([segment,k,len(X_tr),len(X_va),score])\n",
    "\n",
    "    # append outer loop\n",
    "    models[segment] = _models\n",
    "    scores.append(_scores)\n",
    "    feature_info[segment] = {\n",
    "        'cat_features':fixed_cat_features,\n",
    "        'features':_X.columns.tolist(),\n",
    "        'oh_features':_X_oh.columns.tolist(),\n",
    "    }\n",
    "    ohes[segment] = ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b710dcb-dde0-4416-a93b-3c39982bc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "segment_list = final_dict['segment_list']\n",
    "\n",
    "tr_pred_list = []\n",
    "te_pred_list = []\n",
    "for segment in tqdm(segment_list[:1]):\n",
    "    tr_data = final_dict['train'][segment]\n",
    "    te_data = final_dict['test'] [segment]\n",
    "    \n",
    "    type_controller = TypeController(\n",
    "        target_feature='가격',\n",
    "        cat_features=kmeans_label_features,\n",
    "        unuse_features=CFG.UNUSE_FEATURES,\n",
    "        segment_feature='segment',\n",
    "    )\n",
    "    type_controller.fit(\n",
    "        data=tr_data,\n",
    "        global_assignment=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    tr_data = type_controller.transform(tr_data)\n",
    "    te_data = type_controller.transform(te_data)\n",
    "    \n",
    "    ohe = ohes[segment]\n",
    "    tr_data_oh = ohe.transform(tr_data)\n",
    "    te_data_oh = ohe.transform(te_data)\n",
    "    \n",
    "    ## data load\n",
    "    # (1) train\n",
    "    train_data    = tr_data   [feature_info[segment]['features']]\n",
    "    train_data_oh = tr_data_oh[feature_info[segment]['oh_features']]\n",
    "    # (2) test\n",
    "    test_data     = te_data   [feature_info[segment]['features']]\n",
    "    test_data_oh  = te_data_oh[feature_info[segment]['oh_features']]\n",
    "    \n",
    "    ## model\n",
    "    kfold_models = models[segment]\n",
    "    \n",
    "    ## prediction\n",
    "    # (1) train\n",
    "    tr_pred_df = pd.DataFrame({\n",
    "        'true':tr_data[target_feature].values.flatten(),\n",
    "    })\n",
    "    for i in range(len(kfold_models)):\n",
    "        tr_pred_df[f'pred_{i+1}'] = kfold_models[i].predict(train_data,train_data_oh)\n",
    "    tr_pred_df.index = train_data.index\n",
    "    # (2) test\n",
    "    te_pred_df = pd.DataFrame()\n",
    "    for i in range(len(kfold_models)):\n",
    "        te_pred_df[f'pred_{i+1}'] = kfold_models[i].predict(test_data,test_data_oh)\n",
    "    te_pred_df.index = test_data.index\n",
    "        \n",
    "    ## target transformation\n",
    "    if CFG.TARGET_TRANSFORMATION=='log':\n",
    "        tr_pred_df['true'] = np.exp(tr_pred_df['true'])\n",
    "    elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "        tr_pred_df['true'] = np.square(tr_pred_df['true'])\n",
    "    \n",
    "    ## append\n",
    "    tr_pred_list.append(tr_pred_df)\n",
    "    te_pred_list.append(te_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f47481-304d-4323-af1c-babe88751e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data,tr_pred_df],axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0283f9-8f1f-44f4-9f7e-b5eba512d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_pred_df = pd.concat(tr_pred_list,axis=0).sort_index()\n",
    "te_pred_df = pd.concat(te_pred_list,axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d479c-97d9-4424-ae94-12bdc9b3c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tr_pred_df.drop('true',axis=1).apply(lambda x: np.nanmean(x), axis=1)\n",
    "y_true = tr_pred_df.true\n",
    "\n",
    "mean_absolute_error(y_pred=y_pred,y_true=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca575b3-65b7-477d-9dc5-b09c658507b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_df.to_parquet('./out/stacking/tr_pred_df_segment브랜드_ensemble_logy_생산년도그룹추가.parquet')\n",
    "te_pred_df.to_parquet('./out/stacking/te_pred_df_segment브랜드_ensemble_logy_생산년도그룹추가.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc20a2d-0af6-4242-876d-e42dc5158c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_df.to_csv('./out/stacking/tr_pred_df_segment브랜드_ensemble_logy_생산년도그룹추가.csv',index=False)\n",
    "te_pred_df.to_csv('./out/stacking/te_pred_df_segment브랜드_ensemble_logy_생산년도그룹추가.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a047bc-f568-42f0-95a5-8c6d647428b1",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179a4ec-026b-4c9c-8e27-e0036546ea2e",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5be77-2d91-4769-a03e-99847998d583",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(2) ensemble regressor : 5.6123244492\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa2fe0-8480-43ca-8ad6-84fce10d0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "segment_list = final_dict['segment_list']\n",
    "\n",
    "# add predictions\n",
    "X      = pd.concat([X     ,tr_pred_df.drop('true',axis=1)],axis=1)\n",
    "X_test = pd.concat([X_test,te_pred_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a11d2-3378-4201-a9c6-59cc80511b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 30분\n",
    "\n",
    "stacking_models = {}\n",
    "stacking_feature_info = {}\n",
    "stacking_scores = []\n",
    "pbar = tqdm(segment_list)\n",
    "\n",
    "s_i = 0\n",
    "for segment in pbar:\n",
    "    s_i+=1\n",
    "    \n",
    "    # segment에 해당하는 데이터추출\n",
    "    _X = X[X.segment==segment].drop('segment',axis=1)\n",
    "    _y = y[X.segment==segment]\n",
    "    \n",
    "    # kfold\n",
    "    kf = KFold(n_splits=CFG.N_SPLITS,random_state=1000*s_i+CFG.SEED,shuffle=True)\n",
    "    \n",
    "    # unique인 컬럼 제외\n",
    "    unique_info = _X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X = _X.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # categorical feature에서 unique인 컬럼을 제외\n",
    "    fixed_cat_features = [col for col in cat_features if col in _X.columns]\n",
    "    \n",
    "    _models = []\n",
    "    _scores = []\n",
    "    k=0\n",
    "    for tr_idx,val_idx in kf.split(_X,_y):\n",
    "        k+=1\n",
    "        \n",
    "        # kfold dataset\n",
    "        X_tr, X_va = _X.iloc[tr_idx], _X.iloc[val_idx]\n",
    "        y_tr, y_va = _y.iloc[tr_idx], _y.iloc[val_idx]\n",
    "\n",
    "        # progress\n",
    "        progress = 'Segment: [{}], Size: [{:,}], KFold: [{}/{}]'\\\n",
    "            .format(segment,len(_X),k,CFG.N_SPLITS)\n",
    "        pbar.set_description(progress)\n",
    "\n",
    "        # dataset\n",
    "        train_dataset = Pool(X_tr,y_tr,cat_features=fixed_cat_features)\n",
    "        valid_dataset = Pool(X_va,y_va,cat_features=fixed_cat_features)\n",
    "\n",
    "        # define the model\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function='MAE',\n",
    "            random_state=CFG.SEED,\n",
    "            iterations=CFG.EPOCHS,\n",
    "            learning_rate=CFG.LR,\n",
    "            allow_writing_files=False,\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            eval_set=valid_dataset,\n",
    "            early_stopping_rounds=CFG.ES,\n",
    "            verbose=0,\n",
    "            #metric_period=CFG.EPOCHS//5,\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        model.save_model(f'./model_checkpoints/\bstacking_segment_catboost/{segment}_k{k}.cbm')\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model.predict(valid_dataset).flatten()\n",
    "        y_true = y_va.values\n",
    "        \n",
    "        # inverse transform\n",
    "        if CFG.TARGET_TRANSFORMATION=='log':\n",
    "            y_pred = np.exp(y_pred)\n",
    "            y_true = np.exp(y_true)\n",
    "        elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "            y_pred = np.square(y_pred)\n",
    "            y_true = np.square(y_true)  \n",
    "            \n",
    "        # calculate score\n",
    "        score = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "        \n",
    "        # append inner loop\n",
    "        _models.append(model)\n",
    "        _scores.append([segment,k,len(X_tr),len(X_va),score])\n",
    "\n",
    "    # append outer loop\n",
    "    stacking_models[segment] = _models\n",
    "    stacking_scores.append(_scores)\n",
    "    stacking_feature_info[segment] = {'cat_features':fixed_cat_features,'features':_X.columns.tolist()}\n",
    "    \n",
    "    # score report\n",
    "    mean_score_report = pd.Series(np.array(_scores)[:,-1]).astype(float).mean()\n",
    "    print('Segment: {}'.format(segment))\n",
    "    print(\"MAE's for {}-Fold: [{}]\".format(CFG.N_SPLITS,np.array(pd.Series(np.array(_scores)[:,-1]).astype(float).values)))\n",
    "    print(\"Mean of MAE's for {}-Fold: [{:.4f}]\".format(CFG.N_SPLITS,mean_score_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b52be1-e1c0-42f5-957d-d1b013899666",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(\n",
    "    np.array(stacking_scores).reshape(len(stacking_scores)*5,5),\n",
    "    columns=['segment','k','n_tr','n_val','score']\n",
    ")\n",
    "\n",
    "score_df.sort_values(['segment','k']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf56e57-6f26-419a-9fbe-4746126440f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "X_test = test_fn.copy()\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "# add predictions\n",
    "X      = pd.concat([X     ,tr_pred_df.drop('true',axis=1)],axis=1)\n",
    "X_test = pd.concat([X_test,te_pred_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633313fb-66fc-4990-98ed-a6898187b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_list = []\n",
    "te_pred_list = []\n",
    "for segment in tqdm(segment_list):\n",
    "    ## data load\n",
    "    # (1) train\n",
    "    train_data = X[X.segment==segment][stacking_feature_info[segment]['features']]\n",
    "    train_dataset = Pool(train_data,cat_features=stacking_feature_info[segment]['cat_features'])\n",
    "    # (2) test\n",
    "    test_data = X_test[X_test.segment==segment][stacking_feature_info[segment]['features']]\n",
    "    test_dataset = Pool(test_data,cat_features=stacking_feature_info[segment]['cat_features'])\n",
    "    \n",
    "    ## model\n",
    "    kfold_models  = stacking_models[segment]\n",
    "    kfold_weights = 1 / score_df[score_df.segment==segment].score.astype(float).values\n",
    "    kfold_weights /= sum(kfold_weights)\n",
    "    \n",
    "    ## prediction\n",
    "    # (1) train\n",
    "    tr_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'true':y[X.segment==segment].values.flatten(),\n",
    "        'pred':np.mean([model.predict(train_dataset) for model in kfold_models],axis=0),\n",
    "        #'pred':np.sum([weight*model.predict(train_dataset) for weight,model in zip(kfold_weights,kfold_models)],axis=0),\n",
    "    })\n",
    "    tr_pred_df.index = train_data.index\n",
    "    # (2) test\n",
    "    te_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'pred':np.mean([model.predict(test_dataset) for model in kfold_models],axis=0),\n",
    "        #'pred':np.sum([weight*model.predict(test_dataset) for weight,model in zip(kfold_weights,kfold_models)],axis=0),\n",
    "    })\n",
    "    te_pred_df.index = test_data.index\n",
    "    \n",
    "    ## Target Transformation\n",
    "    if CFG.TARGET_TRANSFORMATION=='log':\n",
    "        tr_pred_df['true'] = np.exp(tr_pred_df['true'])\n",
    "        tr_pred_df['pred'] = np.exp(tr_pred_df['pred'])\n",
    "        te_pred_df['pred'] = np.exp(te_pred_df['pred'])\n",
    "    elif CFG.TARGET_TRANSFORMATION=='sqrt':\n",
    "        tr_pred_df['true'] = np.square(tr_pred_df['true'])\n",
    "        tr_pred_df['pred'] = np.square(tr_pred_df['pred'])\n",
    "        te_pred_df['pred'] = np.square(te_pred_df['pred'])\n",
    "    \n",
    "    ## append\n",
    "    tr_pred_list.append(tr_pred_df)\n",
    "    te_pred_list.append(te_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c58da-a007-47a4-9502-a79bf2c3fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_pred_df = pd.concat(tr_pred_list,axis=0).sort_index()\n",
    "mean_absolute_error(y_true=tr_pred_df.true,y_pred=tr_pred_df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e656b61-ccf0-4aae-aa53-627b78c83025",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred_df = pd.concat(te_pred_list,axis=0).sort_index()\n",
    "te_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33940985-d98b-4eaf-9535-f2e8420b8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['가격'] = te_pred_df.pred.values\n",
    "submit.to_csv('./out/23_ensemble_stacking_segment_브랜드_kfold_logy_생산년도그룹추가.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f971ca-800f-4915-b6ec-3ce7646e8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = submit['가격']\n",
    "b = pd.read_csv('./out/16_ensemble_stacking_segment_브랜드_kfold_logy.csv')['가격']\n",
    "\n",
    "mean_absolute_error(a,b), np.sqrt(sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fd4f5-3135-43a5-bcac-b7590ddea438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycaret import regression\n",
    "\n",
    "# d = pd.concat([X_tr,y_tr],axis=1).head()\n",
    "\n",
    "# regression.setup(\n",
    "#     data=d,\n",
    "#     target='가격',\n",
    "#     remove_outliers=False,\n",
    "#     verbose=True,\n",
    "#     normalize=False,\n",
    "#     feature_selection=False,\n",
    "#     session_id=0,\n",
    "#     categorical_features=fixed_cat_features,\n",
    "# )\n",
    "# best = regression.compare_models(n_select=5,fold=3,sort='MAE')\n",
    "# # best\n",
    "\n",
    "# preds = np.array([regression.predict_model(b,X_va)['prediction_label'].values.flatten() for b in best])\n",
    "# preds = np.mean(preds,axis=0)\n",
    "# preds.shape\n",
    "\n",
    "# mean_absolute_error(y_true=np.exp(y_va.values.flatten()),y_pred=np.exp(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
