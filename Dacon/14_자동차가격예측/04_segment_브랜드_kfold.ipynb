{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b987cd1-0680-4fb8-a482-f5567577e80f",
   "metadata": {},
   "source": [
    "# Library Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eeab870-95ed-41a6-b815-1bdf2dee38fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b6f33-3b7d-401a-a710-ea0f089f95a7",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b54b8f-874d-49c7-95db-a535b984b01d",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe03a3b-5d80-448f-ad15-50e77c5128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 0\n",
    "    \n",
    "    SUBSET_DEPTH = 3\n",
    "    INTERACTION = False\n",
    "    FS_ALPHA = 0.01\n",
    "    \n",
    "    N_SPLITS = 5\n",
    "    TARGET_TRANSFORMATION = True\n",
    "    \n",
    "    LR = 0.003\n",
    "    EPOCHS = 30000\n",
    "    ES = 300\n",
    "    XGB_LR = 0.3     # default\n",
    "    XGB_EPOCHS = 1000 # default\n",
    "    XGB_ES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2cfe-7bf1-4cc3-b7e3-90b0886eab54",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbf58b-29ed-4ab7-9a2c-f51c6b08c00a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8ae59-c4c3-47f8-854a-6cfc780a2adc",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a824834-ad27-424a-8f55-7c4ca73eaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da88a595-b068-4b4b-97a9-4e6bcf863305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d6f038-af80-42c0-a33e-e62dcd8bdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04590f97-2b7a-4039-aa51-7528b7f14a91",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b653065-ce2f-4186-89d0-eb71b4bd0986",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30b74aa-75d9-4c9e-a67d-71a97fe529b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CFG.TARGET_TRANSFORMATION:\n",
    "#     train_df['가격'] = np.log(train_df['가격'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c323f-9e86-4fe2-9e09-395d4e290050",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aab2e-3a95-4013-a15e-d141663182ea",
   "metadata": {},
   "source": [
    "## Resetting Columns Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbffe1aa-eef1-4263-933e-492cbbf6cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeResetting:\n",
    "    def __init__(self):\n",
    "        self.cat_features = ['브랜드','차량모델명','판매도시','판매구역','생산년도','모델출시년도']\n",
    "        self.seg_features = []\n",
    "        \n",
    "    def add_categorical_features(self,cat_features):\n",
    "        self.cat_features += cat_features\n",
    "        \n",
    "    def delete_categorical_features(self,cat_features):\n",
    "        self.cat_features = [col for col in self.cat_features if col not in cat_features]\n",
    "        \n",
    "    def add_segment_features(self,segment_features):\n",
    "        self.seg_features = ['segment']\n",
    "        self.cat_features = [col for col in self.cat_features if col not in segment_features]\n",
    "        \n",
    "    def fit(self,data):\n",
    "        if (len(self.seg_features)>0) & ('segment' not in data.columns):\n",
    "            raise ValueError(\"segment column name must be 'segment'\")\n",
    "        self.target_feature = ['가격']\n",
    "        self.unuse_features = ['ID']\n",
    "        self.dummy_features = ['압축천연가스(CNG)','액화석유가스(LPG)','경유','가솔린','하이브리드']\n",
    "        self.num_features   = [col for col in data.columns\n",
    "                               if col not in self.target_feature+self.unuse_features+self.dummy_features+self.cat_features+self.seg_features]\n",
    "        \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        for col in self.dummy_features:\n",
    "            if d[col].dtypes!=int:\n",
    "                d[col] = d[col].astype(int)\n",
    "        for col in self.cat_features:\n",
    "            if d[col].dtypes!=object:\n",
    "                d[col] = d[col].astype(str)\n",
    "        for col in self.num_features:\n",
    "            if d[col].dtypes!=float:\n",
    "                d[col] = d[col].astype(float)\n",
    "        for col in self.seg_features:\n",
    "            if d[col].dtypes!=object:\n",
    "                d[col] = d[col].astype(str)\n",
    "        for col in self.unuse_features:\n",
    "            if col in d.columns:\n",
    "                d.drop(col,axis=1,inplace=True)\n",
    "        return d\n",
    "    \n",
    "    def fit_transform(self,data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "    def get_feature_type(self):\n",
    "        globals()['target_feature'] = self.target_feature\n",
    "        globals()['unuse_features'] = self.unuse_features\n",
    "        globals()['dummy_features'] = self.dummy_features\n",
    "        globals()['cat_features']   = self.cat_features\n",
    "        globals()['num_features']   = self.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5177c8-be0f-4363-8ec8-8849f4531ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_resetor = TypeResetting()\n",
    "type_resetor.fit(train_df)\n",
    "type_resetor.get_feature_type()\n",
    "\n",
    "train_df2 = type_resetor.transform(train_df)\n",
    "test_df2  = type_resetor.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9166d7-d880-4117-a2da-9422f62428ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"always\")\n",
    "\n",
    "def check_only_oneside(train,test,cat_features):\n",
    "    not_test_only_features = []\n",
    "    for iter,col in enumerate(cat_features):\n",
    "        print('[{}/{}] {}'.format(iter+1,len(cat_features),col))\n",
    "        \n",
    "        only_train = list(set(train[col].unique())-set(test[col].unique()))\n",
    "        only_test  = list(set(test[col].unique())-set(train[col].unique()))\n",
    "        print(' - Only Train:',len(only_train))\n",
    "        print(' - Only Test :',len(only_test))\n",
    "        if len(only_test)>0:\n",
    "            print('******Warning******')\n",
    "        else:\n",
    "            not_test_only_features.append(col)\n",
    "        print('')\n",
    "    return not_test_only_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8c40f7-1201-4598-9558-d086e9ab1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브랜드, 차량모델명, 판매구역, 모델출시년도\n",
    "not_test_only_features = check_only_oneside(train_df2,test_df2,cat_features+dummy_features)\n",
    "not_test_only_features = list(set(not_test_only_features)-set(dummy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4fb877-5302-4a4d-8b79-7f5de66c6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_test_only_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204816d7-af14-427d-bc50-32b876febd64",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5254fac-62ad-4ca8-98d9-0cc687d11af2",
   "metadata": {},
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae5c16b-cf03-4b32-894f-10ef35d8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([str(round(int(year)/100,1)) for year in train_df6['생산년도']]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1faf8654-f04e-43ed-be0a-965a18e71664",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6541725-7cf0-4e7d-960b-cb96da805e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain, combinations\n",
    "def all_subsets(ss):\n",
    "    return list(chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1))))\n",
    "\n",
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_quantile(self,x,col):\n",
    "        x = np.array(x).flatten()\n",
    "        x = x[pd.notnull(x)]\n",
    "\n",
    "        agg_df = pd.DataFrame(index=[0])\n",
    "        for q in [0,25,50,75,100]:\n",
    "            agg_df[f'{col}_Q{q}'] = np.quantile(x,q/100)\n",
    "\n",
    "        return agg_df\n",
    "    \n",
    "    def _derived_features(self,data):\n",
    "        d = data.copy()\n",
    "\n",
    "        # (1) 모델출시년도에 생산된 차량인지\n",
    "        d['출시년도생산여부'] = np.where(d['생산년도'].astype(int)==d['모델출시년도'].astype(int),1,0)\n",
    "\n",
    "        # (2) 모델출시 이후에 몇년 지나서 생산됬는지\n",
    "        d['출시이후생산년수'] = d['생산년도'].astype(int)-d['모델출시년도'].astype(int)\n",
    "\n",
    "        # (3) 출시 이전에 생산되었는지\n",
    "        d['출시이전생산여부'] = np.where(d['출시이후생산년수']<0,1,0)\n",
    "\n",
    "        # (4) 브랜드의 국적 (구글링)\n",
    "        d['브랜드국적'] = ['체코' if brand=='skoda' else\n",
    "                        '일본' if brand in ['toyota','nissan','mazda','honda','mitsubishi'] else\n",
    "                        '독일' if brand in ['mercedes-benz','audi','volkswagen','bmw','opel'] else\n",
    "                        '이탈리아' if brand=='fiat' else\n",
    "                        '프랑스' if brand in ['renault','citroen','peugeot'] else\n",
    "                        '미국' if brand=='ford' else\n",
    "                        '한국' if brand in ['kia','hyundai'] else\n",
    "                        '스페인' if brand=='seat' else\n",
    "                        '스웨덴' if brand=='volvo' else\n",
    "                        np.nan for brand in d['브랜드']]\n",
    "\n",
    "        # (5) 브랜드 국적의 대륙명\n",
    "        d['브랜드대륙명'] = ['유럽' if country in ['체코','독일','이탈리아','프랑스','스페인','스웨덴'] else\n",
    "                          '아시아' if country in ['일본','한국'] else\n",
    "                          '아메리카' if country in ['미국'] else\n",
    "                          np.nan for country in d['브랜드국적']]\n",
    "        return d\n",
    "    \n",
    "    def fit(self,data,cat_features,subset_depth=1):\n",
    "        assert '가격' in data.columns, \\\n",
    "            'Input data must be training dataset'\n",
    "        assert len(cat_features)>=subset_depth, \\\n",
    "            'len(cat_features) >= subset_depth'\n",
    "        \n",
    "        self.cat_features = cat_features\n",
    "        self.new_cat_features = ['출시년도생산여부','출시이후생산년수','출시이전생산여부','브랜드국적','브랜드대륙명']\n",
    "        \n",
    "        # (6) 카테고리 변수에 따른 가격의 Quantile값\n",
    "        all_subset_list = all_subsets(cat_features)\n",
    "        all_subset_list = [subset for subset in all_subset_list if (len(subset)<=subset_depth) & (len(subset)>=1)]\n",
    "        \n",
    "        self.agg_dict = {}\n",
    "        for subset in tqdm(all_subset_list,desc=f'Get quantiles of target by categorical features (depth={subset_depth})'):\n",
    "            subset = list(subset)\n",
    "            subset_name = '_'.join(subset)\n",
    "            agg_fn = data.groupby(subset)['가격'].apply(lambda x: self._get_quantile(x,subset_name)).reset_index()\n",
    "            drop_cols = [col for col in agg_fn if col.find('level_')>=0]\n",
    "            agg_fn.drop(columns=drop_cols,inplace=True)\n",
    "            self.agg_dict[subset_name] = agg_fn\n",
    "            \n",
    "    def transform(self,data):\n",
    "        data = self._derived_features(data)\n",
    "        for key,agg_fn in self.agg_dict.items():\n",
    "            data = pd.merge(data,agg_fn,how='left',on=key.split('_'))\n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self,data,cat_features,subset_depth=1):\n",
    "        self.fit(data,cat_features,subset_depth)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a085347-abaa-4e5f-9f9b-f71f226bf84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineering()\n",
    "fe.fit(\n",
    "    data=train_df2,\n",
    "    cat_features=not_test_only_features, \n",
    "    subset_depth=CFG.SUBSET_DEPTH,\n",
    ")\n",
    "train_df3 = fe.transform(train_df2)\n",
    "test_df3  = fe.transform(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29021d0-ce57-42c8-8a78-0f9cc4209d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.new_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa09b13-62ca-425d-bab5-299828454fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_resetor = TypeResetting()\n",
    "type_resetor.add_categorical_features(fe.new_cat_features)\n",
    "type_resetor.fit(train_df3)\n",
    "type_resetor.get_feature_type()\n",
    "\n",
    "train_df3 = type_resetor.transform(train_df3)\n",
    "test_df3  = type_resetor.transform(test_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af8e5c65-95f9-4a32-b63d-c074537853de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df3.shape)\n",
    "train_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65939305-7703-474a-912d-3841f3eed25d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f1466-3f68-43ec-a104-181930321dce",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1e3686-4ea5-4147-a968-4dc93785c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_num_features = [col for col in num_features if col.find('_Q')<0]\n",
    "\n",
    "# i=0\n",
    "# for col in check_num_features:\n",
    "#     i+=1\n",
    "#     print('\\n({}/{}) {}'.format(i,len(check_num_features),col))\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=train_df3['가격'],y=train_df3[col])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920fd41-52ab-4de1-82a4-d9a5b28a4346",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6171ab-e115-43a0-906e-ea644ee74b86",
   "metadata": {},
   "source": [
    "# Add the Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c35453f-0aae-4d1c-b034-f47a24ec1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import trange\n",
    "\n",
    "def get_abs_corr(x,y):\n",
    "    return np.abs(np.corrcoef(x,y))[0,1]\n",
    "\n",
    "class InteractionTerm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,num_features,corr_cutoff=0.7):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        \n",
    "        d = data.copy()\n",
    "        self.interaction_list = []\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    col_i = num_features[i]\n",
    "                    col_j = num_features[j]\n",
    "                    \n",
    "                    # 상관계수가 cutoff보다 큰 경우에는 interaction을 생성하지 않음\n",
    "                    if (get_abs_corr(d[col_i]*d[col_j],d[col_i])>=corr_cutoff) | (get_abs_corr(d[col_i]*d[col_j],d[col_j])>=corr_cutoff):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.interaction_list.append(f'{col_i}*{col_j}')\n",
    "    \n",
    "    def transform(self,data):\n",
    "        d = data.copy()\n",
    "        for interaction in self.interaction_list:\n",
    "            col_i,col_j = interaction.split('*')\n",
    "            d[interaction] = d[col_i]*d[col_j]\n",
    "        return d\n",
    "    \n",
    "    def fit_transform(self,data,num_features,corr_cutoff=0.7):\n",
    "        self.fit(data,num_features,corr_cutoff)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e973e0f2-e229-493c-9ad0-a42c086a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4 = train_df3.copy()\n",
    "test_df4  = test_df3.copy()\n",
    "\n",
    "if CFG.INTERACTION:\n",
    "    interaction_maker = InteractionTerm()\n",
    "    interaction_maker.fit(\n",
    "        data=train_df3,\n",
    "        num_features=num_features,\n",
    "        corr_cutoff=0.7,\n",
    "    )\n",
    "    train_df4 = interaction_maker.transform(train_df4)\n",
    "    test_df4  = interaction_maker.transform(test_df4)\n",
    "\n",
    "    type_resetor = TypeResetting()\n",
    "    type_resetor.add_categorical_features(fe.new_cat_features)\n",
    "    type_resetor.fit(train_df4)\n",
    "    type_resetor.get_feature_type()\n",
    "\n",
    "    train_df4 = type_resetor.transform(train_df4)\n",
    "    test_df4  = type_resetor.transform(test_df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3732aa9-e9d7-41d5-8e8c-667699fe5a30",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec950f8-0d43-4b9a-97ed-6feac553dba3",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880a07b7-5116-410a-a6d0-0d2c50b151b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=0\n",
    "# for i in range(len(num_features)):\n",
    "#     for j in range(len(num_features)):\n",
    "#         if i>j:\n",
    "#             col_i = num_features[i]\n",
    "#             col_j = num_features[j]\n",
    "#             corr = np.corrcoef(train_df4[col_i],train_df4[col_j])[0,1]\n",
    "#             if corr>=0.7:\n",
    "#                 k+=1\n",
    "#                 print(k,col_i,col_j,corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a38e1e0d-3c13-4c6e-81a1-9e18d0f05e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_offset(x):\n",
    "    if min(x)>0:\n",
    "        offset = 0\n",
    "    elif min(x)==0:\n",
    "        offset = 1e-3\n",
    "    else:\n",
    "        offset = min(x)+1e-3\n",
    "        print('minimum = {:.3f}'.format(min(x)))\n",
    "    return np.log(x+offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40318716-a92d-459a-9ff7-9c1d262a107d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8304c3-426d-4a24-89cc-d3c76a5ea249",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6e1cc67-3282-47c3-ada7-8c7776443b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81d460ab-4868-423a-8f9c-08e0b456ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cat_features = [col for col in cat_features if train_df4[col].nunique()<=100]\n",
    "\n",
    "# (1) ANOVA를 해서 p-value가 0.05보다 높은 것들 확인\n",
    "pvalue_list = []\n",
    "for col in tqdm(check_cat_features):\n",
    "    d = train_df4[[col,'가격']].rename(columns={col:'feature'})\n",
    "    \n",
    "    model = ols(f'가격 ~ C(feature)',data=d).fit()\n",
    "    pvalue = anova_lm(model).values[0][-1]\n",
    "    pvalue_list.append([col,pvalue])\n",
    "    \n",
    "pvalue_df = pd.DataFrame(pvalue_list,columns=['feature','pvalue'])\\\n",
    "    .sort_values('pvalue',ascending=False)\n",
    "# pvalue_df[pvalue_df.pvalue>=alpha].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90277925-192b-4514-beb0-675a1dddd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) (1)에서 유의하지않은 feature들은 log적용 후에도 유의하지 않으면 제외\n",
    "pvalue_list2 = []\n",
    "unsignificant_features = pvalue_df[pvalue_df.pvalue>CFG.FS_ALPHA].feature.tolist()\n",
    "for col in tqdm(unsignificant_features):\n",
    "    d = train_df4[[col,'target']].rename(columns={col:'feature'})\n",
    "    d['feature'] = log_offset(d['feature'])\n",
    "    \n",
    "    model = ols(f'feature ~ C(target)',data=d).fit()\n",
    "    pvalue = anova_lm(model).values[0][-1]\n",
    "    pvalue_list2.append([col,pvalue])\n",
    "    \n",
    "pvalue_df2 = pd.DataFrame(pvalue_list2,columns=['feature','pvalue'])\\\n",
    "    .sort_values('pvalue',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a33f82a-221e-4c73-a7e1-6edb0aec18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_features = pvalue_df2[pvalue_df2.pvalue> CFG.FS_ALPHA].feature.tolist()\n",
    "log_features    = pvalue_df2[pvalue_df2.pvalue<=CFG.FS_ALPHA].feature.tolist()\n",
    "print('> delete_features')\n",
    "print('  - length : {}'.format(len(delete_features)))\n",
    "print('  - feature_name : {}'.format(delete_features))\n",
    "print('')\n",
    "print('> log_features')\n",
    "print('  - length : {}'.format(len(log_features)))\n",
    "print('  - feature_name : {}'.format(log_features))\n",
    "\n",
    "train_df5 = train_df4.copy()\n",
    "train_df5.drop(delete_features,axis=1,inplace=True)\n",
    "for col in log_features:\n",
    "    train_df5[col] = log_offset(train_df5[col])\n",
    "    \n",
    "test_df5 = test_df4.copy()\n",
    "test_df5.drop(delete_features,axis=1,inplace=True)\n",
    "for col in log_features:\n",
    "    test_df5[col] = log_offset(test_df5[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176ea54-23ec-469f-b732-88b6afcac30e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bbc25-a915-4b50-9fb3-d6689c37a216",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbb2b20b-bb8e-4081-bdc8-0c121fadbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c13b45c5-3c59-404e-bb14-b023dad4705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) corr test를 해서 p-value가 0.05보다 높은 것들 확인\n",
    "pvalue_list = []\n",
    "for col in num_features:\n",
    "    corr,pvalue = scipy.stats.pearsonr(train_df5['가격'],train_df5[col])\n",
    "    pvalue_list.append([col,pvalue])\n",
    "pvalue_df = pd.DataFrame(pvalue_list,columns=['feature','pvalue'])\\\n",
    "    .sort_values('pvalue',ascending=False)\n",
    "# pvalue_df.round(4).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d178a263-70c2-4a52-9974-fa45e9dd5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) (1)에서 유의하지않은 feature들은 log적용 후에도 유의하지 않으면 제외\n",
    "pvalue_list2 = []\n",
    "unsignificant_features = pvalue_df[pvalue_df.pvalue>CFG.FS_ALPHA].feature.tolist()\n",
    "for col in tqdm(unsignificant_features):\n",
    "    corr,pvalue = scipy.stats.pearsonr(train_df5['가격'],log_offset(train_df5[col]))\n",
    "    pvalue_list2.append([col,pvalue])\n",
    "pvalue_df2 = pd.DataFrame(pvalue_list2,columns=['feature','pvalue'])\\\n",
    "    .sort_values('pvalue',ascending=False)\n",
    "# pvalue_df2.round(4).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc6bc35c-24f3-4e04-89ae-2a0c6db4deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_features = pvalue_df2[pvalue_df2.pvalue> CFG.FS_ALPHA].feature.tolist()\n",
    "log_features    = pvalue_df2[pvalue_df2.pvalue<=CFG.FS_ALPHA].feature.tolist()\n",
    "print('> delete_features')\n",
    "print('  - length : {}'.format(len(delete_features)))\n",
    "print('  - feature_name : {}'.format(delete_features))\n",
    "print('')\n",
    "print('> log_features')\n",
    "print('  - length : {}'.format(len(log_features)))\n",
    "print('  - feature_name : {}'.format(log_features))\n",
    "\n",
    "train_df6 = train_df5.copy()\n",
    "train_df6.drop(delete_features,axis=1,inplace=True)\n",
    "for col in log_features:\n",
    "    train_df6[col] = log_offset(train_df6[col])\n",
    "    \n",
    "test_df6 = test_df5.copy()\n",
    "test_df6.drop(delete_features,axis=1,inplace=True)\n",
    "for col in log_features:\n",
    "    test_df6[col] = log_offset(test_df6[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79601117-685e-475d-84b6-0706f876e338",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b9354-bcc6-448c-969a-e13259c63768",
   "metadata": {},
   "source": [
    "# Make Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d59204-cd48-463d-9d66-f6cdeb958d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment(data,segment: list):\n",
    "    d = data.copy()\n",
    "    d['segment'] = d[segment].apply(lambda x: '___'.join(x),axis=1)\n",
    "    d.drop(columns=segment,inplace=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af98b30f-f615-4098-ad04-dc19a06e19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = ['브랜드']\n",
    "train_df7 = make_segment(train_df6,segment)\n",
    "test_df7  = make_segment(test_df6 ,segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5335ebd-d436-4ef0-aa8a-9652f0b42edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only = list(set(test_df7.segment.unique())-set(train_df7.segment.unique()))\n",
    "assert len(test_only)==0, \\\n",
    "    \"Segment exists only in the test set ({})\".format(len(test_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22f864e5-c30e-41e1-b3bf-45e6a561dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only = list(set(train_df7['segment'].unique())-set(test_df7['segment'].unique()))\n",
    "\n",
    "n_asis = len(train_df7)\n",
    "n_tobe = len(train_df7[~train_df7.segment.isin(train_only)])\n",
    "train_df7 = train_df7[~train_df7.segment.isin(train_only)]\n",
    "print('> Train에만 존재하는 Segment 제거')\n",
    "print(' - 데이터수 : {:,} -> {:,}'.format(n_asis,n_tobe))\n",
    "print(' - 세그먼트수 : {:,}'.format(train_df7['segment'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4579bed8-088d-46c0-b69a-72cad7028995",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = train_df7['segment'].value_counts().sort_values()\n",
    "display(vc.head())\n",
    "print('...')\n",
    "display(vc.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caffe72a-8ac5-4cb9-81c6-8a778faa2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_resetor = TypeResetting()\n",
    "type_resetor.add_categorical_features(fe.new_cat_features)\n",
    "type_resetor.add_segment_features(segment)\n",
    "type_resetor.fit(train_df7)\n",
    "type_resetor.get_feature_type()\n",
    "\n",
    "train_df7 = type_resetor.transform(train_df7)\n",
    "test_df7  = type_resetor.transform(test_df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b6938ef-7438-4e29-9b63-f8b45ae27180",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea57f4f5-bdd5-41f9-a2ea-832514b6bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df7.shape)\n",
    "train_df7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc32ace-0b1a-4235-b3aa-b834096d6c02",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdebcae-a0db-4e33-8f6a-8d655989d04f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "330d67f2-4951-4092-aefd-1be42b8577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def mkdir(paths):\n",
    "    if type(paths)==str:\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            print('> Create Folder: {}'.format(path))\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4abf1609-fe3f-405b-87e6-76c296a00a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## dummy_features는 한가지만 속함\n",
    "# X[dummy_features].apply(lambda x: np.sum(x),axis=1).value_counts()\n",
    "\n",
    "def add_fuel_type(data,dummy_features):\n",
    "    d = data.copy()\n",
    "    d['fuel_type'] = d[dummy_features].apply(\n",
    "        lambda x: dummy_features[np.where(x==1)[0][0]],axis=1)\n",
    "    d.drop(columns=dummy_features,inplace=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01609cc3-d341-4d8c-9295-45934f1e4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir('./model_checkpoints')\n",
    "mkdir('./model_checkpoints/segment_catboost')\n",
    "mkdir('./model_checkpoints/segment_weightedensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4369ef-ddbf-42f5-8d3b-008573b4b6b9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f1b72-9651-42d2-8410-52fd85595f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CatBoost\n",
    "- public score : 6.0357651823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da2683aa-dd52-4402-a77b-7560b27b9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6bfc-9f23-4ad2-bca5-16a3d57c42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eceedea-c652-4a96-b165-e222b01b6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = train_df7.copy()\n",
    "test_fn  = test_df7 .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0364030-ccdf-40df-a8cc-bf6fad1107f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 30분\n",
    "\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "X = add_fuel_type(X,dummy_features)\n",
    "new_cat_features = cat_features + ['fuel_type']\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "models = {}\n",
    "feature_info = {}\n",
    "scores = []\n",
    "pbar = tqdm(segment_list)\n",
    "\n",
    "s_i = 0\n",
    "for segment in pbar:\n",
    "    s_i+=1\n",
    "    \n",
    "    # segment에 해당하는 데이터추출\n",
    "    _X = X[X.segment==segment].drop('segment',axis=1)\n",
    "    _y = y[X.segment==segment]\n",
    "    \n",
    "    # kfold\n",
    "    kf = KFold(n_splits=CFG.N_SPLITS,random_state=1000*s_i+CFG.SEED,shuffle=True)\n",
    "    \n",
    "    # unique인 컬럼 제외\n",
    "    unique_info = _X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X = _X.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # categorical feature에서 unique인 컬럼을 제외\n",
    "    fixed_cat_features = [col for col in new_cat_features if col in _X.columns]\n",
    "    \n",
    "    _models = []\n",
    "    _scores = []\n",
    "    k=0\n",
    "    for tr_idx,val_idx in kf.split(_X,_y):\n",
    "        k+=1\n",
    "        \n",
    "        # kfold dataset\n",
    "        X_tr, X_va = _X.iloc[tr_idx], _X.iloc[val_idx]\n",
    "        y_tr, y_va = _y.iloc[tr_idx], _y.iloc[val_idx]\n",
    "\n",
    "        # progress\n",
    "        progress = 'Segment: {}, Length: Train({}), Validation({}), KFold: {}/{}'\\\n",
    "            .format(segment,len(X_tr),len(X_va),k,CFG.N_SPLITS)\n",
    "        pbar.set_description(progress)\n",
    "\n",
    "        # dataset\n",
    "        train_dataset = Pool(X_tr,y_tr,cat_features=fixed_cat_features)\n",
    "        valid_dataset = Pool(X_va,y_va,cat_features=fixed_cat_features)\n",
    "\n",
    "        # define the model\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function='MAE',\n",
    "            random_state=CFG.SEED,\n",
    "            iterations=CFG.EPOCHS,\n",
    "            learning_rate=CFG.LR,\n",
    "            allow_writing_files=False,\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            eval_set=valid_dataset,\n",
    "            early_stopping_rounds=CFG.ES,\n",
    "            verbose=0,\n",
    "            #metric_period=CFG.EPOCHS//5,\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        model.save_model(f'./model_checkpoints/segment_catboost/{segment}_k{k}.cbm')\n",
    "\n",
    "        # calculate the score\n",
    "        y_pred = model.predict(valid_dataset).flatten()\n",
    "        y_true = y_va.values\n",
    "        score = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "        \n",
    "        # append inner loop\n",
    "        _models.append(model)\n",
    "        _scores.append([segment,k,len(X_tr),len(X_va),score])\n",
    "\n",
    "    # append outer loop\n",
    "    models[segment] = _models\n",
    "    scores.append(_scores)\n",
    "    feature_info[segment] = {'cat_features':fixed_cat_features,'features':_X.columns.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d329a-8a2a-4b11-a184-dd86a24de2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model_checkpoints/segment_cat_models_brand_kf.pkl', 'wb') as f:\n",
    "\tpickle.dump(models, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./model_checkpoints/segment_cat_feature_info_brand_kf.pkl', 'wb') as f:\n",
    "\tpickle.dump(feature_info, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./model_checkpoints/segment_cat_scores_brand_kf.pkl', 'wb') as f:\n",
    "\tpickle.dump(scores, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33753abc-bf26-4ee8-b7a6-847686119b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(\n",
    "#     np.array(scores).reshape(100,5),\n",
    "#     columns=['segment','k','n_tr','n_val','score']\n",
    "# ).sort_values(['segment','k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89dbbe-b7cc-4107-b3bc-197c368d49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "X = add_fuel_type(X,dummy_features)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "X_test = add_fuel_type(test_fn,dummy_features)\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "tr_pred_list = []\n",
    "te_pred_list = []\n",
    "for segment in tqdm(segment_list):\n",
    "    ## data load\n",
    "    # (1) train\n",
    "    train_data = X[X.segment==segment][feature_info[segment]['features']]\n",
    "    train_dataset = Pool(train_data,cat_features=feature_info[segment]['cat_features'])\n",
    "    # (2) test\n",
    "    test_data = X_test[X_test.segment==segment][feature_info[segment]['features']]\n",
    "    test_dataset = Pool(test_data,cat_features=feature_info[segment]['cat_features'])\n",
    "    \n",
    "    ## model\n",
    "    kfold_models = models[segment]\n",
    "    \n",
    "    ## prediction\n",
    "    # (1) train\n",
    "    tr_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'true':y[X.segment==segment].values.flatten(),\n",
    "        'pred':np.mean([model.predict(train_dataset) for model in kfold_models],axis=0),\n",
    "    })\n",
    "    tr_pred_df.index = train_data.index\n",
    "    # (2) test\n",
    "    te_pred_df = pd.DataFrame({\n",
    "        'segment':segment,\n",
    "        'pred':np.mean([model.predict(test_dataset) for model in kfold_models],axis=0),\n",
    "    })\n",
    "    te_pred_df.index = test_data.index\n",
    "    \n",
    "    ## append\n",
    "    tr_pred_list.append(tr_pred_df)\n",
    "    te_pred_list.append(te_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad43d66-1e96-437e-8d88-43c6549e1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_pred_df = pd.concat(tr_pred_list,axis=0).sort_index()\n",
    "mean_absolute_error(y_true=tr_pred_df.true,y_pred=tr_pred_df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a46da9-6aa5-4fd2-95a2-339aa93a4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred_df = pd.concat(te_pred_list,axis=0).sort_index()\n",
    "te_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d926b-6db7-4b8d-9e16-1ca1c0331872",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['가격'] = te_pred_df.pred.values\n",
    "submit.to_csv('./out/7_catboost_segment_브랜드_kfold.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7a7e9-1c4b-47c3-8959-0412cfc188f6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34001f77-6935-4c16-8d86-2ee1e7ab27ef",
   "metadata": {},
   "source": [
    "## Weighted Ensemble\n",
    "- public score : 6.2489713335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e51d603f-56f8-4006-a62b-386cc14d51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,columns):\n",
    "        self.transform_list = []\n",
    "        for col in columns:\n",
    "            for i,value in enumerate(sorted(data[col].unique())):\n",
    "                if i>0:\n",
    "                    self.transform_list.append([col,value])\n",
    "        \n",
    "    def transform(self,data):\n",
    "        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "        new_data = data.copy()\n",
    "        for col,value in self.transform_list:\n",
    "            new_data[f'{col}_{value}'] = np.where(new_data[col]==value,1,0)\n",
    "        drop_columns = pd.unique(np.array(self.transform_list)[:,0])\n",
    "        new_data.drop(columns=drop_columns,inplace=True)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c20d5c0-a506-418d-838c-65c44c753dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,weight=['equal','balanced']):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert weight in ['equal','balanced'], \\\n",
    "            \"weight must be one of ['equal','balanced']\"\n",
    "        self.weight = weight\n",
    "        self._get_regressors()\n",
    "    \n",
    "    def _get_regressors(self):\n",
    "        max_depth = 10\n",
    "        n_jobs = -1\n",
    "        \n",
    "        params_catboost = {\n",
    "            'random_state':CFG.SEED,\n",
    "            'early_stopping_rounds' : CFG.ES,\n",
    "            'learning_rate' : CFG.LR,\n",
    "            'iterations' : CFG.EPOCHS,\n",
    "            'loss_function': 'MAE',\n",
    "            'grow_policy' : 'Lossguide', # 'SymmetricTree','Depthwise'\n",
    "            'use_best_model' : True,\n",
    "            'allow_writing_files' : False,\n",
    "            'verbose' : 0,\n",
    "            'max_depth': max_depth,\n",
    "            'l2_leaf_reg' : 1,\n",
    "        }\n",
    "    \n",
    "        params_xgboost = {\n",
    "            'random_state':CFG.SEED,\n",
    "            'early_stopping_rounds' : CFG.XGB_ES,\n",
    "            'learning_rate' : CFG.XGB_LR,\n",
    "            'n_estimators' : CFG.XGB_EPOCHS,\n",
    "            'objective': 'reg:absoluteerror',\n",
    "            'verbosity' : 0,\n",
    "            'max_depth': max_depth,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "    \n",
    "        params_lgb = {\n",
    "            'objective': 'regression',\n",
    "            'random_state':CFG.SEED,\n",
    "            'early_stopping_round' : CFG.ES,\n",
    "            'learning_rate' : CFG.LR,\n",
    "            'n_estimators' : CFG.EPOCHS,\n",
    "            'metric': 'mean_absolute_error',\n",
    "            'verbosity' : -1,\n",
    "            'max_depth': max_depth,\n",
    "            'n_jobs' : n_jobs,\n",
    "        }\n",
    "        \n",
    "        self.regressors = [\n",
    "            CatBoostRegressor(**params_catboost),\n",
    "            XGBRegressor(**params_xgboost),\n",
    "            LGBMRegressor(**params_lgb),\n",
    "        ]\n",
    "        self.regressors_name = ['CatBoost','XGBoost','LightGBM']\n",
    "    \n",
    "    def fit(self,X,y,eval_set,oh_set,cat_features,verbose=1):\n",
    "        assert len(eval_set)==1, \\\n",
    "            \"eval_set length must be 1. len(eval_set)={}\".format(len(eval_set))\n",
    "        assert len(oh_set)==1, \\\n",
    "            \"oh_set length must be 1. len(oh_set)={}\".format(len(oh_set))\n",
    "        X_val, y_val = eval_set[0]\n",
    "        X_oh, X_val_oh = oh_set[0]\n",
    "        \n",
    "        self.cat_features = cat_features\n",
    "        self.weights = []\n",
    "        self.fitting_elapsed = []\n",
    "        if verbose:\n",
    "            pbar = tqdm(zip(self.regressors_name,self.regressors),total=len(self.regressors))\n",
    "        else:\n",
    "            pbar = zip(self.regressors_name,self.regressors)\n",
    "        for regressor_name,regressor in pbar:\n",
    "            s = time.time()\n",
    "            if verbose:\n",
    "                pbar.set_description(name)\n",
    "            if regressor_name=='CatBoost':\n",
    "                train_dataset = Pool(X,y,cat_features=cat_features)\n",
    "                val_dataset   = Pool(X_val,y_val,cat_features=cat_features)\n",
    "                regressor.fit(\n",
    "                    train_dataset,\n",
    "                    eval_set=val_dataset,\n",
    "                    #metric_period=CFG.EPOCHS//5,\n",
    "                )\n",
    "                val_pred = regressor.predict(val_dataset)\n",
    "            elif regressor_name=='XGBoost':\n",
    "                regressor.fit(\n",
    "                    X_oh,y,\n",
    "                    eval_set=[(X_val_oh,y_val)],\n",
    "                    verbose=0,\n",
    "                )\n",
    "                val_pred = regressor.predict(X_val_oh)\n",
    "            elif regressor_name=='LightGBM':\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                X_tmp = X.copy()\n",
    "                X_val_tmp = X_val.copy()\n",
    "                for col in cat_features:\n",
    "                    X_tmp[col]     = X_tmp[col]    .astype('category')\n",
    "                    X_val_tmp[col] = X_val_tmp[col].astype('category')\n",
    "                regressor.fit(\n",
    "                    X_tmp,y,\n",
    "                    eval_set=[(X_val_tmp,y_val)],\n",
    "                    verbose=-1,\n",
    "                )\n",
    "                val_pred = regressor.predict(X_val_tmp)\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "                \n",
    "            score = mean_absolute_error(y_pred=val_pred,y_true=y_val)\n",
    "            e = time.time()\n",
    "            \n",
    "            self.weights.append(1/score)\n",
    "            self.fitting_elapsed.append(e-s)\n",
    "        \n",
    "        if self.weight=='equal':\n",
    "            self.weights = np.array([1.0 for _ in self.regressors])\n",
    "        self.weights /= sum(self.weights)\n",
    "                \n",
    "    def predict(self,X,X_oh):\n",
    "        assert len(X)==len(X_oh), \\\n",
    "            \"X and X_oh must be same length\"\n",
    "        \n",
    "        pred_list = []\n",
    "        for regressor_name,regressor in zip(self.regressors_name,self.regressors):\n",
    "            if regressor_name=='CatBoost':\n",
    "                dataset = Pool(X,cat_features=self.cat_features)\n",
    "            elif regressor_name=='XGBoost':\n",
    "                dataset = X_oh.copy()\n",
    "            elif regressor_name=='LightGBM':\n",
    "                dataset = X.copy()\n",
    "                for col in self.cat_features:\n",
    "                    dataset[col] = dataset[col].astype('category')\n",
    "            else:\n",
    "                raise ValueError('Unknown Regressor: {}'.format(regressor_name))\n",
    "            \n",
    "            y_pred = regressor.predict(dataset)\n",
    "            y_pred = np.array(y_pred).flatten()\n",
    "            pred_list.append(y_pred)\n",
    "            \n",
    "        final_pred = np.zeros(len(X))\n",
    "        for pred,weight in zip(pred_list,self.weights):\n",
    "            final_pred += np.array(pred)*weight\n",
    "            \n",
    "        return final_pred\n",
    "    \n",
    "    def save_model(self,path):\n",
    "        save_dict = {\n",
    "            'cat_features' : self.cat_features,\n",
    "            'weights' : self.weights,\n",
    "            'fitting_elapsed' : self.fitting_elapsed,\n",
    "            'regressors' : self.regressors,\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_model(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "            self.cat_features = save_dict['cat_features']\n",
    "            self.weights = save_dict['weights']\n",
    "            self.fitting_elapsed = save_dict['fitting_elapsed']\n",
    "            self.regressors = save_dict['regressors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e23e22bc-b630-430c-a9f9-6f539e219c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8ac1666-19fe-4f0c-b651-8d6267292d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f003ed17-9aae-4651-9020-35ae377b036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = train_df7.copy()\n",
    "test_fn  = test_df7 .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ccb213e-a076-45ba-b12d-94449e44d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 6시간\n",
    "\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X,cat_features)\n",
    "X_oh = ohe.transform(X)\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "models = {}\n",
    "feature_info = {}\n",
    "scores = []\n",
    "pbar = tqdm(segment_list)\n",
    "\n",
    "s_i = 0\n",
    "for segment in pbar:\n",
    "    s_i+=1\n",
    "    \n",
    "    # segment에 해당하는 데이터추출\n",
    "    _X    = X   [X   .segment==segment].drop('segment',axis=1)\n",
    "    _X_oh = X_oh[X_oh.segment==segment].drop('segment',axis=1)\n",
    "    _y    = y   [X   .segment==segment]\n",
    "    \n",
    "    # kfold\n",
    "    kf = KFold(n_splits=CFG.N_SPLITS,random_state=1000*s_i+CFG.SEED,shuffle=True)\n",
    "    \n",
    "    # unique인 컬럼 제외\n",
    "    # (1) X\n",
    "    unique_info = _X.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X = _X.drop(unique_cols,axis=1)\n",
    "    # (2) X_oh\n",
    "    unique_info = _X_oh.nunique()\n",
    "    unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "    if len(unique_cols)>0:\n",
    "        _X_oh = _X_oh.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # categorical feature에서 unique인 컬럼을 제외\n",
    "    fixed_cat_features = [col for col in cat_features if col in _X.columns]\n",
    "    \n",
    "    _models = []\n",
    "    _scores = []\n",
    "    k=0\n",
    "    for tr_idx,val_idx in kf.split(_X,_y):\n",
    "        k+=1\n",
    "        \n",
    "        # kfold dataset\n",
    "        X_tr   , X_va    = _X   .iloc[tr_idx], _X   .iloc[val_idx]\n",
    "        X_tr_oh, X_va_oh = _X_oh.iloc[tr_idx], _X_oh.iloc[val_idx]\n",
    "        y_tr   , y_va    = _y   .iloc[tr_idx], _y   .iloc[val_idx]\n",
    "\n",
    "        # progress\n",
    "        progress = 'Segment: {}, Length: Train({}), Validation({}), KFold: {}/{}'\\\n",
    "            .format(segment,len(X_tr),len(X_va),k,CFG.N_SPLITS)\n",
    "        pbar.set_description(progress)\n",
    "\n",
    "        # define the model\n",
    "        ensemble_model = WeightedEnsembleRegressor(weight='equal')\n",
    "\n",
    "        # fit the model\n",
    "        ensemble_model.fit(\n",
    "            X_tr,y_tr,\n",
    "            eval_set=[(X_va,y_va)],\n",
    "            oh_set=[(X_tr_oh,X_va_oh)],\n",
    "            cat_features=fixed_cat_features,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        ensemble_model.save_model(f'./model_checkpoints/segment_weightedensemble/{segment}_k{k}.pickle')\n",
    "\n",
    "        # calculate the score\n",
    "        y_pred = ensemble_model.predict(X_va,X_va_oh).flatten()\n",
    "        y_true = y_va.values\n",
    "        score = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "        \n",
    "        # append inner loop\n",
    "        _models.append(ensemble_model)\n",
    "        _scores.append([segment,k,len(X_tr),len(X_va),score])\n",
    "\n",
    "    # append outer loop\n",
    "    models[segment] = _models\n",
    "    scores.append(_scores)\n",
    "    feature_info[segment] = {\n",
    "        'cat_features':fixed_cat_features,\n",
    "        'features':_X.columns.tolist(),\n",
    "        'oh_features':_X_oh.columns.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92db8ed9-ced3-432d-aeb2-ba20f3e30150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model_checkpoints/segment_weiens_models_brand.pkl', 'wb') as f:\n",
    "\tpickle.dump(models, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./model_checkpoints/segment_weiens_feature_info_brand.pkl', 'wb') as f:\n",
    "\tpickle.dump(feature_info, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./model_checkpoints/segment_weiens_scores_brand.pkl', 'wb') as f:\n",
    "\tpickle.dump(scores, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74057406-7c55-4ee1-90e0-bde923905b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('./model_checkpoints/segment_weiens_models_brand.pkl', 'rb') as f:\n",
    "# \tmodels = pickle.load(f)\n",
    "# with open('./model_checkpoints/segment_weiens_feature_info_brand.pkl', 'rb') as f:\n",
    "# \tfeature_info = pickle.load(f)\n",
    "# with open('./model_checkpoints/segment_weiens_scores_brand.pkl', 'rb') as f:\n",
    "# \tscores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e754b5f4-1a4e-483a-aa41-019289df14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(\n",
    "#     np.array(scores).reshape(100,5),\n",
    "#     columns=['segment','k','n_tr','n_val','score']\n",
    "# ).sort_values(['segment','k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff4feb2f-3fe3-4b15-a2e9-a69e7c283c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "X = train_fn.drop(target_feature,axis=1)\n",
    "y = train_fn[target_feature]\n",
    "\n",
    "X_test = test_fn.copy()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X,cat_features)\n",
    "X_oh = ohe.transform(X)\n",
    "X_test_oh = ohe.transform(X_test)\n",
    "\n",
    "segment_list = X['segment'].unique()\n",
    "\n",
    "tr_pred_list = []\n",
    "te_pred_list = []\n",
    "for segment in tqdm(segment_list):\n",
    "    ## data load\n",
    "    # (1) train\n",
    "    train_data    = X   [X   .segment==segment][feature_info[segment]['features']]\n",
    "    train_data_oh = X_oh[X_oh.segment==segment][feature_info[segment]['oh_features']]\n",
    "    # (2) test\n",
    "    test_data     = X_test   [X_test   .segment==segment][feature_info[segment]['features']]\n",
    "    test_data_oh  = X_test_oh[X_test_oh.segment==segment][feature_info[segment]['oh_features']]\n",
    "    \n",
    "    ## model\n",
    "    kfold_models = models[segment]\n",
    "    \n",
    "    ## prediction\n",
    "    # (1) train\n",
    "    tr_pred_df = pd.DataFrame({\n",
    "        'true':y[X.segment==segment].values.flatten(),\n",
    "        'pred':np.mean([model.predict(train_data,train_data_oh) for model in kfold_models],axis=0),\n",
    "    })\n",
    "    tr_pred_df.index = train_data.index\n",
    "    # (2) test\n",
    "    te_pred_df = pd.DataFrame({\n",
    "        'pred':np.mean([model.predict(test_data,test_data_oh) for model in kfold_models],axis=0),\n",
    "    })\n",
    "    te_pred_df.index = test_data.index\n",
    "    \n",
    "    ## append\n",
    "    tr_pred_list.append(tr_pred_df)\n",
    "    te_pred_list.append(te_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c068d2b4-114a-4556-aebb-49ce81be2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_pred_df = pd.concat(tr_pred_list,axis=0).sort_index()\n",
    "mean_absolute_error(y_true=tr_pred_df.true,y_pred=tr_pred_df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "453d4242-793f-4c8c-a1da-2f6934a98df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abline(intercept,slope,**kwargs):\n",
    "#     axes = plt.gca()\n",
    "#     x_vals = np.array(axes.get_xlim())\n",
    "#     y_vals = intercept + slope * x_vals\n",
    "#     plt.plot(x_vals, y_vals, '--',**kwargs)\n",
    "\n",
    "# offset = 0.05\n",
    "# min_value = min(tr_pred_df.true.min(),tr_pred_df.pred.min())*(1-offset)\n",
    "# max_value = min(tr_pred_df.true.max(),tr_pred_df.pred.max())*(1+offset)\n",
    "\n",
    "# plt.figure(figsize=(15,7))\n",
    "# sns.scatterplot(x=tr_pred_df.true,y=tr_pred_df.pred)\n",
    "# plt.xlim(min_value,max_value)\n",
    "# plt.ylim(min_value,max_value)\n",
    "# abline(0,1,color='red',linestyle='--')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d43b9ad6-0c4f-4593-94e3-70d810909cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred_df = pd.concat(te_pred_list,axis=0).sort_index()\n",
    "te_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52e76369-d936-4816-aed7-c7dcdb3bdde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['가격'] = te_pred_df.pred.values\n",
    "submit.to_csv('./out/8_ensemble_segment_브랜드_kfold.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92abc4-eb0a-4d71-861f-200a40356c20",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6f0e6-6fa4-458d-838e-32a5db3df13d",
   "metadata": {},
   "source": [
    "## 참조 pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e7ce5-6c68-4d17-b54b-c33c821ced59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycaret import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da0375-b3dc-45d9-af61-33e733136308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data = train_fn[train_fn.segment==segment_list[0]]\n",
    "# print(len(data))\n",
    "\n",
    "# regression.setup(data=data,target='가격',remove_outliers=True,verbose=True)\n",
    "# best = regression.compare_models(n_select=5,fold=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
