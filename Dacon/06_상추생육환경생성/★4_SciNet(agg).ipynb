{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4041a5-44ba-4987-a6e7-3c3f40f10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 torch 다운로드\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e5953c-7a86-4c51-a887-423f7beb6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fp = fm.FontProperties(fname='/home/studio-lab-user/Dacon/tools/NanumFont/NanumGothic.ttf', size=10)\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "# # cuda (not Mac)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # mps (Mac)\n",
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47cef-a496-4dc7-9a8b-0f363f65cdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':128,#1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':0.05,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, input_paths, label_paths, test_input_paths, test_label_paths):\n",
    "        \n",
    "        self.input, self.label, self.test_input, self.test_label = None, None, None, None\n",
    "        \n",
    "        self.X_train, self.X_valid = None, None\n",
    "        self.y_train, self.y_valid = None, None\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, label_path in zip(input_paths, label_paths):\n",
    "            case_num = input_path.replace('./data/train_input/CASE_','').replace('.csv','')\n",
    "            \n",
    "            input_df = pd.read_csv(input_path)\n",
    "            label_df = pd.read_csv(label_path)\n",
    "\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_df['case_num'] = case_num\n",
    "            label_df['case_num'] = case_num\n",
    "            \n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(label_df)\n",
    "        \n",
    "        test_input_fn = []\n",
    "        test_label_fn = []\n",
    "        for test_input_path, test_label_path in zip(test_input_paths, test_label_paths):\n",
    "            case_num = test_input_path.replace('./data/test_input/TEST_','').replace('.csv','')\n",
    "            \n",
    "            test_input_df = pd.read_csv(test_input_path)\n",
    "            test_label_df = pd.read_csv(test_label_path)\n",
    "            \n",
    "            test_input_df['case_num'] = case_num\n",
    "            test_label_df['case_num'] = case_num\n",
    "            \n",
    "            test_input_fn.append(test_input_df)\n",
    "            test_label_fn.append(test_label_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0).sort_values(['case_num','DAT','obs_time'])\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_fn ,axis=0)\n",
    "        self.test_label  = pd.concat(test_label_fn ,axis=0)\n",
    "        \n",
    "        self.input     .obs_time = list(np.arange(0,24))*int(self.input     .shape[0]/24)\n",
    "        self.test_input.obs_time = list(np.arange(0,24))*int(self.test_input.shape[0]/24)\n",
    "        \n",
    "    def _data_return(self):\n",
    "        return self.input,self.label,self.test_input,self.test_label\n",
    "            \n",
    "    def _target_log(self):\n",
    "        self.label['predicted_weight_g'] = np.log(self.label['predicted_weight_g'])\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        self.input      = self.input     .drop(['obs_time'],axis=1)\n",
    "        self.test_input = self.test_input.drop(['obs_time'],axis=1)\n",
    "        \n",
    "        self.input      = self.input     .groupby(['case_num','DAT']).mean().reset_index()\n",
    "        self.test_input = self.test_input.groupby(['case_num','DAT']).mean().reset_index()\n",
    "        \n",
    "        self.input.DAT      = self.input     .DAT + 1\n",
    "        self.test_input.DAT = self.test_input.DAT + 1\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # https://dacon.io/competitions/official/236033/talkboard/407304?page=1&dtype=recent\n",
    "    def _scale_dataset(self,outlier):\n",
    "        \n",
    "        minmax_info = {\n",
    "            'none':[0,0],\n",
    "            # 'time':[0,28*24],\n",
    "            # '내부온도관측치':[4,40],\n",
    "            # '내부습도관측치':[0,100],\n",
    "            # 'co2관측치':[0,1200],\n",
    "            # 'ec관측치':[0,8],\n",
    "            # '시간당분무량':[0,3000],\n",
    "            # '일간누적분무량':[0,72000],\n",
    "            # '시간당백색광량':[0,120000],\n",
    "            # '일간누적백색광량':[0,2880000],\n",
    "            # '시간당적색광량':[0,120000],\n",
    "            # '일간누적적색광량':[0,2880000],\n",
    "            # '시간당청색광량':[0,120000],\n",
    "            # '일간누적청색광량':[0,2880000],\n",
    "            # '시간당총광량':[0,120000],\n",
    "            # '일간누적총광량':[0,2880000],\n",
    "        }\n",
    "            \n",
    "        scale_feature = [feature for feature,(min_info,max_info) in minmax_info.items() if feature in self.input.columns]\n",
    "        \n",
    "        # for train dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.input[col] = (self.input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.input[col][self.input[col]<0] = 0\n",
    "                self.input[col][self.input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.input[col][(self.input[col]<0) | (self.input[col]>1)] = np.nan\n",
    "            \n",
    "        # for test dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.test_input[col][self.test_input[col]<0] = 0\n",
    "                self.test_input[col][self.test_input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.test_input[col][(self.test_input[col]<0) | (self.test_input[col]>1)] = np.nan\n",
    "        \n",
    "        another_features = list(set(self.input.select_dtypes(exclude=[object]).columns)-set(scale_feature))\n",
    "        for col in another_features:\n",
    "            min_info,max_info = self.input[col].min(),self.input[col].max()\n",
    "            self.input[col]      = (self.input[col]     -min_info) / (max_info-min_info)\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "        \n",
    "    def _interaction_term(self):\n",
    "        # num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        # num_features = list(set(num_features)-set(['DAT','obs_time']))\n",
    "        num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    self.input     [f'{num_features[i]}*{num_features[j]}'] = self.input     [num_features[i]]*self.input     [num_features[j]]\n",
    "                    self.test_input[f'{num_features[i]}*{num_features[j]}'] = self.test_input[num_features[i]]*self.test_input[num_features[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46afb772-6e30-42b5-b09b-f0cdc584fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf407c58-fe03-456d-9bad-ec0ff6b18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# val_rate = 0.05\n",
    "\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# dataset._preprocess()\n",
    "# dataset._scale_dataset()\n",
    "# input_df, label_df = dataset._data_return()\n",
    "\n",
    "# for case_num in tqdm(sorted(input_df.case_num.unique())):\n",
    "\n",
    "#     input = input_df[input_df.case_num==case_num].drop('case_num',axis=1)\n",
    "#     label = label_df[label_df.case_num==case_num].drop('case_num',axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize=(20,15))\n",
    "#     nrow = 3\n",
    "#     ncol = 5\n",
    "\n",
    "#     iter = 0\n",
    "#     total = len(input.columns)-3\n",
    "#     for col in input.columns:\n",
    "#         if col not in ['time','DAT','obs_time']:\n",
    "#             iter+=1\n",
    "\n",
    "#             y1 = input[col]\n",
    "#             #y1 = (y1-y1.min())/(y1.max()-y1.min())\n",
    "\n",
    "#             y2 = label['predicted_weight_g']\n",
    "#             y2 = (y2-y2.min())/(y2.max()-y2.min())\n",
    "\n",
    "#             y3 = input.groupby('DAT')[col].mean().values\n",
    "\n",
    "#             corr, pvalue = pearsonr(y2,y3)\n",
    "\n",
    "#             fig.add_subplot(ncol,nrow,iter)\n",
    "#             sns.scatterplot(x=input.time  ,y=y1)\n",
    "#             sns.scatterplot(x=label.DAT*24,y=y2,color='red')\n",
    "#             sns.lineplot   (x=label.DAT*24,y=y3,color='blue',linestyle='--',alpha=0.7)\n",
    "#             plt.ylabel('')\n",
    "\n",
    "#             plt.title(f'{col}(corr={corr:.3f}(pvalue={pvalue:.3f}))',fontproperties=fp)\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'./fig/{case_num}.png',dpi=100)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a08-c07c-42bd-9b38-18eccfeba517",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff8530a6-2012-4619-903f-da297915ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=15, hidden_size=256, batch_first=True, bidirectional=False)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         hidden, _ = self.lstm(x)\n",
    "#         output = self.classifier(hidden[:,-1,:])\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee70e82-0375-4a8d-9a02-5100763d8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coding-yoon.tistory.com/131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_sizes, dropout_rates, num_classes, num_layers, bidirectional):\n",
    "#         super(BaseModel, self).__init__()\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_sizes = hidden_sizes\n",
    "#         self.dropout_rates = dropout_rates\n",
    "#         self.num_classes = num_classes\n",
    "#         self.num_layers = num_layers\n",
    "#         self.bidirectional = bidirectional\n",
    "        \n",
    "#         self.node_offset = 2 if self.bidirectional else 1\n",
    "\n",
    "#         self.lstm1 = nn.LSTM(\n",
    "#             input_size=self.input_size,\n",
    "#             hidden_size=self.hidden_sizes[0],\n",
    "#             # batch_first=True,\n",
    "#             bidirectional=self.bidirectional,\n",
    "#             dropout=self.dropout_rates[0],\n",
    "#             num_layers=self.num_layers,\n",
    "#         )\n",
    "        \n",
    "#         self.lstm2 = nn.LSTM(\n",
    "#             input_size=self.hidden_sizes[0]*self.node_offset, # bidirectional\n",
    "#             hidden_size=self.hidden_sizes[1],\n",
    "#             # batch_first=True,\n",
    "#             bidirectional=self.bidirectional,\n",
    "#             dropout=self.dropout_rates[1],\n",
    "#             num_layers=self.num_layers,\n",
    "#         )\n",
    "\n",
    "#         # self.fc = nn.Linear(self.hidden_sizes[1]*self.node_offset, self.num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         # self.elu  = nn.ELU()\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "#         self.module = nn.Sequential(\n",
    "#             nn.Linear(input_size, 128),\n",
    "#             #nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_sizes[0],\n",
    "#                             num_layers=self.num_layers, batch_first=True, dropout = self.dropout_rates[0])\n",
    "\n",
    "#         self.fc = nn.Linear(self.hidden_sizes[0], self.num_classes)\n",
    "        \n",
    "#     # def forward(self, x):\n",
    "#     #     hid, _ = self.lstm1(x)\n",
    "#     #     hid    = self.dropout(hid)\n",
    "#     #     hid    = self.relu(hid)\n",
    "#     #     hid, _ = self.lstm2(hid)\n",
    "#     #     hid    = self.dropout(hid)\n",
    "#     #     out    = self.relu(hid)\n",
    "#     #     # print(out.detach().numpy().shape) # (16, 24, 32)\n",
    "#     #     out    = self.fc(out[:,-1,:])\n",
    "#     #     # out    = self.fc(out)\n",
    "#     #     return out\n",
    "    \n",
    "#     # def forward(self, x):\n",
    "#     #     out = self.module(x)\n",
    "#     #     return out\n",
    "    \n",
    "#     # https://www.kaggle.com/code/omershect/learning-pytorch-lstm-deep-learning-with-m5-data\n",
    "#     def forward(self, x):\n",
    "#         h_0 = Variable(torch.zeros(\n",
    "#             self.num_layers, x.size(0), self.hidden_sizes[0]).to(device))\n",
    "        \n",
    "#         c_0 = Variable(torch.zeros(\n",
    "#             self.num_layers, x.size(0), self.hidden_sizes[0]).to(device))\n",
    "        \n",
    "#         # Propagate input through LSTM\n",
    "#         ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "#         h_out = h_out.view(-1, self.hidden_sizes[0])\n",
    "        \n",
    "#         out = self.fc(h_out)\n",
    "#         out = self.dropout(out)\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train, Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d419b-7332-4e64-a6e5-68aed8e5b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_fn(output, target):\n",
    "    return torch.sqrt(torch.mean((output-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26ee98e-b3ae-4d8e-a18e-8b166686c493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, scheduler, device, \n",
    "    early_stopping, epochs, metric_period=1, best_model_only=True\n",
    "):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['PATIENCE'], verbose = False, path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            # print(output.shape,Y.shape) # torch.Size([4, 28, 1]) torch.Size([4, 24])\n",
    "            # print(output[:5],Y[:5])\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5],output.shape,Y.shape)\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "        epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "        progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "            .format(\n",
    "                epoch_str,\n",
    "                epochs,np.mean(train_loss),\n",
    "                valid_loss,\n",
    "                epoch_e-epoch_s,\n",
    "                epoch_e-start_time,\n",
    "                (epoch_e-epoch_s)*(epochs-epoch)\n",
    "            )\n",
    "        epoch_s = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if best_model_only:\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model = model\n",
    "                if epoch % metric_period == 0:\n",
    "                    print('*'+progress)\n",
    "\n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "            else:\n",
    "                if epoch % metric_period == 0:\n",
    "                    print(' '+progress)\n",
    "                \n",
    "        else:\n",
    "            best_model = model\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                if epoch % metric_period == 0:\n",
    "                    print('*'+progress)\n",
    "\n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "            else:\n",
    "                if epoch % metric_period == 0:\n",
    "                    print(' '+progress)\n",
    "                \n",
    "            path = f'./model/best_model.pt'\n",
    "            torch.save(best_model.state_dict(), path)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731d783-38cb-43e3-8f4f-1f9c6d187cdc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5372837a-12ee-431c-be3b-3102602e774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(input_list).shape\n",
    "# [x.size() for x in input_list]\n",
    "\n",
    "# [x.size() for x in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5023f6d2-1af1-4ccb-86b0-2b241cc9a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.size() for x in input_list]\n",
    "# [x.size() for x in label_list]\n",
    "# torch.Tensor(y_seq.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46086dc5-9016-4d90-9d11-e16031dc5007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [x[0] for x in train_loader]\n",
    "# X_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4fc984-65ef-4c99-ab47-39cdc5072fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# n_split = 2\n",
    "\n",
    "# # Preprocess Class\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# # (1) preprocessing + scaling + interaction term\n",
    "# dataset._preprocess()\n",
    "# # dataset._feature_exp()\n",
    "# dataset._scale_dataset()\n",
    "# # dataset._interaction_term()\n",
    "\n",
    "# # (2) Data Return for check\n",
    "# input_df, label_df, test_df = dataset._data_return()\n",
    "\n",
    "# # (3) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']\n",
    "\n",
    "# # (4) train/validaion split을 위한 index 생성\n",
    "# input_df['idx'] = np.repeat(np.arange(0,28*28),24)\n",
    "\n",
    "# # (5) delete features\n",
    "# input_df = input_df.drop(del_features,axis=1)\n",
    "\n",
    "# test_preds = []\n",
    "# kf = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "# for tr_idx,va_idx in tqdm(kf.split(label_df),total=n_split):\n",
    "    \n",
    "#     X_train = input_df[input_df.idx.isin(tr_idx)].drop(['idx','case_num'],axis=1)\n",
    "#     X_valid = input_df[input_df.idx.isin(va_idx)].drop(['idx','case_num'],axis=1)\n",
    "#     y_train = label_df.iloc[tr_idx]\n",
    "#     y_valid = label_df.iloc[va_idx]\n",
    "    \n",
    "#     # #temp\n",
    "#     # X_train = X_train.groupby(['case_num','DAT']).mean().reset_index().drop(['idx','case_num'],axis=1)\n",
    "#     # X_valid = X_valid.groupby(['case_num','DAT']).mean().reset_index().drop(['idx','case_num'],axis=1)\n",
    "    \n",
    "#     # X_train['DAT'] = (X_train['DAT']-0)/(28-0)\n",
    "#     # X_valid['DAT'] = (X_valid['DAT']-0)/(28-0)\n",
    "#     # X_train = X_train.drop('DAT',axis=1)\n",
    "#     # X_valid = X_valid.drop('DAT',axis=1)\n",
    "    \n",
    "#     train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False)\n",
    "#     # train_datatset = TensorDataset(torch.from_numpy(X_train.values),torch.from_numpy(y_train.values))\n",
    "#     train_loader  = DataLoader(train_dataset, batch_size = 1024, shuffle=False, num_workers=6) # CFG['BATCH_SIZE']\n",
    "#     # train_loader = DataLoader(train_dataset, num_workers=8)\n",
    "\n",
    "#     valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False)\n",
    "#     # valid_datatset = TensorDataset(torch.from_numpy(X_valid.values),torch.from_numpy(y_valid.values))\n",
    "#     valid_loader  = DataLoader(valid_dataset, batch_size = 1024, shuffle=False, num_workers=6) # CFG['BATCH_SIZE']\n",
    "#     # valid_loader = DataLoader(valid_dataset, num_workers=8)\n",
    "    \n",
    "#     # input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "#     model = BaseModel(\n",
    "#         input_size = X_train.shape[1],\n",
    "#         hidden_sizes=[64,32],\n",
    "#         dropout_rates=[0.2,0.2],\n",
    "#         num_classes=1,\n",
    "#         num_layers=1,\n",
    "#         bidirectional=False,\n",
    "#     )\n",
    "#     # model = GRUModel(input_dim=X_train.shape[1], hidden_dim=64, layer_dim=1, output_dim=1)\n",
    "    \n",
    "#     model.eval()\n",
    "#     optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])#, weight_decay=1e-5)\n",
    "#     # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=False)\n",
    "\n",
    "#     best_model = train(\n",
    "#         model,\n",
    "#         optimizer=optimizer,\n",
    "#         train_loader=train_loader,\n",
    "#         valid_loader=valid_loader,\n",
    "#         scheduler=scheduler,\n",
    "#         device=device,\n",
    "#         early_stopping=False,\n",
    "#         metric_period=1,\n",
    "#         epochs=2,\n",
    "#     )\n",
    "    \n",
    "#     test_df = test_df[X_train.columns]\n",
    "#     test_dataset = CustomTestDataset(input=test_df)\n",
    "#     test_loader  = DataLoader(test_dataset, batch_size = 1024, shuffle=False, num_workers=0)\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     test_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for X in iter(test_loader):\n",
    "#             X = X.float().to(device)\n",
    "\n",
    "#             model_pred = model(X)\n",
    "#             # model_pred = torch.exp(model_pred)\n",
    "#             model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "#             test_pred += model_pred\n",
    "            \n",
    "#     test_preds.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913d75a3-a641-4a4e-8e1e-bc329caa3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Preprocess Class\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# # (1) preprocessing + scaling + interaction term\n",
    "# dataset._preprocess()\n",
    "# # dataset._target_log()\n",
    "# dataset._scale_dataset()\n",
    "# # dataset._interaction_term()\n",
    "\n",
    "# # (2) Data Return for check\n",
    "# input_df, label_df, test_df = dataset._data_return()\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# # label_df = label_df['predicted_weight_g']\n",
    "\n",
    "# plot_df = input_df.copy()\n",
    "# features = input_df.drop(['DAT','case_num','time'],axis=1).columns\n",
    "\n",
    "# for col in ['predicted_weight_g']:\n",
    "#     label_df[col] = (label_df[col]-label_df[col].min()) / (label_df[col].max()-label_df[col].min())\n",
    "# for col in features:\n",
    "#     plot_df[col] = (plot_df[col] - plot_df[col].min()) / (plot_df[col].max()-plot_df[col].min())\n",
    "\n",
    "# for col in features:\n",
    "#     palette = sns.color_palette(\"pastel\",plot_df.case_num.nunique())\n",
    "#     plt.figure(figsize=(15,7))\n",
    "\n",
    "#     i=0\n",
    "#     for case_num in plot_df.case_num.unique():\n",
    "#         d1 = plot_df[plot_df.case_num==case_num]\n",
    "#         d2 = label_df[label_df.case_num==case_num]\n",
    "#         sns.lineplot(x=d1['time']*d2['DAT'].max()*24,y=d1[col],alpha=0.5,color=palette[i])\n",
    "#         sns.lineplot(x=d2['DAT']*24,y=d2['predicted_weight_g'],color=palette[i])\n",
    "#         i+=1\n",
    "\n",
    "#     plt.xlabel('')\n",
    "#     plt.ylabel('')\n",
    "#     plt.title(col,fontproperties=fp)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e95a60-344a-4d23-a7aa-913d23ea5c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agg_df = plot_df.groupby(['time']).mean().reset_index()\n",
    "\n",
    "# for col in features:\n",
    "\n",
    "#     x = plot_df['time']\n",
    "#     y1 = plot_df[col]\n",
    "#     y2 = (label_df-label_df.min()) / (label_df.max()-label_df.min())\n",
    "#     y3 = agg_df[col]\n",
    "\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=x,y=y1,alpha=0.5)\n",
    "#     sns.scatterplot(x=y2.index*24,y=y2,color='red')\n",
    "#     sns.lineplot(x=agg_df['time'],y=y3,color='black')\n",
    "#     plt.title(col,fontproperties=fp)\n",
    "#     plt.ylabel('')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d57fd0-ea32-4fb4-a626-1006236b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_label_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "all_test_label_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d65ad91-4f38-4423-99f9-c1e8089c2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess Class\n",
    "dataset = Preprocess(\n",
    "    input_paths = all_input_list,\n",
    "    label_paths = all_label_list,\n",
    "    test_input_paths = all_test_input_list,\n",
    "    test_label_paths = all_test_label_list,\n",
    ")\n",
    "\n",
    "# (1) preprocessing + scaling + interaction term\n",
    "dataset._preprocess()\n",
    "# dataset._target_log()\n",
    "dataset._scale_dataset(outlier='keep')\n",
    "dataset._interaction_term()\n",
    "\n",
    "# (2) Data Return for check\n",
    "input_df, label_df, test_input_df, test_label_df = dataset._data_return()\n",
    "\n",
    "# # (3) Delete Std zero features\n",
    "# std_zero_features = []\n",
    "# for case_num in input_df.case_num.unique():\n",
    "#     tmp = input_df[input_df.case_num==case_num]\n",
    "#     std_zero_feature = tmp.std().index[tmp.std()==0].tolist()\n",
    "#     std_zero_features += std_zero_feature\n",
    "    \n",
    "# std_zero_features = pd.unique(std_zero_features)\n",
    "\n",
    "# input_df = input_df.drop(std_zero_features,axis=1)\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e1319e6-fb96-4d3a-88a6-6523d1d18edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_num</th>\n",
       "      <th>DAT</th>\n",
       "      <th>내부온도관측치</th>\n",
       "      <th>내부습도관측치</th>\n",
       "      <th>co2관측치</th>\n",
       "      <th>ec관측치</th>\n",
       "      <th>시간당분무량</th>\n",
       "      <th>일간누적분무량</th>\n",
       "      <th>시간당백색광량</th>\n",
       "      <th>일간누적백색광량</th>\n",
       "      <th>시간당적색광량</th>\n",
       "      <th>일간누적적색광량</th>\n",
       "      <th>시간당청색광량</th>\n",
       "      <th>일간누적청색광량</th>\n",
       "      <th>시간당총광량</th>\n",
       "      <th>일간누적총광량</th>\n",
       "      <th>내부온도관측치*DAT</th>\n",
       "      <th>내부습도관측치*DAT</th>\n",
       "      <th>내부습도관측치*내부온도관측치</th>\n",
       "      <th>co2관측치*DAT</th>\n",
       "      <th>co2관측치*내부온도관측치</th>\n",
       "      <th>co2관측치*내부습도관측치</th>\n",
       "      <th>ec관측치*DAT</th>\n",
       "      <th>ec관측치*내부온도관측치</th>\n",
       "      <th>ec관측치*내부습도관측치</th>\n",
       "      <th>ec관측치*co2관측치</th>\n",
       "      <th>시간당분무량*DAT</th>\n",
       "      <th>시간당분무량*내부온도관측치</th>\n",
       "      <th>시간당분무량*내부습도관측치</th>\n",
       "      <th>시간당분무량*co2관측치</th>\n",
       "      <th>시간당분무량*ec관측치</th>\n",
       "      <th>일간누적분무량*DAT</th>\n",
       "      <th>일간누적분무량*내부온도관측치</th>\n",
       "      <th>일간누적분무량*내부습도관측치</th>\n",
       "      <th>일간누적분무량*co2관측치</th>\n",
       "      <th>일간누적분무량*ec관측치</th>\n",
       "      <th>일간누적분무량*시간당분무량</th>\n",
       "      <th>시간당백색광량*DAT</th>\n",
       "      <th>시간당백색광량*내부온도관측치</th>\n",
       "      <th>시간당백색광량*내부습도관측치</th>\n",
       "      <th>시간당백색광량*co2관측치</th>\n",
       "      <th>시간당백색광량*ec관측치</th>\n",
       "      <th>시간당백색광량*시간당분무량</th>\n",
       "      <th>시간당백색광량*일간누적분무량</th>\n",
       "      <th>일간누적백색광량*DAT</th>\n",
       "      <th>일간누적백색광량*내부온도관측치</th>\n",
       "      <th>일간누적백색광량*내부습도관측치</th>\n",
       "      <th>일간누적백색광량*co2관측치</th>\n",
       "      <th>일간누적백색광량*ec관측치</th>\n",
       "      <th>일간누적백색광량*시간당분무량</th>\n",
       "      <th>일간누적백색광량*일간누적분무량</th>\n",
       "      <th>일간누적백색광량*시간당백색광량</th>\n",
       "      <th>시간당적색광량*DAT</th>\n",
       "      <th>시간당적색광량*내부온도관측치</th>\n",
       "      <th>시간당적색광량*내부습도관측치</th>\n",
       "      <th>시간당적색광량*co2관측치</th>\n",
       "      <th>시간당적색광량*ec관측치</th>\n",
       "      <th>시간당적색광량*시간당분무량</th>\n",
       "      <th>시간당적색광량*일간누적분무량</th>\n",
       "      <th>시간당적색광량*시간당백색광량</th>\n",
       "      <th>시간당적색광량*일간누적백색광량</th>\n",
       "      <th>일간누적적색광량*DAT</th>\n",
       "      <th>일간누적적색광량*내부온도관측치</th>\n",
       "      <th>일간누적적색광량*내부습도관측치</th>\n",
       "      <th>일간누적적색광량*co2관측치</th>\n",
       "      <th>일간누적적색광량*ec관측치</th>\n",
       "      <th>일간누적적색광량*시간당분무량</th>\n",
       "      <th>일간누적적색광량*일간누적분무량</th>\n",
       "      <th>일간누적적색광량*시간당백색광량</th>\n",
       "      <th>일간누적적색광량*일간누적백색광량</th>\n",
       "      <th>일간누적적색광량*시간당적색광량</th>\n",
       "      <th>시간당청색광량*DAT</th>\n",
       "      <th>시간당청색광량*내부온도관측치</th>\n",
       "      <th>시간당청색광량*내부습도관측치</th>\n",
       "      <th>시간당청색광량*co2관측치</th>\n",
       "      <th>시간당청색광량*ec관측치</th>\n",
       "      <th>시간당청색광량*시간당분무량</th>\n",
       "      <th>시간당청색광량*일간누적분무량</th>\n",
       "      <th>시간당청색광량*시간당백색광량</th>\n",
       "      <th>시간당청색광량*일간누적백색광량</th>\n",
       "      <th>시간당청색광량*시간당적색광량</th>\n",
       "      <th>시간당청색광량*일간누적적색광량</th>\n",
       "      <th>일간누적청색광량*DAT</th>\n",
       "      <th>일간누적청색광량*내부온도관측치</th>\n",
       "      <th>일간누적청색광량*내부습도관측치</th>\n",
       "      <th>일간누적청색광량*co2관측치</th>\n",
       "      <th>일간누적청색광량*ec관측치</th>\n",
       "      <th>일간누적청색광량*시간당분무량</th>\n",
       "      <th>일간누적청색광량*일간누적분무량</th>\n",
       "      <th>일간누적청색광량*시간당백색광량</th>\n",
       "      <th>일간누적청색광량*일간누적백색광량</th>\n",
       "      <th>일간누적청색광량*시간당적색광량</th>\n",
       "      <th>일간누적청색광량*일간누적적색광량</th>\n",
       "      <th>일간누적청색광량*시간당청색광량</th>\n",
       "      <th>시간당총광량*DAT</th>\n",
       "      <th>시간당총광량*내부온도관측치</th>\n",
       "      <th>시간당총광량*내부습도관측치</th>\n",
       "      <th>시간당총광량*co2관측치</th>\n",
       "      <th>시간당총광량*ec관측치</th>\n",
       "      <th>시간당총광량*시간당분무량</th>\n",
       "      <th>시간당총광량*일간누적분무량</th>\n",
       "      <th>시간당총광량*시간당백색광량</th>\n",
       "      <th>시간당총광량*일간누적백색광량</th>\n",
       "      <th>시간당총광량*시간당적색광량</th>\n",
       "      <th>시간당총광량*일간누적적색광량</th>\n",
       "      <th>시간당총광량*시간당청색광량</th>\n",
       "      <th>시간당총광량*일간누적청색광량</th>\n",
       "      <th>일간누적총광량*DAT</th>\n",
       "      <th>일간누적총광량*내부온도관측치</th>\n",
       "      <th>일간누적총광량*내부습도관측치</th>\n",
       "      <th>일간누적총광량*co2관측치</th>\n",
       "      <th>일간누적총광량*ec관측치</th>\n",
       "      <th>일간누적총광량*시간당분무량</th>\n",
       "      <th>일간누적총광량*일간누적분무량</th>\n",
       "      <th>일간누적총광량*시간당백색광량</th>\n",
       "      <th>일간누적총광량*일간누적백색광량</th>\n",
       "      <th>일간누적총광량*시간당적색광량</th>\n",
       "      <th>일간누적총광량*일간누적적색광량</th>\n",
       "      <th>일간누적총광량*시간당청색광량</th>\n",
       "      <th>일간누적총광량*일간누적청색광량</th>\n",
       "      <th>일간누적총광량*시간당총광량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564266</td>\n",
       "      <td>0.823008</td>\n",
       "      <td>0.279704</td>\n",
       "      <td>0.279170</td>\n",
       "      <td>0.083194</td>\n",
       "      <td>0.082095</td>\n",
       "      <td>0.875564</td>\n",
       "      <td>0.961504</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.061177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481469</td>\n",
       "      <td>0.503443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157828</td>\n",
       "      <td>0.230199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157526</td>\n",
       "      <td>0.229759</td>\n",
       "      <td>0.078085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.068469</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>0.067565</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494051</td>\n",
       "      <td>0.720596</td>\n",
       "      <td>0.244899</td>\n",
       "      <td>0.244431</td>\n",
       "      <td>0.072842</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542544</td>\n",
       "      <td>0.791325</td>\n",
       "      <td>0.268937</td>\n",
       "      <td>0.268423</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.078935</td>\n",
       "      <td>0.841858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038540</td>\n",
       "      <td>0.056212</td>\n",
       "      <td>0.019104</td>\n",
       "      <td>0.019067</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.059801</td>\n",
       "      <td>0.065671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034520</td>\n",
       "      <td>0.050349</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.053564</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271676</td>\n",
       "      <td>0.396252</td>\n",
       "      <td>0.134669</td>\n",
       "      <td>0.134411</td>\n",
       "      <td>0.040055</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.421557</td>\n",
       "      <td>0.462934</td>\n",
       "      <td>0.032884</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284076</td>\n",
       "      <td>0.414338</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>0.140546</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.041330</td>\n",
       "      <td>0.440797</td>\n",
       "      <td>0.484063</td>\n",
       "      <td>0.034385</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.592162</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>0.262877</td>\n",
       "      <td>0.277330</td>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>0.963773</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.061192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482642</td>\n",
       "      <td>0.504578</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.030489</td>\n",
       "      <td>0.487468</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.164224</td>\n",
       "      <td>0.228298</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.049534</td>\n",
       "      <td>0.068860</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.023198</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.069630</td>\n",
       "      <td>0.022235</td>\n",
       "      <td>0.023458</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>0.519787</td>\n",
       "      <td>0.722588</td>\n",
       "      <td>0.230748</td>\n",
       "      <td>0.243434</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.570710</td>\n",
       "      <td>0.793379</td>\n",
       "      <td>0.253354</td>\n",
       "      <td>0.267283</td>\n",
       "      <td>0.080618</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.845980</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.040474</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.059996</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.036236</td>\n",
       "      <td>0.050374</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.053713</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.285802</td>\n",
       "      <td>0.397311</td>\n",
       "      <td>0.126876</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.040824</td>\n",
       "      <td>0.423653</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.298792</td>\n",
       "      <td>0.415369</td>\n",
       "      <td>0.132642</td>\n",
       "      <td>0.139934</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.486299</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.545077</td>\n",
       "      <td>0.816391</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.278995</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.085435</td>\n",
       "      <td>0.875723</td>\n",
       "      <td>0.961246</td>\n",
       "      <td>0.068185</td>\n",
       "      <td>0.061022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481510</td>\n",
       "      <td>0.503251</td>\n",
       "      <td>0.040376</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>0.444996</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.155881</td>\n",
       "      <td>0.233471</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.152074</td>\n",
       "      <td>0.227769</td>\n",
       "      <td>0.079787</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.047049</td>\n",
       "      <td>0.070467</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.046569</td>\n",
       "      <td>0.069748</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>0.023836</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.477337</td>\n",
       "      <td>0.714933</td>\n",
       "      <td>0.250438</td>\n",
       "      <td>0.244323</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.074817</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>0.523953</td>\n",
       "      <td>0.784753</td>\n",
       "      <td>0.274896</td>\n",
       "      <td>0.268183</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>0.841785</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.037166</td>\n",
       "      <td>0.055665</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.005885</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>0.065542</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.053439</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.262460</td>\n",
       "      <td>0.393101</td>\n",
       "      <td>0.137702</td>\n",
       "      <td>0.134339</td>\n",
       "      <td>0.041562</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.421670</td>\n",
       "      <td>0.462850</td>\n",
       "      <td>0.032832</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037278</td>\n",
       "      <td>0.274310</td>\n",
       "      <td>0.410849</td>\n",
       "      <td>0.143919</td>\n",
       "      <td>0.140404</td>\n",
       "      <td>0.043438</td>\n",
       "      <td>0.042995</td>\n",
       "      <td>0.440708</td>\n",
       "      <td>0.483748</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>0.030709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.516685</td>\n",
       "      <td>0.813247</td>\n",
       "      <td>0.620639</td>\n",
       "      <td>0.280708</td>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>0.963773</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.061192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482642</td>\n",
       "      <td>0.504578</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.420193</td>\n",
       "      <td>0.068960</td>\n",
       "      <td>0.320675</td>\n",
       "      <td>0.504732</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>0.228285</td>\n",
       "      <td>0.174219</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.043220</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.051916</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.043704</td>\n",
       "      <td>0.068788</td>\n",
       "      <td>0.052497</td>\n",
       "      <td>0.023744</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.097531</td>\n",
       "      <td>0.453535</td>\n",
       "      <td>0.713851</td>\n",
       "      <td>0.544783</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.107086</td>\n",
       "      <td>0.497968</td>\n",
       "      <td>0.783786</td>\n",
       "      <td>0.598155</td>\n",
       "      <td>0.270539</td>\n",
       "      <td>0.080618</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.845980</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.035315</td>\n",
       "      <td>0.055585</td>\n",
       "      <td>0.042421</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.059996</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.031617</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.037978</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.053713</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>0.249374</td>\n",
       "      <td>0.392507</td>\n",
       "      <td>0.299546</td>\n",
       "      <td>0.135482</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.040824</td>\n",
       "      <td>0.423653</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056064</td>\n",
       "      <td>0.260708</td>\n",
       "      <td>0.410346</td>\n",
       "      <td>0.313161</td>\n",
       "      <td>0.141639</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.486299</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.536601</td>\n",
       "      <td>0.823996</td>\n",
       "      <td>0.286433</td>\n",
       "      <td>0.281209</td>\n",
       "      <td>0.082588</td>\n",
       "      <td>0.078750</td>\n",
       "      <td>0.876083</td>\n",
       "      <td>0.961461</td>\n",
       "      <td>0.068218</td>\n",
       "      <td>0.061043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481710</td>\n",
       "      <td>0.503366</td>\n",
       "      <td>0.079496</td>\n",
       "      <td>0.122073</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.042434</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.236019</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.150897</td>\n",
       "      <td>0.231715</td>\n",
       "      <td>0.080547</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.068052</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>0.022145</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.129790</td>\n",
       "      <td>0.470107</td>\n",
       "      <td>0.721889</td>\n",
       "      <td>0.250939</td>\n",
       "      <td>0.246362</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.068992</td>\n",
       "      <td>0.142439</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>0.792240</td>\n",
       "      <td>0.275394</td>\n",
       "      <td>0.270371</td>\n",
       "      <td>0.079405</td>\n",
       "      <td>0.075715</td>\n",
       "      <td>0.842320</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.056211</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.059765</td>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.017485</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0.053479</td>\n",
       "      <td>0.058690</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071364</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>0.396927</td>\n",
       "      <td>0.137977</td>\n",
       "      <td>0.135461</td>\n",
       "      <td>0.039783</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.463145</td>\n",
       "      <td>0.032861</td>\n",
       "      <td>0.029405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074573</td>\n",
       "      <td>0.270107</td>\n",
       "      <td>0.414772</td>\n",
       "      <td>0.144181</td>\n",
       "      <td>0.141551</td>\n",
       "      <td>0.041572</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.440991</td>\n",
       "      <td>0.483967</td>\n",
       "      <td>0.034339</td>\n",
       "      <td>0.030727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_num       DAT   내부온도관측치   내부습도관측치    co2관측치     ec관측치    시간당분무량  \\\n",
       "0       01  0.000000  0.564266  0.823008  0.279704  0.279170  0.083194   \n",
       "1       01  0.037037  0.592162  0.823201  0.262877  0.277330  0.083649   \n",
       "2       01  0.074074  0.545077  0.816391  0.285979  0.278995  0.086316   \n",
       "3       01  0.111111  0.516685  0.813247  0.620639  0.280708  0.083649   \n",
       "4       01  0.148148  0.536601  0.823996  0.286433  0.281209  0.082588   \n",
       "\n",
       "    일간누적분무량   시간당백색광량  일간누적백색광량   시간당적색광량  일간누적적색광량  시간당청색광량  일간누적청색광량  \\\n",
       "0  0.082095  0.875564  0.961504  0.068300  0.061177      0.0       0.0   \n",
       "1  0.084585  0.877779  0.963773  0.068350  0.061192      0.0       0.0   \n",
       "2  0.085435  0.875723  0.961246  0.068185  0.061022      0.0       0.0   \n",
       "3  0.084585  0.877779  0.963773  0.068350  0.061192      0.0       0.0   \n",
       "4  0.078750  0.876083  0.961461  0.068218  0.061043      0.0       0.0   \n",
       "\n",
       "     시간당총광량   일간누적총광량  내부온도관측치*DAT  내부습도관측치*DAT  내부습도관측치*내부온도관측치  co2관측치*DAT  \\\n",
       "0  0.481469  0.503443     0.000000     0.000000         0.464396    0.000000   \n",
       "1  0.482642  0.504578     0.021932     0.030489         0.487468    0.009736   \n",
       "2  0.481510  0.503251     0.040376     0.060473         0.444996    0.021184   \n",
       "3  0.482642  0.504578     0.057409     0.090361         0.420193    0.068960   \n",
       "4  0.481710  0.503366     0.079496     0.122073         0.442157    0.042434   \n",
       "\n",
       "   co2관측치*내부온도관측치  co2관측치*내부습도관측치  ec관측치*DAT  ec관측치*내부온도관측치  ec관측치*내부습도관측치  \\\n",
       "0        0.157828        0.230199   0.000000       0.157526       0.229759   \n",
       "1        0.155666        0.216401   0.010271       0.164224       0.228298   \n",
       "2        0.155881        0.233471   0.020666       0.152074       0.227769   \n",
       "3        0.320675        0.504732   0.031190       0.145038       0.228285   \n",
       "4        0.153700        0.236019   0.041661       0.150897       0.231715   \n",
       "\n",
       "   ec관측치*co2관측치  시간당분무량*DAT  시간당분무량*내부온도관측치  시간당분무량*내부습도관측치  시간당분무량*co2관측치  \\\n",
       "0      0.078085    0.000000        0.046944        0.068469       0.023270   \n",
       "1      0.072904    0.003098        0.049534        0.068860       0.021989   \n",
       "2      0.079787    0.006394        0.047049        0.070467       0.024684   \n",
       "3      0.174219    0.009294        0.043220        0.068027       0.051916   \n",
       "4      0.080547    0.012235        0.044317        0.068052       0.023656   \n",
       "\n",
       "   시간당분무량*ec관측치  일간누적분무량*DAT  일간누적분무량*내부온도관측치  일간누적분무량*내부습도관측치  \\\n",
       "0      0.023225     0.000000         0.046323         0.067565   \n",
       "1      0.023198     0.003133         0.050088         0.069630   \n",
       "2      0.024082     0.006329         0.046569         0.069748   \n",
       "3      0.023481     0.009398         0.043704         0.068788   \n",
       "4      0.023224     0.011667         0.042257         0.064890   \n",
       "\n",
       "   일간누적분무량*co2관측치  일간누적분무량*ec관측치  일간누적분무량*시간당분무량  시간당백색광량*DAT  \\\n",
       "0        0.022962       0.022918        0.006830     0.000000   \n",
       "1        0.022235       0.023458        0.007075     0.032510   \n",
       "2        0.024433       0.023836        0.007374     0.064868   \n",
       "3        0.052497       0.023744        0.007075     0.097531   \n",
       "4        0.022557       0.022145        0.006504     0.129790   \n",
       "\n",
       "   시간당백색광량*내부온도관측치  시간당백색광량*내부습도관측치  시간당백색광량*co2관측치  시간당백색광량*ec관측치  \\\n",
       "0         0.494051         0.720596        0.244899       0.244431   \n",
       "1         0.519787         0.722588        0.230748       0.243434   \n",
       "2         0.477337         0.714933        0.250438       0.244323   \n",
       "3         0.453535         0.713851        0.544783       0.246400   \n",
       "4         0.470107         0.721889        0.250939       0.246362   \n",
       "\n",
       "   시간당백색광량*시간당분무량  시간당백색광량*일간누적분무량  일간누적백색광량*DAT  일간누적백색광량*내부온도관측치  \\\n",
       "0        0.072842         0.071879      0.000000          0.542544   \n",
       "1        0.073425         0.074247      0.035695          0.570710   \n",
       "2        0.075589         0.074817      0.071203          0.523953   \n",
       "3        0.073425         0.074247      0.107086          0.497968   \n",
       "4        0.072354         0.068992      0.142439          0.515921   \n",
       "\n",
       "   일간누적백색광량*내부습도관측치  일간누적백색광량*co2관측치  일간누적백색광량*ec관측치  일간누적백색광량*시간당분무량  \\\n",
       "0          0.791325         0.268937        0.268423         0.079991   \n",
       "1          0.793379         0.253354        0.267283         0.080618   \n",
       "2          0.784753         0.274896        0.268183         0.082971   \n",
       "3          0.783786         0.598155        0.270539         0.080618   \n",
       "4          0.792240         0.275394        0.270371         0.079405   \n",
       "\n",
       "   일간누적백색광량*일간누적분무량  일간누적백색광량*시간당백색광량  시간당적색광량*DAT  시간당적색광량*내부온도관측치  \\\n",
       "0          0.078935          0.841858     0.000000         0.038540   \n",
       "1          0.081521          0.845980     0.002531         0.040474   \n",
       "2          0.082124          0.841785     0.005051         0.037166   \n",
       "3          0.081521          0.845980     0.007594         0.035315   \n",
       "4          0.075715          0.842320     0.010106         0.036606   \n",
       "\n",
       "   시간당적색광량*내부습도관측치  시간당적색광량*co2관측치  시간당적색광량*ec관측치  시간당적색광량*시간당분무량  \\\n",
       "0         0.056212        0.019104       0.019067        0.005682   \n",
       "1         0.056266        0.017968       0.018955        0.005717   \n",
       "2         0.055665        0.019499       0.019023        0.005885   \n",
       "3         0.055585        0.042421       0.019186        0.005717   \n",
       "4         0.056211        0.019540       0.019183        0.005634   \n",
       "\n",
       "   시간당적색광량*일간누적분무량  시간당적색광량*시간당백색광량  시간당적색광량*일간누적백색광량  일간누적적색광량*DAT  \\\n",
       "0         0.005607         0.059801          0.065671      0.000000   \n",
       "1         0.005781         0.059996          0.065874      0.002266   \n",
       "2         0.005825         0.059711          0.065542      0.004520   \n",
       "3         0.005781         0.059996          0.065874      0.006799   \n",
       "4         0.005372         0.059765          0.065589      0.009043   \n",
       "\n",
       "   일간누적적색광량*내부온도관측치  일간누적적색광량*내부습도관측치  일간누적적색광량*co2관측치  일간누적적색광량*ec관측치  \\\n",
       "0          0.034520          0.050349         0.017111        0.017079   \n",
       "1          0.036236          0.050374         0.016086        0.016970   \n",
       "2          0.033262          0.049818         0.017451        0.017025   \n",
       "3          0.031617          0.049764         0.037978        0.017177   \n",
       "4          0.032756          0.050299         0.017485        0.017166   \n",
       "\n",
       "   일간누적적색광량*시간당분무량  일간누적적색광량*일간누적분무량  일간누적적색광량*시간당백색광량  일간누적적색광량*일간누적백색광량  \\\n",
       "0         0.005090          0.005022          0.053564           0.058822   \n",
       "1         0.005119          0.005176          0.053713           0.058976   \n",
       "2         0.005267          0.005213          0.053439           0.058657   \n",
       "3         0.005119          0.005176          0.053713           0.058976   \n",
       "4         0.005041          0.004807          0.053479           0.058690   \n",
       "\n",
       "   일간누적적색광량*시간당적색광량  시간당청색광량*DAT  시간당청색광량*내부온도관측치  시간당청색광량*내부습도관측치  \\\n",
       "0          0.004178          0.0              0.0              0.0   \n",
       "1          0.004182          0.0              0.0              0.0   \n",
       "2          0.004161          0.0              0.0              0.0   \n",
       "3          0.004182          0.0              0.0              0.0   \n",
       "4          0.004164          0.0              0.0              0.0   \n",
       "\n",
       "   시간당청색광량*co2관측치  시간당청색광량*ec관측치  시간당청색광량*시간당분무량  시간당청색광량*일간누적분무량  \\\n",
       "0             0.0            0.0             0.0              0.0   \n",
       "1             0.0            0.0             0.0              0.0   \n",
       "2             0.0            0.0             0.0              0.0   \n",
       "3             0.0            0.0             0.0              0.0   \n",
       "4             0.0            0.0             0.0              0.0   \n",
       "\n",
       "   시간당청색광량*시간당백색광량  시간당청색광량*일간누적백색광량  시간당청색광량*시간당적색광량  시간당청색광량*일간누적적색광량  \\\n",
       "0              0.0               0.0              0.0               0.0   \n",
       "1              0.0               0.0              0.0               0.0   \n",
       "2              0.0               0.0              0.0               0.0   \n",
       "3              0.0               0.0              0.0               0.0   \n",
       "4              0.0               0.0              0.0               0.0   \n",
       "\n",
       "   일간누적청색광량*DAT  일간누적청색광량*내부온도관측치  일간누적청색광량*내부습도관측치  일간누적청색광량*co2관측치  \\\n",
       "0           0.0               0.0               0.0              0.0   \n",
       "1           0.0               0.0               0.0              0.0   \n",
       "2           0.0               0.0               0.0              0.0   \n",
       "3           0.0               0.0               0.0              0.0   \n",
       "4           0.0               0.0               0.0              0.0   \n",
       "\n",
       "   일간누적청색광량*ec관측치  일간누적청색광량*시간당분무량  일간누적청색광량*일간누적분무량  일간누적청색광량*시간당백색광량  \\\n",
       "0             0.0              0.0               0.0               0.0   \n",
       "1             0.0              0.0               0.0               0.0   \n",
       "2             0.0              0.0               0.0               0.0   \n",
       "3             0.0              0.0               0.0               0.0   \n",
       "4             0.0              0.0               0.0               0.0   \n",
       "\n",
       "   일간누적청색광량*일간누적백색광량  일간누적청색광량*시간당적색광량  일간누적청색광량*일간누적적색광량  일간누적청색광량*시간당청색광량  \\\n",
       "0                0.0               0.0                0.0               0.0   \n",
       "1                0.0               0.0                0.0               0.0   \n",
       "2                0.0               0.0                0.0               0.0   \n",
       "3                0.0               0.0                0.0               0.0   \n",
       "4                0.0               0.0                0.0               0.0   \n",
       "\n",
       "   시간당총광량*DAT  시간당총광량*내부온도관측치  시간당총광량*내부습도관측치  시간당총광량*co2관측치  시간당총광량*ec관측치  \\\n",
       "0    0.000000        0.271676        0.396252       0.134669      0.134411   \n",
       "1    0.017876        0.285802        0.397311       0.126876      0.133851   \n",
       "2    0.035667        0.262460        0.393101       0.137702      0.134339   \n",
       "3    0.053627        0.249374        0.392507       0.299546      0.135482   \n",
       "4    0.071364        0.258486        0.396927       0.137977      0.135461   \n",
       "\n",
       "   시간당총광량*시간당분무량  시간당총광량*일간누적분무량  시간당총광량*시간당백색광량  시간당총광량*일간누적백색광량  \\\n",
       "0       0.040055        0.039526        0.421557         0.462934   \n",
       "1       0.040372        0.040824        0.423653         0.465158   \n",
       "2       0.041562        0.041138        0.421670         0.462850   \n",
       "3       0.040372        0.040824        0.423653         0.465158   \n",
       "4       0.039783        0.037935        0.422018         0.463145   \n",
       "\n",
       "   시간당총광량*시간당적색광량  시간당총광량*일간누적적색광량  시간당총광량*시간당청색광량  시간당총광량*일간누적청색광량  \\\n",
       "0        0.032884         0.029455             0.0              0.0   \n",
       "1        0.032989         0.029534             0.0              0.0   \n",
       "2        0.032832         0.029383             0.0              0.0   \n",
       "3        0.032989         0.029534             0.0              0.0   \n",
       "4        0.032861         0.029405             0.0              0.0   \n",
       "\n",
       "   일간누적총광량*DAT  일간누적총광량*내부온도관측치  일간누적총광량*내부습도관측치  일간누적총광량*co2관측치  \\\n",
       "0     0.000000         0.284076         0.414338        0.140815   \n",
       "1     0.018688         0.298792         0.415369        0.132642   \n",
       "2     0.037278         0.274310         0.410849        0.143919   \n",
       "3     0.056064         0.260708         0.410346        0.313161   \n",
       "4     0.074573         0.270107         0.414772        0.144181   \n",
       "\n",
       "   일간누적총광량*ec관측치  일간누적총광량*시간당분무량  일간누적총광량*일간누적분무량  일간누적총광량*시간당백색광량  \\\n",
       "0       0.140546        0.041884         0.041330         0.440797   \n",
       "1       0.139934        0.042207         0.042680         0.442908   \n",
       "2       0.140404        0.043438         0.042995         0.440708   \n",
       "3       0.141639        0.042207         0.042680         0.442908   \n",
       "4       0.141551        0.041572         0.039640         0.440991   \n",
       "\n",
       "   일간누적총광량*일간누적백색광량  일간누적총광량*시간당적색광량  일간누적총광량*일간누적적색광량  일간누적총광량*시간당청색광량  \\\n",
       "0          0.484063         0.034385          0.030799              0.0   \n",
       "1          0.486299         0.034488          0.030876              0.0   \n",
       "2          0.483748         0.034314          0.030709              0.0   \n",
       "3          0.486299         0.034488          0.030876              0.0   \n",
       "4          0.483967         0.034339          0.030727              0.0   \n",
       "\n",
       "   일간누적총광량*일간누적청색광량  일간누적총광량*시간당총광량  \n",
       "0               0.0        0.242392  \n",
       "1               0.0        0.243531  \n",
       "2               0.0        0.242320  \n",
       "3               0.0        0.243531  \n",
       "4               0.0        0.242476  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8979bc0a-1589-44b6-9cc6-6733be3e1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_df.isnull().sum()\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f93b40f-3e0c-4f7e-b84c-8c0958395769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_num', 'DAT', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in input_df.columns if col.find('*')<0])\n",
    "# print(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9ef6a2c-b58e-4d62-a88c-074132ab45fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size = 26, valid_size : 2\n",
      "[20, 3]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "n = input_df.case_num.nunique()\n",
    "val_rate = 0.1\n",
    "\n",
    "valid_size = int(n*val_rate)\n",
    "train_size = n-valid_size\n",
    "print(f'train_size = {train_size}, valid_size : {valid_size}')\n",
    "\n",
    "random_num = random.sample(range(n),valid_size)\n",
    "print(random_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ff2f9b-cc59-417e-b6de-7856edc83829",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_case_num = input_df.case_num.unique()[random_num].tolist()\n",
    "tr_case_num = list(set(input_df.case_num.unique())-set(va_case_num))\n",
    "\n",
    "X_train = input_df[input_df.case_num.isin(tr_case_num)]#.drop(columns=['case_num','time','DAT','obs_time'])\n",
    "X_valid = input_df[input_df.case_num.isin(va_case_num)]#.drop(columns=['case_num','time','DAT','obs_time'])\n",
    "\n",
    "y_train = label_df[label_df.case_num.isin(tr_case_num)]#['predicted_weight_g']\n",
    "y_valid = label_df[label_df.case_num.isin(va_case_num)]#['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d361a096-12a3-4ce0-94ac-2fe685a16e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(input_df,label_df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2b39b8c-e09c-4c99-9020-b36bccf4e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728, 121) (56, 121) (728, 3) (56, 3)\n"
     ]
    }
   ],
   "source": [
    "# # X_train = pd.get_dummies(X_train,columns=['DAT','obs_time'])\n",
    "# # X_valid = pd.get_dummies(X_valid,columns=['DAT','obs_time'])\n",
    "\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "280965b6-90fa-4e1f-b4b9-cd3052ebe30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c78b35f-987a-4a7b-a99f-888eed206868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a706f1d-de3f-43fe-9976-fd013f443fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in input_df.select_dtypes(exclude=[object]).columns:\n",
    "#     a = input_df[col].reset_index(drop=True)\n",
    "#     b = label_df.predicted_weight_g.reset_index(drop=True)\n",
    "#     plt.figure(figsize=(15,7))\n",
    "#     sns.scatterplot(x=a,y=b,hue=input_df.case_num.reset_index(drop=True))\n",
    "#     plt.title(col,fontproperties=fp)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c3c8b5e-f612-483e-9da1-4f310c6c094d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='case_num', ylabel='predicted_weight_g'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=label_df.case_num,y=label_df.predicted_weight_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b327f6-d784-44a9-8c6b-59cfe63ea1a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 참조용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89196afd-c692-4b6c-854b-372373e553cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cure-lab/SCINet\n",
    "# https://github.com/fd17/SciNet_PyTorch/blob/master/models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SciNet(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, latent_dim, layer_dim):\n",
    "\t\t\"\"\"Initialize SciNet Model.\n",
    "\t\t\n",
    "\t\tParams\n",
    "\t\t======\n",
    "\t\t\tinput_dim (int): number of inputs\n",
    "\t\t\toutput_dim (int): number of outputs\n",
    "\t\t\tlatent_dim (int): number of latent neurons\n",
    "\t\t\tLayer_dim (int): number of neurons in hidden layers\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(SciNet, self).__init__()\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\t\tself.enc1 = nn.Linear(input_dim, layer_dim)\n",
    "\t\tself.enc2 = nn.Linear(layer_dim, layer_dim)\n",
    "\t\tself.latent = nn.Linear(layer_dim, latent_dim*2)\n",
    "\t\tself.dec1 = nn.Linear(latent_dim+1, layer_dim)\n",
    "\t\tself.dec2 = nn.Linear(layer_dim,layer_dim)\n",
    "\t\tself.out = nn.Linear(layer_dim, output_dim)       \n",
    "\t  \n",
    "\tdef encoder(self, x):\n",
    "\t\tz = F.elu(self.enc1(x))\n",
    "\t\tz = F.elu(self.enc2(z))\n",
    "\t\tz = self.latent(z)\n",
    "\t\tself.mu = z[:, 0:self.latent_dim]\n",
    "\t\tself.log_sigma = z[:, self.latent_dim:]\n",
    "\t\tself.sigma = torch.exp(self.log_sigma)        \n",
    "\n",
    "\t\t# Use reparametrization trick to sample from gaussian\n",
    "\t\teps = torch.randn(x.size(0), self.latent_dim)\n",
    "\t\tz_sample = self.mu + self.sigma * eps        \n",
    "\n",
    "\t\t# Compute KL loss\n",
    "\t\tself.kl_loss = kl_divergence(self.mu, self.log_sigma, dim=self.latent_dim)\n",
    "\n",
    "\t\treturn z_sample\n",
    "\t\n",
    "\tdef decoder(self, z):\n",
    "\t\tx = F.elu(self.dec1(z))\n",
    "\t\tx = F.elu(self.dec2(x))        \n",
    "\t\treturn self.out(x)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\tq = obs[:,-1].reshape(obs.size(0),1)\n",
    "\t\tobs = obs[:,0:-1]\n",
    "\t\tself.latent_r = self.encoder(obs) \n",
    "\t\tdec_input = torch.cat( (q, self.latent_r), 1)\n",
    "\n",
    "\t\treturn self.decoder(dec_input)\n",
    "\n",
    "\n",
    "def kl_divergence(means, log_sigma, dim, target_sigma=0.1):\n",
    "\t\"\"\"\n",
    "\tComputes Kullback–Leibler divergence for arrays of mean and log(sigma)\n",
    "\t\"\"\"\n",
    "\ttarget_sigma = torch.Tensor([target_sigma])\n",
    "\treturn 1 / 2. * torch.mean(torch.mean(1 / target_sigma**2 * means**2 +\n",
    "\t\t\ttorch.exp(2 * log_sigma) / target_sigma**2 - 2 * log_sigma + 2 * torch.log(target_sigma), dim=1) - dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02acf626-e892-4194-b2ed-5f802c85e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def pendulum(t, A0, delta0, k, b, m):\n",
    "\t\"\"\"\n",
    "\tSolution x(t) for pendulum differential equation\n",
    "\t\tmx'' = -kx + bx'\n",
    "\tReturns position at time t\n",
    "\tParameters:\n",
    "\t\t- t: time\n",
    "\t\t- A0: starting amplitude\n",
    "\t\t- delta0: phase\n",
    "\t\t- k: spring constant\n",
    "\t\t- b: damping factor\n",
    "\t\"\"\"\n",
    "\tA = 1 - b**2 / (4 * m * k)\n",
    "\tif A < 0:\n",
    "\t\treturn None\n",
    "\tw = np.sqrt(k/m)* np.sqrt(A)\n",
    "\tresult = A0 * np.exp( - t * b / (2. * m) ) * np.cos(w * t + delta0)\n",
    "\treturn result\n",
    "\n",
    "def target_loss(pred,answer):\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tpred = pred[:,0]\n",
    "\t\n",
    "\treturn torch.mean(torch.sum((pred - answer)**2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a93d7a3e-599a-4a26-87e4-de8339ea32c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SciNet(\n",
       "  (enc1): Linear(in_features=121, out_features=64, bias=True)\n",
       "  (enc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (latent): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (dec1): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (dec2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scinet = SciNet(input_dim=X_train.shape[1], output_dim=1, latent_dim=3, layer_dim=64)\n",
    "scinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d90ba876-adf6-4da2-97f6-ce8debd72be2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m epoch_error \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_batch, minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m     13\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m minibatch\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "optimizer = optim.Adam(scinet.parameters())\n",
    "hist_error = []\n",
    "hist_loss = []\n",
    "beta = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    epoch_error = []\n",
    "    epoch_loss = []\n",
    "    for i_batch, minibatch in enumerate(train_loader):\n",
    "\n",
    "        inputs, outputs = minibatch\n",
    "        optimizer.zero_grad()\n",
    "        pred = scinet.forward(inputs)\n",
    "        \n",
    "        loss = target_loss(pred, outputs) + beta * scinet.kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        error = torch.mean(torch.sqrt((pred[:,0]-outputs)**2)).detach().numpy()\n",
    "        epoch_error.append(error)\n",
    "        epoch_loss.append(loss.data.detach().numpy())\n",
    "    hist_error.append(np.mean(epoch_error))\n",
    "    hist_loss.append(np.mean(epoch_loss))\n",
    "    print(\"Epoch %d -- loss %f, RMS error %f \" % (epoch+1, hist_loss[-1], hist_error[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d9fee-508f-4d4f-b2d5-4e7a68691887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "# model = BaseModel(\n",
    "#     input_size = X_train.shape[1],\n",
    "#     hidden_sizes=[32,16],\n",
    "#     dropout_rates=[0.2,0.2],\n",
    "#     num_classes=1,\n",
    "#     num_layers=1,\n",
    "#     bidirectional=False,\n",
    "# )\n",
    "# # model = GRUModel(input_dim=X_train.shape[1], hidden_dim=64, layer_dim=1, output_dim=1)\n",
    "\n",
    "# model.eval()\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-6)# lr = CFG[\"LEARNING_RATE\"], weight_decay=1e-5)\n",
    "# # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=False)\n",
    "\n",
    "# best_model = train(\n",
    "#     model,\n",
    "#     optimizer=optimizer,\n",
    "#     train_loader=train_loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     early_stopping=False,\n",
    "#     metric_period=1,\n",
    "#     epochs=1024,\n",
    "# )\n",
    "\n",
    "# test_df = test_df[X_train.columns]\n",
    "# test_dataset = CustomTestDataset(input=test_df)\n",
    "# test_loader  = DataLoader(test_dataset, shuffle=False, num_workers=0)\n",
    "\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# test_pred = []\n",
    "# with torch.no_grad():\n",
    "#     for X in iter(test_loader):\n",
    "#         X = X.float().to(device)\n",
    "\n",
    "#         model_pred = model(X)\n",
    "#         # model_pred = torch.exp(model_pred)\n",
    "#         model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "\n",
    "#         test_pred += model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5189c-9130-4ab4-8d64-ffc051c7667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b36a1-ca35-499f-ac31-9e1de876df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted_weight_g'] = test_pred\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8c26e-2811-4d5e-9737-be9e9982741c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f1d1e-d7b4-430b-9293-a4b4d060d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GRUCell(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size,  3*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3*hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self) :\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters() : \n",
    "            w.data.uniform_(-std, std)\n",
    "            \n",
    "    def forward(self, x, hidden) : \n",
    "        x = x.view(-1, x.size(1))\n",
    "        \n",
    "        gate_x = self.x2h(x)\n",
    "        gate_h = self.h2h(hidden)\n",
    "        \n",
    "        gate_x = gate_x.squeeze()\n",
    "        gate_h = gate_h.squeeze()\n",
    "        \n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        \n",
    "        resetgate = F.sigmoid(i_r + h_r)\n",
    "        inputgate = F.sigmoid(i_i + h_i)\n",
    "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
    "        \n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "    \n",
    "class GRUModel(nn.Module) :\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True) :\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        if torch.cuda.is_available() :\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else :\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        outs = []\n",
    "        hn = h0[0, :, :]\n",
    "        \n",
    "        for seq in range(x.size(1)) : \n",
    "            hn = self.gru_cell(x[:, seq, :], hn)\n",
    "            outs.append(hn)\n",
    "        out = outs[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec6fe-a87d-4fe0-8f91-97136bd96e57",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cc3e9c6-5ca0-43cf-99d4-4826cedf9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,input,label,infer_mode,seq_length):\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        input = input.sort_values(['case_num','DAT'])\n",
    "        label = label.sort_values(['case_num','DAT'])\n",
    "\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        for i in range(int(label.shape[0]/seq_length)):\n",
    "            i_df = input.iloc[i*seq_length:(i+1)*seq_length,:].drop('case_num',axis=1)\n",
    "            l_df = label.iloc[i*seq_length:(i+1)*seq_length]['predicted_weight_g']\n",
    "            self.input_list.append(torch.Tensor(i_df.values))\n",
    "            self.label_list.append(torch.Tensor(l_df.values))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data  = self.input_list[index]\n",
    "        label = self.label_list[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1d32f07-3371-41ec-a567-7b8fecd3b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 2\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False, seq_length=seq_length)\n",
    "train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False, seq_length=seq_length)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True, seq_length=seq_length)\n",
    "test_loader  = DataLoader(test_dataset  , batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a976b98b-4c3e-4885-86b2-1d4ba0011c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5da276cf-00c9-431c-a324-b65165cc96ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "# [y for x,y in iter(train_loader)]\n",
    "sum([y.size(0) for x,y in iter(train_loader)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bed0cce1-1de6-445f-9e65-37c46e3eaadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x,y in iter(train_loader)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a12dff64-a9d8-43b2-b137-20fb0bf2308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([16, 2, 120]), torch.Size([16, 2])),\n",
       " (torch.Size([12, 2, 120]), torch.Size([12, 2]))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[0].size(),x[1].size()) for x in train_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19ff934e-3796-4a0b-9a87-8b1e605d0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        hidden  = [100, 100, 100, 100]\n",
    "        # dropout = [0.5, 0.5, 0.5, 0.5]\n",
    "        dropout = [0.2, 0.2, 0.2, 0.2]\n",
    "        num_layers = [1,1,1,1]\n",
    "        bidirectional = False\n",
    "        if bidirectional:\n",
    "            offset = 2\n",
    "        else:\n",
    "            offset = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden[0],\n",
    "            dropout=dropout[0],\n",
    "            num_layers=num_layers[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=offset*hidden[0],\n",
    "            hidden_size=hidden[1],\n",
    "            dropout=dropout[1],\n",
    "            num_layers=num_layers[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=offset*hidden[1],\n",
    "            hidden_size=hidden[2],\n",
    "            dropout=dropout[2],\n",
    "            num_layers=num_layers[2],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=offset*hidden[2],\n",
    "            hidden_size=hidden[3],\n",
    "            dropout=dropout[3],\n",
    "            num_layers=num_layers[3],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        \n",
    "        self.fc = nn.Linear(offset * hidden[3], 1)\n",
    "        self.fc = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st\n",
    "        x, _ = self.lstm1(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 2nd\n",
    "        x, _ = self.lstm2(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 3rd\n",
    "        x, _ = self.lstm3(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # 4th\n",
    "        x, _ = self.lstm4(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.leakyrelu(x)\n",
    "        # fully connected layer\n",
    "        x    = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a523935d-af75-45ce-976c-c78f41578e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        # print(x.shape,x_reshape.shape)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fcb61ab-2360-4805-8407-1005e3ca5b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update)\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                 kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update)\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x)\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd =SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "    \n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen):\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: \n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCINet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, num_decoder_layer = 1, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.num_decoder_layer = num_decoder_layer\n",
    "\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        self.div_projection = nn.ModuleList()\n",
    "        self.overlap_len = self.input_len//4\n",
    "        self.div_len = self.input_len//6\n",
    "\n",
    "        if self.num_decoder_layer > 1:\n",
    "            self.projection1 = nn.Linear(self.input_len, self.output_len)\n",
    "            for layer_idx in range(self.num_decoder_layer-1):\n",
    "                div_projection = nn.ModuleList()\n",
    "                for i in range(6):\n",
    "                    lens = min(i*self.div_len+self.overlap_len,self.input_len) - i*self.div_len\n",
    "                    div_projection.append(nn.Linear(lens, self.div_len))\n",
    "                self.div_projection.append(div_projection)\n",
    "\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x += pe[:, :, :-1]\n",
    "            else:\n",
    "                x += self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        if self.RIN:\n",
    "            print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            #mean\n",
    "            x = x - means\n",
    "            #var\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # affine\n",
    "            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x += res1\n",
    "        if self.num_decoder_layer == 1:\n",
    "            x = self.projection1(x)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    "            for div_projection in self.div_projection:\n",
    "                output = torch.zeros(x.shape,dtype=x.dtype).cuda()\n",
    "                for i, div_layer in enumerate(div_projection):\n",
    "                    div_x = x[:,:,i*self.div_len:min(i*self.div_len+self.overlap_len,self.input_len)]\n",
    "                    output[:,:,i*self.div_len:(i+1)*self.div_len] = div_layer(div_x)\n",
    "                x = output\n",
    "            x = self.projection1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            res2 = x\n",
    "            x = self.blocks2(x)\n",
    "            x += res2\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x, MidOutPut\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b693071-44da-46f4-a007-ccbfa2a9b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class SCINet_decompose(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet_decompose, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.decomp = series_decomp(25)\n",
    "        self.trend = nn.Linear(input_len,input_len)\n",
    "        self.trend_dec = nn.Linear(input_len,output_len)\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        x, trend = self.decomp(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # seq_means = x[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means = x[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # x = x - seq_means\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "            # print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means2 = trend.mean(1, keepdim=True).detach()\n",
    "            trend = trend - means2\n",
    "            stdev2 = torch.sqrt(torch.var(trend, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            trend /= stdev2\n",
    "            # seq_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # trend = trend - seq_means2 \n",
    "            trend = trend * self.affine_weight2 + self.affine_bias2\n",
    "        \n",
    "\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x = x + pe[:, :, :-1]\n",
    "            else:\n",
    "                x = x + self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        \n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x = self.projection1(x)\n",
    "\n",
    "        trend = trend.permute(0,2,1)\n",
    "        trend = self.trend(trend)  \n",
    "        trend = self.trend_dec(trend).permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                # x = x + pred_means\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            x = self.blocks2(x)\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend, MidOutPut\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e41047c2-5bff-4835-9c26-6f523fbbf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCINet_Model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(SCINet_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # 24,4,1,1,2,0.5,False,1,True,1\n",
    "        window_size = 1 # in (fixed)\n",
    "        horizon = 1      # out\n",
    "        hidden_size = 1\n",
    "        groups = 1\n",
    "        kernel = 1\n",
    "        dropout = 0.5\n",
    "        single_step_output_One = False\n",
    "        num_levels = 1\n",
    "        positionalEcoding = True\n",
    "        num_stacks = 1\n",
    "        self.scinet = SCINet(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        self.scinet_decompose = SCINet_decompose(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        \n",
    "        # hidden  = [64, 64, 64]\n",
    "        # dropout = [0.2, 0.5, 0.5]\n",
    "        # num_layers = [1,1,1]\n",
    "        # self.lstm1 = nn.LSTM(\n",
    "        #     input_size=input_size,\n",
    "        #     hidden_size=hidden[0],\n",
    "        #     dropout=dropout[0],\n",
    "        #     num_layers=num_layers[0],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm2 = nn.LSTM(\n",
    "        #     input_size=2*hidden[0],\n",
    "        #     hidden_size=hidden[1],\n",
    "        #     dropout=dropout[1],\n",
    "        #     num_layers=num_layers[1],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm3 = nn.LSTM(\n",
    "        #     input_size=2*hidden[1],\n",
    "        #     hidden_size=hidden[2],\n",
    "        #     dropout=dropout[2],\n",
    "        #     num_layers=num_layers[2],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.fc = nn.Linear(2*hidden[0], 1)\n",
    "        self.fc_1 = nn.Linear(input_size, 16)\n",
    "        self.fc_1 = TimeDistributed(self.fc_1)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        self.fc_2 = TimeDistributed(self.fc_2)\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.scinet(x)\n",
    "#         # x = self.bn(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm1(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm2(x)\n",
    "#         # x = self.selu(x)\n",
    "#         # x,_ = self.lstm3(x)\n",
    "#         # x = self.selu(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leakyrelu(x)\n",
    "#         x = self.fc_2(x[:,-1,:]) # [:,:,-1]\n",
    "        \n",
    "#         # x = self.fc_2(x[:,-1,:])\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.scinet(x)\n",
    "#         x = self.scinet_decompose(x)\n",
    "#         x1,x2 = x[0],x[1]\n",
    "#         x = torch.cat([x1,x2],dim=1)\n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.scinet(x)\n",
    "        x = self.scinet_decompose(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbcf73e7-3976-4026-a656-fa0818d2dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "# model = Model(input_size = input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7402fe51-d06e-4b6b-bfec-33ebb0db009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 120])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0].shape for x in train_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "830d0cba-75de-42d6-8648-b597d0c68f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71f221e0-702b-4346-b149-b9514bd809c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[0001/1024] tr_loss : 49.01098, val_loss : 32.74091, elapsed : 0.18s, total : 0.18s, remaining : 187.59s\n",
      "*[0002/1024] tr_loss : 48.99673, val_loss : 32.72286, elapsed : 0.16s, total : 0.35s, remaining : 165.64s\n",
      "*[0003/1024] tr_loss : 48.97692, val_loss : 32.69406, elapsed : 0.16s, total : 0.50s, remaining : 161.21s\n",
      "*[0004/1024] tr_loss : 48.94448, val_loss : 32.63502, elapsed : 0.16s, total : 0.66s, remaining : 158.35s\n",
      "*[0005/1024] tr_loss : 48.87850, val_loss : 32.47966, elapsed : 0.16s, total : 0.82s, remaining : 167.06s\n",
      "*[0006/1024] tr_loss : 48.70977, val_loss : 32.02017, elapsed : 0.16s, total : 0.98s, remaining : 164.27s\n",
      "*[0007/1024] tr_loss : 48.23020, val_loss : 30.98006, elapsed : 0.16s, total : 1.14s, remaining : 163.23s\n",
      "*[0008/1024] tr_loss : 47.20781, val_loss : 29.71524, elapsed : 0.16s, total : 1.30s, remaining : 158.53s\n",
      "*[0009/1024] tr_loss : 46.00685, val_loss : 28.84282, elapsed : 0.16s, total : 1.46s, remaining : 160.98s\n",
      "*[0010/1024] tr_loss : 45.10253, val_loss : 28.36180, elapsed : 0.16s, total : 1.62s, remaining : 163.74s\n",
      "*[0011/1024] tr_loss : 44.51655, val_loss : 28.09090, elapsed : 0.17s, total : 1.79s, remaining : 171.24s\n",
      "*[0012/1024] tr_loss : 44.13271, val_loss : 27.92151, elapsed : 0.16s, total : 1.95s, remaining : 160.14s\n",
      "*[0013/1024] tr_loss : 43.86232, val_loss : 27.80263, elapsed : 0.16s, total : 2.11s, remaining : 159.60s\n",
      "*[0014/1024] tr_loss : 43.65359, val_loss : 27.70885, elapsed : 0.15s, total : 2.26s, remaining : 155.82s\n",
      "*[0015/1024] tr_loss : 43.47309, val_loss : 27.62291, elapsed : 0.19s, total : 2.45s, remaining : 187.77s\n",
      "*[0016/1024] tr_loss : 43.29308, val_loss : 27.54546, elapsed : 0.19s, total : 2.63s, remaining : 187.97s\n",
      "*[0017/1024] tr_loss : 43.12575, val_loss : 27.48482, elapsed : 0.18s, total : 2.81s, remaining : 182.96s\n",
      "*[0018/1024] tr_loss : 42.98776, val_loss : 27.43709, elapsed : 0.18s, total : 2.99s, remaining : 178.95s\n",
      "*[0019/1024] tr_loss : 42.86916, val_loss : 27.39769, elapsed : 0.17s, total : 3.16s, remaining : 167.53s\n",
      "*[0020/1024] tr_loss : 42.76234, val_loss : 27.36414, elapsed : 0.18s, total : 3.34s, remaining : 178.99s\n",
      "*[0021/1024] tr_loss : 42.66362, val_loss : 27.33512, elapsed : 0.18s, total : 3.51s, remaining : 175.59s\n",
      "*[0022/1024] tr_loss : 42.57100, val_loss : 27.30981, elapsed : 0.17s, total : 3.68s, remaining : 172.13s\n",
      "*[0023/1024] tr_loss : 42.48324, val_loss : 27.28767, elapsed : 0.18s, total : 3.87s, remaining : 183.54s\n",
      "*[0024/1024] tr_loss : 42.39953, val_loss : 27.26828, elapsed : 0.18s, total : 4.04s, remaining : 175.24s\n",
      "*[0025/1024] tr_loss : 42.31928, val_loss : 27.25134, elapsed : 0.16s, total : 4.21s, remaining : 162.98s\n",
      "*[0026/1024] tr_loss : 42.24204, val_loss : 27.23660, elapsed : 0.16s, total : 4.37s, remaining : 162.25s\n",
      "*[0027/1024] tr_loss : 42.16745, val_loss : 27.22384, elapsed : 0.16s, total : 4.53s, remaining : 162.52s\n",
      "*[0028/1024] tr_loss : 42.09523, val_loss : 27.21290, elapsed : 0.16s, total : 4.69s, remaining : 156.51s\n",
      "*[0029/1024] tr_loss : 42.02517, val_loss : 27.20363, elapsed : 0.16s, total : 4.85s, remaining : 156.02s\n",
      "*[0030/1024] tr_loss : 41.95705, val_loss : 27.19589, elapsed : 0.16s, total : 5.01s, remaining : 160.22s\n",
      "*[0031/1024] tr_loss : 41.89074, val_loss : 27.18958, elapsed : 0.15s, total : 5.16s, remaining : 153.90s\n",
      "*[0032/1024] tr_loss : 41.82610, val_loss : 27.18459, elapsed : 0.16s, total : 5.32s, remaining : 156.41s\n",
      "*[0033/1024] tr_loss : 41.76300, val_loss : 27.18083, elapsed : 0.16s, total : 5.48s, remaining : 158.79s\n",
      "*[0034/1024] tr_loss : 41.70136, val_loss : 27.17823, elapsed : 0.16s, total : 5.64s, remaining : 153.60s\n",
      "*[0035/1024] tr_loss : 41.64108, val_loss : 27.17671, elapsed : 0.15s, total : 5.79s, remaining : 152.78s\n",
      "*[0036/1024] tr_loss : 41.58209, val_loss : 27.17619, elapsed : 0.15s, total : 5.94s, remaining : 151.78s\n",
      " [0037/1024] tr_loss : 41.52432, val_loss : 27.17663, elapsed : 0.15s, total : 6.10s, remaining : 152.47s\n",
      " [0038/1024] tr_loss : 41.46772, val_loss : 27.17797, elapsed : 0.15s, total : 6.25s, remaining : 152.72s\n",
      " [0039/1024] tr_loss : 41.41222, val_loss : 27.18015, elapsed : 0.16s, total : 6.41s, remaining : 153.01s\n",
      " [0040/1024] tr_loss : 41.35778, val_loss : 27.18314, elapsed : 0.15s, total : 6.56s, remaining : 149.02s\n",
      " [0041/1024] tr_loss : 41.30435, val_loss : 27.18687, elapsed : 0.15s, total : 6.71s, remaining : 149.30s\n",
      " [0042/1024] tr_loss : 41.25190, val_loss : 27.19131, elapsed : 0.15s, total : 6.87s, remaining : 150.85s\n",
      " [0043/1024] tr_loss : 41.20039, val_loss : 27.19643, elapsed : 0.16s, total : 7.02s, remaining : 154.11s\n",
      " [0044/1024] tr_loss : 41.14979, val_loss : 27.20218, elapsed : 0.15s, total : 7.18s, remaining : 149.90s\n",
      " [0045/1024] tr_loss : 41.10006, val_loss : 27.20854, elapsed : 0.16s, total : 7.34s, remaining : 159.16s\n",
      " [0046/1024] tr_loss : 41.05118, val_loss : 27.21547, elapsed : 0.15s, total : 7.49s, remaining : 150.90s\n",
      " [0047/1024] tr_loss : 41.00312, val_loss : 27.22294, elapsed : 0.16s, total : 7.65s, remaining : 156.62s\n",
      " [0048/1024] tr_loss : 40.95585, val_loss : 27.23092, elapsed : 0.15s, total : 7.81s, remaining : 148.55s\n",
      " [0049/1024] tr_loss : 40.90937, val_loss : 27.23940, elapsed : 0.15s, total : 7.96s, remaining : 148.16s\n",
      " [0050/1024] tr_loss : 40.86363, val_loss : 27.24833, elapsed : 0.15s, total : 8.11s, remaining : 148.71s\n",
      " [0051/1024] tr_loss : 40.81863, val_loss : 27.25771, elapsed : 0.16s, total : 8.27s, remaining : 151.24s\n",
      " [0052/1024] tr_loss : 40.77434, val_loss : 27.26750, elapsed : 0.16s, total : 8.42s, remaining : 154.05s\n",
      " [0053/1024] tr_loss : 40.73075, val_loss : 27.27769, elapsed : 0.15s, total : 8.58s, remaining : 150.50s\n",
      " [0054/1024] tr_loss : 40.68785, val_loss : 27.28826, elapsed : 0.16s, total : 8.74s, remaining : 151.51s\n",
      " [0055/1024] tr_loss : 40.64560, val_loss : 27.29919, elapsed : 0.15s, total : 8.89s, remaining : 149.34s\n",
      " [0056/1024] tr_loss : 40.60401, val_loss : 27.31045, elapsed : 0.17s, total : 9.06s, remaining : 164.25s\n",
      " [0057/1024] tr_loss : 40.56305, val_loss : 27.32204, elapsed : 0.17s, total : 9.23s, remaining : 164.51s\n",
      " [0058/1024] tr_loss : 40.52271, val_loss : 27.33394, elapsed : 0.16s, total : 9.39s, remaining : 157.86s\n",
      " [0059/1024] tr_loss : 40.48299, val_loss : 27.34613, elapsed : 0.15s, total : 9.54s, remaining : 146.53s\n",
      " [0060/1024] tr_loss : 40.44386, val_loss : 27.35860, elapsed : 0.15s, total : 9.70s, remaining : 147.84s\n",
      " [0061/1024] tr_loss : 40.40531, val_loss : 27.37133, elapsed : 0.17s, total : 9.87s, remaining : 162.88s\n",
      " [0062/1024] tr_loss : 40.36733, val_loss : 27.38432, elapsed : 0.17s, total : 10.04s, remaining : 163.64s\n",
      " [0063/1024] tr_loss : 40.32992, val_loss : 27.39754, elapsed : 0.17s, total : 10.21s, remaining : 166.42s\n",
      " [0064/1024] tr_loss : 40.29305, val_loss : 27.41098, elapsed : 0.16s, total : 10.37s, remaining : 154.51s\n",
      " [0065/1024] tr_loss : 40.25672, val_loss : 27.42465, elapsed : 0.18s, total : 10.56s, remaining : 176.07s\n",
      " [0066/1024] tr_loss : 40.22093, val_loss : 27.43851, elapsed : 0.17s, total : 10.73s, remaining : 165.59s\n",
      " [0067/1024] tr_loss : 40.18565, val_loss : 27.45257, elapsed : 0.16s, total : 10.89s, remaining : 155.19s\n",
      " [0068/1024] tr_loss : 40.15088, val_loss : 27.46682, elapsed : 0.15s, total : 11.04s, remaining : 145.65s\n",
      " [0069/1024] tr_loss : 40.11661, val_loss : 27.48123, elapsed : 0.15s, total : 11.20s, remaining : 147.36s\n",
      " [0070/1024] tr_loss : 40.08283, val_loss : 27.49581, elapsed : 0.16s, total : 11.36s, remaining : 153.72s\n",
      " [0071/1024] tr_loss : 40.04953, val_loss : 27.51055, elapsed : 0.16s, total : 11.52s, remaining : 151.97s\n",
      " [0072/1024] tr_loss : 40.01671, val_loss : 27.52543, elapsed : 0.15s, total : 11.67s, remaining : 145.94s\n",
      " [0073/1024] tr_loss : 39.98434, val_loss : 27.54045, elapsed : 0.15s, total : 11.83s, remaining : 146.92s\n",
      " [0074/1024] tr_loss : 39.95243, val_loss : 27.55560, elapsed : 0.15s, total : 11.98s, remaining : 145.14s\n",
      " [0075/1024] tr_loss : 39.92096, val_loss : 27.57088, elapsed : 0.16s, total : 12.13s, remaining : 147.43s\n",
      " [0076/1024] tr_loss : 39.88992, val_loss : 27.58627, elapsed : 0.15s, total : 12.29s, remaining : 145.30s\n",
      " [0077/1024] tr_loss : 39.85931, val_loss : 27.60177, elapsed : 0.15s, total : 12.44s, remaining : 145.48s\n",
      " [0078/1024] tr_loss : 39.82911, val_loss : 27.61737, elapsed : 0.15s, total : 12.59s, remaining : 144.04s\n",
      " [0079/1024] tr_loss : 39.79931, val_loss : 27.63307, elapsed : 0.16s, total : 12.75s, remaining : 147.45s\n",
      " [0080/1024] tr_loss : 39.76991, val_loss : 27.64885, elapsed : 0.15s, total : 12.90s, remaining : 145.10s\n",
      " [0081/1024] tr_loss : 39.74089, val_loss : 27.66472, elapsed : 0.16s, total : 13.06s, remaining : 146.63s\n",
      " [0082/1024] tr_loss : 39.71223, val_loss : 27.68066, elapsed : 0.15s, total : 13.21s, remaining : 143.08s\n",
      " [0083/1024] tr_loss : 39.68392, val_loss : 27.69666, elapsed : 0.15s, total : 13.36s, remaining : 144.36s\n",
      " [0084/1024] tr_loss : 39.65594, val_loss : 27.71273, elapsed : 0.15s, total : 13.52s, remaining : 142.75s\n",
      " [0085/1024] tr_loss : 39.62827, val_loss : 27.72886, elapsed : 0.19s, total : 13.71s, remaining : 181.70s\n",
      " [0086/1024] tr_loss : 39.60088, val_loss : 27.74502, elapsed : 0.16s, total : 13.86s, remaining : 146.04s\n",
      " [0087/1024] tr_loss : 39.57375, val_loss : 27.76123, elapsed : 0.19s, total : 14.06s, remaining : 178.96s\n",
      " [0088/1024] tr_loss : 39.54682, val_loss : 27.77746, elapsed : 0.16s, total : 14.21s, remaining : 148.68s\n",
      " [0089/1024] tr_loss : 39.52004, val_loss : 27.79370, elapsed : 0.15s, total : 14.37s, remaining : 144.65s\n",
      " [0090/1024] tr_loss : 39.49332, val_loss : 27.80993, elapsed : 0.15s, total : 14.52s, remaining : 141.58s\n",
      " [0091/1024] tr_loss : 39.46655, val_loss : 27.82613, elapsed : 0.15s, total : 14.68s, remaining : 143.70s\n",
      " [0092/1024] tr_loss : 39.43953, val_loss : 27.84226, elapsed : 0.15s, total : 14.83s, remaining : 141.44s\n",
      " [0093/1024] tr_loss : 39.41196, val_loss : 27.85825, elapsed : 0.15s, total : 14.98s, remaining : 144.30s\n",
      " [0094/1024] tr_loss : 39.38328, val_loss : 27.87399, elapsed : 0.15s, total : 15.13s, remaining : 141.73s\n",
      " [0095/1024] tr_loss : 39.35245, val_loss : 27.88924, elapsed : 0.16s, total : 15.29s, remaining : 146.32s\n",
      " [0096/1024] tr_loss : 39.31716, val_loss : 27.90338, elapsed : 0.16s, total : 15.45s, remaining : 145.83s\n",
      " [0097/1024] tr_loss : 39.27011, val_loss : 27.91402, elapsed : 0.16s, total : 15.61s, remaining : 146.71s\n",
      " [0098/1024] tr_loss : 39.17341, val_loss : 27.90715, elapsed : 0.15s, total : 15.76s, remaining : 140.57s\n",
      " [0099/1024] tr_loss : 38.96815, val_loss : 27.86404, elapsed : 0.16s, total : 15.91s, remaining : 143.61s\n",
      " [0100/1024] tr_loss : 38.74357, val_loss : 27.84473, elapsed : 0.15s, total : 16.07s, remaining : 140.77s\n",
      " [0101/1024] tr_loss : 38.49326, val_loss : 27.79247, elapsed : 0.16s, total : 16.22s, remaining : 144.94s\n",
      " [0102/1024] tr_loss : 38.10466, val_loss : 27.70938, elapsed : 0.15s, total : 16.38s, remaining : 140.57s\n",
      " [0103/1024] tr_loss : 37.79690, val_loss : 27.64078, elapsed : 0.16s, total : 16.53s, remaining : 144.26s\n",
      " [0104/1024] tr_loss : 37.62778, val_loss : 27.58301, elapsed : 0.15s, total : 16.69s, remaining : 141.18s\n",
      " [0105/1024] tr_loss : 37.50343, val_loss : 27.49380, elapsed : 0.15s, total : 16.84s, remaining : 142.17s\n",
      " [0106/1024] tr_loss : 37.39325, val_loss : 27.32860, elapsed : 0.15s, total : 17.00s, remaining : 142.28s\n",
      "*[0107/1024] tr_loss : 37.29082, val_loss : 27.05825, elapsed : 0.17s, total : 17.16s, remaining : 153.82s\n",
      "*[0108/1024] tr_loss : 37.19581, val_loss : 26.75338, elapsed : 0.16s, total : 17.33s, remaining : 147.97s\n",
      "*[0109/1024] tr_loss : 37.10862, val_loss : 26.52583, elapsed : 0.16s, total : 17.48s, remaining : 145.41s\n",
      "*[0110/1024] tr_loss : 37.02527, val_loss : 26.36287, elapsed : 0.16s, total : 17.64s, remaining : 142.93s\n",
      "*[0111/1024] tr_loss : 36.94324, val_loss : 26.20602, elapsed : 0.16s, total : 17.80s, remaining : 146.30s\n",
      "*[0112/1024] tr_loss : 36.86238, val_loss : 26.04435, elapsed : 0.15s, total : 17.96s, remaining : 141.06s\n",
      "*[0113/1024] tr_loss : 36.78271, val_loss : 25.90200, elapsed : 0.16s, total : 18.12s, remaining : 145.07s\n",
      "*[0114/1024] tr_loss : 36.70476, val_loss : 25.77284, elapsed : 0.16s, total : 18.27s, remaining : 141.13s\n",
      "*[0115/1024] tr_loss : 36.62833, val_loss : 25.65971, elapsed : 0.16s, total : 18.43s, remaining : 144.70s\n",
      "*[0116/1024] tr_loss : 36.55325, val_loss : 25.55784, elapsed : 0.16s, total : 18.59s, remaining : 141.49s\n",
      "*[0117/1024] tr_loss : 36.47928, val_loss : 25.46949, elapsed : 0.16s, total : 18.74s, remaining : 143.54s\n",
      "*[0118/1024] tr_loss : 36.40638, val_loss : 25.38667, elapsed : 0.16s, total : 18.90s, remaining : 141.33s\n",
      "*[0119/1024] tr_loss : 36.33441, val_loss : 25.30172, elapsed : 0.16s, total : 19.06s, remaining : 144.39s\n",
      "*[0120/1024] tr_loss : 36.26323, val_loss : 25.21800, elapsed : 0.16s, total : 19.21s, remaining : 140.43s\n",
      "*[0121/1024] tr_loss : 36.19280, val_loss : 25.12926, elapsed : 0.16s, total : 19.37s, remaining : 143.16s\n",
      "*[0122/1024] tr_loss : 36.12309, val_loss : 25.03952, elapsed : 0.15s, total : 19.53s, remaining : 139.23s\n",
      "*[0123/1024] tr_loss : 36.05386, val_loss : 24.94472, elapsed : 0.16s, total : 19.69s, remaining : 142.77s\n",
      "*[0124/1024] tr_loss : 35.98532, val_loss : 24.85309, elapsed : 0.16s, total : 19.85s, remaining : 146.09s\n",
      "*[0125/1024] tr_loss : 35.91733, val_loss : 24.76445, elapsed : 0.16s, total : 20.01s, remaining : 145.25s\n",
      "*[0126/1024] tr_loss : 35.84985, val_loss : 24.67714, elapsed : 0.16s, total : 20.17s, remaining : 144.91s\n",
      "*[0127/1024] tr_loss : 35.78279, val_loss : 24.59265, elapsed : 0.16s, total : 20.33s, remaining : 145.57s\n",
      "*[0128/1024] tr_loss : 35.71627, val_loss : 24.51186, elapsed : 0.16s, total : 20.49s, remaining : 138.91s\n",
      "*[0129/1024] tr_loss : 35.65002, val_loss : 24.42956, elapsed : 0.16s, total : 20.65s, remaining : 141.90s\n",
      "*[0130/1024] tr_loss : 35.58416, val_loss : 24.34900, elapsed : 0.16s, total : 20.81s, remaining : 140.91s\n",
      "*[0131/1024] tr_loss : 35.51875, val_loss : 24.27474, elapsed : 0.16s, total : 20.96s, remaining : 141.29s\n",
      "*[0132/1024] tr_loss : 35.45346, val_loss : 24.19867, elapsed : 0.16s, total : 21.12s, remaining : 142.89s\n",
      "*[0133/1024] tr_loss : 35.38855, val_loss : 24.12644, elapsed : 0.17s, total : 21.29s, remaining : 147.92s\n",
      "*[0134/1024] tr_loss : 35.32423, val_loss : 24.05751, elapsed : 0.15s, total : 21.44s, remaining : 137.63s\n",
      "*[0135/1024] tr_loss : 35.26007, val_loss : 23.98731, elapsed : 0.16s, total : 21.60s, remaining : 140.40s\n",
      "*[0136/1024] tr_loss : 35.19623, val_loss : 23.92229, elapsed : 0.15s, total : 21.76s, remaining : 137.37s\n",
      "*[0137/1024] tr_loss : 35.13258, val_loss : 23.86038, elapsed : 0.16s, total : 21.91s, remaining : 139.84s\n",
      "*[0138/1024] tr_loss : 35.06926, val_loss : 23.80471, elapsed : 0.16s, total : 22.08s, remaining : 142.60s\n",
      "*[0139/1024] tr_loss : 35.00624, val_loss : 23.75215, elapsed : 0.16s, total : 22.23s, remaining : 140.54s\n",
      "*[0140/1024] tr_loss : 34.94345, val_loss : 23.69870, elapsed : 0.16s, total : 22.39s, remaining : 138.26s\n",
      "*[0141/1024] tr_loss : 34.88081, val_loss : 23.64587, elapsed : 0.16s, total : 22.55s, remaining : 141.05s\n",
      "*[0142/1024] tr_loss : 34.81829, val_loss : 23.59470, elapsed : 0.16s, total : 22.71s, remaining : 137.03s\n",
      "*[0143/1024] tr_loss : 34.75591, val_loss : 23.54086, elapsed : 0.16s, total : 22.86s, remaining : 138.78s\n",
      "*[0144/1024] tr_loss : 34.69353, val_loss : 23.48225, elapsed : 0.16s, total : 23.02s, remaining : 136.87s\n",
      "*[0145/1024] tr_loss : 34.63141, val_loss : 23.42480, elapsed : 0.16s, total : 23.18s, remaining : 137.78s\n",
      "*[0146/1024] tr_loss : 34.56935, val_loss : 23.35930, elapsed : 0.15s, total : 23.33s, remaining : 135.59s\n",
      "*[0147/1024] tr_loss : 34.50758, val_loss : 23.29702, elapsed : 0.16s, total : 23.49s, remaining : 138.77s\n",
      "*[0148/1024] tr_loss : 34.44594, val_loss : 23.23044, elapsed : 0.16s, total : 23.64s, remaining : 136.65s\n",
      "*[0149/1024] tr_loss : 34.38451, val_loss : 23.16186, elapsed : 0.17s, total : 23.81s, remaining : 145.39s\n",
      "*[0150/1024] tr_loss : 34.32321, val_loss : 23.09079, elapsed : 0.16s, total : 23.97s, remaining : 137.24s\n",
      "*[0151/1024] tr_loss : 34.26212, val_loss : 23.02142, elapsed : 0.16s, total : 24.13s, remaining : 140.81s\n",
      "*[0152/1024] tr_loss : 34.20113, val_loss : 22.94789, elapsed : 0.16s, total : 24.29s, remaining : 136.78s\n",
      "*[0153/1024] tr_loss : 34.14058, val_loss : 22.87868, elapsed : 0.16s, total : 24.45s, remaining : 138.44s\n",
      "*[0154/1024] tr_loss : 34.08016, val_loss : 22.80700, elapsed : 0.15s, total : 24.60s, remaining : 133.99s\n",
      "*[0155/1024] tr_loss : 34.02009, val_loss : 22.74624, elapsed : 0.16s, total : 24.76s, remaining : 137.24s\n",
      "*[0156/1024] tr_loss : 33.96011, val_loss : 22.68257, elapsed : 0.15s, total : 24.91s, remaining : 133.56s\n",
      "*[0157/1024] tr_loss : 33.90051, val_loss : 22.62259, elapsed : 0.16s, total : 25.07s, remaining : 138.37s\n",
      "*[0158/1024] tr_loss : 33.84124, val_loss : 22.56719, elapsed : 0.15s, total : 25.22s, remaining : 133.50s\n",
      "*[0159/1024] tr_loss : 33.78192, val_loss : 22.51847, elapsed : 0.16s, total : 25.39s, remaining : 139.63s\n",
      "*[0160/1024] tr_loss : 33.72275, val_loss : 22.47184, elapsed : 0.17s, total : 25.55s, remaining : 144.25s\n",
      "*[0161/1024] tr_loss : 33.66407, val_loss : 22.42827, elapsed : 0.16s, total : 25.72s, remaining : 140.22s\n",
      "*[0162/1024] tr_loss : 33.60507, val_loss : 22.38915, elapsed : 0.17s, total : 25.88s, remaining : 143.93s\n",
      "*[0163/1024] tr_loss : 33.54652, val_loss : 22.34934, elapsed : 0.16s, total : 26.04s, remaining : 137.49s\n",
      "*[0164/1024] tr_loss : 33.48814, val_loss : 22.31061, elapsed : 0.16s, total : 26.20s, remaining : 134.87s\n",
      "*[0165/1024] tr_loss : 33.42986, val_loss : 22.27506, elapsed : 0.16s, total : 26.36s, remaining : 135.13s\n",
      "*[0166/1024] tr_loss : 33.37153, val_loss : 22.24297, elapsed : 0.16s, total : 26.51s, remaining : 133.10s\n",
      "*[0167/1024] tr_loss : 33.31364, val_loss : 22.21027, elapsed : 0.16s, total : 26.67s, remaining : 136.30s\n",
      "*[0168/1024] tr_loss : 33.25594, val_loss : 22.18115, elapsed : 0.15s, total : 26.82s, remaining : 131.22s\n",
      "*[0169/1024] tr_loss : 33.19789, val_loss : 22.14684, elapsed : 0.16s, total : 26.98s, remaining : 135.38s\n",
      "*[0170/1024] tr_loss : 33.14046, val_loss : 22.11634, elapsed : 0.16s, total : 27.14s, remaining : 132.77s\n",
      "*[0171/1024] tr_loss : 33.08313, val_loss : 22.08358, elapsed : 0.16s, total : 27.30s, remaining : 137.60s\n",
      "*[0172/1024] tr_loss : 33.02621, val_loss : 22.04826, elapsed : 0.16s, total : 27.46s, remaining : 134.67s\n",
      "*[0173/1024] tr_loss : 32.96913, val_loss : 22.01227, elapsed : 0.16s, total : 27.62s, remaining : 135.44s\n",
      "*[0174/1024] tr_loss : 32.91259, val_loss : 21.98194, elapsed : 0.16s, total : 27.77s, remaining : 133.22s\n",
      "*[0175/1024] tr_loss : 32.85582, val_loss : 21.95144, elapsed : 0.16s, total : 27.93s, remaining : 133.19s\n",
      "*[0176/1024] tr_loss : 32.79890, val_loss : 21.91526, elapsed : 0.15s, total : 28.09s, remaining : 131.39s\n",
      "*[0177/1024] tr_loss : 32.74236, val_loss : 21.87827, elapsed : 0.16s, total : 28.25s, remaining : 136.33s\n",
      "*[0178/1024] tr_loss : 32.68556, val_loss : 21.84619, elapsed : 0.16s, total : 28.40s, remaining : 131.15s\n",
      "*[0179/1024] tr_loss : 32.62932, val_loss : 21.81087, elapsed : 0.16s, total : 28.56s, remaining : 133.37s\n",
      "*[0180/1024] tr_loss : 32.57282, val_loss : 21.76999, elapsed : 0.15s, total : 28.71s, remaining : 130.77s\n",
      "*[0181/1024] tr_loss : 32.51720, val_loss : 21.73628, elapsed : 0.16s, total : 28.87s, remaining : 132.82s\n",
      "*[0182/1024] tr_loss : 32.46031, val_loss : 21.70159, elapsed : 0.16s, total : 29.03s, remaining : 130.65s\n",
      "*[0183/1024] tr_loss : 32.40526, val_loss : 21.66941, elapsed : 0.16s, total : 29.19s, remaining : 135.50s\n",
      "*[0184/1024] tr_loss : 32.34871, val_loss : 21.63465, elapsed : 0.16s, total : 29.35s, remaining : 133.38s\n",
      "*[0185/1024] tr_loss : 32.29263, val_loss : 21.60639, elapsed : 0.16s, total : 29.51s, remaining : 132.96s\n",
      "*[0186/1024] tr_loss : 32.23591, val_loss : 21.57187, elapsed : 0.16s, total : 29.67s, remaining : 135.28s\n",
      "*[0187/1024] tr_loss : 32.17993, val_loss : 21.53633, elapsed : 0.16s, total : 29.83s, remaining : 134.16s\n",
      "*[0188/1024] tr_loss : 32.12264, val_loss : 21.49522, elapsed : 0.16s, total : 29.99s, remaining : 133.00s\n",
      "*[0189/1024] tr_loss : 32.06802, val_loss : 21.46321, elapsed : 0.16s, total : 30.15s, remaining : 132.33s\n",
      "*[0190/1024] tr_loss : 32.01070, val_loss : 21.42796, elapsed : 0.16s, total : 30.30s, remaining : 131.71s\n",
      "*[0191/1024] tr_loss : 31.95398, val_loss : 21.38895, elapsed : 0.16s, total : 30.46s, remaining : 130.91s\n",
      "*[0192/1024] tr_loss : 31.89623, val_loss : 21.34491, elapsed : 0.16s, total : 30.62s, remaining : 129.05s\n",
      "*[0193/1024] tr_loss : 31.83973, val_loss : 21.29238, elapsed : 0.16s, total : 30.77s, remaining : 132.17s\n",
      "*[0194/1024] tr_loss : 31.78117, val_loss : 21.24052, elapsed : 0.16s, total : 30.93s, remaining : 132.42s\n",
      "*[0195/1024] tr_loss : 31.72203, val_loss : 21.19439, elapsed : 0.16s, total : 31.09s, remaining : 130.25s\n",
      "*[0196/1024] tr_loss : 31.66514, val_loss : 21.14370, elapsed : 0.16s, total : 31.25s, remaining : 129.28s\n",
      "*[0197/1024] tr_loss : 31.60295, val_loss : 21.07900, elapsed : 0.16s, total : 31.41s, remaining : 132.05s\n",
      "*[0198/1024] tr_loss : 31.54309, val_loss : 21.01738, elapsed : 0.16s, total : 31.56s, remaining : 129.46s\n",
      "*[0199/1024] tr_loss : 31.48299, val_loss : 20.95773, elapsed : 0.16s, total : 31.73s, remaining : 134.14s\n",
      "*[0200/1024] tr_loss : 31.42154, val_loss : 20.88214, elapsed : 0.17s, total : 31.89s, remaining : 137.04s\n",
      "*[0201/1024] tr_loss : 31.36161, val_loss : 20.80890, elapsed : 0.16s, total : 32.06s, remaining : 134.32s\n",
      "*[0202/1024] tr_loss : 31.30374, val_loss : 20.75461, elapsed : 0.17s, total : 32.23s, remaining : 140.23s\n",
      "*[0203/1024] tr_loss : 31.24445, val_loss : 20.69744, elapsed : 0.16s, total : 32.39s, remaining : 134.30s\n",
      "*[0204/1024] tr_loss : 31.18565, val_loss : 20.64380, elapsed : 0.16s, total : 32.55s, remaining : 129.95s\n",
      "*[0205/1024] tr_loss : 31.12697, val_loss : 20.61266, elapsed : 0.17s, total : 32.72s, remaining : 138.24s\n",
      "*[0206/1024] tr_loss : 31.06660, val_loss : 20.57978, elapsed : 0.16s, total : 32.87s, remaining : 126.89s\n",
      "*[0207/1024] tr_loss : 31.00803, val_loss : 20.53314, elapsed : 0.16s, total : 33.03s, remaining : 129.50s\n",
      "*[0208/1024] tr_loss : 30.95267, val_loss : 20.50785, elapsed : 0.16s, total : 33.19s, remaining : 130.89s\n",
      " [0209/1024] tr_loss : 30.89299, val_loss : 20.50902, elapsed : 0.17s, total : 33.36s, remaining : 135.43s\n",
      "*[0210/1024] tr_loss : 30.83212, val_loss : 20.45157, elapsed : 0.15s, total : 33.51s, remaining : 125.02s\n",
      "*[0211/1024] tr_loss : 30.77618, val_loss : 20.40154, elapsed : 0.16s, total : 33.67s, remaining : 131.42s\n",
      " [0212/1024] tr_loss : 30.72402, val_loss : 20.42934, elapsed : 0.16s, total : 33.83s, remaining : 127.25s\n",
      " [0213/1024] tr_loss : 30.66370, val_loss : 20.49892, elapsed : 0.16s, total : 33.99s, remaining : 129.14s\n",
      " [0214/1024] tr_loss : 30.59772, val_loss : 20.52689, elapsed : 0.16s, total : 34.15s, remaining : 127.73s\n",
      "*[0215/1024] tr_loss : 30.53616, val_loss : 20.33171, elapsed : 0.16s, total : 34.31s, remaining : 131.60s\n",
      "*[0216/1024] tr_loss : 30.49397, val_loss : 20.31609, elapsed : 0.16s, total : 34.47s, remaining : 126.57s\n",
      " [0217/1024] tr_loss : 30.45601, val_loss : 20.51118, elapsed : 0.16s, total : 34.63s, remaining : 129.97s\n",
      " [0218/1024] tr_loss : 30.37109, val_loss : 20.73068, elapsed : 0.15s, total : 34.78s, remaining : 123.48s\n",
      " [0219/1024] tr_loss : 30.31495, val_loss : 20.39125, elapsed : 0.16s, total : 34.94s, remaining : 124.93s\n",
      "*[0220/1024] tr_loss : 30.25902, val_loss : 20.04643, elapsed : 0.15s, total : 35.09s, remaining : 122.35s\n",
      " [0221/1024] tr_loss : 30.30295, val_loss : 20.51560, elapsed : 0.16s, total : 35.25s, remaining : 126.63s\n",
      " [0222/1024] tr_loss : 30.14630, val_loss : 20.63951, elapsed : 0.15s, total : 35.40s, remaining : 124.09s\n",
      "*[0223/1024] tr_loss : 30.08886, val_loss : 19.99027, elapsed : 0.16s, total : 35.56s, remaining : 126.13s\n",
      "*[0224/1024] tr_loss : 30.03264, val_loss : 19.93699, elapsed : 0.16s, total : 35.71s, remaining : 125.24s\n",
      " [0225/1024] tr_loss : 30.08990, val_loss : 20.50117, elapsed : 0.16s, total : 35.87s, remaining : 127.64s\n",
      " [0226/1024] tr_loss : 29.93749, val_loss : 20.47064, elapsed : 0.16s, total : 36.03s, remaining : 124.13s\n",
      "*[0227/1024] tr_loss : 29.87640, val_loss : 19.75856, elapsed : 0.16s, total : 36.19s, remaining : 125.93s\n",
      " [0228/1024] tr_loss : 29.83565, val_loss : 19.84428, elapsed : 0.16s, total : 36.34s, remaining : 123.57s\n",
      " [0229/1024] tr_loss : 29.81905, val_loss : 20.17154, elapsed : 0.16s, total : 36.50s, remaining : 124.60s\n",
      " [0230/1024] tr_loss : 29.69464, val_loss : 20.18703, elapsed : 0.16s, total : 36.66s, remaining : 129.14s\n",
      "*[0231/1024] tr_loss : 29.64680, val_loss : 19.69130, elapsed : 0.16s, total : 36.82s, remaining : 124.53s\n",
      "*[0232/1024] tr_loss : 29.58946, val_loss : 19.68247, elapsed : 0.16s, total : 36.98s, remaining : 124.43s\n",
      " [0233/1024] tr_loss : 29.63701, val_loss : 20.09092, elapsed : 0.16s, total : 37.14s, remaining : 126.23s\n",
      " [0234/1024] tr_loss : 29.49147, val_loss : 20.16876, elapsed : 0.15s, total : 37.29s, remaining : 121.69s\n",
      "*[0235/1024] tr_loss : 29.45753, val_loss : 19.53418, elapsed : 0.16s, total : 37.45s, remaining : 123.47s\n",
      " [0236/1024] tr_loss : 29.41274, val_loss : 19.62741, elapsed : 0.16s, total : 37.60s, remaining : 124.05s\n",
      " [0237/1024] tr_loss : 29.37896, val_loss : 19.88363, elapsed : 0.16s, total : 37.76s, remaining : 125.14s\n",
      " [0238/1024] tr_loss : 29.26441, val_loss : 19.96343, elapsed : 0.16s, total : 37.92s, remaining : 122.34s\n",
      "*[0239/1024] tr_loss : 29.22789, val_loss : 19.47369, elapsed : 0.16s, total : 38.08s, remaining : 125.63s\n",
      " [0240/1024] tr_loss : 29.16918, val_loss : 19.47824, elapsed : 0.16s, total : 38.24s, remaining : 124.64s\n",
      " [0241/1024] tr_loss : 29.22535, val_loss : 19.95419, elapsed : 0.16s, total : 38.40s, remaining : 124.31s\n",
      " [0242/1024] tr_loss : 29.07955, val_loss : 19.96777, elapsed : 0.15s, total : 38.55s, remaining : 119.31s\n",
      "*[0243/1024] tr_loss : 29.04834, val_loss : 19.38333, elapsed : 0.16s, total : 38.71s, remaining : 122.03s\n",
      " [0244/1024] tr_loss : 29.04552, val_loss : 19.56470, elapsed : 0.15s, total : 38.86s, remaining : 120.34s\n",
      " [0245/1024] tr_loss : 28.92421, val_loss : 19.61095, elapsed : 0.16s, total : 39.02s, remaining : 122.72s\n",
      " [0246/1024] tr_loss : 28.84555, val_loss : 19.69450, elapsed : 0.16s, total : 39.18s, remaining : 126.93s\n",
      " [0247/1024] tr_loss : 28.79720, val_loss : 19.50321, elapsed : 0.16s, total : 39.34s, remaining : 121.15s\n",
      "*[0248/1024] tr_loss : 28.75622, val_loss : 19.31694, elapsed : 0.19s, total : 39.52s, remaining : 145.03s\n",
      " [0249/1024] tr_loss : 28.80940, val_loss : 19.56406, elapsed : 0.16s, total : 39.68s, remaining : 124.55s\n",
      " [0250/1024] tr_loss : 28.65188, val_loss : 19.65614, elapsed : 0.15s, total : 39.84s, remaining : 118.05s\n",
      " [0251/1024] tr_loss : 28.59140, val_loss : 19.55255, elapsed : 0.16s, total : 39.99s, remaining : 121.17s\n",
      "*[0252/1024] tr_loss : 28.55650, val_loss : 19.24125, elapsed : 0.16s, total : 40.15s, remaining : 123.82s\n",
      " [0253/1024] tr_loss : 28.57848, val_loss : 19.32456, elapsed : 0.16s, total : 40.32s, remaining : 127.09s\n",
      " [0254/1024] tr_loss : 28.50489, val_loss : 19.68221, elapsed : 0.15s, total : 40.47s, remaining : 117.74s\n",
      " [0255/1024] tr_loss : 28.39849, val_loss : 19.88463, elapsed : 0.16s, total : 40.63s, remaining : 121.22s\n",
      "*[0256/1024] tr_loss : 28.41026, val_loss : 19.16282, elapsed : 0.15s, total : 40.78s, remaining : 118.10s\n",
      " [0257/1024] tr_loss : 28.34459, val_loss : 19.23671, elapsed : 0.16s, total : 40.95s, remaining : 124.54s\n",
      " [0258/1024] tr_loss : 28.43643, val_loss : 20.01350, elapsed : 0.16s, total : 41.10s, remaining : 118.81s\n",
      " [0259/1024] tr_loss : 28.25622, val_loss : 19.43613, elapsed : 0.16s, total : 41.26s, remaining : 119.86s\n",
      "*[0260/1024] tr_loss : 28.17290, val_loss : 19.15641, elapsed : 0.15s, total : 41.41s, remaining : 117.56s\n",
      " [0261/1024] tr_loss : 28.27484, val_loss : 19.74338, elapsed : 0.16s, total : 41.57s, remaining : 120.99s\n",
      " [0262/1024] tr_loss : 28.10220, val_loss : 19.33421, elapsed : 0.15s, total : 41.73s, remaining : 117.81s\n",
      "*[0263/1024] tr_loss : 28.01561, val_loss : 19.10716, elapsed : 0.16s, total : 41.89s, remaining : 121.80s\n",
      " [0264/1024] tr_loss : 28.12837, val_loss : 19.64020, elapsed : 0.16s, total : 42.04s, remaining : 118.51s\n",
      " [0265/1024] tr_loss : 27.96938, val_loss : 19.25210, elapsed : 0.16s, total : 42.20s, remaining : 119.55s\n",
      "*[0266/1024] tr_loss : 27.86897, val_loss : 19.04990, elapsed : 0.15s, total : 42.35s, remaining : 116.92s\n",
      " [0267/1024] tr_loss : 27.98072, val_loss : 19.57890, elapsed : 0.16s, total : 42.51s, remaining : 120.93s\n",
      " [0268/1024] tr_loss : 27.84046, val_loss : 19.13677, elapsed : 0.16s, total : 42.67s, remaining : 118.34s\n",
      "*[0269/1024] tr_loss : 27.72563, val_loss : 18.99839, elapsed : 0.16s, total : 42.83s, remaining : 122.93s\n",
      " [0270/1024] tr_loss : 27.82113, val_loss : 19.54389, elapsed : 0.16s, total : 42.99s, remaining : 116.90s\n",
      " [0271/1024] tr_loss : 27.70637, val_loss : 19.08712, elapsed : 0.16s, total : 43.15s, remaining : 119.69s\n",
      "*[0272/1024] tr_loss : 27.58377, val_loss : 18.95012, elapsed : 0.15s, total : 43.30s, remaining : 114.84s\n",
      " [0273/1024] tr_loss : 27.66074, val_loss : 19.44019, elapsed : 0.20s, total : 43.50s, remaining : 150.04s\n",
      " [0274/1024] tr_loss : 27.56305, val_loss : 19.10515, elapsed : 0.16s, total : 43.66s, remaining : 119.64s\n",
      "*[0275/1024] tr_loss : 27.42980, val_loss : 18.88757, elapsed : 0.16s, total : 43.82s, remaining : 121.81s\n",
      " [0276/1024] tr_loss : 27.47934, val_loss : 19.16582, elapsed : 0.16s, total : 43.98s, remaining : 117.45s\n",
      " [0277/1024] tr_loss : 27.36314, val_loss : 19.25911, elapsed : 0.16s, total : 44.14s, remaining : 120.06s\n",
      "*[0278/1024] tr_loss : 27.31534, val_loss : 18.83899, elapsed : 0.15s, total : 44.29s, remaining : 114.62s\n",
      " [0279/1024] tr_loss : 27.27564, val_loss : 18.86146, elapsed : 0.16s, total : 44.45s, remaining : 118.62s\n",
      " [0280/1024] tr_loss : 27.31419, val_loss : 19.36098, elapsed : 0.16s, total : 44.61s, remaining : 115.99s\n",
      " [0281/1024] tr_loss : 27.21594, val_loss : 19.13038, elapsed : 0.16s, total : 44.77s, remaining : 116.89s\n",
      "*[0282/1024] tr_loss : 27.09989, val_loss : 18.82219, elapsed : 0.15s, total : 44.92s, remaining : 113.52s\n",
      " [0283/1024] tr_loss : 27.12378, val_loss : 18.92934, elapsed : 0.16s, total : 45.08s, remaining : 117.30s\n",
      " [0284/1024] tr_loss : 27.03264, val_loss : 19.21172, elapsed : 0.15s, total : 45.23s, remaining : 113.75s\n",
      " [0285/1024] tr_loss : 26.97324, val_loss : 19.14112, elapsed : 0.16s, total : 45.39s, remaining : 116.87s\n",
      "*[0286/1024] tr_loss : 26.93034, val_loss : 18.81207, elapsed : 0.15s, total : 45.54s, remaining : 113.40s\n",
      "*[0287/1024] tr_loss : 26.89437, val_loss : 18.79262, elapsed : 0.16s, total : 45.70s, remaining : 117.20s\n",
      " [0288/1024] tr_loss : 26.97347, val_loss : 19.38439, elapsed : 0.16s, total : 45.86s, remaining : 114.16s\n",
      " [0289/1024] tr_loss : 26.87604, val_loss : 19.16427, elapsed : 0.16s, total : 46.01s, remaining : 115.25s\n",
      "*[0290/1024] tr_loss : 26.73938, val_loss : 18.75906, elapsed : 0.15s, total : 46.17s, remaining : 113.76s\n",
      " [0291/1024] tr_loss : 26.75099, val_loss : 18.82169, elapsed : 0.16s, total : 46.33s, remaining : 119.82s\n",
      " [0292/1024] tr_loss : 26.68627, val_loss : 19.14361, elapsed : 0.16s, total : 46.49s, remaining : 114.21s\n",
      " [0293/1024] tr_loss : 26.60912, val_loss : 19.17980, elapsed : 0.16s, total : 46.64s, remaining : 114.61s\n",
      " [0294/1024] tr_loss : 26.58775, val_loss : 18.76416, elapsed : 0.15s, total : 46.80s, remaining : 111.93s\n",
      "*[0295/1024] tr_loss : 26.51476, val_loss : 18.69180, elapsed : 0.16s, total : 46.96s, remaining : 114.71s\n",
      " [0296/1024] tr_loss : 26.59272, val_loss : 19.05732, elapsed : 0.16s, total : 47.11s, remaining : 113.43s\n",
      " [0297/1024] tr_loss : 26.46339, val_loss : 19.21806, elapsed : 0.16s, total : 47.27s, remaining : 114.19s\n",
      "*[0298/1024] tr_loss : 26.41636, val_loss : 18.63400, elapsed : 0.15s, total : 47.42s, remaining : 111.61s\n",
      "*[0299/1024] tr_loss : 26.35826, val_loss : 18.63383, elapsed : 0.16s, total : 47.58s, remaining : 115.10s\n",
      " [0300/1024] tr_loss : 26.44006, val_loss : 19.11481, elapsed : 0.16s, total : 47.74s, remaining : 115.03s\n",
      " [0301/1024] tr_loss : 26.31424, val_loss : 19.06150, elapsed : 0.16s, total : 47.90s, remaining : 116.74s\n",
      "*[0302/1024] tr_loss : 26.22381, val_loss : 18.57963, elapsed : 0.15s, total : 48.06s, remaining : 111.20s\n",
      "*[0303/1024] tr_loss : 26.18839, val_loss : 18.54592, elapsed : 0.16s, total : 48.22s, remaining : 115.38s\n",
      " [0304/1024] tr_loss : 26.21492, val_loss : 18.97098, elapsed : 0.16s, total : 48.37s, remaining : 111.87s\n",
      " [0305/1024] tr_loss : 26.10277, val_loss : 19.04553, elapsed : 0.15s, total : 48.53s, remaining : 111.31s\n",
      "*[0306/1024] tr_loss : 26.06145, val_loss : 18.54502, elapsed : 0.15s, total : 48.68s, remaining : 110.02s\n",
      "*[0307/1024] tr_loss : 25.99439, val_loss : 18.47657, elapsed : 0.16s, total : 48.84s, remaining : 112.71s\n",
      " [0308/1024] tr_loss : 26.05321, val_loss : 18.75011, elapsed : 0.15s, total : 48.99s, remaining : 110.67s\n",
      " [0309/1024] tr_loss : 25.93338, val_loss : 19.07109, elapsed : 0.16s, total : 49.15s, remaining : 111.74s\n",
      " [0310/1024] tr_loss : 25.90284, val_loss : 18.51003, elapsed : 0.15s, total : 49.30s, remaining : 109.27s\n",
      "*[0311/1024] tr_loss : 25.81754, val_loss : 18.42635, elapsed : 0.16s, total : 49.46s, remaining : 112.36s\n",
      " [0312/1024] tr_loss : 25.86513, val_loss : 18.68083, elapsed : 0.16s, total : 49.61s, remaining : 110.69s\n",
      " [0313/1024] tr_loss : 25.74498, val_loss : 19.04373, elapsed : 0.16s, total : 49.77s, remaining : 111.01s\n",
      " [0314/1024] tr_loss : 25.72944, val_loss : 18.51622, elapsed : 0.15s, total : 49.92s, remaining : 109.87s\n",
      "*[0315/1024] tr_loss : 25.63355, val_loss : 18.38209, elapsed : 0.16s, total : 50.08s, remaining : 110.79s\n",
      " [0316/1024] tr_loss : 25.67169, val_loss : 18.54169, elapsed : 0.15s, total : 50.24s, remaining : 109.49s\n",
      " [0317/1024] tr_loss : 25.55452, val_loss : 18.85718, elapsed : 0.16s, total : 50.39s, remaining : 109.79s\n",
      " [0318/1024] tr_loss : 25.52958, val_loss : 18.57326, elapsed : 0.15s, total : 50.54s, remaining : 108.09s\n",
      "*[0319/1024] tr_loss : 25.45608, val_loss : 18.31536, elapsed : 0.16s, total : 50.70s, remaining : 111.77s\n",
      "*[0320/1024] tr_loss : 25.43142, val_loss : 18.27518, elapsed : 0.16s, total : 50.86s, remaining : 112.52s\n",
      " [0321/1024] tr_loss : 25.45047, val_loss : 18.51243, elapsed : 0.16s, total : 51.02s, remaining : 112.63s\n",
      " [0322/1024] tr_loss : 25.34870, val_loss : 18.78168, elapsed : 0.15s, total : 51.18s, remaining : 108.36s\n",
      " [0323/1024] tr_loss : 25.35498, val_loss : 18.47690, elapsed : 0.16s, total : 51.34s, remaining : 111.30s\n",
      "*[0324/1024] tr_loss : 25.24820, val_loss : 18.18082, elapsed : 0.16s, total : 51.49s, remaining : 108.80s\n",
      " [0325/1024] tr_loss : 25.28991, val_loss : 18.33874, elapsed : 0.16s, total : 51.65s, remaining : 113.20s\n",
      " [0326/1024] tr_loss : 25.17756, val_loss : 18.74154, elapsed : 0.15s, total : 51.80s, remaining : 105.78s\n",
      " [0327/1024] tr_loss : 25.16010, val_loss : 18.64273, elapsed : 0.16s, total : 51.96s, remaining : 108.35s\n",
      " [0328/1024] tr_loss : 25.10079, val_loss : 18.19659, elapsed : 0.15s, total : 52.11s, remaining : 106.80s\n",
      "*[0329/1024] tr_loss : 25.03855, val_loss : 18.12546, elapsed : 0.16s, total : 52.27s, remaining : 111.05s\n",
      " [0330/1024] tr_loss : 25.09049, val_loss : 18.20479, elapsed : 0.16s, total : 52.43s, remaining : 108.22s\n",
      " [0331/1024] tr_loss : 24.97136, val_loss : 18.72598, elapsed : 0.16s, total : 52.59s, remaining : 107.99s\n",
      " [0332/1024] tr_loss : 24.97971, val_loss : 18.52201, elapsed : 0.15s, total : 52.74s, remaining : 106.58s\n",
      "*[0333/1024] tr_loss : 24.86932, val_loss : 18.08467, elapsed : 0.16s, total : 52.90s, remaining : 107.70s\n",
      " [0334/1024] tr_loss : 24.85214, val_loss : 18.08721, elapsed : 0.16s, total : 53.05s, remaining : 107.12s\n",
      " [0335/1024] tr_loss : 24.83811, val_loss : 18.28170, elapsed : 0.16s, total : 53.21s, remaining : 106.86s\n",
      " [0336/1024] tr_loss : 24.75486, val_loss : 18.65022, elapsed : 0.15s, total : 53.36s, remaining : 105.93s\n",
      " [0337/1024] tr_loss : 24.79057, val_loss : 18.36062, elapsed : 0.16s, total : 53.52s, remaining : 107.44s\n",
      "*[0338/1024] tr_loss : 24.66635, val_loss : 18.04441, elapsed : 0.16s, total : 53.67s, remaining : 106.50s\n",
      "*[0339/1024] tr_loss : 24.67486, val_loss : 18.00794, elapsed : 0.16s, total : 53.83s, remaining : 111.28s\n",
      " [0340/1024] tr_loss : 24.61136, val_loss : 18.40621, elapsed : 0.16s, total : 53.99s, remaining : 106.83s\n",
      " [0341/1024] tr_loss : 24.56535, val_loss : 18.57218, elapsed : 0.16s, total : 54.15s, remaining : 106.85s\n",
      " [0342/1024] tr_loss : 24.58383, val_loss : 18.29261, elapsed : 0.16s, total : 54.30s, remaining : 106.71s\n",
      "*[0343/1024] tr_loss : 24.45657, val_loss : 17.96148, elapsed : 0.16s, total : 54.46s, remaining : 106.16s\n",
      " [0344/1024] tr_loss : 24.49378, val_loss : 18.00493, elapsed : 0.16s, total : 54.62s, remaining : 106.93s\n",
      " [0345/1024] tr_loss : 24.39711, val_loss : 18.49622, elapsed : 0.16s, total : 54.77s, remaining : 105.38s\n",
      " [0346/1024] tr_loss : 24.39448, val_loss : 18.55946, elapsed : 0.15s, total : 54.92s, remaining : 102.59s\n",
      " [0347/1024] tr_loss : 24.33784, val_loss : 18.06209, elapsed : 0.16s, total : 55.08s, remaining : 106.97s\n",
      "*[0348/1024] tr_loss : 24.26928, val_loss : 17.86377, elapsed : 0.15s, total : 55.23s, remaining : 102.52s\n",
      " [0349/1024] tr_loss : 24.27753, val_loss : 18.07097, elapsed : 0.16s, total : 55.39s, remaining : 107.86s\n",
      " [0350/1024] tr_loss : 24.18770, val_loss : 18.47459, elapsed : 0.15s, total : 55.55s, remaining : 102.93s\n",
      " [0351/1024] tr_loss : 24.20177, val_loss : 18.41253, elapsed : 0.16s, total : 55.70s, remaining : 105.55s\n",
      " [0352/1024] tr_loss : 24.12138, val_loss : 17.98521, elapsed : 0.16s, total : 55.86s, remaining : 104.65s\n",
      "*[0353/1024] tr_loss : 24.07832, val_loss : 17.82444, elapsed : 0.16s, total : 56.02s, remaining : 106.74s\n",
      " [0354/1024] tr_loss : 24.06743, val_loss : 18.07178, elapsed : 0.16s, total : 56.17s, remaining : 105.41s\n",
      " [0355/1024] tr_loss : 23.98338, val_loss : 18.45710, elapsed : 0.16s, total : 56.33s, remaining : 106.07s\n",
      " [0356/1024] tr_loss : 24.00029, val_loss : 18.37674, elapsed : 0.15s, total : 56.48s, remaining : 101.63s\n",
      " [0357/1024] tr_loss : 23.92873, val_loss : 17.93344, elapsed : 0.16s, total : 56.64s, remaining : 105.40s\n",
      "*[0358/1024] tr_loss : 23.90058, val_loss : 17.69375, elapsed : 0.15s, total : 56.80s, remaining : 101.74s\n",
      " [0359/1024] tr_loss : 23.85641, val_loss : 18.06187, elapsed : 0.16s, total : 56.95s, remaining : 105.25s\n",
      " [0360/1024] tr_loss : 23.78659, val_loss : 18.19717, elapsed : 0.15s, total : 57.11s, remaining : 102.07s\n",
      " [0361/1024] tr_loss : 23.78626, val_loss : 18.40749, elapsed : 0.16s, total : 57.26s, remaining : 104.06s\n",
      " [0362/1024] tr_loss : 23.71189, val_loss : 17.76459, elapsed : 0.15s, total : 57.42s, remaining : 101.66s\n",
      " [0363/1024] tr_loss : 23.67963, val_loss : 17.87334, elapsed : 0.16s, total : 57.57s, remaining : 103.25s\n",
      " [0364/1024] tr_loss : 23.72346, val_loss : 17.77839, elapsed : 0.15s, total : 57.73s, remaining : 101.65s\n",
      " [0365/1024] tr_loss : 23.60712, val_loss : 18.35004, elapsed : 0.16s, total : 57.89s, remaining : 103.58s\n",
      " [0366/1024] tr_loss : 23.60148, val_loss : 18.26461, elapsed : 0.15s, total : 58.04s, remaining : 100.89s\n",
      " [0367/1024] tr_loss : 23.56694, val_loss : 17.98992, elapsed : 0.16s, total : 58.20s, remaining : 105.31s\n",
      "*[0368/1024] tr_loss : 23.49407, val_loss : 17.63113, elapsed : 0.16s, total : 58.36s, remaining : 106.16s\n",
      " [0369/1024] tr_loss : 23.51842, val_loss : 17.84463, elapsed : 0.16s, total : 58.52s, remaining : 104.68s\n",
      " [0370/1024] tr_loss : 23.41696, val_loss : 18.19759, elapsed : 0.15s, total : 58.67s, remaining : 99.69s\n",
      " [0371/1024] tr_loss : 23.41332, val_loss : 18.45116, elapsed : 0.16s, total : 58.83s, remaining : 104.15s\n",
      " [0372/1024] tr_loss : 23.38353, val_loss : 17.93797, elapsed : 0.15s, total : 58.99s, remaining : 99.36s\n",
      " [0373/1024] tr_loss : 23.30017, val_loss : 17.64221, elapsed : 0.16s, total : 59.15s, remaining : 105.42s\n",
      " [0374/1024] tr_loss : 23.32618, val_loss : 17.77850, elapsed : 0.16s, total : 59.31s, remaining : 102.84s\n",
      " [0375/1024] tr_loss : 23.22999, val_loss : 18.20812, elapsed : 0.16s, total : 59.46s, remaining : 102.51s\n",
      " [0376/1024] tr_loss : 23.22080, val_loss : 18.29045, elapsed : 0.15s, total : 59.62s, remaining : 99.68s\n",
      " [0377/1024] tr_loss : 23.19338, val_loss : 17.91896, elapsed : 0.16s, total : 59.77s, remaining : 100.40s\n",
      "*[0378/1024] tr_loss : 23.11220, val_loss : 17.61602, elapsed : 0.15s, total : 59.93s, remaining : 98.73s\n",
      " [0379/1024] tr_loss : 23.12874, val_loss : 17.70327, elapsed : 0.16s, total : 60.09s, remaining : 103.07s\n",
      " [0380/1024] tr_loss : 23.03792, val_loss : 18.02647, elapsed : 0.19s, total : 60.27s, remaining : 121.69s\n",
      " [0381/1024] tr_loss : 23.02538, val_loss : 18.25917, elapsed : 0.16s, total : 60.43s, remaining : 101.24s\n",
      " [0382/1024] tr_loss : 23.00504, val_loss : 17.87714, elapsed : 0.15s, total : 60.59s, remaining : 98.34s\n",
      "*[0383/1024] tr_loss : 22.92606, val_loss : 17.60968, elapsed : 0.16s, total : 60.74s, remaining : 101.35s\n",
      " [0384/1024] tr_loss : 22.94781, val_loss : 17.65739, elapsed : 0.16s, total : 60.90s, remaining : 101.08s\n",
      " [0385/1024] tr_loss : 22.86525, val_loss : 17.99838, elapsed : 0.16s, total : 61.06s, remaining : 101.31s\n",
      " [0386/1024] tr_loss : 22.83875, val_loss : 18.23427, elapsed : 0.15s, total : 61.21s, remaining : 97.43s\n",
      " [0387/1024] tr_loss : 22.83354, val_loss : 17.87741, elapsed : 0.16s, total : 61.37s, remaining : 100.74s\n",
      "*[0388/1024] tr_loss : 22.73453, val_loss : 17.58660, elapsed : 0.15s, total : 61.52s, remaining : 97.23s\n",
      "*[0389/1024] tr_loss : 22.73521, val_loss : 17.56906, elapsed : 0.16s, total : 61.68s, remaining : 100.90s\n",
      " [0390/1024] tr_loss : 22.66111, val_loss : 17.82986, elapsed : 0.16s, total : 61.84s, remaining : 99.12s\n",
      " [0391/1024] tr_loss : 22.62720, val_loss : 18.09780, elapsed : 0.16s, total : 62.00s, remaining : 99.47s\n",
      " [0392/1024] tr_loss : 22.65430, val_loss : 17.88882, elapsed : 0.16s, total : 62.15s, remaining : 98.39s\n",
      " [0393/1024] tr_loss : 22.54433, val_loss : 17.59072, elapsed : 0.16s, total : 62.31s, remaining : 99.13s\n",
      " [0394/1024] tr_loss : 22.54433, val_loss : 17.57773, elapsed : 0.16s, total : 62.47s, remaining : 99.85s\n",
      "*[0395/1024] tr_loss : 22.58426, val_loss : 17.55340, elapsed : 0.16s, total : 62.62s, remaining : 98.02s\n",
      " [0396/1024] tr_loss : 22.45743, val_loss : 18.14307, elapsed : 0.16s, total : 62.78s, remaining : 98.79s\n",
      " [0397/1024] tr_loss : 22.47479, val_loss : 17.92382, elapsed : 0.16s, total : 62.94s, remaining : 97.98s\n",
      " [0398/1024] tr_loss : 22.44604, val_loss : 17.75439, elapsed : 0.15s, total : 63.09s, remaining : 96.10s\n",
      "*[0399/1024] tr_loss : 22.36289, val_loss : 17.36318, elapsed : 0.16s, total : 63.25s, remaining : 97.69s\n",
      " [0400/1024] tr_loss : 22.30610, val_loss : 17.62853, elapsed : 0.16s, total : 63.40s, remaining : 97.49s\n",
      " [0401/1024] tr_loss : 22.32548, val_loss : 17.84219, elapsed : 0.16s, total : 63.56s, remaining : 97.68s\n",
      " [0402/1024] tr_loss : 22.27963, val_loss : 17.97709, elapsed : 0.15s, total : 63.71s, remaining : 94.76s\n",
      " [0403/1024] tr_loss : 22.19116, val_loss : 17.55085, elapsed : 0.16s, total : 63.87s, remaining : 97.90s\n",
      " [0404/1024] tr_loss : 22.19029, val_loss : 17.51674, elapsed : 0.15s, total : 64.02s, remaining : 94.92s\n",
      " [0405/1024] tr_loss : 22.23349, val_loss : 17.52316, elapsed : 0.16s, total : 64.18s, remaining : 98.28s\n",
      " [0406/1024] tr_loss : 22.06293, val_loss : 17.80565, elapsed : 0.15s, total : 64.34s, remaining : 94.49s\n",
      " [0407/1024] tr_loss : 22.06246, val_loss : 17.65132, elapsed : 0.16s, total : 64.49s, remaining : 96.25s\n",
      " [0408/1024] tr_loss : 22.06461, val_loss : 17.63024, elapsed : 0.15s, total : 64.64s, remaining : 94.23s\n",
      " [0409/1024] tr_loss : 22.02183, val_loss : 17.36466, elapsed : 0.16s, total : 64.80s, remaining : 96.81s\n",
      " [0410/1024] tr_loss : 21.91323, val_loss : 17.57472, elapsed : 0.16s, total : 64.96s, remaining : 96.71s\n",
      " [0411/1024] tr_loss : 21.91317, val_loss : 17.72437, elapsed : 0.16s, total : 65.12s, remaining : 96.50s\n",
      " [0412/1024] tr_loss : 21.86021, val_loss : 17.72383, elapsed : 0.15s, total : 65.27s, remaining : 92.56s\n",
      " [0413/1024] tr_loss : 21.79674, val_loss : 17.45783, elapsed : 0.16s, total : 65.43s, remaining : 98.36s\n",
      " [0414/1024] tr_loss : 21.81869, val_loss : 17.58854, elapsed : 0.15s, total : 65.58s, remaining : 93.62s\n",
      " [0415/1024] tr_loss : 21.81655, val_loss : 17.44290, elapsed : 0.16s, total : 65.74s, remaining : 95.67s\n",
      " [0416/1024] tr_loss : 21.72454, val_loss : 17.79747, elapsed : 0.15s, total : 65.89s, remaining : 92.50s\n",
      " [0417/1024] tr_loss : 21.72139, val_loss : 17.82199, elapsed : 0.16s, total : 66.05s, remaining : 94.97s\n",
      " [0418/1024] tr_loss : 21.72590, val_loss : 17.77553, elapsed : 0.15s, total : 66.20s, remaining : 92.42s\n",
      " [0419/1024] tr_loss : 21.60142, val_loss : 17.47150, elapsed : 0.16s, total : 66.36s, remaining : 94.81s\n",
      " [0420/1024] tr_loss : 21.63615, val_loss : 17.50872, elapsed : 0.15s, total : 66.51s, remaining : 92.60s\n",
      " [0421/1024] tr_loss : 21.64791, val_loss : 17.54624, elapsed : 0.16s, total : 66.67s, remaining : 94.94s\n",
      " [0422/1024] tr_loss : 21.55355, val_loss : 18.14682, elapsed : 0.15s, total : 66.82s, remaining : 91.84s\n",
      " [0423/1024] tr_loss : 21.56630, val_loss : 17.68347, elapsed : 0.16s, total : 66.98s, remaining : 94.42s\n",
      " [0424/1024] tr_loss : 21.54611, val_loss : 17.81896, elapsed : 0.15s, total : 67.13s, remaining : 92.46s\n",
      " [0425/1024] tr_loss : 21.46513, val_loss : 17.36738, elapsed : 0.16s, total : 67.29s, remaining : 93.85s\n",
      " [0426/1024] tr_loss : 21.38107, val_loss : 17.66776, elapsed : 0.15s, total : 67.44s, remaining : 91.61s\n",
      " [0427/1024] tr_loss : 21.38653, val_loss : 17.89340, elapsed : 0.19s, total : 67.64s, remaining : 115.51s\n",
      " [0428/1024] tr_loss : 21.34158, val_loss : 17.80697, elapsed : 0.16s, total : 67.79s, remaining : 94.41s\n",
      " [0429/1024] tr_loss : 21.24583, val_loss : 17.59603, elapsed : 0.16s, total : 67.95s, remaining : 93.12s\n",
      " [0430/1024] tr_loss : 21.25298, val_loss : 17.55381, elapsed : 0.16s, total : 68.11s, remaining : 93.93s\n",
      " [0431/1024] tr_loss : 21.26250, val_loss : 17.65824, elapsed : 0.16s, total : 68.27s, remaining : 96.56s\n",
      " [0432/1024] tr_loss : 21.15009, val_loss : 18.20174, elapsed : 0.15s, total : 68.42s, remaining : 89.93s\n",
      " [0433/1024] tr_loss : 21.12364, val_loss : 17.91119, elapsed : 0.16s, total : 68.58s, remaining : 93.07s\n",
      " [0434/1024] tr_loss : 21.09971, val_loss : 17.85702, elapsed : 0.15s, total : 68.74s, remaining : 91.15s\n",
      " [0435/1024] tr_loss : 21.02409, val_loss : 17.60819, elapsed : 0.16s, total : 68.89s, remaining : 92.30s\n",
      " [0436/1024] tr_loss : 20.99452, val_loss : 17.79133, elapsed : 0.15s, total : 69.04s, remaining : 89.64s\n",
      " [0437/1024] tr_loss : 20.98683, val_loss : 18.04603, elapsed : 0.16s, total : 69.20s, remaining : 92.22s\n",
      " [0438/1024] tr_loss : 20.87389, val_loss : 18.29126, elapsed : 0.15s, total : 69.36s, remaining : 90.23s\n",
      " [0439/1024] tr_loss : 20.86991, val_loss : 17.79602, elapsed : 0.16s, total : 69.51s, remaining : 91.16s\n",
      " [0440/1024] tr_loss : 20.83673, val_loss : 17.78416, elapsed : 0.15s, total : 69.67s, remaining : 89.69s\n",
      " [0441/1024] tr_loss : 20.79557, val_loss : 17.59000, elapsed : 0.16s, total : 69.82s, remaining : 90.88s\n",
      " [0442/1024] tr_loss : 20.82894, val_loss : 17.71762, elapsed : 0.15s, total : 69.97s, remaining : 88.21s\n",
      " [0443/1024] tr_loss : 20.82004, val_loss : 18.40575, elapsed : 0.16s, total : 70.13s, remaining : 90.77s\n",
      " [0444/1024] tr_loss : 20.73218, val_loss : 18.10960, elapsed : 0.15s, total : 70.28s, remaining : 88.42s\n",
      " [0445/1024] tr_loss : 20.71994, val_loss : 17.87733, elapsed : 0.16s, total : 70.44s, remaining : 89.89s\n",
      " [0446/1024] tr_loss : 20.62152, val_loss : 17.61904, elapsed : 0.16s, total : 70.60s, remaining : 92.84s\n",
      " [0447/1024] tr_loss : 20.66912, val_loss : 17.50732, elapsed : 0.16s, total : 70.76s, remaining : 90.94s\n",
      " [0448/1024] tr_loss : 20.63272, val_loss : 18.23034, elapsed : 0.16s, total : 70.91s, remaining : 90.16s\n",
      " [0449/1024] tr_loss : 20.59669, val_loss : 17.87005, elapsed : 0.16s, total : 71.07s, remaining : 91.63s\n",
      " [0450/1024] tr_loss : 20.55330, val_loss : 17.74081, elapsed : 0.15s, total : 71.22s, remaining : 87.86s\n",
      " [0451/1024] tr_loss : 20.47880, val_loss : 17.41672, elapsed : 0.16s, total : 71.38s, remaining : 89.71s\n",
      " [0452/1024] tr_loss : 20.45748, val_loss : 17.56525, elapsed : 0.16s, total : 71.54s, remaining : 89.06s\n",
      " [0453/1024] tr_loss : 20.42736, val_loss : 17.95398, elapsed : 0.16s, total : 71.70s, remaining : 91.65s\n",
      " [0454/1024] tr_loss : 20.39488, val_loss : 17.58676, elapsed : 0.15s, total : 71.85s, remaining : 87.39s\n",
      " [0455/1024] tr_loss : 20.33647, val_loss : 17.61370, elapsed : 0.17s, total : 72.02s, remaining : 94.69s\n",
      "*[0456/1024] tr_loss : 20.36686, val_loss : 17.23903, elapsed : 0.16s, total : 72.18s, remaining : 90.93s\n",
      " [0457/1024] tr_loss : 20.27304, val_loss : 17.66182, elapsed : 0.17s, total : 72.35s, remaining : 97.29s\n",
      " [0458/1024] tr_loss : 20.23269, val_loss : 17.60583, elapsed : 0.15s, total : 72.50s, remaining : 87.30s\n",
      " [0459/1024] tr_loss : 20.20407, val_loss : 17.51854, elapsed : 0.17s, total : 72.67s, remaining : 94.45s\n",
      " [0460/1024] tr_loss : 20.14276, val_loss : 17.34297, elapsed : 0.16s, total : 72.83s, remaining : 88.74s\n",
      "*[0461/1024] tr_loss : 20.17305, val_loss : 17.19449, elapsed : 0.16s, total : 72.99s, remaining : 92.72s\n",
      " [0462/1024] tr_loss : 20.10111, val_loss : 17.65924, elapsed : 0.16s, total : 73.15s, remaining : 90.78s\n",
      " [0463/1024] tr_loss : 20.07423, val_loss : 17.35770, elapsed : 0.17s, total : 73.32s, remaining : 92.69s\n",
      " [0464/1024] tr_loss : 20.04698, val_loss : 17.41766, elapsed : 0.16s, total : 73.48s, remaining : 87.50s\n",
      "*[0465/1024] tr_loss : 20.04060, val_loss : 17.06648, elapsed : 0.24s, total : 73.71s, remaining : 132.77s\n",
      " [0466/1024] tr_loss : 19.97331, val_loss : 17.34293, elapsed : 0.16s, total : 73.87s, remaining : 90.10s\n",
      " [0467/1024] tr_loss : 19.90943, val_loss : 17.35539, elapsed : 0.16s, total : 74.03s, remaining : 87.40s\n",
      " [0468/1024] tr_loss : 19.88634, val_loss : 17.25310, elapsed : 0.16s, total : 74.19s, remaining : 88.40s\n",
      " [0469/1024] tr_loss : 19.83621, val_loss : 17.22986, elapsed : 0.16s, total : 74.35s, remaining : 86.04s\n",
      "*[0470/1024] tr_loss : 19.89637, val_loss : 16.96626, elapsed : 0.15s, total : 74.50s, remaining : 84.25s\n",
      " [0471/1024] tr_loss : 19.81527, val_loss : 17.38113, elapsed : 0.16s, total : 74.66s, remaining : 88.75s\n",
      " [0472/1024] tr_loss : 19.77372, val_loss : 17.15877, elapsed : 0.15s, total : 74.81s, remaining : 84.34s\n",
      " [0473/1024] tr_loss : 19.74209, val_loss : 17.23509, elapsed : 0.15s, total : 74.97s, remaining : 85.14s\n",
      " [0474/1024] tr_loss : 19.72123, val_loss : 16.96914, elapsed : 0.15s, total : 75.12s, remaining : 83.89s\n",
      " [0475/1024] tr_loss : 19.68714, val_loss : 17.03257, elapsed : 0.16s, total : 75.27s, remaining : 85.58s\n",
      " [0476/1024] tr_loss : 19.62613, val_loss : 17.27506, elapsed : 0.15s, total : 75.43s, remaining : 83.68s\n",
      " [0477/1024] tr_loss : 19.61952, val_loss : 17.03366, elapsed : 0.16s, total : 75.59s, remaining : 87.16s\n",
      " [0478/1024] tr_loss : 19.56436, val_loss : 17.16422, elapsed : 0.15s, total : 75.74s, remaining : 83.33s\n",
      "*[0479/1024] tr_loss : 19.64333, val_loss : 16.80835, elapsed : 0.16s, total : 75.89s, remaining : 84.79s\n",
      " [0480/1024] tr_loss : 19.54497, val_loss : 17.14481, elapsed : 0.16s, total : 76.05s, remaining : 84.98s\n",
      " [0481/1024] tr_loss : 19.52286, val_loss : 17.08964, elapsed : 0.16s, total : 76.21s, remaining : 85.03s\n",
      " [0482/1024] tr_loss : 19.49062, val_loss : 17.04286, elapsed : 0.16s, total : 76.36s, remaining : 84.44s\n",
      " [0483/1024] tr_loss : 19.43490, val_loss : 16.92129, elapsed : 0.16s, total : 76.52s, remaining : 85.37s\n",
      "*[0484/1024] tr_loss : 19.46201, val_loss : 16.72819, elapsed : 0.15s, total : 76.67s, remaining : 82.12s\n",
      " [0485/1024] tr_loss : 19.38907, val_loss : 17.15684, elapsed : 0.16s, total : 76.83s, remaining : 86.70s\n",
      " [0486/1024] tr_loss : 19.36906, val_loss : 16.82024, elapsed : 0.15s, total : 76.99s, remaining : 81.99s\n",
      " [0487/1024] tr_loss : 19.33502, val_loss : 17.03928, elapsed : 0.16s, total : 77.14s, remaining : 84.50s\n",
      "*[0488/1024] tr_loss : 19.42197, val_loss : 16.59441, elapsed : 0.16s, total : 77.30s, remaining : 83.15s\n",
      " [0489/1024] tr_loss : 19.28686, val_loss : 17.00308, elapsed : 0.16s, total : 77.46s, remaining : 84.87s\n",
      " [0490/1024] tr_loss : 19.28422, val_loss : 16.85426, elapsed : 0.15s, total : 77.61s, remaining : 81.89s\n",
      " [0491/1024] tr_loss : 19.23340, val_loss : 16.92447, elapsed : 0.16s, total : 77.77s, remaining : 84.12s\n",
      "*[0492/1024] tr_loss : 19.24692, val_loss : 16.58658, elapsed : 0.15s, total : 77.92s, remaining : 81.90s\n",
      " [0493/1024] tr_loss : 19.16086, val_loss : 16.79415, elapsed : 0.16s, total : 78.08s, remaining : 84.94s\n",
      " [0494/1024] tr_loss : 19.10666, val_loss : 16.79223, elapsed : 0.15s, total : 78.23s, remaining : 80.53s\n",
      " [0495/1024] tr_loss : 19.07782, val_loss : 16.77570, elapsed : 0.16s, total : 78.39s, remaining : 82.76s\n",
      " [0496/1024] tr_loss : 19.08371, val_loss : 16.63849, elapsed : 0.15s, total : 78.54s, remaining : 80.46s\n",
      " [0497/1024] tr_loss : 19.06678, val_loss : 16.60177, elapsed : 0.16s, total : 78.70s, remaining : 82.65s\n",
      " [0498/1024] tr_loss : 18.99159, val_loss : 16.92662, elapsed : 0.15s, total : 78.85s, remaining : 80.31s\n",
      " [0499/1024] tr_loss : 18.97967, val_loss : 16.62338, elapsed : 0.16s, total : 79.01s, remaining : 84.36s\n",
      " [0500/1024] tr_loss : 18.94488, val_loss : 16.81865, elapsed : 0.15s, total : 79.17s, remaining : 80.62s\n",
      "*[0501/1024] tr_loss : 19.05778, val_loss : 16.42765, elapsed : 0.16s, total : 79.33s, remaining : 83.11s\n",
      " [0502/1024] tr_loss : 18.93699, val_loss : 16.87724, elapsed : 0.17s, total : 79.49s, remaining : 86.18s\n",
      " [0503/1024] tr_loss : 18.93262, val_loss : 16.64668, elapsed : 0.16s, total : 79.66s, remaining : 85.56s\n",
      " [0504/1024] tr_loss : 18.88076, val_loss : 16.81311, elapsed : 0.16s, total : 79.81s, remaining : 81.50s\n",
      "*[0505/1024] tr_loss : 18.96432, val_loss : 16.34645, elapsed : 0.16s, total : 79.97s, remaining : 83.54s\n",
      " [0506/1024] tr_loss : 18.83341, val_loss : 16.74168, elapsed : 0.16s, total : 80.13s, remaining : 82.06s\n",
      " [0507/1024] tr_loss : 18.80610, val_loss : 16.60299, elapsed : 0.16s, total : 80.29s, remaining : 83.90s\n",
      " [0508/1024] tr_loss : 18.76820, val_loss : 16.72409, elapsed : 0.16s, total : 80.45s, remaining : 81.19s\n",
      "*[0509/1024] tr_loss : 18.88432, val_loss : 16.20756, elapsed : 0.16s, total : 80.62s, remaining : 84.16s\n",
      " [0510/1024] tr_loss : 18.70655, val_loss : 16.74184, elapsed : 0.16s, total : 80.77s, remaining : 80.30s\n",
      " [0511/1024] tr_loss : 18.72460, val_loss : 16.53027, elapsed : 0.16s, total : 80.93s, remaining : 83.22s\n",
      " [0512/1024] tr_loss : 18.68215, val_loss : 16.65362, elapsed : 0.15s, total : 81.09s, remaining : 78.57s\n",
      "*[0513/1024] tr_loss : 18.81312, val_loss : 16.10913, elapsed : 0.17s, total : 81.25s, remaining : 84.78s\n",
      " [0514/1024] tr_loss : 18.65545, val_loss : 16.76359, elapsed : 0.16s, total : 81.41s, remaining : 80.91s\n",
      " [0515/1024] tr_loss : 18.62881, val_loss : 16.47657, elapsed : 0.16s, total : 81.58s, remaining : 83.08s\n",
      " [0516/1024] tr_loss : 18.59110, val_loss : 16.54408, elapsed : 0.16s, total : 81.73s, remaining : 78.83s\n",
      "*[0517/1024] tr_loss : 18.68784, val_loss : 16.08017, elapsed : 0.16s, total : 81.89s, remaining : 80.86s\n",
      " [0518/1024] tr_loss : 18.53505, val_loss : 16.71290, elapsed : 0.17s, total : 82.06s, remaining : 83.69s\n",
      " [0519/1024] tr_loss : 18.49612, val_loss : 16.44220, elapsed : 0.16s, total : 82.22s, remaining : 81.98s\n",
      " [0520/1024] tr_loss : 18.49992, val_loss : 16.34563, elapsed : 0.15s, total : 82.37s, remaining : 77.24s\n",
      " [0521/1024] tr_loss : 18.49672, val_loss : 16.10914, elapsed : 0.16s, total : 82.53s, remaining : 80.16s\n",
      " [0522/1024] tr_loss : 18.40725, val_loss : 16.56381, elapsed : 0.15s, total : 82.68s, remaining : 76.75s\n",
      " [0523/1024] tr_loss : 18.34738, val_loss : 16.38037, elapsed : 0.16s, total : 82.85s, remaining : 81.07s\n",
      " [0524/1024] tr_loss : 18.41313, val_loss : 16.16262, elapsed : 0.15s, total : 83.00s, remaining : 76.65s\n",
      " [0525/1024] tr_loss : 18.31600, val_loss : 16.19062, elapsed : 0.16s, total : 83.16s, remaining : 80.34s\n",
      " [0526/1024] tr_loss : 18.27200, val_loss : 16.42376, elapsed : 0.15s, total : 83.31s, remaining : 76.18s\n",
      " [0527/1024] tr_loss : 18.22741, val_loss : 16.35205, elapsed : 0.16s, total : 83.47s, remaining : 77.88s\n",
      "*[0528/1024] tr_loss : 18.34188, val_loss : 16.02081, elapsed : 0.15s, total : 83.62s, remaining : 75.94s\n",
      " [0529/1024] tr_loss : 18.19380, val_loss : 16.32135, elapsed : 0.16s, total : 83.78s, remaining : 79.73s\n",
      " [0530/1024] tr_loss : 18.19146, val_loss : 16.31468, elapsed : 0.15s, total : 83.94s, remaining : 76.43s\n",
      " [0531/1024] tr_loss : 18.14479, val_loss : 16.36722, elapsed : 0.16s, total : 84.10s, remaining : 77.96s\n",
      "*[0532/1024] tr_loss : 18.27774, val_loss : 15.94686, elapsed : 0.15s, total : 84.25s, remaining : 74.98s\n",
      " [0533/1024] tr_loss : 18.10562, val_loss : 16.52069, elapsed : 0.16s, total : 84.41s, remaining : 78.39s\n",
      " [0534/1024] tr_loss : 18.20463, val_loss : 16.42539, elapsed : 0.15s, total : 84.56s, remaining : 74.98s\n",
      " [0535/1024] tr_loss : 18.11079, val_loss : 16.53131, elapsed : 0.20s, total : 84.76s, remaining : 97.55s\n",
      "*[0536/1024] tr_loss : 18.11926, val_loss : 15.86952, elapsed : 0.16s, total : 84.92s, remaining : 76.57s\n",
      " [0537/1024] tr_loss : 17.92333, val_loss : 16.34889, elapsed : 0.16s, total : 85.08s, remaining : 79.21s\n",
      " [0538/1024] tr_loss : 17.98481, val_loss : 16.37735, elapsed : 0.15s, total : 85.23s, remaining : 74.05s\n",
      " [0539/1024] tr_loss : 17.89521, val_loss : 16.32880, elapsed : 0.16s, total : 85.39s, remaining : 76.00s\n",
      "*[0540/1024] tr_loss : 17.98730, val_loss : 15.63769, elapsed : 0.15s, total : 85.54s, remaining : 74.50s\n",
      " [0541/1024] tr_loss : 17.81727, val_loss : 16.02305, elapsed : 0.16s, total : 85.71s, remaining : 78.61s\n",
      " [0542/1024] tr_loss : 17.81141, val_loss : 16.09621, elapsed : 0.15s, total : 85.86s, remaining : 74.13s\n",
      " [0543/1024] tr_loss : 17.74351, val_loss : 16.16280, elapsed : 0.16s, total : 86.02s, remaining : 76.00s\n",
      "*[0544/1024] tr_loss : 17.84782, val_loss : 15.50963, elapsed : 0.15s, total : 86.17s, remaining : 73.26s\n",
      " [0545/1024] tr_loss : 17.67169, val_loss : 15.90504, elapsed : 0.16s, total : 86.33s, remaining : 77.42s\n",
      " [0546/1024] tr_loss : 17.66059, val_loss : 15.89934, elapsed : 0.15s, total : 86.49s, remaining : 73.27s\n",
      " [0547/1024] tr_loss : 17.61321, val_loss : 16.07226, elapsed : 0.16s, total : 86.65s, remaining : 77.42s\n",
      "*[0548/1024] tr_loss : 17.70873, val_loss : 15.37057, elapsed : 0.15s, total : 86.80s, remaining : 73.07s\n",
      " [0549/1024] tr_loss : 17.55592, val_loss : 15.77257, elapsed : 0.16s, total : 86.96s, remaining : 76.06s\n",
      " [0550/1024] tr_loss : 17.54524, val_loss : 15.81771, elapsed : 0.15s, total : 87.12s, remaining : 72.61s\n",
      " [0551/1024] tr_loss : 17.49376, val_loss : 15.91144, elapsed : 0.16s, total : 87.27s, remaining : 74.06s\n",
      "*[0552/1024] tr_loss : 17.63019, val_loss : 15.25679, elapsed : 0.15s, total : 87.43s, remaining : 72.25s\n",
      " [0553/1024] tr_loss : 17.44500, val_loss : 15.71794, elapsed : 0.16s, total : 87.59s, remaining : 75.75s\n",
      " [0554/1024] tr_loss : 17.45624, val_loss : 15.75085, elapsed : 0.15s, total : 87.74s, remaining : 72.30s\n",
      " [0555/1024] tr_loss : 17.39836, val_loss : 15.87901, elapsed : 0.16s, total : 87.90s, remaining : 74.17s\n",
      "*[0556/1024] tr_loss : 17.50253, val_loss : 15.19794, elapsed : 0.15s, total : 88.05s, remaining : 72.11s\n",
      " [0557/1024] tr_loss : 17.34464, val_loss : 15.57534, elapsed : 0.16s, total : 88.21s, remaining : 74.76s\n",
      " [0558/1024] tr_loss : 17.33753, val_loss : 15.60323, elapsed : 0.15s, total : 88.36s, remaining : 71.13s\n",
      " [0559/1024] tr_loss : 17.28511, val_loss : 15.81294, elapsed : 0.16s, total : 88.52s, remaining : 72.94s\n",
      "*[0560/1024] tr_loss : 17.44927, val_loss : 15.04887, elapsed : 0.15s, total : 88.67s, remaining : 70.85s\n",
      " [0561/1024] tr_loss : 17.26224, val_loss : 15.53325, elapsed : 0.17s, total : 88.84s, remaining : 76.91s\n",
      " [0562/1024] tr_loss : 17.24782, val_loss : 15.52279, elapsed : 0.15s, total : 88.99s, remaining : 70.72s\n",
      " [0563/1024] tr_loss : 17.19460, val_loss : 15.73818, elapsed : 0.16s, total : 89.15s, remaining : 73.75s\n",
      "*[0564/1024] tr_loss : 17.34938, val_loss : 14.93690, elapsed : 0.15s, total : 89.31s, remaining : 70.71s\n",
      " [0565/1024] tr_loss : 17.16004, val_loss : 15.44154, elapsed : 0.16s, total : 89.47s, remaining : 73.37s\n",
      " [0566/1024] tr_loss : 17.17165, val_loss : 15.47328, elapsed : 0.15s, total : 89.62s, remaining : 70.02s\n",
      " [0567/1024] tr_loss : 17.10876, val_loss : 15.65126, elapsed : 0.16s, total : 89.78s, remaining : 71.44s\n",
      "*[0568/1024] tr_loss : 17.25999, val_loss : 14.88945, elapsed : 0.15s, total : 89.93s, remaining : 69.80s\n",
      " [0569/1024] tr_loss : 17.07109, val_loss : 15.43775, elapsed : 0.16s, total : 90.09s, remaining : 72.90s\n",
      " [0570/1024] tr_loss : 17.07639, val_loss : 15.44478, elapsed : 0.15s, total : 90.24s, remaining : 69.46s\n",
      " [0571/1024] tr_loss : 17.03249, val_loss : 15.44729, elapsed : 0.16s, total : 90.40s, remaining : 71.49s\n",
      "*[0572/1024] tr_loss : 17.15431, val_loss : 14.84320, elapsed : 0.16s, total : 90.56s, remaining : 71.88s\n",
      " [0573/1024] tr_loss : 16.98436, val_loss : 15.32218, elapsed : 0.16s, total : 90.72s, remaining : 73.68s\n",
      " [0574/1024] tr_loss : 16.97977, val_loss : 15.40374, elapsed : 0.15s, total : 90.88s, remaining : 69.01s\n",
      " [0575/1024] tr_loss : 16.95633, val_loss : 15.27248, elapsed : 0.16s, total : 91.04s, remaining : 71.28s\n",
      " [0576/1024] tr_loss : 17.02004, val_loss : 14.84700, elapsed : 0.16s, total : 91.19s, remaining : 70.03s\n",
      " [0577/1024] tr_loss : 16.87159, val_loss : 15.24783, elapsed : 0.16s, total : 91.35s, remaining : 71.23s\n",
      " [0578/1024] tr_loss : 16.86473, val_loss : 15.34696, elapsed : 0.15s, total : 91.50s, remaining : 67.95s\n",
      " [0579/1024] tr_loss : 16.87515, val_loss : 15.08709, elapsed : 0.16s, total : 91.66s, remaining : 69.97s\n",
      "*[0580/1024] tr_loss : 16.88131, val_loss : 14.83768, elapsed : 0.15s, total : 91.81s, remaining : 67.93s\n",
      " [0581/1024] tr_loss : 16.76001, val_loss : 15.12247, elapsed : 0.16s, total : 91.98s, remaining : 71.78s\n",
      " [0582/1024] tr_loss : 16.74281, val_loss : 15.31281, elapsed : 0.15s, total : 92.13s, remaining : 67.78s\n",
      " [0583/1024] tr_loss : 16.79479, val_loss : 14.93595, elapsed : 0.16s, total : 92.29s, remaining : 68.88s\n",
      " [0584/1024] tr_loss : 16.73726, val_loss : 14.84958, elapsed : 0.15s, total : 92.44s, remaining : 66.97s\n",
      " [0585/1024] tr_loss : 16.63247, val_loss : 15.02180, elapsed : 0.16s, total : 92.59s, remaining : 68.63s\n",
      " [0586/1024] tr_loss : 16.61845, val_loss : 15.17160, elapsed : 0.15s, total : 92.75s, remaining : 67.37s\n",
      " [0587/1024] tr_loss : 16.67996, val_loss : 14.88706, elapsed : 0.16s, total : 92.91s, remaining : 71.70s\n",
      "*[0588/1024] tr_loss : 16.65222, val_loss : 14.77908, elapsed : 0.16s, total : 93.07s, remaining : 67.78s\n",
      " [0589/1024] tr_loss : 16.53834, val_loss : 14.96604, elapsed : 0.16s, total : 93.23s, remaining : 70.19s\n",
      " [0590/1024] tr_loss : 16.53935, val_loss : 15.10431, elapsed : 0.16s, total : 93.38s, remaining : 67.43s\n",
      " [0591/1024] tr_loss : 16.57760, val_loss : 14.93111, elapsed : 0.16s, total : 93.54s, remaining : 68.05s\n",
      "*[0592/1024] tr_loss : 16.60956, val_loss : 14.71933, elapsed : 0.15s, total : 93.69s, remaining : 66.03s\n",
      " [0593/1024] tr_loss : 16.46986, val_loss : 15.00723, elapsed : 0.16s, total : 93.85s, remaining : 68.76s\n",
      " [0594/1024] tr_loss : 16.48182, val_loss : 15.02362, elapsed : 0.15s, total : 94.01s, remaining : 65.72s\n",
      " [0595/1024] tr_loss : 16.48870, val_loss : 14.99724, elapsed : 0.16s, total : 94.17s, remaining : 68.52s\n",
      "*[0596/1024] tr_loss : 16.56830, val_loss : 14.61965, elapsed : 0.16s, total : 94.32s, remaining : 66.85s\n",
      " [0597/1024] tr_loss : 16.40719, val_loss : 14.95659, elapsed : 0.16s, total : 94.48s, remaining : 68.15s\n",
      " [0598/1024] tr_loss : 16.43360, val_loss : 15.03791, elapsed : 0.16s, total : 94.64s, remaining : 66.25s\n",
      " [0599/1024] tr_loss : 16.43515, val_loss : 14.98900, elapsed : 0.16s, total : 94.80s, remaining : 66.84s\n",
      "*[0600/1024] tr_loss : 16.50300, val_loss : 14.61864, elapsed : 0.15s, total : 94.95s, remaining : 64.77s\n",
      " [0601/1024] tr_loss : 16.33196, val_loss : 14.93390, elapsed : 0.16s, total : 95.11s, remaining : 68.03s\n",
      " [0602/1024] tr_loss : 16.37481, val_loss : 14.98641, elapsed : 0.15s, total : 95.26s, remaining : 65.03s\n",
      " [0603/1024] tr_loss : 16.38154, val_loss : 14.90863, elapsed : 0.17s, total : 95.43s, remaining : 70.40s\n",
      "*[0604/1024] tr_loss : 16.46887, val_loss : 14.60278, elapsed : 0.15s, total : 95.58s, remaining : 64.67s\n",
      " [0605/1024] tr_loss : 16.31108, val_loss : 14.94754, elapsed : 0.16s, total : 95.74s, remaining : 67.19s\n",
      " [0606/1024] tr_loss : 16.29567, val_loss : 15.08064, elapsed : 0.15s, total : 95.90s, remaining : 63.91s\n",
      "*[0607/1024] tr_loss : 16.40938, val_loss : 14.47100, elapsed : 0.16s, total : 96.06s, remaining : 67.34s\n",
      " [0608/1024] tr_loss : 16.25534, val_loss : 14.82879, elapsed : 0.15s, total : 96.21s, remaining : 64.29s\n",
      " [0609/1024] tr_loss : 16.26034, val_loss : 14.74480, elapsed : 0.16s, total : 96.37s, remaining : 66.24s\n",
      " [0610/1024] tr_loss : 16.22743, val_loss : 15.20714, elapsed : 0.15s, total : 96.53s, remaining : 63.87s\n",
      "*[0611/1024] tr_loss : 16.48661, val_loss : 14.13213, elapsed : 0.16s, total : 96.68s, remaining : 64.84s\n",
      " [0612/1024] tr_loss : 16.24204, val_loss : 15.10568, elapsed : 0.16s, total : 96.84s, remaining : 63.87s\n",
      " [0613/1024] tr_loss : 16.30170, val_loss : 14.93044, elapsed : 0.16s, total : 97.00s, remaining : 67.04s\n",
      " [0614/1024] tr_loss : 16.32667, val_loss : 14.47352, elapsed : 0.16s, total : 97.16s, remaining : 65.30s\n",
      " [0615/1024] tr_loss : 16.14782, val_loss : 14.57983, elapsed : 0.16s, total : 97.32s, remaining : 65.88s\n",
      " [0616/1024] tr_loss : 16.08314, val_loss : 14.72622, elapsed : 0.15s, total : 97.48s, remaining : 62.98s\n",
      " [0617/1024] tr_loss : 16.05978, val_loss : 15.12275, elapsed : 0.16s, total : 97.64s, remaining : 64.18s\n",
      "*[0618/1024] tr_loss : 16.36870, val_loss : 14.08977, elapsed : 0.15s, total : 97.79s, remaining : 62.83s\n",
      " [0619/1024] tr_loss : 16.11710, val_loss : 14.91274, elapsed : 0.16s, total : 97.96s, remaining : 66.72s\n",
      " [0620/1024] tr_loss : 16.10259, val_loss : 14.89375, elapsed : 0.19s, total : 98.14s, remaining : 76.08s\n",
      "*[0621/1024] tr_loss : 16.27270, val_loss : 14.04765, elapsed : 0.17s, total : 98.31s, remaining : 68.14s\n",
      " [0622/1024] tr_loss : 15.96520, val_loss : 14.60162, elapsed : 0.16s, total : 98.47s, remaining : 65.05s\n",
      " [0623/1024] tr_loss : 15.93190, val_loss : 14.65680, elapsed : 0.16s, total : 98.64s, remaining : 64.75s\n",
      " [0624/1024] tr_loss : 15.98714, val_loss : 14.70501, elapsed : 0.15s, total : 98.79s, remaining : 61.39s\n",
      " [0625/1024] tr_loss : 16.05064, val_loss : 14.18277, elapsed : 0.16s, total : 98.95s, remaining : 63.16s\n",
      " [0626/1024] tr_loss : 15.94028, val_loss : 14.64228, elapsed : 0.16s, total : 99.11s, remaining : 62.89s\n",
      " [0627/1024] tr_loss : 15.87753, val_loss : 15.03350, elapsed : 0.16s, total : 99.26s, remaining : 62.80s\n",
      "*[0628/1024] tr_loss : 16.20819, val_loss : 13.83723, elapsed : 0.15s, total : 99.42s, remaining : 60.96s\n",
      " [0629/1024] tr_loss : 15.87327, val_loss : 14.72840, elapsed : 0.16s, total : 99.58s, remaining : 63.27s\n",
      " [0630/1024] tr_loss : 15.85930, val_loss : 14.74313, elapsed : 0.15s, total : 99.73s, remaining : 60.01s\n",
      " [0631/1024] tr_loss : 16.02590, val_loss : 14.05533, elapsed : 0.16s, total : 99.89s, remaining : 61.68s\n",
      " [0632/1024] tr_loss : 15.75526, val_loss : 14.38308, elapsed : 0.15s, total : 100.04s, remaining : 59.90s\n",
      " [0633/1024] tr_loss : 15.72785, val_loss : 14.45653, elapsed : 0.20s, total : 100.24s, remaining : 76.63s\n",
      " [0634/1024] tr_loss : 15.74972, val_loss : 14.66692, elapsed : 0.16s, total : 100.39s, remaining : 61.36s\n",
      " [0635/1024] tr_loss : 15.88726, val_loss : 14.03509, elapsed : 0.16s, total : 100.55s, remaining : 61.46s\n",
      " [0636/1024] tr_loss : 15.74517, val_loss : 14.44192, elapsed : 0.15s, total : 100.71s, remaining : 59.39s\n",
      " [0637/1024] tr_loss : 15.67300, val_loss : 14.80519, elapsed : 0.16s, total : 100.86s, remaining : 60.70s\n",
      "*[0638/1024] tr_loss : 15.97917, val_loss : 13.73293, elapsed : 0.15s, total : 101.01s, remaining : 59.06s\n",
      " [0639/1024] tr_loss : 15.62970, val_loss : 14.49623, elapsed : 0.16s, total : 101.18s, remaining : 61.99s\n",
      " [0640/1024] tr_loss : 15.62635, val_loss : 14.49882, elapsed : 0.15s, total : 101.33s, remaining : 58.38s\n",
      " [0641/1024] tr_loss : 15.75433, val_loss : 14.13295, elapsed : 0.16s, total : 101.48s, remaining : 59.99s\n",
      " [0642/1024] tr_loss : 15.61394, val_loss : 14.18400, elapsed : 0.16s, total : 101.64s, remaining : 59.79s\n",
      " [0643/1024] tr_loss : 15.57412, val_loss : 14.33278, elapsed : 0.16s, total : 101.80s, remaining : 60.29s\n",
      " [0644/1024] tr_loss : 15.52321, val_loss : 14.70050, elapsed : 0.15s, total : 101.95s, remaining : 58.03s\n",
      " [0645/1024] tr_loss : 15.79336, val_loss : 13.82124, elapsed : 0.16s, total : 102.11s, remaining : 59.71s\n",
      " [0646/1024] tr_loss : 15.55856, val_loss : 14.43448, elapsed : 0.15s, total : 102.26s, remaining : 57.73s\n",
      " [0647/1024] tr_loss : 15.51059, val_loss : 14.63715, elapsed : 0.16s, total : 102.42s, remaining : 59.10s\n",
      "*[0648/1024] tr_loss : 15.75724, val_loss : 13.67731, elapsed : 0.15s, total : 102.57s, remaining : 57.54s\n",
      " [0649/1024] tr_loss : 15.42510, val_loss : 14.32672, elapsed : 0.16s, total : 102.74s, remaining : 61.33s\n",
      " [0650/1024] tr_loss : 15.42635, val_loss : 14.30534, elapsed : 0.16s, total : 102.90s, remaining : 60.92s\n",
      " [0651/1024] tr_loss : 15.47103, val_loss : 14.29691, elapsed : 0.16s, total : 103.06s, remaining : 60.76s\n",
      " [0652/1024] tr_loss : 15.52069, val_loss : 13.94515, elapsed : 0.15s, total : 103.22s, remaining : 57.60s\n",
      " [0653/1024] tr_loss : 15.41235, val_loss : 14.27063, elapsed : 0.17s, total : 103.39s, remaining : 62.64s\n",
      " [0654/1024] tr_loss : 15.32590, val_loss : 14.60928, elapsed : 0.16s, total : 103.54s, remaining : 57.76s\n",
      "*[0655/1024] tr_loss : 15.61928, val_loss : 13.64868, elapsed : 0.16s, total : 103.70s, remaining : 58.74s\n",
      " [0656/1024] tr_loss : 15.29641, val_loss : 14.35175, elapsed : 0.15s, total : 103.86s, remaining : 56.93s\n",
      " [0657/1024] tr_loss : 15.29219, val_loss : 14.31690, elapsed : 0.16s, total : 104.01s, remaining : 58.18s\n",
      " [0658/1024] tr_loss : 15.36710, val_loss : 14.06356, elapsed : 0.15s, total : 104.17s, remaining : 56.52s\n",
      " [0659/1024] tr_loss : 15.32097, val_loss : 14.04213, elapsed : 0.16s, total : 104.33s, remaining : 57.60s\n",
      " [0660/1024] tr_loss : 15.29022, val_loss : 14.24018, elapsed : 0.15s, total : 104.48s, remaining : 55.75s\n",
      " [0661/1024] tr_loss : 15.20336, val_loss : 14.53480, elapsed : 0.16s, total : 104.64s, remaining : 57.04s\n",
      " [0662/1024] tr_loss : 15.47028, val_loss : 13.67980, elapsed : 0.15s, total : 104.79s, remaining : 55.34s\n",
      " [0663/1024] tr_loss : 15.19021, val_loss : 14.33972, elapsed : 0.16s, total : 104.95s, remaining : 56.40s\n",
      " [0664/1024] tr_loss : 15.19547, val_loss : 14.32433, elapsed : 0.15s, total : 105.10s, remaining : 55.65s\n",
      " [0665/1024] tr_loss : 15.27092, val_loss : 13.97166, elapsed : 0.16s, total : 105.26s, remaining : 56.53s\n",
      " [0666/1024] tr_loss : 15.22248, val_loss : 14.03016, elapsed : 0.15s, total : 105.41s, remaining : 54.35s\n",
      " [0667/1024] tr_loss : 15.21098, val_loss : 14.22131, elapsed : 0.16s, total : 105.57s, remaining : 56.62s\n",
      " [0668/1024] tr_loss : 15.12514, val_loss : 14.56091, elapsed : 0.15s, total : 105.72s, remaining : 54.81s\n",
      "*[0669/1024] tr_loss : 15.38371, val_loss : 13.63904, elapsed : 0.16s, total : 105.88s, remaining : 55.70s\n",
      " [0670/1024] tr_loss : 15.07440, val_loss : 14.27352, elapsed : 0.16s, total : 106.04s, remaining : 55.83s\n",
      " [0671/1024] tr_loss : 15.06935, val_loss : 14.33133, elapsed : 0.16s, total : 106.19s, remaining : 55.46s\n",
      " [0672/1024] tr_loss : 15.18748, val_loss : 13.82560, elapsed : 0.15s, total : 106.35s, remaining : 53.77s\n",
      " [0673/1024] tr_loss : 15.05383, val_loss : 14.03821, elapsed : 0.16s, total : 106.50s, remaining : 55.13s\n",
      " [0674/1024] tr_loss : 15.06724, val_loss : 14.13395, elapsed : 0.15s, total : 106.66s, remaining : 53.48s\n",
      " [0675/1024] tr_loss : 14.99325, val_loss : 14.53134, elapsed : 0.16s, total : 106.82s, remaining : 55.47s\n",
      "*[0676/1024] tr_loss : 15.23089, val_loss : 13.59610, elapsed : 0.15s, total : 106.97s, remaining : 53.48s\n",
      " [0677/1024] tr_loss : 14.96111, val_loss : 14.23673, elapsed : 0.17s, total : 107.14s, remaining : 57.93s\n",
      " [0678/1024] tr_loss : 14.92413, val_loss : 14.36750, elapsed : 0.16s, total : 107.30s, remaining : 55.83s\n",
      " [0679/1024] tr_loss : 15.10408, val_loss : 13.63499, elapsed : 0.17s, total : 107.46s, remaining : 56.93s\n",
      " [0680/1024] tr_loss : 14.89391, val_loss : 14.11227, elapsed : 0.15s, total : 107.62s, remaining : 53.18s\n",
      " [0681/1024] tr_loss : 14.95565, val_loss : 14.06198, elapsed : 0.16s, total : 107.78s, remaining : 55.29s\n",
      " [0682/1024] tr_loss : 14.89327, val_loss : 14.46677, elapsed : 0.16s, total : 107.94s, remaining : 55.02s\n",
      "*[0683/1024] tr_loss : 15.09498, val_loss : 13.55564, elapsed : 0.16s, total : 108.10s, remaining : 54.94s\n",
      " [0684/1024] tr_loss : 14.86515, val_loss : 14.18941, elapsed : 0.16s, total : 108.26s, remaining : 52.76s\n",
      " [0685/1024] tr_loss : 14.81826, val_loss : 14.38488, elapsed : 0.16s, total : 108.42s, remaining : 54.84s\n",
      " [0686/1024] tr_loss : 14.99744, val_loss : 13.57468, elapsed : 0.15s, total : 108.57s, remaining : 52.36s\n",
      " [0687/1024] tr_loss : 14.77273, val_loss : 14.03217, elapsed : 0.16s, total : 108.73s, remaining : 53.50s\n",
      " [0688/1024] tr_loss : 14.83458, val_loss : 14.02281, elapsed : 0.15s, total : 108.88s, remaining : 51.23s\n",
      " [0689/1024] tr_loss : 14.77486, val_loss : 14.44472, elapsed : 0.16s, total : 109.05s, remaining : 54.65s\n",
      " [0690/1024] tr_loss : 14.95915, val_loss : 13.57193, elapsed : 0.16s, total : 109.20s, remaining : 52.09s\n",
      " [0691/1024] tr_loss : 14.72495, val_loss : 14.16039, elapsed : 0.16s, total : 109.36s, remaining : 52.72s\n",
      " [0692/1024] tr_loss : 14.68292, val_loss : 14.38815, elapsed : 0.15s, total : 109.51s, remaining : 50.73s\n",
      " [0693/1024] tr_loss : 14.84266, val_loss : 13.59162, elapsed : 0.16s, total : 109.67s, remaining : 52.17s\n",
      " [0694/1024] tr_loss : 14.64578, val_loss : 14.02011, elapsed : 0.15s, total : 109.82s, remaining : 50.05s\n",
      " [0695/1024] tr_loss : 14.72206, val_loss : 14.08892, elapsed : 0.16s, total : 109.98s, remaining : 52.14s\n",
      " [0696/1024] tr_loss : 14.65840, val_loss : 14.47750, elapsed : 0.15s, total : 110.14s, remaining : 50.46s\n",
      " [0697/1024] tr_loss : 14.83005, val_loss : 13.57658, elapsed : 0.16s, total : 110.30s, remaining : 53.05s\n",
      " [0698/1024] tr_loss : 14.61903, val_loss : 14.18317, elapsed : 0.15s, total : 110.45s, remaining : 50.16s\n",
      " [0699/1024] tr_loss : 14.55612, val_loss : 14.37958, elapsed : 0.16s, total : 110.61s, remaining : 51.95s\n",
      " [0700/1024] tr_loss : 14.68599, val_loss : 13.62631, elapsed : 0.15s, total : 110.77s, remaining : 49.94s\n",
      " [0701/1024] tr_loss : 14.51987, val_loss : 14.10581, elapsed : 0.16s, total : 110.92s, remaining : 50.88s\n",
      " [0702/1024] tr_loss : 14.60322, val_loss : 14.16947, elapsed : 0.15s, total : 111.08s, remaining : 49.49s\n",
      " [0703/1024] tr_loss : 14.49557, val_loss : 14.40436, elapsed : 0.16s, total : 111.24s, remaining : 51.25s\n",
      " [0704/1024] tr_loss : 14.66504, val_loss : 13.76261, elapsed : 0.15s, total : 111.39s, remaining : 48.84s\n",
      " [0705/1024] tr_loss : 14.44978, val_loss : 14.34287, elapsed : 0.16s, total : 111.55s, remaining : 49.89s\n",
      " [0706/1024] tr_loss : 14.40873, val_loss : 14.29859, elapsed : 0.15s, total : 111.70s, remaining : 48.75s\n",
      " [0707/1024] tr_loss : 14.44066, val_loss : 13.78112, elapsed : 0.16s, total : 111.86s, remaining : 50.35s\n",
      " [0708/1024] tr_loss : 14.35911, val_loss : 14.28191, elapsed : 0.16s, total : 112.01s, remaining : 49.01s\n",
      " [0709/1024] tr_loss : 14.40551, val_loss : 14.20471, elapsed : 0.16s, total : 112.17s, remaining : 50.57s\n",
      " [0710/1024] tr_loss : 14.30392, val_loss : 14.39980, elapsed : 0.15s, total : 112.33s, remaining : 48.03s\n",
      " [0711/1024] tr_loss : 14.48390, val_loss : 13.68040, elapsed : 0.16s, total : 112.49s, remaining : 50.25s\n",
      " [0712/1024] tr_loss : 14.23571, val_loss : 14.44119, elapsed : 0.15s, total : 112.64s, remaining : 48.27s\n",
      " [0713/1024] tr_loss : 14.23684, val_loss : 14.28072, elapsed : 0.16s, total : 112.80s, remaining : 49.12s\n",
      " [0714/1024] tr_loss : 14.24530, val_loss : 13.96380, elapsed : 0.15s, total : 112.95s, remaining : 47.72s\n",
      " [0715/1024] tr_loss : 14.25687, val_loss : 14.19688, elapsed : 0.20s, total : 113.16s, remaining : 62.33s\n",
      " [0716/1024] tr_loss : 14.20868, val_loss : 14.30126, elapsed : 0.16s, total : 113.31s, remaining : 48.67s\n",
      " [0717/1024] tr_loss : 14.15490, val_loss : 14.37757, elapsed : 0.16s, total : 113.48s, remaining : 49.27s\n",
      " [0718/1024] tr_loss : 14.34567, val_loss : 13.67444, elapsed : 0.15s, total : 113.63s, remaining : 46.92s\n",
      " [0719/1024] tr_loss : 14.10702, val_loss : 14.50806, elapsed : 0.16s, total : 113.79s, remaining : 48.38s\n",
      " [0720/1024] tr_loss : 14.16321, val_loss : 14.28909, elapsed : 0.15s, total : 113.94s, remaining : 46.59s\n",
      " [0721/1024] tr_loss : 14.10862, val_loss : 14.16693, elapsed : 0.16s, total : 114.10s, remaining : 48.10s\n",
      " [0722/1024] tr_loss : 14.24909, val_loss : 14.06144, elapsed : 0.16s, total : 114.26s, remaining : 49.28s\n",
      " [0723/1024] tr_loss : 14.09441, val_loss : 14.50558, elapsed : 0.16s, total : 114.42s, remaining : 47.91s\n",
      " [0724/1024] tr_loss : 14.07007, val_loss : 14.34959, elapsed : 0.15s, total : 114.58s, remaining : 46.29s\n",
      " [0725/1024] tr_loss : 14.15449, val_loss : 13.78013, elapsed : 0.16s, total : 114.74s, remaining : 49.26s\n",
      " [0726/1024] tr_loss : 14.03846, val_loss : 14.39442, elapsed : 0.15s, total : 114.89s, remaining : 45.40s\n",
      " [0727/1024] tr_loss : 14.08333, val_loss : 14.31262, elapsed : 0.16s, total : 115.05s, remaining : 47.29s\n",
      " [0728/1024] tr_loss : 14.03316, val_loss : 14.35454, elapsed : 0.16s, total : 115.21s, remaining : 45.90s\n",
      " [0729/1024] tr_loss : 14.24119, val_loss : 13.72575, elapsed : 0.17s, total : 115.38s, remaining : 49.46s\n",
      " [0730/1024] tr_loss : 13.96713, val_loss : 14.63242, elapsed : 0.16s, total : 115.53s, remaining : 45.89s\n",
      " [0731/1024] tr_loss : 14.03146, val_loss : 14.38294, elapsed : 0.16s, total : 115.69s, remaining : 46.50s\n",
      " [0732/1024] tr_loss : 14.01814, val_loss : 14.06488, elapsed : 0.15s, total : 115.84s, remaining : 44.80s\n",
      " [0733/1024] tr_loss : 14.05882, val_loss : 14.27453, elapsed : 0.16s, total : 116.00s, remaining : 46.36s\n",
      " [0734/1024] tr_loss : 14.00833, val_loss : 14.39018, elapsed : 0.15s, total : 116.16s, remaining : 44.58s\n",
      " [0735/1024] tr_loss : 13.99535, val_loss : 14.49926, elapsed : 0.16s, total : 116.32s, remaining : 46.17s\n",
      " [0736/1024] tr_loss : 14.16772, val_loss : 13.69237, elapsed : 0.15s, total : 116.47s, remaining : 44.33s\n",
      " [0737/1024] tr_loss : 13.87071, val_loss : 14.71366, elapsed : 0.16s, total : 116.63s, remaining : 46.75s\n",
      " [0738/1024] tr_loss : 13.94505, val_loss : 14.38893, elapsed : 0.16s, total : 116.79s, remaining : 44.58s\n",
      " [0739/1024] tr_loss : 13.89042, val_loss : 14.17739, elapsed : 0.17s, total : 116.96s, remaining : 47.89s\n",
      " [0740/1024] tr_loss : 14.01045, val_loss : 14.09440, elapsed : 0.16s, total : 117.12s, remaining : 45.11s\n",
      " [0741/1024] tr_loss : 13.86668, val_loss : 14.47755, elapsed : 0.16s, total : 117.27s, remaining : 44.31s\n",
      " [0742/1024] tr_loss : 13.84831, val_loss : 14.44916, elapsed : 0.16s, total : 117.43s, remaining : 44.11s\n",
      " [0743/1024] tr_loss : 14.03482, val_loss : 13.58731, elapsed : 0.16s, total : 117.59s, remaining : 45.87s\n",
      " [0744/1024] tr_loss : 13.78060, val_loss : 14.70188, elapsed : 0.15s, total : 117.75s, remaining : 42.76s\n",
      " [0745/1024] tr_loss : 13.88737, val_loss : 14.35363, elapsed : 0.16s, total : 117.91s, remaining : 44.75s\n",
      " [0746/1024] tr_loss : 13.76800, val_loss : 14.32973, elapsed : 0.15s, total : 118.06s, remaining : 42.82s\n",
      " [0747/1024] tr_loss : 13.92106, val_loss : 14.01426, elapsed : 0.16s, total : 118.22s, remaining : 43.66s\n",
      " [0748/1024] tr_loss : 13.73037, val_loss : 14.58557, elapsed : 0.15s, total : 118.37s, remaining : 42.16s\n",
      " [0749/1024] tr_loss : 13.68627, val_loss : 14.46615, elapsed : 0.16s, total : 118.53s, remaining : 43.37s\n",
      " [0750/1024] tr_loss : 13.76008, val_loss : 13.72413, elapsed : 0.15s, total : 118.68s, remaining : 42.29s\n",
      " [0751/1024] tr_loss : 13.58171, val_loss : 14.78273, elapsed : 0.16s, total : 118.84s, remaining : 42.82s\n",
      " [0752/1024] tr_loss : 13.70148, val_loss : 14.16133, elapsed : 0.15s, total : 118.99s, remaining : 41.33s\n",
      " [0753/1024] tr_loss : 13.55860, val_loss : 14.36757, elapsed : 0.16s, total : 119.16s, remaining : 44.41s\n",
      " [0754/1024] tr_loss : 13.68466, val_loss : 13.86467, elapsed : 0.15s, total : 119.31s, remaining : 41.53s\n",
      " [0755/1024] tr_loss : 13.46029, val_loss : 14.72126, elapsed : 0.16s, total : 119.47s, remaining : 42.35s\n",
      " [0756/1024] tr_loss : 13.50251, val_loss : 14.37952, elapsed : 0.15s, total : 119.62s, remaining : 41.41s\n",
      " [0757/1024] tr_loss : 13.48199, val_loss : 14.12391, elapsed : 0.16s, total : 119.78s, remaining : 42.58s\n",
      " [0758/1024] tr_loss : 13.50238, val_loss : 14.46458, elapsed : 0.15s, total : 119.93s, remaining : 40.53s\n",
      " [0759/1024] tr_loss : 13.41391, val_loss : 14.40619, elapsed : 0.16s, total : 120.09s, remaining : 41.72s\n",
      " [0760/1024] tr_loss : 13.39025, val_loss : 14.59922, elapsed : 0.15s, total : 120.25s, remaining : 40.91s\n",
      " [0761/1024] tr_loss : 13.51798, val_loss : 13.76045, elapsed : 0.16s, total : 120.40s, remaining : 41.46s\n",
      " [0762/1024] tr_loss : 13.28848, val_loss : 14.78766, elapsed : 0.15s, total : 120.56s, remaining : 39.92s\n",
      " [0763/1024] tr_loss : 13.30907, val_loss : 14.15160, elapsed : 0.16s, total : 120.71s, remaining : 41.09s\n",
      " [0764/1024] tr_loss : 13.32359, val_loss : 14.55351, elapsed : 0.15s, total : 120.87s, remaining : 40.04s\n",
      " [0765/1024] tr_loss : 13.46501, val_loss : 13.68128, elapsed : 0.16s, total : 121.03s, remaining : 41.47s\n",
      " [0766/1024] tr_loss : 13.18483, val_loss : 14.74415, elapsed : 0.15s, total : 121.18s, remaining : 39.43s\n",
      " [0767/1024] tr_loss : 13.20935, val_loss : 14.08852, elapsed : 0.16s, total : 121.34s, remaining : 40.55s\n",
      " [0768/1024] tr_loss : 13.25566, val_loss : 14.42259, elapsed : 0.15s, total : 121.49s, remaining : 38.96s\n",
      " [0769/1024] tr_loss : 13.34663, val_loss : 13.82259, elapsed : 0.16s, total : 121.65s, remaining : 40.32s\n",
      " [0770/1024] tr_loss : 13.14257, val_loss : 14.56278, elapsed : 0.15s, total : 121.80s, remaining : 38.72s\n",
      " [0771/1024] tr_loss : 13.14106, val_loss : 13.92506, elapsed : 0.16s, total : 121.96s, remaining : 39.74s\n",
      " [0772/1024] tr_loss : 13.21976, val_loss : 14.39271, elapsed : 0.19s, total : 122.15s, remaining : 47.34s\n",
      " [0773/1024] tr_loss : 13.25252, val_loss : 13.99611, elapsed : 0.16s, total : 122.30s, remaining : 39.92s\n",
      " [0774/1024] tr_loss : 13.17455, val_loss : 14.56570, elapsed : 0.16s, total : 122.46s, remaining : 39.35s\n",
      " [0775/1024] tr_loss : 13.13987, val_loss : 13.86856, elapsed : 0.17s, total : 122.63s, remaining : 42.75s\n",
      " [0776/1024] tr_loss : 13.21590, val_loss : 14.61890, elapsed : 0.15s, total : 122.79s, remaining : 38.32s\n",
      " [0777/1024] tr_loss : 13.21674, val_loss : 14.24264, elapsed : 0.16s, total : 122.95s, remaining : 39.07s\n",
      " [0778/1024] tr_loss : 13.15998, val_loss : 14.59365, elapsed : 0.15s, total : 123.10s, remaining : 37.97s\n",
      " [0779/1024] tr_loss : 13.10967, val_loss : 14.00225, elapsed : 0.16s, total : 123.26s, remaining : 38.86s\n",
      " [0780/1024] tr_loss : 13.14766, val_loss : 14.81487, elapsed : 0.16s, total : 123.42s, remaining : 38.16s\n",
      " [0781/1024] tr_loss : 13.12227, val_loss : 14.52509, elapsed : 0.16s, total : 123.58s, remaining : 38.89s\n",
      " [0782/1024] tr_loss : 13.12749, val_loss : 14.65108, elapsed : 0.15s, total : 123.73s, remaining : 37.23s\n",
      " [0783/1024] tr_loss : 13.09905, val_loss : 14.11380, elapsed : 0.16s, total : 123.89s, remaining : 39.65s\n",
      " [0784/1024] tr_loss : 13.14221, val_loss : 14.90872, elapsed : 0.15s, total : 124.05s, remaining : 37.16s\n",
      " [0785/1024] tr_loss : 13.08501, val_loss : 14.79955, elapsed : 0.16s, total : 124.21s, remaining : 38.42s\n",
      " [0786/1024] tr_loss : 13.10597, val_loss : 14.70138, elapsed : 0.15s, total : 124.36s, remaining : 36.62s\n",
      " [0787/1024] tr_loss : 13.10474, val_loss : 14.08184, elapsed : 0.16s, total : 124.52s, remaining : 37.35s\n",
      " [0788/1024] tr_loss : 13.08047, val_loss : 14.97917, elapsed : 0.15s, total : 124.67s, remaining : 35.99s\n",
      " [0789/1024] tr_loss : 13.00537, val_loss : 14.71559, elapsed : 0.16s, total : 124.83s, remaining : 37.52s\n",
      " [0790/1024] tr_loss : 13.03649, val_loss : 14.75530, elapsed : 0.15s, total : 124.99s, remaining : 35.60s\n",
      " [0791/1024] tr_loss : 13.08392, val_loss : 14.04056, elapsed : 0.16s, total : 125.15s, remaining : 37.33s\n",
      " [0792/1024] tr_loss : 12.95835, val_loss : 15.10777, elapsed : 0.15s, total : 125.30s, remaining : 35.64s\n",
      " [0793/1024] tr_loss : 12.91661, val_loss : 14.42909, elapsed : 0.16s, total : 125.46s, remaining : 36.09s\n",
      " [0794/1024] tr_loss : 12.91305, val_loss : 14.56247, elapsed : 0.16s, total : 125.61s, remaining : 36.51s\n",
      " [0795/1024] tr_loss : 13.10154, val_loss : 14.02876, elapsed : 0.20s, total : 125.82s, remaining : 46.34s\n",
      " [0796/1024] tr_loss : 12.91267, val_loss : 15.22765, elapsed : 0.16s, total : 125.98s, remaining : 36.19s\n",
      " [0797/1024] tr_loss : 12.92790, val_loss : 14.34218, elapsed : 0.16s, total : 126.14s, remaining : 36.29s\n",
      " [0798/1024] tr_loss : 12.84392, val_loss : 14.31243, elapsed : 0.15s, total : 126.29s, remaining : 34.61s\n",
      " [0799/1024] tr_loss : 13.05355, val_loss : 14.13235, elapsed : 0.16s, total : 126.45s, remaining : 36.18s\n",
      " [0800/1024] tr_loss : 12.89616, val_loss : 15.01664, elapsed : 0.16s, total : 126.61s, remaining : 35.33s\n",
      " [0801/1024] tr_loss : 12.94530, val_loss : 14.59238, elapsed : 0.16s, total : 126.77s, remaining : 35.45s\n",
      " [0802/1024] tr_loss : 12.87914, val_loss : 14.12605, elapsed : 0.15s, total : 126.92s, remaining : 34.07s\n",
      " [0803/1024] tr_loss : 12.97873, val_loss : 14.75648, elapsed : 0.16s, total : 127.08s, remaining : 35.72s\n",
      " [0804/1024] tr_loss : 12.85843, val_loss : 14.76682, elapsed : 0.16s, total : 127.24s, remaining : 35.42s\n",
      " [0805/1024] tr_loss : 12.89176, val_loss : 14.70447, elapsed : 0.16s, total : 127.40s, remaining : 34.84s\n",
      " [0806/1024] tr_loss : 12.90471, val_loss : 14.00283, elapsed : 0.15s, total : 127.56s, remaining : 33.46s\n",
      " [0807/1024] tr_loss : 12.80715, val_loss : 15.02199, elapsed : 0.16s, total : 127.71s, remaining : 34.18s\n",
      " [0808/1024] tr_loss : 12.74441, val_loss : 14.41892, elapsed : 0.15s, total : 127.87s, remaining : 33.13s\n",
      " [0809/1024] tr_loss : 12.74593, val_loss : 14.53390, elapsed : 0.16s, total : 128.02s, remaining : 34.00s\n",
      " [0810/1024] tr_loss : 12.90780, val_loss : 13.94500, elapsed : 0.15s, total : 128.18s, remaining : 32.69s\n",
      " [0811/1024] tr_loss : 12.69420, val_loss : 15.14391, elapsed : 0.16s, total : 128.33s, remaining : 33.40s\n",
      " [0812/1024] tr_loss : 12.70937, val_loss : 14.26545, elapsed : 0.15s, total : 128.49s, remaining : 32.16s\n",
      " [0813/1024] tr_loss : 12.63848, val_loss : 14.28106, elapsed : 0.16s, total : 128.64s, remaining : 33.41s\n",
      " [0814/1024] tr_loss : 12.83635, val_loss : 14.11515, elapsed : 0.16s, total : 128.80s, remaining : 33.63s\n",
      " [0815/1024] tr_loss : 12.69322, val_loss : 15.00655, elapsed : 0.16s, total : 128.96s, remaining : 33.10s\n",
      " [0816/1024] tr_loss : 12.71525, val_loss : 14.35394, elapsed : 0.15s, total : 129.12s, remaining : 32.15s\n",
      " [0817/1024] tr_loss : 12.63287, val_loss : 14.13757, elapsed : 0.16s, total : 129.28s, remaining : 33.25s\n",
      " [0818/1024] tr_loss : 12.75676, val_loss : 14.36419, elapsed : 0.15s, total : 129.43s, remaining : 31.65s\n",
      " [0819/1024] tr_loss : 12.66795, val_loss : 14.76821, elapsed : 0.16s, total : 129.59s, remaining : 33.19s\n",
      " [0820/1024] tr_loss : 12.69848, val_loss : 14.57682, elapsed : 0.15s, total : 129.75s, remaining : 31.25s\n",
      " [0821/1024] tr_loss : 12.60765, val_loss : 14.10039, elapsed : 0.16s, total : 129.91s, remaining : 32.72s\n",
      " [0822/1024] tr_loss : 12.75117, val_loss : 14.72304, elapsed : 0.16s, total : 130.06s, remaining : 31.43s\n",
      " [0823/1024] tr_loss : 12.66284, val_loss : 14.69485, elapsed : 0.16s, total : 130.22s, remaining : 31.89s\n",
      " [0824/1024] tr_loss : 12.67623, val_loss : 14.60338, elapsed : 0.15s, total : 130.38s, remaining : 30.54s\n",
      " [0825/1024] tr_loss : 12.62560, val_loss : 14.15046, elapsed : 0.16s, total : 130.53s, remaining : 31.52s\n",
      " [0826/1024] tr_loss : 12.61994, val_loss : 14.92252, elapsed : 0.15s, total : 130.69s, remaining : 30.26s\n",
      " [0827/1024] tr_loss : 12.55346, val_loss : 14.64243, elapsed : 0.16s, total : 130.85s, remaining : 31.61s\n",
      " [0828/1024] tr_loss : 12.56104, val_loss : 14.56265, elapsed : 0.16s, total : 131.00s, remaining : 30.44s\n",
      " [0829/1024] tr_loss : 12.59502, val_loss : 14.15040, elapsed : 0.17s, total : 131.17s, remaining : 33.15s\n",
      " [0830/1024] tr_loss : 12.53715, val_loss : 15.05761, elapsed : 0.16s, total : 131.33s, remaining : 30.46s\n",
      " [0831/1024] tr_loss : 12.49162, val_loss : 14.51567, elapsed : 0.16s, total : 131.49s, remaining : 31.11s\n",
      " [0832/1024] tr_loss : 12.47158, val_loss : 14.49420, elapsed : 0.15s, total : 131.64s, remaining : 29.57s\n",
      " [0833/1024] tr_loss : 12.57198, val_loss : 14.16891, elapsed : 0.16s, total : 131.81s, remaining : 31.51s\n",
      " [0834/1024] tr_loss : 12.47437, val_loss : 15.27494, elapsed : 0.15s, total : 131.96s, remaining : 29.44s\n",
      " [0835/1024] tr_loss : 12.45925, val_loss : 14.42596, elapsed : 0.16s, total : 132.13s, remaining : 30.66s\n",
      " [0836/1024] tr_loss : 12.40987, val_loss : 14.46766, elapsed : 0.16s, total : 132.28s, remaining : 29.64s\n",
      " [0837/1024] tr_loss : 12.52096, val_loss : 14.20220, elapsed : 0.17s, total : 132.45s, remaining : 31.36s\n",
      " [0838/1024] tr_loss : 12.41628, val_loss : 15.25540, elapsed : 0.15s, total : 132.61s, remaining : 28.76s\n",
      " [0839/1024] tr_loss : 12.41520, val_loss : 14.39052, elapsed : 0.17s, total : 132.77s, remaining : 30.98s\n",
      " [0840/1024] tr_loss : 12.35493, val_loss : 14.43165, elapsed : 0.16s, total : 132.93s, remaining : 28.74s\n",
      " [0841/1024] tr_loss : 12.46535, val_loss : 14.19282, elapsed : 0.17s, total : 133.10s, remaining : 30.37s\n",
      " [0842/1024] tr_loss : 12.37892, val_loss : 15.22638, elapsed : 0.16s, total : 133.25s, remaining : 28.43s\n",
      " [0843/1024] tr_loss : 12.37675, val_loss : 14.45398, elapsed : 0.16s, total : 133.42s, remaining : 29.63s\n",
      " [0844/1024] tr_loss : 12.32372, val_loss : 14.49144, elapsed : 0.16s, total : 133.57s, remaining : 28.22s\n",
      " [0845/1024] tr_loss : 12.39470, val_loss : 14.22965, elapsed : 0.16s, total : 133.74s, remaining : 28.91s\n",
      " [0846/1024] tr_loss : 12.33823, val_loss : 15.18800, elapsed : 0.16s, total : 133.89s, remaining : 27.60s\n",
      " [0847/1024] tr_loss : 12.32939, val_loss : 14.51103, elapsed : 0.16s, total : 134.05s, remaining : 28.07s\n",
      " [0848/1024] tr_loss : 12.28151, val_loss : 14.55054, elapsed : 0.16s, total : 134.21s, remaining : 28.00s\n",
      " [0849/1024] tr_loss : 12.29624, val_loss : 14.20646, elapsed : 0.16s, total : 134.37s, remaining : 27.57s\n",
      " [0850/1024] tr_loss : 12.31065, val_loss : 15.12096, elapsed : 0.15s, total : 134.52s, remaining : 26.63s\n",
      " [0851/1024] tr_loss : 12.25877, val_loss : 14.61578, elapsed : 0.16s, total : 134.68s, remaining : 28.02s\n",
      " [0852/1024] tr_loss : 12.23319, val_loss : 14.57159, elapsed : 0.15s, total : 134.84s, remaining : 26.66s\n",
      " [0853/1024] tr_loss : 12.19652, val_loss : 14.14335, elapsed : 0.16s, total : 135.00s, remaining : 27.68s\n",
      " [0854/1024] tr_loss : 12.32334, val_loss : 14.99110, elapsed : 0.16s, total : 135.15s, remaining : 26.63s\n",
      " [0855/1024] tr_loss : 12.25673, val_loss : 14.71801, elapsed : 0.16s, total : 135.31s, remaining : 27.12s\n",
      " [0856/1024] tr_loss : 12.24416, val_loss : 14.62198, elapsed : 0.15s, total : 135.47s, remaining : 26.00s\n",
      " [0857/1024] tr_loss : 12.16140, val_loss : 14.09894, elapsed : 0.17s, total : 135.64s, remaining : 27.85s\n",
      " [0858/1024] tr_loss : 12.31912, val_loss : 14.84798, elapsed : 0.16s, total : 135.79s, remaining : 25.78s\n",
      " [0859/1024] tr_loss : 12.22287, val_loss : 14.76533, elapsed : 0.19s, total : 135.99s, remaining : 32.02s\n",
      " [0860/1024] tr_loss : 12.18450, val_loss : 14.61950, elapsed : 0.16s, total : 136.14s, remaining : 25.83s\n",
      " [0861/1024] tr_loss : 12.10406, val_loss : 14.06584, elapsed : 0.16s, total : 136.30s, remaining : 25.77s\n",
      " [0862/1024] tr_loss : 12.32084, val_loss : 14.84284, elapsed : 0.15s, total : 136.45s, remaining : 24.74s\n",
      " [0863/1024] tr_loss : 12.19658, val_loss : 14.92698, elapsed : 0.16s, total : 136.62s, remaining : 26.16s\n",
      " [0864/1024] tr_loss : 12.20551, val_loss : 14.67726, elapsed : 0.15s, total : 136.77s, remaining : 24.52s\n",
      " [0865/1024] tr_loss : 12.08611, val_loss : 14.03760, elapsed : 0.16s, total : 136.93s, remaining : 25.05s\n",
      " [0866/1024] tr_loss : 12.37759, val_loss : 14.74800, elapsed : 0.16s, total : 137.08s, remaining : 24.72s\n",
      " [0867/1024] tr_loss : 12.21449, val_loss : 15.26553, elapsed : 0.16s, total : 137.25s, remaining : 25.50s\n",
      " [0868/1024] tr_loss : 12.25347, val_loss : 14.55228, elapsed : 0.15s, total : 137.40s, remaining : 24.14s\n",
      " [0869/1024] tr_loss : 12.09739, val_loss : 14.05171, elapsed : 0.16s, total : 137.56s, remaining : 24.60s\n",
      " [0870/1024] tr_loss : 12.42612, val_loss : 14.52534, elapsed : 0.16s, total : 137.72s, remaining : 24.21s\n",
      " [0871/1024] tr_loss : 12.19022, val_loss : 15.47849, elapsed : 0.16s, total : 137.88s, remaining : 25.07s\n",
      " [0872/1024] tr_loss : 12.25708, val_loss : 14.40272, elapsed : 0.16s, total : 138.04s, remaining : 24.65s\n",
      " [0873/1024] tr_loss : 12.09375, val_loss : 14.14371, elapsed : 0.16s, total : 138.21s, remaining : 24.66s\n",
      " [0874/1024] tr_loss : 12.36385, val_loss : 14.48991, elapsed : 0.15s, total : 138.36s, remaining : 23.05s\n",
      " [0875/1024] tr_loss : 12.12047, val_loss : 15.46857, elapsed : 0.16s, total : 138.52s, remaining : 23.42s\n",
      " [0876/1024] tr_loss : 12.22290, val_loss : 14.48702, elapsed : 0.15s, total : 138.67s, remaining : 22.78s\n",
      " [0877/1024] tr_loss : 12.05244, val_loss : 14.13583, elapsed : 0.16s, total : 138.83s, remaining : 23.40s\n",
      " [0878/1024] tr_loss : 12.32481, val_loss : 14.63033, elapsed : 0.15s, total : 138.98s, remaining : 22.21s\n",
      " [0879/1024] tr_loss : 12.06173, val_loss : 15.44238, elapsed : 0.16s, total : 139.14s, remaining : 23.08s\n",
      " [0880/1024] tr_loss : 12.19798, val_loss : 14.51734, elapsed : 0.15s, total : 139.29s, remaining : 21.87s\n",
      " [0881/1024] tr_loss : 12.02734, val_loss : 14.12790, elapsed : 0.16s, total : 139.45s, remaining : 22.55s\n",
      " [0882/1024] tr_loss : 12.24402, val_loss : 14.62609, elapsed : 0.15s, total : 139.60s, remaining : 21.58s\n",
      " [0883/1024] tr_loss : 11.99528, val_loss : 15.30390, elapsed : 0.16s, total : 139.76s, remaining : 22.25s\n",
      " [0884/1024] tr_loss : 12.10471, val_loss : 14.52397, elapsed : 0.15s, total : 139.91s, remaining : 21.34s\n",
      " [0885/1024] tr_loss : 11.98228, val_loss : 14.16884, elapsed : 0.16s, total : 140.07s, remaining : 22.05s\n",
      " [0886/1024] tr_loss : 12.19481, val_loss : 14.66422, elapsed : 0.15s, total : 140.23s, remaining : 21.17s\n",
      " [0887/1024] tr_loss : 11.95856, val_loss : 15.38935, elapsed : 0.16s, total : 140.39s, remaining : 22.21s\n",
      " [0888/1024] tr_loss : 12.06930, val_loss : 14.55764, elapsed : 0.16s, total : 140.54s, remaining : 21.11s\n",
      " [0889/1024] tr_loss : 11.94277, val_loss : 14.16946, elapsed : 0.16s, total : 140.70s, remaining : 21.61s\n",
      " [0890/1024] tr_loss : 12.16049, val_loss : 14.70556, elapsed : 0.15s, total : 140.86s, remaining : 20.60s\n",
      " [0891/1024] tr_loss : 11.92785, val_loss : 15.41054, elapsed : 0.17s, total : 141.02s, remaining : 22.08s\n",
      " [0892/1024] tr_loss : 12.04383, val_loss : 14.53207, elapsed : 0.16s, total : 141.18s, remaining : 20.74s\n",
      " [0893/1024] tr_loss : 11.92392, val_loss : 14.21674, elapsed : 0.16s, total : 141.34s, remaining : 21.11s\n",
      " [0894/1024] tr_loss : 12.13047, val_loss : 14.78071, elapsed : 0.16s, total : 141.50s, remaining : 20.87s\n",
      " [0895/1024] tr_loss : 11.90164, val_loss : 15.40449, elapsed : 0.16s, total : 141.66s, remaining : 20.91s\n",
      " [0896/1024] tr_loss : 12.00536, val_loss : 14.62238, elapsed : 0.16s, total : 141.82s, remaining : 20.00s\n",
      " [0897/1024] tr_loss : 11.88353, val_loss : 14.18464, elapsed : 0.17s, total : 141.99s, remaining : 21.05s\n",
      " [0898/1024] tr_loss : 12.09249, val_loss : 14.86589, elapsed : 0.15s, total : 142.14s, remaining : 19.36s\n",
      " [0899/1024] tr_loss : 11.87420, val_loss : 15.41148, elapsed : 0.17s, total : 142.31s, remaining : 20.73s\n",
      " [0900/1024] tr_loss : 11.98001, val_loss : 14.67717, elapsed : 0.16s, total : 142.46s, remaining : 19.51s\n",
      " [0901/1024] tr_loss : 11.86110, val_loss : 14.23224, elapsed : 0.16s, total : 142.63s, remaining : 19.91s\n",
      " [0902/1024] tr_loss : 12.02517, val_loss : 14.88434, elapsed : 0.16s, total : 142.78s, remaining : 19.39s\n",
      " [0903/1024] tr_loss : 11.80785, val_loss : 15.30481, elapsed : 0.16s, total : 142.94s, remaining : 19.08s\n",
      " [0904/1024] tr_loss : 11.88781, val_loss : 14.58728, elapsed : 0.16s, total : 143.10s, remaining : 18.99s\n",
      " [0905/1024] tr_loss : 11.79847, val_loss : 14.23366, elapsed : 0.16s, total : 143.26s, remaining : 18.98s\n",
      " [0906/1024] tr_loss : 11.97106, val_loss : 15.01152, elapsed : 0.15s, total : 143.41s, remaining : 18.09s\n",
      " [0907/1024] tr_loss : 11.79366, val_loss : 15.39485, elapsed : 0.16s, total : 143.58s, remaining : 19.13s\n",
      " [0908/1024] tr_loss : 11.89176, val_loss : 14.66059, elapsed : 0.16s, total : 143.73s, remaining : 18.16s\n",
      " [0909/1024] tr_loss : 11.77953, val_loss : 14.21109, elapsed : 0.16s, total : 143.90s, remaining : 18.82s\n",
      " [0910/1024] tr_loss : 11.97371, val_loss : 14.93889, elapsed : 0.15s, total : 144.05s, remaining : 17.65s\n",
      " [0911/1024] tr_loss : 11.76211, val_loss : 15.53777, elapsed : 0.16s, total : 144.21s, remaining : 18.29s\n",
      " [0912/1024] tr_loss : 11.87455, val_loss : 14.69419, elapsed : 0.15s, total : 144.37s, remaining : 17.09s\n",
      " [0913/1024] tr_loss : 11.75365, val_loss : 14.23586, elapsed : 0.16s, total : 144.53s, remaining : 17.84s\n",
      " [0914/1024] tr_loss : 11.97411, val_loss : 14.94251, elapsed : 0.17s, total : 144.70s, remaining : 18.54s\n",
      " [0915/1024] tr_loss : 11.75049, val_loss : 15.63210, elapsed : 0.17s, total : 144.87s, remaining : 18.61s\n",
      " [0916/1024] tr_loss : 11.88388, val_loss : 14.69144, elapsed : 0.18s, total : 145.05s, remaining : 19.43s\n",
      " [0917/1024] tr_loss : 11.74288, val_loss : 14.22892, elapsed : 0.20s, total : 145.25s, remaining : 21.54s\n",
      " [0918/1024] tr_loss : 11.95094, val_loss : 14.91883, elapsed : 0.16s, total : 145.41s, remaining : 17.24s\n",
      " [0919/1024] tr_loss : 11.72050, val_loss : 15.60625, elapsed : 0.16s, total : 145.57s, remaining : 17.07s\n",
      " [0920/1024] tr_loss : 11.83731, val_loss : 14.72890, elapsed : 0.16s, total : 145.74s, remaining : 16.88s\n",
      " [0921/1024] tr_loss : 11.70647, val_loss : 14.27965, elapsed : 0.16s, total : 145.90s, remaining : 16.64s\n",
      " [0922/1024] tr_loss : 11.87624, val_loss : 15.08901, elapsed : 0.15s, total : 146.05s, remaining : 15.73s\n",
      " [0923/1024] tr_loss : 11.67704, val_loss : 15.54417, elapsed : 0.16s, total : 146.21s, remaining : 16.27s\n",
      " [0924/1024] tr_loss : 11.77015, val_loss : 14.76111, elapsed : 0.15s, total : 146.37s, remaining : 15.48s\n",
      " [0925/1024] tr_loss : 11.64958, val_loss : 14.27833, elapsed : 0.17s, total : 146.53s, remaining : 16.52s\n",
      " [0926/1024] tr_loss : 11.82692, val_loss : 15.14132, elapsed : 0.15s, total : 146.69s, remaining : 15.10s\n",
      " [0927/1024] tr_loss : 11.63992, val_loss : 15.62232, elapsed : 0.20s, total : 146.89s, remaining : 19.80s\n",
      " [0928/1024] tr_loss : 11.74123, val_loss : 14.80981, elapsed : 0.16s, total : 147.06s, remaining : 15.66s\n",
      " [0929/1024] tr_loss : 11.61929, val_loss : 14.26240, elapsed : 0.17s, total : 147.23s, remaining : 16.29s\n",
      " [0930/1024] tr_loss : 11.81651, val_loss : 15.15779, elapsed : 0.16s, total : 147.38s, remaining : 14.74s\n",
      " [0931/1024] tr_loss : 11.61827, val_loss : 15.66300, elapsed : 0.18s, total : 147.56s, remaining : 16.32s\n",
      " [0932/1024] tr_loss : 11.72049, val_loss : 14.83107, elapsed : 0.16s, total : 147.72s, remaining : 14.77s\n",
      " [0933/1024] tr_loss : 11.59296, val_loss : 14.29418, elapsed : 0.16s, total : 147.88s, remaining : 14.64s\n",
      " [0934/1024] tr_loss : 11.78891, val_loss : 15.15922, elapsed : 0.16s, total : 148.04s, remaining : 14.39s\n",
      " [0935/1024] tr_loss : 11.59302, val_loss : 15.73688, elapsed : 0.16s, total : 148.20s, remaining : 14.28s\n",
      " [0936/1024] tr_loss : 11.70293, val_loss : 14.85763, elapsed : 0.15s, total : 148.36s, remaining : 13.52s\n",
      " [0937/1024] tr_loss : 11.56792, val_loss : 14.33443, elapsed : 0.17s, total : 148.52s, remaining : 14.56s\n",
      " [0938/1024] tr_loss : 11.75993, val_loss : 15.17579, elapsed : 0.16s, total : 148.68s, remaining : 13.51s\n",
      " [0939/1024] tr_loss : 11.57087, val_loss : 15.76242, elapsed : 0.16s, total : 148.84s, remaining : 13.63s\n",
      " [0940/1024] tr_loss : 11.68391, val_loss : 14.83965, elapsed : 0.16s, total : 149.00s, remaining : 13.05s\n",
      " [0941/1024] tr_loss : 11.54144, val_loss : 14.33149, elapsed : 0.16s, total : 149.16s, remaining : 13.43s\n",
      " [0942/1024] tr_loss : 11.70687, val_loss : 15.24544, elapsed : 0.15s, total : 149.31s, remaining : 12.50s\n",
      " [0943/1024] tr_loss : 11.52364, val_loss : 15.78747, elapsed : 0.16s, total : 149.47s, remaining : 12.74s\n",
      " [0944/1024] tr_loss : 11.62482, val_loss : 14.90056, elapsed : 0.15s, total : 149.62s, remaining : 12.11s\n",
      " [0945/1024] tr_loss : 11.50020, val_loss : 14.32340, elapsed : 0.16s, total : 149.78s, remaining : 12.48s\n",
      " [0946/1024] tr_loss : 11.68914, val_loss : 15.16637, elapsed : 0.15s, total : 149.93s, remaining : 11.81s\n",
      " [0947/1024] tr_loss : 11.49503, val_loss : 15.91529, elapsed : 0.16s, total : 150.08s, remaining : 12.11s\n",
      " [0948/1024] tr_loss : 11.62209, val_loss : 14.87463, elapsed : 0.15s, total : 150.24s, remaining : 11.61s\n",
      " [0949/1024] tr_loss : 11.48066, val_loss : 14.32105, elapsed : 0.16s, total : 150.39s, remaining : 11.69s\n",
      " [0950/1024] tr_loss : 11.66590, val_loss : 15.12259, elapsed : 0.15s, total : 150.55s, remaining : 11.29s\n",
      " [0951/1024] tr_loss : 11.47293, val_loss : 15.97451, elapsed : 0.16s, total : 150.70s, remaining : 11.56s\n",
      " [0952/1024] tr_loss : 11.60830, val_loss : 14.86342, elapsed : 0.15s, total : 150.86s, remaining : 10.95s\n",
      " [0953/1024] tr_loss : 11.45644, val_loss : 14.30376, elapsed : 0.16s, total : 151.01s, remaining : 11.19s\n",
      " [0954/1024] tr_loss : 11.65111, val_loss : 14.97732, elapsed : 0.16s, total : 151.17s, remaining : 11.04s\n",
      " [0955/1024] tr_loss : 11.45011, val_loss : 16.05943, elapsed : 0.16s, total : 151.33s, remaining : 10.86s\n",
      " [0956/1024] tr_loss : 11.60213, val_loss : 14.85959, elapsed : 0.15s, total : 151.48s, remaining : 10.33s\n",
      " [0957/1024] tr_loss : 11.44049, val_loss : 14.34077, elapsed : 0.16s, total : 151.64s, remaining : 10.49s\n",
      " [0958/1024] tr_loss : 11.60471, val_loss : 15.11821, elapsed : 0.15s, total : 151.79s, remaining : 10.15s\n",
      " [0959/1024] tr_loss : 11.42472, val_loss : 16.01273, elapsed : 0.16s, total : 151.95s, remaining : 10.25s\n",
      " [0960/1024] tr_loss : 11.55966, val_loss : 14.88932, elapsed : 0.15s, total : 152.10s, remaining : 9.76s\n",
      " [0961/1024] tr_loss : 11.40776, val_loss : 14.35254, elapsed : 0.16s, total : 152.26s, remaining : 9.82s\n",
      " [0962/1024] tr_loss : 11.58487, val_loss : 14.93884, elapsed : 0.15s, total : 152.41s, remaining : 9.44s\n",
      " [0963/1024] tr_loss : 11.38662, val_loss : 16.08169, elapsed : 0.16s, total : 152.57s, remaining : 9.57s\n",
      " [0964/1024] tr_loss : 11.54478, val_loss : 14.88279, elapsed : 0.15s, total : 152.72s, remaining : 9.18s\n",
      " [0965/1024] tr_loss : 11.37953, val_loss : 14.37022, elapsed : 0.16s, total : 152.88s, remaining : 9.23s\n",
      " [0966/1024] tr_loss : 11.53823, val_loss : 15.05914, elapsed : 0.15s, total : 153.03s, remaining : 8.91s\n",
      " [0967/1024] tr_loss : 11.35487, val_loss : 16.14830, elapsed : 0.16s, total : 153.19s, remaining : 8.98s\n",
      " [0968/1024] tr_loss : 11.50090, val_loss : 14.97166, elapsed : 0.15s, total : 153.34s, remaining : 8.45s\n",
      " [0969/1024] tr_loss : 11.34703, val_loss : 14.41300, elapsed : 0.16s, total : 153.50s, remaining : 8.67s\n",
      " [0970/1024] tr_loss : 11.50326, val_loss : 14.92624, elapsed : 0.16s, total : 153.65s, remaining : 8.53s\n",
      " [0971/1024] tr_loss : 11.34209, val_loss : 16.21634, elapsed : 0.16s, total : 153.81s, remaining : 8.31s\n",
      " [0972/1024] tr_loss : 11.48546, val_loss : 15.02151, elapsed : 0.15s, total : 153.96s, remaining : 7.83s\n",
      " [0973/1024] tr_loss : 11.32075, val_loss : 14.47968, elapsed : 0.16s, total : 154.12s, remaining : 8.12s\n",
      " [0974/1024] tr_loss : 11.44470, val_loss : 14.98223, elapsed : 0.16s, total : 154.28s, remaining : 7.89s\n",
      " [0975/1024] tr_loss : 11.29448, val_loss : 16.22209, elapsed : 0.16s, total : 154.44s, remaining : 7.84s\n",
      " [0976/1024] tr_loss : 11.42244, val_loss : 15.10566, elapsed : 0.16s, total : 154.60s, remaining : 7.57s\n",
      " [0977/1024] tr_loss : 11.27394, val_loss : 14.62131, elapsed : 0.16s, total : 154.75s, remaining : 7.32s\n",
      " [0978/1024] tr_loss : 11.37022, val_loss : 14.92755, elapsed : 0.15s, total : 154.90s, remaining : 6.97s\n",
      " [0979/1024] tr_loss : 11.26636, val_loss : 16.21927, elapsed : 0.16s, total : 155.06s, remaining : 7.08s\n",
      " [0980/1024] tr_loss : 11.36653, val_loss : 15.25873, elapsed : 0.15s, total : 155.21s, remaining : 6.70s\n",
      " [0981/1024] tr_loss : 11.23781, val_loss : 14.86530, elapsed : 0.16s, total : 155.37s, remaining : 6.89s\n",
      " [0982/1024] tr_loss : 11.27544, val_loss : 14.97706, elapsed : 0.16s, total : 155.53s, remaining : 6.51s\n",
      " [0983/1024] tr_loss : 11.23476, val_loss : 16.20854, elapsed : 0.16s, total : 155.69s, remaining : 6.67s\n",
      " [0984/1024] tr_loss : 11.28334, val_loss : 15.44001, elapsed : 0.16s, total : 155.85s, remaining : 6.22s\n",
      " [0985/1024] tr_loss : 11.20149, val_loss : 15.23775, elapsed : 0.16s, total : 156.01s, remaining : 6.31s\n",
      " [0986/1024] tr_loss : 11.16843, val_loss : 14.92264, elapsed : 0.16s, total : 156.16s, remaining : 5.94s\n",
      " [0987/1024] tr_loss : 11.22735, val_loss : 16.08085, elapsed : 0.16s, total : 156.32s, remaining : 5.84s\n",
      " [0988/1024] tr_loss : 11.21528, val_loss : 15.68673, elapsed : 0.15s, total : 156.48s, remaining : 5.49s\n",
      " [0989/1024] tr_loss : 11.18697, val_loss : 15.54956, elapsed : 0.16s, total : 156.64s, remaining : 5.66s\n",
      " [0990/1024] tr_loss : 11.11611, val_loss : 14.73374, elapsed : 0.16s, total : 156.79s, remaining : 5.35s\n",
      " [0991/1024] tr_loss : 11.21813, val_loss : 15.69845, elapsed : 0.16s, total : 156.95s, remaining : 5.27s\n",
      " [0992/1024] tr_loss : 11.16079, val_loss : 15.83192, elapsed : 0.15s, total : 157.11s, remaining : 4.90s\n",
      " [0993/1024] tr_loss : 11.11499, val_loss : 15.89118, elapsed : 0.16s, total : 157.27s, remaining : 4.91s\n",
      " [0994/1024] tr_loss : 11.11936, val_loss : 14.84203, elapsed : 0.16s, total : 157.42s, remaining : 4.67s\n",
      " [0995/1024] tr_loss : 11.15648, val_loss : 15.13923, elapsed : 0.16s, total : 157.58s, remaining : 4.60s\n",
      " [0996/1024] tr_loss : 11.20495, val_loss : 15.45037, elapsed : 0.15s, total : 157.73s, remaining : 4.30s\n",
      " [0997/1024] tr_loss : 11.08750, val_loss : 16.46452, elapsed : 0.16s, total : 157.89s, remaining : 4.25s\n",
      " [0998/1024] tr_loss : 11.18562, val_loss : 15.56414, elapsed : 0.16s, total : 158.05s, remaining : 4.05s\n",
      " [0999/1024] tr_loss : 11.11396, val_loss : 15.33269, elapsed : 0.16s, total : 158.21s, remaining : 4.10s\n",
      " [1000/1024] tr_loss : 11.11026, val_loss : 15.19038, elapsed : 0.16s, total : 158.37s, remaining : 3.78s\n",
      " [1001/1024] tr_loss : 11.14978, val_loss : 16.43130, elapsed : 0.16s, total : 158.53s, remaining : 3.65s\n",
      " [1002/1024] tr_loss : 11.17203, val_loss : 15.99699, elapsed : 0.15s, total : 158.68s, remaining : 3.35s\n",
      " [1003/1024] tr_loss : 11.13871, val_loss : 15.75566, elapsed : 0.16s, total : 158.84s, remaining : 3.34s\n",
      " [1004/1024] tr_loss : 11.03080, val_loss : 14.82620, elapsed : 0.16s, total : 159.00s, remaining : 3.20s\n",
      " [1005/1024] tr_loss : 11.15185, val_loss : 15.87148, elapsed : 0.16s, total : 159.16s, remaining : 3.12s\n",
      " [1006/1024] tr_loss : 11.11430, val_loss : 16.16922, elapsed : 0.15s, total : 159.32s, remaining : 2.76s\n",
      " [1007/1024] tr_loss : 11.03266, val_loss : 16.14362, elapsed : 0.16s, total : 159.48s, remaining : 2.71s\n",
      " [1008/1024] tr_loss : 11.00286, val_loss : 15.14508, elapsed : 0.15s, total : 159.63s, remaining : 2.44s\n",
      " [1009/1024] tr_loss : 11.02321, val_loss : 15.34158, elapsed : 0.16s, total : 159.79s, remaining : 2.39s\n",
      " [1010/1024] tr_loss : 11.10209, val_loss : 15.62822, elapsed : 0.15s, total : 159.94s, remaining : 2.16s\n",
      " [1011/1024] tr_loss : 11.07639, val_loss : 16.75029, elapsed : 0.16s, total : 160.10s, remaining : 2.05s\n",
      " [1012/1024] tr_loss : 11.12760, val_loss : 15.91395, elapsed : 0.15s, total : 160.25s, remaining : 1.84s\n",
      " [1013/1024] tr_loss : 11.03729, val_loss : 15.23129, elapsed : 0.16s, total : 160.41s, remaining : 1.73s\n",
      " [1014/1024] tr_loss : 11.05264, val_loss : 15.40432, elapsed : 0.15s, total : 160.56s, remaining : 1.53s\n",
      " [1015/1024] tr_loss : 11.08323, val_loss : 16.50512, elapsed : 0.16s, total : 160.72s, remaining : 1.42s\n",
      " [1016/1024] tr_loss : 11.07624, val_loss : 16.46273, elapsed : 0.15s, total : 160.87s, remaining : 1.23s\n",
      " [1017/1024] tr_loss : 11.09605, val_loss : 15.81112, elapsed : 0.16s, total : 161.03s, remaining : 1.12s\n",
      " [1018/1024] tr_loss : 10.96539, val_loss : 14.92148, elapsed : 0.15s, total : 161.19s, remaining : 0.93s\n",
      " [1019/1024] tr_loss : 11.13925, val_loss : 16.06444, elapsed : 0.16s, total : 161.35s, remaining : 0.80s\n",
      " [1020/1024] tr_loss : 11.10920, val_loss : 16.81412, elapsed : 0.15s, total : 161.50s, remaining : 0.61s\n",
      " [1021/1024] tr_loss : 11.08341, val_loss : 15.92004, elapsed : 0.20s, total : 161.70s, remaining : 0.60s\n",
      " [1022/1024] tr_loss : 10.95088, val_loss : 15.01474, elapsed : 0.16s, total : 161.86s, remaining : 0.32s\n",
      " [1023/1024] tr_loss : 11.11261, val_loss : 15.64729, elapsed : 0.16s, total : 162.02s, remaining : 0.16s\n",
      " [1024/1024] tr_loss : 11.08715, val_loss : 16.96802, elapsed : 0.15s, total : 162.18s, remaining : 0.00s\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "model = Model(input_size = input_size)\n",
    "# model = SCINet_Model(input_size = input_size)\n",
    "# model = BaseModel(\n",
    "#     input_size = input_size,\n",
    "#     hidden_sizes=[400,300],\n",
    "#     dropout_rates=[0.2,0.2],\n",
    "#     num_classes=seq_length,\n",
    "#     num_layers=2,\n",
    "#     bidirectional=True,\n",
    "# )\n",
    "\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "CFG['PATIENCE']=100\n",
    "best_model = train(\n",
    "    model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    scheduler=None,#scheduler,\n",
    "    device=device,\n",
    "    early_stopping=False,\n",
    "    metric_period=1,\n",
    "    epochs=1024,\n",
    "    best_model_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86409d-8a20-4a3b-97c7-067e4a2956d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "561cd5d5-3c8c-463c-9544-b08ff83ace31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "path = f'./model/best_model_999.pt'\n",
    "# path = f'./model/best_model.pt'\n",
    "\n",
    "torch.save(best_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "153f48a7-6517-40ac-9024-adc1e99b8a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = Model(input_size = input_size) # SCINet_Model\n",
    "best_model.load_state_dict(torch.load(path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c16fd45c-dbf6-41c2-a2b4-997084b88a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "true_list = []\n",
    "with torch.no_grad():\n",
    "    for X,y in iter(valid_loader): # train_loader, valid_loader\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X)\n",
    "        # model_pred = torch.exp(model_pred)\n",
    "        \n",
    "        pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "        true_list += y         .cpu().numpy().reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0c3eddf-1719-4355-ab47-3989ffbe0acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x.shape for x,y in iter(train_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "182c750e-6ffc-44ca-959a-565416951e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38f46acc-77b7-48fa-b5ef-22ee051ddd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 56)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in np.array(pred_list)]),len([x for x in np.array(true_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e86cd058-96cf-4058-a483-cb03354cad5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size): \u001b[38;5;66;03m# 3,25\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     plot_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#[i*28:(i+1)*28]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     plot_df \u001b[38;5;241m=\u001b[39m plot_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# plot_df = np.exp(plot_df)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/core/frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    656\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    657\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "for i in range(size): # 3,25\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        'pred' : pred_list,\n",
    "        'true' : true_list,\n",
    "    })#[i*28:(i+1)*28]\n",
    "    plot_df = plot_df.reset_index(drop=True)\n",
    "    # plot_df = np.exp(plot_df)\n",
    "\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.true,color='black')\n",
    "    sns.lineplot(x=plot_df.index,y=plot_df.pred,color='red')\n",
    "    # plt.title(random_num[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e4b32-cebe-4f62-815c-dd1cce12a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(plot_df.pred,plot_df.true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc1a96-e2f7-422f-82b6-559a8370770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X in iter(test_loader):\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = best_model(X)\n",
    "        # model_pred = torch.exp(model_pred)\n",
    "\n",
    "        pred_list += model_pred.cpu().numpy().reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96f71-2ae7-444e-9437-7bbb251a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test_label_df.sort_values(['case_num','DAT'])\n",
    "sub['predicted_weight_g'] = pred_list\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num].drop('case_num',axis=1)\n",
    "    s.to_csv(f'./out/lstm/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/home/studio-lab-user/Dacon/6_상추생육환경생성')\n",
    "os.chdir(\"./out/lstm/\")\n",
    "submission = zipfile.ZipFile(\"../lstm.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/home/studio-lab-user/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5ffdd-2fbc-427b-b7a5-ce0873940a76",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbcc8d-efd2-48a1-b8a1-9cf42b70b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cure-lab/SCINet/blob/main/models/SCINet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f28558-5146-4832-82ac-1b0dff485f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--window_size', type=int, default=96)\n",
    "parser.add_argument('--horizon', type=int, default=12)\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--groups', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--hidden-size', default=1, type=int, help='hidden channel of module')\n",
    "parser.add_argument('--INN', default=1, type=int, help='use INN or basic strategy')\n",
    "parser.add_argument('--kernel', default=3, type=int, help='kernel size')\n",
    "parser.add_argument('--dilation', default=1, type=int, help='dilation')\n",
    "parser.add_argument('--positionalEcoding', type=bool, default=True)\n",
    "\n",
    "parser.add_argument('--single_step_output_One', type=int, default=0)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab2a6b-d005-484b-97b9-1c557f485610",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12\n",
    "window_size = 96\n",
    "hidden_size = 1\n",
    "groups = 1\n",
    "kernel = 3\n",
    "dropout = 0.5\n",
    "single_step_output_One = 0\n",
    "positionalEcoding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a0d44-f37c-4ea4-a640-81b7b3416450",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c34f0-e165-42f7-a61c-a897a505e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCINet(\n",
    "    output_len = horizon, input_len= window_size, input_dim = 9, hid_size = hidden_size, num_stacks = 1,\n",
    "    num_levels = 3, concat_len = 0, groups = groups, kernel = kernel, dropout = dropout,\n",
    "    single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, modified = True\n",
    ").to(device)\n",
    "x = torch.randn(32, 96, 9).to(device)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fd63f-bb54-4ef4-9663-83bde8b4e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
