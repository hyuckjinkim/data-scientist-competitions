{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfd3826-7ff1-4a26-b431-6f34d34b5125",
   "metadata": {},
   "source": [
    "- kmeans 등으로 y의 max랑 구분지어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4041a5-44ba-4987-a6e7-3c3f40f10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 torch 다운로드\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e5953c-7a86-4c51-a887-423f7beb6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fp = fm.FontProperties(fname='/home/studio-lab-user/Dacon/tools/NanumFont/NanumGothic.ttf', size=10)\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "# # cuda (not Mac)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # mps (Mac)\n",
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47cef-a496-4dc7-9a8b-0f363f65cdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':128,#1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':0.05,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0fc924-3361-45c3-9f77-a764ef8fbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x,size):\n",
    "\n",
    "    ma = []\n",
    "    for i in range(len(x)):\n",
    "        if i<size:\n",
    "            values = x.values[:(i+1)]\n",
    "        else:\n",
    "            values = x.values[(i-size+1):(i+1)]\n",
    "        ma_value = values.mean()\n",
    "        ma.append(ma_value)\n",
    "        \n",
    "    return ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, input_paths, label_paths, test_input_paths, test_label_paths):\n",
    "        \n",
    "        self.input, self.label, self.test_input, self.test_label = None, None, None, None\n",
    "        \n",
    "        self.X_train, self.X_valid = None, None\n",
    "        self.y_train, self.y_valid = None, None\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, label_path in zip(input_paths, label_paths):\n",
    "            case_num = input_path.replace('./data/train_input/CASE_','').replace('.csv','')\n",
    "            \n",
    "            input_df = pd.read_csv(input_path)\n",
    "            label_df = pd.read_csv(label_path)\n",
    "\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_df['case_num'] = case_num\n",
    "            label_df['case_num'] = case_num\n",
    "            \n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(label_df)\n",
    "        \n",
    "        test_input_fn = []\n",
    "        test_label_fn = []\n",
    "        for test_input_path, test_label_path in zip(test_input_paths, test_label_paths):\n",
    "            case_num = test_input_path.replace('./data/test_input/TEST_','').replace('.csv','')\n",
    "            \n",
    "            test_input_df = pd.read_csv(test_input_path)\n",
    "            test_label_df = pd.read_csv(test_label_path)\n",
    "            \n",
    "            test_input_df['case_num'] = case_num\n",
    "            test_label_df['case_num'] = case_num\n",
    "            \n",
    "            test_input_fn.append(test_input_df)\n",
    "            test_label_fn.append(test_label_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0).sort_values(['case_num','DAT','obs_time'])\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_fn ,axis=0)\n",
    "        self.test_label  = pd.concat(test_label_fn ,axis=0)\n",
    "        \n",
    "        self.input     .obs_time = list(np.arange(0,24))*int(self.input     .shape[0]/24)\n",
    "        self.test_input.obs_time = list(np.arange(0,24))*int(self.test_input.shape[0]/24)\n",
    "        \n",
    "    def _data_return(self):\n",
    "        return self.input,self.label,self.test_input,self.test_label\n",
    "            \n",
    "    def _target_log(self):\n",
    "        self.label['predicted_weight_g'] = np.log(self.label['predicted_weight_g'])\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        # 1. time 추가 : 1~672 (24시간 x 28일)\n",
    "        self.input     ['time'] = [i+1 for i in range(28*24)]*self.input     .case_num.nunique()\n",
    "        self.test_input['time'] = [i+1 for i in range(28*24)]*self.test_input.case_num.nunique()\n",
    "\n",
    "        features = [\n",
    "            'DAT', 'obs_time', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'case_num', 'time'\n",
    "        ]\n",
    "        # del_features = [\n",
    "        #     '시간당분무량','시간당백색광량','시간당적색광량','시간당청색광량','시간당총광량',\n",
    "        #     # '일간누적분무량','일간누적백색광량','일간누적적색광량','일간누적청색광량','일간누적총광량'\n",
    "        # ]\n",
    "        # self.input     .drop(columns=del_features,inplace=True)\n",
    "        # self.test_input.drop(columns=del_features,inplace=True)\n",
    "        \n",
    "        # 2. 각 컬럼들의 파생변수\n",
    "        input_df       = []\n",
    "        test_input_df  = []\n",
    "        for case_num in self.input.case_num.unique():\n",
    "            i_df = self.input     [self.input     .case_num==case_num]\n",
    "            t_df = self.test_input[self.test_input.case_num==case_num]\n",
    "            \n",
    "            for col in list(set(self.input.columns)-set(['case_num','DAT','obs_time','time'])):\n",
    "                for i in range(4):\n",
    "                    # (1) 이전시간 값\n",
    "                    i_df[f'{col}_bf{i+1}'] = i_df[col].shift(i+1).fillna(0)\n",
    "                    t_df[f'{col}_bf{i+1}'] = t_df[col].shift(i+1).fillna(0)\n",
    "                \n",
    "                    # (2) 전시간대 대비 상승했는지 여부\n",
    "                    i_df[f'{col}_higher_than_{i+1}d'] = np.where(i_df[col]>i_df[col].shift(i+1),1,0)\n",
    "                    t_df[f'{col}_higher_than_{i+1}d'] = np.where(t_df[col]>t_df[col].shift(i+1),1,0)\n",
    "\n",
    "                    # (3) 전시간대 대비 상승률 -> 넣으면 NaN 발생\n",
    "                    if i_df[col].min()<=0:\n",
    "                        offset = i_df[col].min()\n",
    "                        i_df[col] = i_df[col] + offset\n",
    "                        t_df[col] = t_df[col] + offset\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'] = (i_df[col] - i_df[col].shift(i+1)) / i_df[col]\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'] = (t_df[col] - t_df[col].shift(i+1)) / t_df[col]\n",
    "\n",
    "                    # -inf -> min, inf -> max\n",
    "                    tmp = i_df[f'{col}_{i+1}d_rise_rate'].copy()\n",
    "                    tmp = tmp[(tmp!=-np.inf) & (tmp!=np.inf)]\n",
    "                    min_info, max_info = tmp.min(), tmp.max()\n",
    "\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    \n",
    "                    # fill nan to zero\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    \n",
    "                    # (4) moving average\n",
    "                    for size in [2,4,7]:\n",
    "                        i_df[f'{col}_ma{size}'] = moving_average(i_df[col],size=size)\n",
    "                        t_df[f'{col}_ma{size}'] = moving_average(t_df[col],size=size)\n",
    "                \n",
    "                # (5) cumulative sum\n",
    "                i_df[f'{col}_cumsum'] = i_df[col].cumsum()\n",
    "                t_df[f'{col}_cumsum'] = t_df[col].cumsum()\n",
    "\n",
    "            input_df     .append(i_df)\n",
    "            test_input_df.append(t_df)\n",
    "        \n",
    "        # concat\n",
    "        self.input       = pd.concat(input_df     ,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_df,axis=0)\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "        #---------------------------------------------------------------------------\n",
    "        # agg\n",
    "        #---------------------------------------------------------------------------\n",
    "        self.input      = self.input     .drop(['obs_time'],axis=1)\n",
    "        self.test_input = self.test_input.drop(['obs_time'],axis=1)\n",
    "        \n",
    "        self.input      = self.input     .groupby(['case_num','DAT']).mean().reset_index()\n",
    "        self.test_input = self.test_input.groupby(['case_num','DAT']).mean().reset_index()\n",
    "        \n",
    "        self.input.DAT      = self.input     .DAT + 1\n",
    "        self.test_input.DAT = self.test_input.DAT + 1\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # https://dacon.io/competitions/official/236033/talkboard/407304?page=1&dtype=recent\n",
    "    def _scale_dataset(self,outlier):\n",
    "        \n",
    "        minmax_info = {\n",
    "            #'None':[0,0],\n",
    "            'DAT':[1,28],\n",
    "            'obs_time':[0,23],\n",
    "            'time':[0,28*24],\n",
    "            '내부온도관측치':[4,40],\n",
    "            '내부습도관측치':[0,100],\n",
    "            'co2관측치':[0,1200],\n",
    "            'ec관측치':[0,8],\n",
    "            '시간당분무량':[0,3000],\n",
    "            '일간누적분무량':[0,72000],\n",
    "            '시간당백색광량':[0,120000],\n",
    "            '일간누적백색광량':[0,2880000],\n",
    "            '시간당적색광량':[0,120000],\n",
    "            '일간누적적색광량':[0,2880000],\n",
    "            '시간당청색광량':[0,120000],\n",
    "            '일간누적청색광량':[0,2880000],\n",
    "            '시간당총광량':[0,120000],\n",
    "            '일간누적총광량':[0,2880000],\n",
    "        }\n",
    "            \n",
    "        scale_feature = [feature for feature,(min_info,max_info) in minmax_info.items() if feature in self.input.columns]\n",
    "        \n",
    "        # for train dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.input[col] = (self.input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.input[col][self.input[col]<0] = 0\n",
    "                self.input[col][self.input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.input[col][(self.input[col]<0) | (self.input[col]>1)] = np.nan\n",
    "            \n",
    "        # for test dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.test_input[col][self.test_input[col]<0] = 0\n",
    "                self.test_input[col][self.test_input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.test_input[col][(self.test_input[col]<0) | (self.test_input[col]>1)] = np.nan\n",
    "        \n",
    "        # another features\n",
    "        another_features = list(set(self.input.select_dtypes(exclude=[object]).columns)-set(scale_feature))\n",
    "        for col in another_features:\n",
    "            if self.input[col].min()==0:\n",
    "                offset=1e-4\n",
    "                self.input[col]      = self.input     [col]+offset\n",
    "                self.test_input[col] = self.test_input[col]+offset\n",
    "            \n",
    "            min_info,max_info = self.input[col].min(),self.input[col].max()    \n",
    "            self.input[col]      = (self.input[col]     -min_info) / (max_info-min_info)\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "        \n",
    "    def _interaction_term(self):\n",
    "        # num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        num_features = [\n",
    "            'DAT','time',\n",
    "            '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량',\n",
    "        ]\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    self.input     [f'{num_features[i]}*{num_features[j]}'] = self.input     [num_features[i]]*self.input     [num_features[j]]\n",
    "                    self.test_input[f'{num_features[i]}*{num_features[j]}'] = self.test_input[num_features[i]]*self.test_input[num_features[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46afb772-6e30-42b5-b09b-f0cdc584fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf407c58-fe03-456d-9bad-ec0ff6b18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# val_rate = 0.05\n",
    "\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# dataset._preprocess()\n",
    "# dataset._scale_dataset()\n",
    "# input_df, label_df = dataset._data_return()\n",
    "\n",
    "# for case_num in tqdm(sorted(input_df.case_num.unique())):\n",
    "\n",
    "#     input = input_df[input_df.case_num==case_num].drop('case_num',axis=1)\n",
    "#     label = label_df[label_df.case_num==case_num].drop('case_num',axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize=(20,15))\n",
    "#     nrow = 3\n",
    "#     ncol = 5\n",
    "\n",
    "#     iter = 0\n",
    "#     total = len(input.columns)-3\n",
    "#     for col in input.columns:\n",
    "#         if col not in ['time','DAT','obs_time']:\n",
    "#             iter+=1\n",
    "\n",
    "#             y1 = input[col]\n",
    "#             #y1 = (y1-y1.min())/(y1.max()-y1.min())\n",
    "\n",
    "#             y2 = label['predicted_weight_g']\n",
    "#             y2 = (y2-y2.min())/(y2.max()-y2.min())\n",
    "\n",
    "#             y3 = input.groupby('DAT')[col].mean().values\n",
    "\n",
    "#             corr, pvalue = pearsonr(y2,y3)\n",
    "\n",
    "#             fig.add_subplot(ncol,nrow,iter)\n",
    "#             sns.scatterplot(x=input.time  ,y=y1)\n",
    "#             sns.scatterplot(x=label.DAT*24,y=y2,color='red')\n",
    "#             sns.lineplot   (x=label.DAT*24,y=y3,color='blue',linestyle='--',alpha=0.7)\n",
    "#             plt.ylabel('')\n",
    "\n",
    "#             plt.title(f'{col}(corr={corr:.3f}(pvalue={pvalue:.3f}))',fontproperties=fp)\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'./fig/{case_num}.png',dpi=100)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731d783-38cb-43e3-8f4f-1f9c6d187cdc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d57fd0-ea32-4fb4-a626-1006236b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_label_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "all_test_label_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d65ad91-4f38-4423-99f9-c1e8089c2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.3 s, sys: 1.5 s, total: 51.8 s\n",
      "Wall time: 56.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess Class\n",
    "dataset = Preprocess(\n",
    "    input_paths = all_input_list,\n",
    "    label_paths = all_label_list,\n",
    "    test_input_paths = all_test_input_list,\n",
    "    test_label_paths = all_test_label_list,\n",
    ")\n",
    "\n",
    "# (1) preprocessing + scaling + interaction term\n",
    "dataset._preprocess()\n",
    "# dataset._target_log()\n",
    "dataset._scale_dataset(outlier='keep')\n",
    "# dataset._interaction_term()\n",
    "\n",
    "# (2) Data Return for check\n",
    "input_df, label_df, test_input_df, test_label_df = dataset._data_return()\n",
    "\n",
    "# # (3) Delete Std zero features\n",
    "# std_zero_features = []\n",
    "# for case_num in input_df.case_num.unique():\n",
    "#     tmp = input_df[input_df.case_num==case_num]\n",
    "#     std_zero_feature = tmp.std().index[tmp.std()==0].tolist()\n",
    "#     std_zero_features += std_zero_feature\n",
    "    \n",
    "# std_zero_features = pd.unique(std_zero_features)\n",
    "\n",
    "# input_df = input_df.drop(std_zero_features,axis=1)\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec50457-eded-4068-a3ef-8487af93b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_info = input_df.isnull().sum()\n",
    "null_info[null_info!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521202eb-ce6f-4e3c-a229-ffb2f5bf7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = input_df.copy()\n",
    "\n",
    "# cols = [col for col in d.columns if col.find('_higher_than_')>=0]\n",
    "# std_zero_cols = d[cols].std()==0\n",
    "# del_cols = std_zero_cols[std_zero_cols].index.tolist()\n",
    "# print(del_cols)\n",
    "\n",
    "# # for i in range(len(cols)):\n",
    "# #     print(f'({i}/{len(cols)}) {cols[i]}')\n",
    "# #     plt.figure(figsize=(8,7))\n",
    "# #     sns.lineplot(x=d.time,y=d[cols[i]])\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8979bc0a-1589-44b6-9cc6-6733be3e1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 241)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_df.isnull().sum()\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f93b40f-3e0c-4f7e-b84c-8c0958395769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_num', 'DAT', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'time', '일간누적청색광량_bf1', '일간누적청색광량_higher_than_1d', '일간누적청색광량_1d_rise_rate', '일간누적청색광량_ma2', '일간누적청색광량_ma4', '일간누적청색광량_ma7', '일간누적청색광량_bf2', '일간누적청색광량_higher_than_2d', '일간누적청색광량_2d_rise_rate', '일간누적청색광량_bf3', '일간누적청색광량_higher_than_3d', '일간누적청색광량_3d_rise_rate', '일간누적청색광량_bf4', '일간누적청색광량_higher_than_4d', '일간누적청색광량_4d_rise_rate', '일간누적청색광량_cumsum', '시간당적색광량_bf1', '시간당적색광량_higher_than_1d', '시간당적색광량_1d_rise_rate', '시간당적색광량_ma2', '시간당적색광량_ma4', '시간당적색광량_ma7', '시간당적색광량_bf2', '시간당적색광량_higher_than_2d', '시간당적색광량_2d_rise_rate', '시간당적색광량_bf3', '시간당적색광량_higher_than_3d', '시간당적색광량_3d_rise_rate', '시간당적색광량_bf4', '시간당적색광량_higher_than_4d', '시간당적색광량_4d_rise_rate', '시간당적색광량_cumsum', '시간당백색광량_bf1', '시간당백색광량_higher_than_1d', '시간당백색광량_1d_rise_rate', '시간당백색광량_ma2', '시간당백색광량_ma4', '시간당백색광량_ma7', '시간당백색광량_bf2', '시간당백색광량_higher_than_2d', '시간당백색광량_2d_rise_rate', '시간당백색광량_bf3', '시간당백색광량_higher_than_3d', '시간당백색광량_3d_rise_rate', '시간당백색광량_bf4', '시간당백색광량_higher_than_4d', '시간당백색광량_4d_rise_rate', '시간당백색광량_cumsum', '내부습도관측치_bf1', '내부습도관측치_higher_than_1d', '내부습도관측치_1d_rise_rate', '내부습도관측치_ma2', '내부습도관측치_ma4', '내부습도관측치_ma7', '내부습도관측치_bf2', '내부습도관측치_higher_than_2d', '내부습도관측치_2d_rise_rate', '내부습도관측치_bf3', '내부습도관측치_higher_than_3d', '내부습도관측치_3d_rise_rate', '내부습도관측치_bf4', '내부습도관측치_higher_than_4d', '내부습도관측치_4d_rise_rate', '내부습도관측치_cumsum', 'ec관측치_bf1', 'ec관측치_higher_than_1d', 'ec관측치_1d_rise_rate', 'ec관측치_ma2', 'ec관측치_ma4', 'ec관측치_ma7', 'ec관측치_bf2', 'ec관측치_higher_than_2d', 'ec관측치_2d_rise_rate', 'ec관측치_bf3', 'ec관측치_higher_than_3d', 'ec관측치_3d_rise_rate', 'ec관측치_bf4', 'ec관측치_higher_than_4d', 'ec관측치_4d_rise_rate', 'ec관측치_cumsum', '시간당청색광량_bf1', '시간당청색광량_higher_than_1d', '시간당청색광량_1d_rise_rate', '시간당청색광량_ma2', '시간당청색광량_ma4', '시간당청색광량_ma7', '시간당청색광량_bf2', '시간당청색광량_higher_than_2d', '시간당청색광량_2d_rise_rate', '시간당청색광량_bf3', '시간당청색광량_higher_than_3d', '시간당청색광량_3d_rise_rate', '시간당청색광량_bf4', '시간당청색광량_higher_than_4d', '시간당청색광량_4d_rise_rate', '시간당청색광량_cumsum', '시간당분무량_bf1', '시간당분무량_higher_than_1d', '시간당분무량_1d_rise_rate', '시간당분무량_ma2', '시간당분무량_ma4', '시간당분무량_ma7', '시간당분무량_bf2', '시간당분무량_higher_than_2d', '시간당분무량_2d_rise_rate', '시간당분무량_bf3', '시간당분무량_higher_than_3d', '시간당분무량_3d_rise_rate', '시간당분무량_bf4', '시간당분무량_higher_than_4d', '시간당분무량_4d_rise_rate', '시간당분무량_cumsum', '내부온도관측치_bf1', '내부온도관측치_higher_than_1d', '내부온도관측치_1d_rise_rate', '내부온도관측치_ma2', '내부온도관측치_ma4', '내부온도관측치_ma7', '내부온도관측치_bf2', '내부온도관측치_higher_than_2d', '내부온도관측치_2d_rise_rate', '내부온도관측치_bf3', '내부온도관측치_higher_than_3d', '내부온도관측치_3d_rise_rate', '내부온도관측치_bf4', '내부온도관측치_higher_than_4d', '내부온도관측치_4d_rise_rate', '내부온도관측치_cumsum', '일간누적적색광량_bf1', '일간누적적색광량_higher_than_1d', '일간누적적색광량_1d_rise_rate', '일간누적적색광량_ma2', '일간누적적색광량_ma4', '일간누적적색광량_ma7', '일간누적적색광량_bf2', '일간누적적색광량_higher_than_2d', '일간누적적색광량_2d_rise_rate', '일간누적적색광량_bf3', '일간누적적색광량_higher_than_3d', '일간누적적색광량_3d_rise_rate', '일간누적적색광량_bf4', '일간누적적색광량_higher_than_4d', '일간누적적색광량_4d_rise_rate', '일간누적적색광량_cumsum', '시간당총광량_bf1', '시간당총광량_higher_than_1d', '시간당총광량_1d_rise_rate', '시간당총광량_ma2', '시간당총광량_ma4', '시간당총광량_ma7', '시간당총광량_bf2', '시간당총광량_higher_than_2d', '시간당총광량_2d_rise_rate', '시간당총광량_bf3', '시간당총광량_higher_than_3d', '시간당총광량_3d_rise_rate', '시간당총광량_bf4', '시간당총광량_higher_than_4d', '시간당총광량_4d_rise_rate', '시간당총광량_cumsum', '일간누적총광량_bf1', '일간누적총광량_higher_than_1d', '일간누적총광량_1d_rise_rate', '일간누적총광량_ma2', '일간누적총광량_ma4', '일간누적총광량_ma7', '일간누적총광량_bf2', '일간누적총광량_higher_than_2d', '일간누적총광량_2d_rise_rate', '일간누적총광량_bf3', '일간누적총광량_higher_than_3d', '일간누적총광량_3d_rise_rate', '일간누적총광량_bf4', '일간누적총광량_higher_than_4d', '일간누적총광량_4d_rise_rate', '일간누적총광량_cumsum', '일간누적백색광량_bf1', '일간누적백색광량_higher_than_1d', '일간누적백색광량_1d_rise_rate', '일간누적백색광량_ma2', '일간누적백색광량_ma4', '일간누적백색광량_ma7', '일간누적백색광량_bf2', '일간누적백색광량_higher_than_2d', '일간누적백색광량_2d_rise_rate', '일간누적백색광량_bf3', '일간누적백색광량_higher_than_3d', '일간누적백색광량_3d_rise_rate', '일간누적백색광량_bf4', '일간누적백색광량_higher_than_4d', '일간누적백색광량_4d_rise_rate', '일간누적백색광량_cumsum', 'co2관측치_bf1', 'co2관측치_higher_than_1d', 'co2관측치_1d_rise_rate', 'co2관측치_ma2', 'co2관측치_ma4', 'co2관측치_ma7', 'co2관측치_bf2', 'co2관측치_higher_than_2d', 'co2관측치_2d_rise_rate', 'co2관측치_bf3', 'co2관측치_higher_than_3d', 'co2관측치_3d_rise_rate', 'co2관측치_bf4', 'co2관측치_higher_than_4d', 'co2관측치_4d_rise_rate', 'co2관측치_cumsum', '일간누적분무량_bf1', '일간누적분무량_higher_than_1d', '일간누적분무량_1d_rise_rate', '일간누적분무량_ma2', '일간누적분무량_ma4', '일간누적분무량_ma7', '일간누적분무량_bf2', '일간누적분무량_higher_than_2d', '일간누적분무량_2d_rise_rate', '일간누적분무량_bf3', '일간누적분무량_higher_than_3d', '일간누적분무량_3d_rise_rate', '일간누적분무량_bf4', '일간누적분무량_higher_than_4d', '일간누적분무량_4d_rise_rate', '일간누적분무량_cumsum']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in input_df.columns if col.find('*')<0])\n",
    "# print(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d3f562-5196-4fc2-a3bb-7c45455a14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_df = []\n",
    "for col in input_df.drop(columns=['case_num']).columns:\n",
    "    r_value, p_value = pearsonr(input_df[col],label_df['predicted_weight_g'])\n",
    "    corr_df.append([col,r_value,p_value])\n",
    "    \n",
    "corr_df = pd.DataFrame(corr_df,columns=['feature','r_value','p_value'])\n",
    "# corr_df.sort_values('p_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccd58fa-b0bf-4263-8494-7b8c94a34a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['시간당적색광량_higher_than_1d' '시간당적색광량_higher_than_2d'\n",
      " '시간당적색광량_higher_than_3d' '시간당적색광량_higher_than_4d' '시간당백색광량_bf2'\n",
      " '시간당백색광량_bf3' '시간당백색광량_cumsum' '내부습도관측치_1d_rise_rate'\n",
      " '내부습도관측치_higher_than_2d' '내부습도관측치_2d_rise_rate' '내부습도관측치_higher_than_3d'\n",
      " '내부습도관측치_3d_rise_rate' '내부습도관측치_higher_than_4d' '내부습도관측치_4d_rise_rate'\n",
      " 'ec관측치_higher_than_1d' 'ec관측치_1d_rise_rate' 'ec관측치_higher_than_2d'\n",
      " 'ec관측치_2d_rise_rate' 'ec관측치_higher_than_3d' 'ec관측치_3d_rise_rate'\n",
      " 'ec관측치_higher_than_4d' 'ec관측치_4d_rise_rate' '시간당청색광량_1d_rise_rate'\n",
      " '시간당청색광량_cumsum' '시간당분무량_1d_rise_rate' '시간당분무량_2d_rise_rate'\n",
      " '시간당분무량_3d_rise_rate' '시간당분무량_4d_rise_rate' '내부온도관측치_higher_than_1d'\n",
      " '내부온도관측치_1d_rise_rate' '내부온도관측치_higher_than_2d' '내부온도관측치_2d_rise_rate'\n",
      " '내부온도관측치_higher_than_3d' '내부온도관측치_3d_rise_rate' '내부온도관측치_higher_than_4d'\n",
      " '내부온도관측치_4d_rise_rate' '시간당총광량_bf2' '시간당총광량_cumsum'\n",
      " '일간누적총광량_1d_rise_rate' '일간누적총광량_2d_rise_rate' '일간누적총광량_3d_rise_rate'\n",
      " '일간누적총광량_4d_rise_rate' '일간누적백색광량_1d_rise_rate' '일간누적백색광량_2d_rise_rate'\n",
      " '일간누적백색광량_3d_rise_rate' '일간누적백색광량_4d_rise_rate' 'co2관측치_higher_than_1d'\n",
      " 'co2관측치_higher_than_2d' 'co2관측치_higher_than_3d' 'co2관측치_higher_than_4d'\n",
      " 'co2관측치_4d_rise_rate' '일간누적분무량_1d_rise_rate' '일간누적분무량_2d_rise_rate'\n",
      " '일간누적분무량_3d_rise_rate' '일간누적분무량_4d_rise_rate']\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "del_feature = corr_df.feature[corr_df.p_value>alpha].tolist()\n",
    "len(del_feature)\n",
    "print(np.array(del_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290fe567-1d52-4079-bb25-9016f212a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df     .drop(columns=del_feature,inplace=True)\n",
    "test_input_df.drop(columns=del_feature,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a2304db-0c27-4e98-b2b0-941e29a8d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asis(241) -> tobe(186)\n"
     ]
    }
   ],
   "source": [
    "print(f'asis({input_df.shape[1]+len(del_feature)}) -> tobe({input_df.shape[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dff77-20cc-4880-9341-8835231ba4b5",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec8035-20fe-4240-8016-9395b0ba39f3",
   "metadata": {},
   "source": [
    "# Pre-Fit Catboost\n",
    "- 아래 모델링과 비슷하게 case_num으로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29d91e-845a-4840-9bc1-bde8e2f01499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '1'\n",
    "\n",
    "paths = [f'./out/kf_cat_{save_mark}',f'./out/kf_cat_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7eefc-abbc-4b04-b7fd-91e5de35e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f85fd8-e663-4efb-a991-831442b86ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = case_num[tr_idx]\n",
    "    va_case_num = case_num[va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)].drop(columns=['case_num'])\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)].drop(columns=['case_num'])\n",
    "\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]['predicted_weight_g']\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]['predicted_weight_g']\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=5000,metric_period=1000,random_state=42)\n",
    "    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "    tr_pred = model.predict(X_train)\n",
    "    va_pred = model.predict(X_valid)\n",
    "    \n",
    "    pred_input_df[f'pred_{kf_iter}'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "    pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "    \n",
    "    pred_test_df[f'pred_{kf_iter}'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "    pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f983dc-aaf6-4c44-addc-7db47354d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9cb6e-bc8f-435d-86dc-5fd4d565cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "pred_input_df['preds'] = pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1)\n",
    "\n",
    "mean_squared_error(pred_input_df['preds'],pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2158-b6ec-450b-afc3-faadbf1fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols].apply(lambda x: x.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1af6e-29c4-4dbb-9976-dbe525d318ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_cat_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f1b7a-a56d-477c-9596-2f7a57168d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_cat_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_cat_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a792e-c83d-4629-a29e-b0fd8b3d50c0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c3eac-4578-46fc-98aa-fecf49e5b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df      = pd.read_csv(f'./out/kf_cat_1/pred_input_df_10.csv').drop('predicted_weight_g',axis=1)\n",
    "test_input_df = pd.read_csv(f'./out/kf_cat_1/pred_test_df_10.csv') .drop('predicted_weight_g',axis=1)\n",
    "\n",
    "input_df.case_num      = ['0'+str(x) if x<10 else str(x) for x in input_df     .case_num]\n",
    "test_input_df.case_num = ['0'+str(x) if x<10 else str(x) for x in test_input_df.case_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dfe5b-c6d9-4026-ae22-b66a86857fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '2'\n",
    "\n",
    "paths = [f'./out/kf_cat_{save_mark}',f'./out/kf_cat_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db29b2-6c1b-4143-b086-f97f9fbc9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9832a59-938c-42f1-b10b-942e90cfe717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = case_num[tr_idx]\n",
    "    va_case_num = case_num[va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)].drop(columns=['case_num'])\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)].drop(columns=['case_num'])\n",
    "\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]['predicted_weight_g']\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]['predicted_weight_g']\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=5000,metric_period=1000,random_state=42)\n",
    "    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "    tr_pred = model.predict(X_train)\n",
    "    va_pred = model.predict(X_valid)\n",
    "    \n",
    "    pred_input_df[f'pred2_{kf_iter}'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "    pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "    \n",
    "    pred_test_df[f'pred2_{kf_iter}'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "    pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5ea94-7157-4932-92af-902cbfdcc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f425a-db42-4c2f-b830-cdcdeaf0e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred2')>=0]\n",
    "pred_input_df['preds'] = pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1)\n",
    "\n",
    "mean_squared_error(pred_input_df['preds'],pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611e4e6-9568-45cb-9fe5-fa26c7304e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=pred_input_df['preds'],y=pred_input_df['predicted_weight_g'])\n",
    "abline(slope=1,intercept=0,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1d128-d5a5-418f-94cb-1ddba1d10961",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred2_')>=0]\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols].apply(lambda x: x.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d519c-032e-416c-af38-dd4136ac2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_cat_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9870ee-d2ac-4eb9-83d4-0bd6969cab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_cat_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_cat_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c49774-3089-49fd-a657-f1181f754f65",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a08-c07c-42bd-9b38-18eccfeba517",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a523935d-af75-45ce-976c-c78f41578e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        # print(x.shape,x_reshape.shape)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446b0f-9dba-4a2b-b25e-5e20da605124",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "194faee7-d316-4c85-9cad-8629cffdacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        hidden  = [40]*4      # 40\n",
    "        dropout = [0.5]*4     # 0.5\n",
    "        num_layers = [1]*4    # 1\n",
    "        bidirectional = False # False\n",
    "        if bidirectional:\n",
    "            offset = 2\n",
    "        else:\n",
    "            offset = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden[0],\n",
    "            dropout=dropout[0],\n",
    "            num_layers=num_layers[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=offset*hidden[0],\n",
    "            hidden_size=hidden[1],\n",
    "            dropout=dropout[1],\n",
    "            num_layers=num_layers[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=offset*hidden[1],\n",
    "            hidden_size=hidden[2],\n",
    "            dropout=dropout[2],\n",
    "            num_layers=num_layers[2],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=offset*hidden[2],\n",
    "            hidden_size=hidden[3],\n",
    "            dropout=dropout[3],\n",
    "            num_layers=num_layers[3],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.elu  = nn.ELU()\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.fc = nn.Linear(offset * hidden[3], 1)\n",
    "        self.fc = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st\n",
    "        x, _ = self.lstm1(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 2nd\n",
    "        x, _ = self.lstm2(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 3rd\n",
    "        x, _ = self.lstm3(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 4th\n",
    "        x, _ = self.lstm4(x)\n",
    "        x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # fully connected layer\n",
    "        x    = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac7bec-be69-4cf4-a663-2c5d0e813db0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fcb61ab-2360-4805-8407-1005e3ca5b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update)\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                 kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update)\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x)\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd =SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "    \n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen):\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: \n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCINet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, num_decoder_layer = 1, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.num_decoder_layer = num_decoder_layer\n",
    "\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        self.div_projection = nn.ModuleList()\n",
    "        self.overlap_len = self.input_len//4\n",
    "        self.div_len = self.input_len//6\n",
    "\n",
    "        if self.num_decoder_layer > 1:\n",
    "            self.projection1 = nn.Linear(self.input_len, self.output_len)\n",
    "            for layer_idx in range(self.num_decoder_layer-1):\n",
    "                div_projection = nn.ModuleList()\n",
    "                for i in range(6):\n",
    "                    lens = min(i*self.div_len+self.overlap_len,self.input_len) - i*self.div_len\n",
    "                    div_projection.append(nn.Linear(lens, self.div_len))\n",
    "                self.div_projection.append(div_projection)\n",
    "\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x += pe[:, :, :-1]\n",
    "            else:\n",
    "                x += self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        if self.RIN:\n",
    "            print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            #mean\n",
    "            x = x - means\n",
    "            #var\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # affine\n",
    "            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x += res1\n",
    "        if self.num_decoder_layer == 1:\n",
    "            x = self.projection1(x)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    "            for div_projection in self.div_projection:\n",
    "                output = torch.zeros(x.shape,dtype=x.dtype).cuda()\n",
    "                for i, div_layer in enumerate(div_projection):\n",
    "                    div_x = x[:,:,i*self.div_len:min(i*self.div_len+self.overlap_len,self.input_len)]\n",
    "                    output[:,:,i*self.div_len:(i+1)*self.div_len] = div_layer(div_x)\n",
    "                x = output\n",
    "            x = self.projection1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            res2 = x\n",
    "            x = self.blocks2(x)\n",
    "            x += res2\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x, MidOutPut\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b693071-44da-46f4-a007-ccbfa2a9b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class SCINet_decompose(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet_decompose, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.decomp = series_decomp(25)\n",
    "        self.trend = nn.Linear(input_len,input_len)\n",
    "        self.trend_dec = nn.Linear(input_len,output_len)\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        x, trend = self.decomp(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # seq_means = x[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means = x[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # x = x - seq_means\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "            # print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means2 = trend.mean(1, keepdim=True).detach()\n",
    "            trend = trend - means2\n",
    "            stdev2 = torch.sqrt(torch.var(trend, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            trend /= stdev2\n",
    "            # seq_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # trend = trend - seq_means2 \n",
    "            trend = trend * self.affine_weight2 + self.affine_bias2\n",
    "        \n",
    "\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x = x + pe[:, :, :-1]\n",
    "            else:\n",
    "                x = x + self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        \n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x = self.projection1(x)\n",
    "\n",
    "        trend = trend.permute(0,2,1)\n",
    "        trend = self.trend(trend)  \n",
    "        trend = self.trend_dec(trend).permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                # x = x + pred_means\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            x = self.blocks2(x)\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend, MidOutPut\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41047c2-5bff-4835-9c26-6f523fbbf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCINet_Model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(SCINet_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # 24,4,1,1,2,0.5,False,1,True,1\n",
    "        window_size = 1 # in (fixed)\n",
    "        horizon = 1      # out\n",
    "        hidden_size = 1\n",
    "        groups = 1\n",
    "        kernel = 1\n",
    "        dropout = 0.5\n",
    "        single_step_output_One = False\n",
    "        num_levels = 1\n",
    "        positionalEcoding = True\n",
    "        num_stacks = 1\n",
    "        self.scinet = SCINet(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        self.scinet_decompose = SCINet_decompose(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        \n",
    "        # hidden  = [64, 64, 64]\n",
    "        # dropout = [0.2, 0.5, 0.5]\n",
    "        # num_layers = [1,1,1]\n",
    "        # self.lstm1 = nn.LSTM(\n",
    "        #     input_size=input_size,\n",
    "        #     hidden_size=hidden[0],\n",
    "        #     dropout=dropout[0],\n",
    "        #     num_layers=num_layers[0],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm2 = nn.LSTM(\n",
    "        #     input_size=2*hidden[0],\n",
    "        #     hidden_size=hidden[1],\n",
    "        #     dropout=dropout[1],\n",
    "        #     num_layers=num_layers[1],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm3 = nn.LSTM(\n",
    "        #     input_size=2*hidden[1],\n",
    "        #     hidden_size=hidden[2],\n",
    "        #     dropout=dropout[2],\n",
    "        #     num_layers=num_layers[2],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.fc = nn.Linear(2*hidden[0], 1)\n",
    "        self.fc_1 = nn.Linear(input_size, 16)\n",
    "        self.fc_1 = TimeDistributed(self.fc_1)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        self.fc_2 = TimeDistributed(self.fc_2)\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        \n",
    "        self.nlinear = NLinear(input_size,1)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.scinet(x)\n",
    "#         # x = self.bn(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm1(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm2(x)\n",
    "#         # x = self.selu(x)\n",
    "#         # x,_ = self.lstm3(x)\n",
    "#         # x = self.selu(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leakyrelu(x)\n",
    "#         x = self.fc_2(x[:,-1,:]) # [:,:,-1]\n",
    "        \n",
    "#         # x = self.fc_2(x[:,-1,:])\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.scinet(x)\n",
    "#         x = self.scinet_decompose(x)\n",
    "#         x1,x2 = x[0],x[1]\n",
    "#         x = torch.cat([x1,x2],dim=1)\n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.scinet(x)\n",
    "        x = self.scinet_decompose(x)\n",
    "        # x = self.fc(x[:,-1,:])\n",
    "        x = self.nlinear(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fe908-42f5-43ec-b343-e9d60b7bdce1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a341e468-e635-460c-b7f9-ab5475e77b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization-Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len, td=True):\n",
    "        super(NLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
    "        if td:\n",
    "            self.Linear = TimeDistributed(self.Linear)\n",
    "        # Use this line if you want to visualize the weights\n",
    "        self.Linear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "        self._reinitialize()\n",
    "    \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'Linear' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seq_last = x[:,-1:,:].detach()\n",
    "        x = x - seq_last\n",
    "        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = x + seq_last\n",
    "        return x # [Batch, Output length, Channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12749617-0499-43b8-8499-4236f292e471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLinear_Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, input_size):\n",
    "        super(NLinear_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.BatchNorm1d(nodes[1]) , \n",
    "        nodes = [40]*4\n",
    "        dropout = 0.5\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            NLinear(seq_len ,nodes[0]), self.dropout, self.activation,\n",
    "            NLinear(nodes[0],nodes[1]), self.dropout, self.activation,\n",
    "            NLinear(nodes[1],nodes[2]), self.dropout, self.activation,\n",
    "            NLinear(nodes[2],nodes[3]), self.dropout, self.activation,\n",
    "            NLinear(nodes[3],pred_len),\n",
    "        )\n",
    "\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        # self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear_1(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.nlinear_2(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train, Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "620d419b-7332-4e64-a6e5-68aed8e5b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_fn(output, target):\n",
    "    return torch.sqrt(torch.mean((output-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26ee98e-b3ae-4d8e-a18e-8b166686c493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, scheduler, device, \n",
    "    early_stopping, epochs, metric_period=1, best_model_only=True, verbose=True,\n",
    "):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['ES_PATIENCE'], verbose = CFG['ES_VERBOSE'], path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            # print(output.shape,Y.shape) # torch.Size([4, 28, 1]) torch.Size([4, 24])\n",
    "            # print(output[:5],Y[:5])\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5],output.shape,Y.shape)\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % metric_period == 0:\n",
    "                epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "                progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "                    .format(\n",
    "                        epoch_str,\n",
    "                        epochs,np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)\n",
    "                    )\n",
    "                epoch_s = time.time()\n",
    "\n",
    "                if best_loss > valid_loss:\n",
    "                    mark = '*'\n",
    "                else:\n",
    "                    mark = ' '\n",
    "            \n",
    "                print(mark+progress)\n",
    "            \n",
    "        if best_model_only:\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model = model\n",
    "                \n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec6fe-a87d-4fe0-8f91-97136bd96e57",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc3e9c6-5ca0-43cf-99d4-4826cedf9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,input,label,infer_mode,seq_length):\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        input = input.sort_values(['case_num','DAT'])\n",
    "        label = label.sort_values(['case_num','DAT'])\n",
    "\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        for i in range(int(label.shape[0]/seq_length)):\n",
    "            i_df = input.iloc[i*seq_length:(i+1)*seq_length,:].drop('case_num',axis=1)\n",
    "            l_df = label.iloc[i*seq_length:(i+1)*seq_length]['predicted_weight_g']\n",
    "            \n",
    "            self.input_list.append(torch.Tensor(i_df.values))\n",
    "            self.label_list.append(torch.Tensor(l_df.values))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data  = self.input_list[index]\n",
    "        label = self.label_list[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc703f27-fc08-42ed-b242-4e7764b505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_df      = pd.read_csv(f'./out/kf_cat_1/pred_input_df_10.csv').drop('predicted_weight_g',axis=1)\n",
    "# test_input_df = pd.read_csv(f'./out/kf_cat_1/pred_test_df_10.csv') .drop('predicted_weight_g',axis=1)\n",
    "\n",
    "# input_df.case_num      = ['0'+str(x) if x<10 else str(x) for x in input_df     .case_num]\n",
    "# test_input_df.case_num = ['0'+str(x) if x<10 else str(x) for x in test_input_df.case_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31adc92f-7d3b-4a78-9898-3ad69b0406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "input_dataset = CustomDataset(input=input_df, label=label_df, infer_mode=False, seq_length=seq_length)\n",
    "input_loader  = DataLoader(input_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True, seq_length=seq_length)\n",
    "test_loader  = DataLoader(test_dataset  , batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c132bc04-4a20-43ad-aec5-648acc3fd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe9bd444-65c1-4f0e-9559-75e3d99c0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '4'\n",
    "\n",
    "paths = [f'./out/kf_lstm_{save_mark}',f'./out/kf_lstm_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbbf5a68-454d-457d-9db5-98056e39363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3446f4278f4465f8b6ff47ef8cce41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "(1/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      " [00100/16000] tr_loss : 12.33777, val_loss : 12.95909, elapsed : 16.64s, total : 16.64s, remaining : 264537.11s\n",
      " [00200/16000] tr_loss : 4.19237, val_loss : 11.30626, elapsed : 17.98s, total : 34.62s, remaining : 284067.58s\n",
      " [00300/16000] tr_loss : 3.70820, val_loss : 7.10229, elapsed : 18.84s, total : 53.46s, remaining : 295801.94s\n",
      " [00400/16000] tr_loss : 2.70567, val_loss : 9.86875, elapsed : 17.60s, total : 71.06s, remaining : 274622.70s\n",
      " [00500/16000] tr_loss : 2.63649, val_loss : 10.95492, elapsed : 17.69s, total : 88.75s, remaining : 274181.42s\n",
      " [00600/16000] tr_loss : 2.01964, val_loss : 9.35521, elapsed : 17.39s, total : 106.14s, remaining : 267849.04s\n",
      " [00700/16000] tr_loss : 2.01543, val_loss : 6.30625, elapsed : 17.94s, total : 124.09s, remaining : 274516.15s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(2/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      " [00100/16000] tr_loss : 8.69351, val_loss : 29.18207, elapsed : 17.26s, total : 17.26s, remaining : 274509.24s\n",
      " [00200/16000] tr_loss : 3.88588, val_loss : 24.98796, elapsed : 16.84s, total : 34.10s, remaining : 266019.75s\n",
      " [00300/16000] tr_loss : 2.85001, val_loss : 21.60372, elapsed : 16.88s, total : 50.98s, remaining : 265041.45s\n",
      " [00400/16000] tr_loss : 2.37155, val_loss : 23.45819, elapsed : 16.81s, total : 67.79s, remaining : 262220.40s\n",
      " [00500/16000] tr_loss : 2.24303, val_loss : 22.07774, elapsed : 17.19s, total : 84.99s, remaining : 266502.96s\n",
      " [00600/16000] tr_loss : 1.89236, val_loss : 19.47962, elapsed : 16.78s, total : 101.76s, remaining : 258369.63s\n",
      " [00700/16000] tr_loss : 1.74115, val_loss : 20.34688, elapsed : 17.11s, total : 118.87s, remaining : 261783.10s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(3/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      " [00100/16000] tr_loss : 8.27849, val_loss : 8.83951, elapsed : 17.33s, total : 17.33s, remaining : 275484.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = case_num[tr_idx]\n",
    "    va_case_num = case_num[va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)]\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)]\n",
    "\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (2) custom dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False, seq_length=seq_length)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "    valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False, seq_length=seq_length)\n",
    "    valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "    \n",
    "    # [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "    # [y for x,y in iter(train_loader)]\n",
    "    # sum([y.size(0) for x,y in iter(train_loader)])\n",
    "\n",
    "    # len([x for x,y in iter(train_loader)])\n",
    "\n",
    "    # [(x[0].size(),x[1].size()) for x in train_loader]\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (3) modeling\n",
    "    #------------------------------------------------------------------------------------\n",
    "    seed_everything(CFG['SEED'])\n",
    "\n",
    "    input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "    model = LSTM_Model(input_size=input_size)\n",
    "    # model = NLinear_Model(seq_len=1,pred_len=1,input_size=input_size)\n",
    "    # model = Model(input_size=input_size)\n",
    "    # model = SCINet_Model(input_size=input_size)\n",
    "    # model = BaseModel(\n",
    "    #     input_size = input_size,\n",
    "    #     hidden_sizes=[400,300],\n",
    "    #     dropout_rates=[0.2,0.2],\n",
    "    #     num_classes=seq_length,\n",
    "    #     num_layers=2,\n",
    "    #     bidirectional=True,\n",
    "    # )\n",
    "\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-3, weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=100, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "    CFG['ES_PATIENCE'] = 500\n",
    "    CFG['ES_VERBOSE']  = 0\n",
    "    best_model = train(\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        scheduler=None,#scheduler,\n",
    "        device=device,\n",
    "        early_stopping=True,\n",
    "        metric_period=100,\n",
    "        epochs=16000,\n",
    "        best_model_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-1) predict : input dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X,y in iter(input_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_input_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_input_df.to_csv(f'./out/kf_lstm_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-2) predict : test dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_test_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_test_df.to_csv(f'./out/kf_lstm_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19d5ab-866c-4ce1-b491-b51630e72369",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceade27b-04ab-4d84-ad78-dddf9d5bdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "\n",
    "mse_list = []\n",
    "for case_num in pred_input_df.case_num.unique():\n",
    "    d = pred_input_df[pred_input_df.case_num==case_num]\n",
    "\n",
    "    _mse = mean_squared_error(\n",
    "        # d['pred_2'],\n",
    "        d[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "        d['predicted_weight_g'],\n",
    "    )\n",
    "    mse_list.append([case_num,_mse])\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(\n",
    "        # x=d['pred_2'],\n",
    "        x=d[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "        y=d['predicted_weight_g'],\n",
    "    )\n",
    "    abline(slope=1,intercept=0,color='red')\n",
    "    plt.title(f'CASE={case_num} : MSE = {_mse:.5f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685e738-9cfd-461a-b60c-bedb5dff3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10+1):\n",
    "    _mse = mean_squared_error(\n",
    "        pred_input_df['predicted_weight_g'],\n",
    "        pred_input_df[f'pred_{i}']\n",
    "        # pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "    )\n",
    "    print(f'pred_{i} :',_mse)\n",
    "    \n",
    "print('all :',mean_squared_error(pred_input_df['predicted_weight_g'], pred_input_df[pred_cols].apply(lambda x: x.mean(),axis=1)))\n",
    "# print('2&4 :',mean_squared_error(pred_input_df['predicted_weight_g'], pred_input_df[['pred_2','pred_4']].apply(lambda x: x.mean(),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512188dd-3fa9-469f-9fb2-71ce16393e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(\n",
    "    x=pred_input_df['predicted_weight_g'],\n",
    "    y=pred_input_df[pred_cols].apply(lambda x: x.mean(),axis=1),\n",
    ")\n",
    "abline(slope=1,intercept=0,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad6f5f-ef5b-4e5f-b38e-eef2e3143c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mse<10 : y최대가 80정도 이하\n",
    "# # mse>10 : y최대가 80정도인 것도 포함되지만, 높은 것들이 많음\n",
    "# d = pred_input_df[pred_input_df.case_num.isin([case_num for case_num, mse in mse_list if mse>10])]\n",
    "# for case_num in d.case_num.unique():\n",
    "#     dd = d[d.case_num==case_num]\n",
    "#     print(case_num)\n",
    "#     sns.lineplot(x=dd.DAT,y=(dd.pred_2+dd.pred_4)/2)\n",
    "#     sns.lineplot(x=dd.DAT,y=dd.predicted_weight_g)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0964749-d7cd-4210-86c4-eaa690550636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "# pred_cols = ['pred_2','pred_4']\n",
    "pred_input_df['preds'] = pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1)\n",
    "\n",
    "mean_squared_error(pred_input_df['preds'],pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c11c-ea95-4eeb-bddb-d66fc61c602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "# pred_cols = ['pred_2','pred_4']\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols].apply(lambda x: x.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbc1d9-07b4-45bf-9bb4-1acd0a0ab023",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96f71-2ae7-444e-9437-7bbb251a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_lstm_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_lstm_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_lstm_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545177e-99ce-4047-88c5-f634eba2d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.8433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafea69c-4e00-4499-ab70-f01151caf51c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d317a2-e427-408b-ac40-22903a9232e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "\n",
    "mse_list = []\n",
    "for col in pred_cols:\n",
    "    _mse = mean_squared_error(pred_input_df[col],pred_input_df['predicted_weight_g'])\n",
    "    mse_list.append(_mse)\n",
    "    \n",
    "# weights = [1]*len(mse_list)/sum(mse_list)\n",
    "weights = 1/(mse_list/sum(mse_list))\n",
    "weights = weights / sum(weights)\n",
    "\n",
    "final_pred = weights * pred_input_df[pred_cols]\n",
    "final_pred = final_pred.apply(lambda x: x.sum(),axis=1)\n",
    "\n",
    "mean_squared_error(final_pred,pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69715bc9-c9af-46fb-8aaf-f3beb082877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = weights * pred_test_df[pred_cols]\n",
    "test_pred = test_pred.apply(lambda x: x.sum(),axis=1)\n",
    "pred_test_df['predicted_weight_g'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49647cd3-313e-46e2-9cbc-10cee9933425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_lstm_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b0cc-fd0c-46b5-9b22-d98b99060717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_lstm_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_lstm_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972264af-354b-4204-a161-12ef5e941910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
