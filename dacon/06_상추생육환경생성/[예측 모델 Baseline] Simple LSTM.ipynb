{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list  = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_target_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_list   = sorted(glob.glob('./data/test_input/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7f28f0-e2fe-40c4-958a-23a4fea0283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, input_paths, target_paths, infer_mode):\n",
    "#         self.input_paths = input_paths\n",
    "#         self.target_paths = target_paths\n",
    "#         self.infer_mode = infer_mode\n",
    "        \n",
    "#         self.data_list = []\n",
    "#         self.label_list = []\n",
    "#         print('Data Pre-processing..')\n",
    "#         for input_path, target_path in tqdm(zip(self.input_paths, self.target_paths)):\n",
    "#             input_df = pd.read_csv(input_path)\n",
    "#             target_df = pd.read_csv(target_path)\n",
    "            \n",
    "#             input_df = input_df.drop(columns=['obs_time'])\n",
    "#             input_df = input_df.fillna(0)\n",
    "            \n",
    "#             input_length = int(len(input_df)/24)\n",
    "#             target_length = int(len(target_df))\n",
    "            \n",
    "#             for idx in range(target_length):\n",
    "#                 time_series = input_df[24*idx:24*(idx+1)].values\n",
    "#                 self.data_list.append(torch.Tensor(time_series))\n",
    "#             for label in target_df[\"predicted_weight_g\"]:\n",
    "#                 self.label_list.append(label)\n",
    "#         print('Done.')\n",
    "              \n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.data_list[index]\n",
    "#         label = self.label_list[index]\n",
    "#         if self.infer_mode == False:\n",
    "#             return data, label\n",
    "#         else:\n",
    "#             return data\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, input_paths, target_paths, test_paths):\n",
    "        self.input_paths = input_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.test_paths = test_paths\n",
    "        \n",
    "        self.input = None\n",
    "        self.label = None\n",
    "        self.test  = None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, target_path in zip(self.input_paths, self.target_paths):\n",
    "            input_df  = pd.read_csv(input_path)\n",
    "            target_df = pd.read_csv(target_path)\n",
    "\n",
    "            input_df = input_df.drop(columns=['obs_time'])\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(target_df)\n",
    "        \n",
    "        test_fn = []\n",
    "        for test_path in self.test_paths:\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            test_df = test_df.drop(columns=['obs_time'])\n",
    "            test_fn.append(test_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0)\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test  = pd.concat(test_fn ,axis=0)\n",
    "\n",
    "    def _train_test_split(self, val_rate):\n",
    "        val_size = int(self.input.DAT.nunique() * val_rate)\n",
    "\n",
    "        tr_idx = self.input.DAT <  max(self.input.DAT)-val_size\n",
    "        va_idx = self.input.DAT >= max(self.input.DAT)-val_size\n",
    "        \n",
    "        X_train = self.input[tr_idx]\n",
    "        X_valid = self.input[va_idx]\n",
    "        y_train = self.label[self.label.DAT.isin(X_train.DAT.unique()+1)]\n",
    "        y_valid = self.label[self.label.DAT.isin(X_valid.DAT.unique()+1)]\n",
    "\n",
    "        print(f'val_rate={val_rate}, val_size={val_size}')\n",
    "        print(f'train DAT : [{X_train.DAT.min()}~{X_train.DAT.max()}], validation DAT : [{X_valid.DAT.min()}~{X_valid.DAT.max()}]')\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "    def _scale_dataset(self,X_train,X_valid):\n",
    "        \n",
    "        scalers = {}\n",
    "        # for train dataset\n",
    "        for col in X_train.columns:\n",
    "            _scaler = MinMaxScaler()\n",
    "            X_train[col] = _scaler.fit_transform(np.array(X_train[col]).reshape(-1,1))\n",
    "            scalers[col] = _scaler\n",
    "            \n",
    "        # for validation dataset\n",
    "        for col in X_valid.columns:\n",
    "            _scaler = scalers[col]\n",
    "            X_valid[col] = _scaler.transform(np.array(X_valid[col]).reshape(-1,1))\n",
    "            \n",
    "        # for test dataset\n",
    "        X_test = self.test.copy()\n",
    "        for col in X_test.columns:\n",
    "            _scaler = scalers[col]\n",
    "            X_test[col] = _scaler.transform(np.array(X_test[col]).reshape(-1,1))\n",
    "            \n",
    "        return X_train, X_valid, X_test\n",
    "\n",
    "    def _transform_dataset(self,y_train,y_valid):\n",
    "        y_train['predicted_weight_g'] = np.log(y_train['predicted_weight_g'])\n",
    "        y_valid['predicted_weight_g'] = np.log(y_valid['predicted_weight_g'])\n",
    "        \n",
    "        return y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8622711d-c9d9-49c3-a321-f1618e4c11ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_rate=0.1, val_size=2\n",
      "train DAT : [0~24], validation DAT : [25~27]\n"
     ]
    }
   ],
   "source": [
    "val_rate = 0.1\n",
    "\n",
    "dataset = CustomDataset(\n",
    "    input_paths = all_input_list,\n",
    "    target_paths = all_target_list,\n",
    "    test_paths = all_test_list,\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = dataset._train_test_split(val_rate=val_rate)\n",
    "\n",
    "X_train, X_valid, X_test = dataset._scale_dataset(X_train,X_valid)\n",
    "# y_train, y_valid = dataset._transform_dataset(y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aa507e6-73a0-4c1c-b38b-3eb84f7ae425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self,X,y,infer_mode):\n",
    "        self.X = X\n",
    "        self.y = y.sort_values('DAT')\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        self.X_ret = []\n",
    "        self.y_ret = []\n",
    "        \n",
    "        for x_idx, y_idx in zip(sorted(X.DAT.unique()),sorted(y.DAT.unique())):\n",
    "            X_dat = X[X.DAT==x_idx]\n",
    "            y_dat = y[y.DAT==y_idx][['predicted_weight_g']].values[0][0]\n",
    "            \n",
    "            self.X_ret.append(torch.Tensor(X_dat.values))\n",
    "            self.y_ret.append(y_dat)\n",
    "            \n",
    "        # self.X_ret = np.array(self.X_ret)\n",
    "        # self.y_ret = np.array(self.y_ret).reshape(-1)\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        data  = self.X_ret[index]\n",
    "        label = self.y_ret[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03fa89d5-37bc-4773-a9cd-edf15269baa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16800, 15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TorchDataset(X_train, y_train, False)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=6)\n",
    "\n",
    "val_dataset = TorchDataset(X_valid, y_valid, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=6)\n",
    "\n",
    "test_dataset = TorchDataset(X_test, y_valid, True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff8530a6-2012-4619-903f-da297915ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=15, hidden_size=256, batch_first=True, bidirectional=False)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         hidden, _ = self.lstm(x)\n",
    "#         output = self.classifier(hidden[:,-1,:])\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fee70e82-0375-4a8d-9a02-5100763d8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coding-yoon.tistory.com/131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropout_rates, num_classes, num_layers):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_sizes[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.dropout_rates[0],\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[0]*2, # bidirectional\n",
    "            hidden_size=self.hidden_sizes[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.dropout_rates[1],\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_sizes[1]*2, self.num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hid, _ = self.lstm1(x)\n",
    "        hid    = self.relu(hid)\n",
    "        hid, _ = self.lstm2(hid)\n",
    "        out    = self.relu(hid)\n",
    "        out    = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, optimizer, train_loader, valid_loader, scheduler, device, early_stopping, epochs, metric_period=1):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['PATIENCE'], verbose = False, path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            # Getting gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        if epoch % metric_period == 0:\n",
    "            epoch_e = time.time()\n",
    "            epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "            #print(f'[{epoch}/{epochs}] Train Loss : [{np.mean(train_loss):.5f}], Valid Loss : [{valid_loss:.5f}], elapsed : [{epoch_e-epoch_s:.2f}s]')\n",
    "            progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "                .format(\n",
    "                    epoch_str,\n",
    "                    epochs,np.mean(train_loss),\n",
    "                    valid_loss,\n",
    "                    epoch_e-epoch_s,\n",
    "                    epoch_e-start_time,\n",
    "                    (epoch_e-epoch_s)*(epochs-epoch)\n",
    "                )\n",
    "            print(progress)\n",
    "            epoch_s = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if best_loss > valid_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "            # Y          = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5])\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (lstm1): LSTM(15, 32, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (lstm2): LSTM(64, 16, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = BaseModel(input_size = input_size, hidden_sizes=[32,16],dropout_rates=[0.2,0.2], num_classes=1, num_layers=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "751cd077-7f38-4783-b46d-7c225373d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0010/1024] tr_loss : 30.27542, val_loss : 82.26220, elapsed : 4.79s, total : 4.79s, remaining : 4853.61s\n",
      "[0020/1024] tr_loss : 29.98543, val_loss : 81.87723, elapsed : 4.93s, total : 9.71s, remaining : 4945.00s\n",
      "[0030/1024] tr_loss : 29.56663, val_loss : 81.33752, elapsed : 4.92s, total : 14.64s, remaining : 4894.01s\n",
      "[0040/1024] tr_loss : 29.14476, val_loss : 80.78839, elapsed : 4.89s, total : 19.53s, remaining : 4812.29s\n",
      "[0050/1024] tr_loss : 28.80274, val_loss : 80.32913, elapsed : 4.88s, total : 24.41s, remaining : 4754.44s\n",
      "[0060/1024] tr_loss : 28.52729, val_loss : 79.94487, elapsed : 4.94s, total : 29.35s, remaining : 4761.56s\n",
      "[0070/1024] tr_loss : 28.29657, val_loss : 79.61050, elapsed : 4.86s, total : 34.21s, remaining : 4634.26s\n",
      "[0080/1024] tr_loss : 28.09639, val_loss : 79.30959, elapsed : 4.89s, total : 39.10s, remaining : 4614.90s\n",
      "[0090/1024] tr_loss : 27.91768, val_loss : 79.03111, elapsed : 4.93s, total : 44.03s, remaining : 4607.74s\n",
      "[0100/1024] tr_loss : 27.75550, val_loss : 78.76907, elapsed : 4.95s, total : 48.98s, remaining : 4569.85s\n",
      "[0110/1024] tr_loss : 27.60739, val_loss : 78.52094, elapsed : 4.86s, total : 53.84s, remaining : 4442.72s\n",
      "[0120/1024] tr_loss : 27.47165, val_loss : 78.28506, elapsed : 4.90s, total : 58.74s, remaining : 4431.32s\n",
      "[0130/1024] tr_loss : 27.34688, val_loss : 78.06002, elapsed : 4.87s, total : 63.61s, remaining : 4351.31s\n",
      "[0140/1024] tr_loss : 27.23197, val_loss : 77.84479, elapsed : 4.83s, total : 68.43s, remaining : 4265.37s\n",
      "[0150/1024] tr_loss : 27.12601, val_loss : 77.63861, elapsed : 4.96s, total : 73.39s, remaining : 4336.23s\n",
      "[0160/1024] tr_loss : 27.02823, val_loss : 77.44090, elapsed : 4.85s, total : 78.24s, remaining : 4188.40s\n",
      "[0170/1024] tr_loss : 26.93795, val_loss : 77.25116, elapsed : 4.88s, total : 83.12s, remaining : 4164.08s\n",
      "[0180/1024] tr_loss : 26.85458, val_loss : 77.06902, elapsed : 4.91s, total : 88.03s, remaining : 4142.02s\n",
      "[0190/1024] tr_loss : 26.77753, val_loss : 76.89408, elapsed : 4.83s, total : 92.86s, remaining : 4029.15s\n",
      "[0200/1024] tr_loss : 26.70632, val_loss : 76.72601, elapsed : 4.86s, total : 97.72s, remaining : 4002.17s\n",
      "[0210/1024] tr_loss : 26.64043, val_loss : 76.56450, elapsed : 4.82s, total : 102.53s, remaining : 3919.77s\n",
      "[0220/1024] tr_loss : 26.57945, val_loss : 76.40922, elapsed : 4.80s, total : 107.33s, remaining : 3855.90s\n",
      "[0230/1024] tr_loss : 26.52294, val_loss : 76.25989, elapsed : 4.88s, total : 112.21s, remaining : 3876.98s\n",
      "[0240/1024] tr_loss : 26.47055, val_loss : 76.11621, elapsed : 4.84s, total : 117.05s, remaining : 3792.55s\n",
      "[0250/1024] tr_loss : 26.42191, val_loss : 75.97791, elapsed : 4.86s, total : 121.91s, remaining : 3764.87s\n",
      "[0260/1024] tr_loss : 26.37672, val_loss : 75.84472, elapsed : 4.84s, total : 126.75s, remaining : 3696.17s\n",
      "[0270/1024] tr_loss : 26.33467, val_loss : 75.71640, elapsed : 5.06s, total : 131.81s, remaining : 3815.77s\n",
      "[0280/1024] tr_loss : 26.29549, val_loss : 75.59269, elapsed : 4.85s, total : 136.66s, remaining : 3608.11s\n",
      "[0290/1024] tr_loss : 26.25896, val_loss : 75.47336, elapsed : 4.85s, total : 141.51s, remaining : 3557.83s\n",
      "[0300/1024] tr_loss : 26.22484, val_loss : 75.35819, elapsed : 5.15s, total : 146.66s, remaining : 3729.47s\n",
      "[0310/1024] tr_loss : 26.19294, val_loss : 75.24697, elapsed : 5.12s, total : 151.78s, remaining : 3655.20s\n",
      "[0320/1024] tr_loss : 26.16307, val_loss : 75.13950, elapsed : 5.13s, total : 156.91s, remaining : 3610.42s\n",
      "[0330/1024] tr_loss : 26.13507, val_loss : 75.03559, elapsed : 5.07s, total : 161.98s, remaining : 3520.17s\n",
      "[0340/1024] tr_loss : 26.10878, val_loss : 74.93507, elapsed : 5.22s, total : 167.20s, remaining : 3567.19s\n",
      "[0350/1024] tr_loss : 26.08407, val_loss : 74.83775, elapsed : 6.14s, total : 173.33s, remaining : 4136.07s\n",
      "[0360/1024] tr_loss : 26.06081, val_loss : 74.74349, elapsed : 7.81s, total : 181.15s, remaining : 5186.46s\n",
      "[0370/1024] tr_loss : 26.03890, val_loss : 74.65214, elapsed : 7.88s, total : 189.02s, remaining : 5151.98s\n",
      "[0380/1024] tr_loss : 26.01823, val_loss : 74.56354, elapsed : 7.75s, total : 196.78s, remaining : 4993.22s\n",
      "[0390/1024] tr_loss : 25.99871, val_loss : 74.47758, elapsed : 7.87s, total : 204.65s, remaining : 4992.47s\n",
      "[0400/1024] tr_loss : 25.98024, val_loss : 74.39411, elapsed : 7.62s, total : 212.28s, remaining : 4757.31s\n",
      "[0410/1024] tr_loss : 25.96276, val_loss : 74.31303, elapsed : 7.74s, total : 220.02s, remaining : 4751.11s\n",
      "[0420/1024] tr_loss : 25.94620, val_loss : 74.23423, elapsed : 8.06s, total : 228.08s, remaining : 4869.94s\n",
      "[0430/1024] tr_loss : 25.93048, val_loss : 74.15759, elapsed : 7.73s, total : 235.81s, remaining : 4590.21s\n",
      "[0440/1024] tr_loss : 25.91556, val_loss : 74.08303, elapsed : 8.02s, total : 243.83s, remaining : 4684.28s\n",
      "[0450/1024] tr_loss : 25.90136, val_loss : 74.01044, elapsed : 7.95s, total : 251.78s, remaining : 4565.90s\n",
      "[0460/1024] tr_loss : 25.88786, val_loss : 73.93974, elapsed : 7.64s, total : 259.42s, remaining : 4309.62s\n",
      "[0470/1024] tr_loss : 25.87499, val_loss : 73.87085, elapsed : 7.82s, total : 267.24s, remaining : 4332.33s\n",
      "[0480/1024] tr_loss : 25.86273, val_loss : 73.80369, elapsed : 7.77s, total : 275.01s, remaining : 4225.51s\n",
      "[0490/1024] tr_loss : 25.85103, val_loss : 73.73819, elapsed : 7.80s, total : 282.81s, remaining : 4163.75s\n",
      "[0500/1024] tr_loss : 25.83984, val_loss : 73.67426, elapsed : 8.05s, total : 290.86s, remaining : 4217.21s\n",
      "[0510/1024] tr_loss : 25.82916, val_loss : 73.61187, elapsed : 7.98s, total : 298.84s, remaining : 4099.74s\n",
      "[0520/1024] tr_loss : 25.81894, val_loss : 73.55094, elapsed : 6.69s, total : 305.52s, remaining : 3370.77s\n",
      "[0530/1024] tr_loss : 25.80915, val_loss : 73.49140, elapsed : 5.25s, total : 310.77s, remaining : 2593.27s\n",
      "[0540/1024] tr_loss : 25.79977, val_loss : 73.43321, elapsed : 5.19s, total : 315.97s, remaining : 2513.60s\n",
      "[0550/1024] tr_loss : 25.79077, val_loss : 73.37632, elapsed : 5.16s, total : 321.13s, remaining : 2446.68s\n",
      "[0560/1024] tr_loss : 25.78214, val_loss : 73.32067, elapsed : 5.18s, total : 326.31s, remaining : 2403.01s\n",
      "[0570/1024] tr_loss : 25.77386, val_loss : 73.26621, elapsed : 5.17s, total : 331.48s, remaining : 2347.68s\n",
      "[0580/1024] tr_loss : 25.76590, val_loss : 73.21291, elapsed : 5.13s, total : 336.61s, remaining : 2277.57s\n",
      "[0590/1024] tr_loss : 25.75824, val_loss : 73.16072, elapsed : 5.16s, total : 341.77s, remaining : 2240.79s\n",
      "[0600/1024] tr_loss : 25.75087, val_loss : 73.10960, elapsed : 5.34s, total : 347.11s, remaining : 2263.87s\n",
      "[0610/1024] tr_loss : 25.74378, val_loss : 73.05951, elapsed : 5.56s, total : 352.67s, remaining : 2301.28s\n",
      "[0620/1024] tr_loss : 25.73695, val_loss : 73.01041, elapsed : 5.27s, total : 357.94s, remaining : 2129.04s\n",
      "[0630/1024] tr_loss : 25.73037, val_loss : 72.96228, elapsed : 6.67s, total : 364.61s, remaining : 2628.61s\n",
      "[0640/1024] tr_loss : 25.72402, val_loss : 72.91508, elapsed : 7.81s, total : 372.42s, remaining : 2998.15s\n",
      "[0650/1024] tr_loss : 25.71789, val_loss : 72.86877, elapsed : 7.64s, total : 380.06s, remaining : 2856.39s\n",
      "[0660/1024] tr_loss : 25.71198, val_loss : 72.82333, elapsed : 6.80s, total : 386.86s, remaining : 2474.48s\n",
      "[0670/1024] tr_loss : 25.70627, val_loss : 72.77874, elapsed : 5.15s, total : 392.00s, remaining : 1821.77s\n",
      "[0680/1024] tr_loss : 25.70075, val_loss : 72.73495, elapsed : 5.16s, total : 397.16s, remaining : 1774.68s\n",
      "[0690/1024] tr_loss : 25.69541, val_loss : 72.69195, elapsed : 5.10s, total : 402.26s, remaining : 1702.18s\n",
      "[0700/1024] tr_loss : 25.69025, val_loss : 72.64970, elapsed : 5.14s, total : 407.40s, remaining : 1664.50s\n",
      "[0710/1024] tr_loss : 25.68525, val_loss : 72.60821, elapsed : 5.21s, total : 412.61s, remaining : 1636.88s\n",
      "[0720/1024] tr_loss : 25.68041, val_loss : 72.56743, elapsed : 5.23s, total : 417.84s, remaining : 1590.55s\n",
      "[0730/1024] tr_loss : 25.67573, val_loss : 72.52734, elapsed : 5.06s, total : 422.90s, remaining : 1487.65s\n",
      "[0740/1024] tr_loss : 25.67119, val_loss : 72.48792, elapsed : 5.18s, total : 428.09s, remaining : 1472.32s\n",
      "[0750/1024] tr_loss : 25.66679, val_loss : 72.44916, elapsed : 5.22s, total : 433.31s, remaining : 1431.31s\n",
      "[0760/1024] tr_loss : 25.66252, val_loss : 72.41103, elapsed : 5.66s, total : 438.97s, remaining : 1493.60s\n",
      "[0770/1024] tr_loss : 25.65838, val_loss : 72.37351, elapsed : 7.84s, total : 446.81s, remaining : 1991.67s\n",
      "[0780/1024] tr_loss : 25.65436, val_loss : 72.33661, elapsed : 5.99s, total : 452.81s, remaining : 1462.14s\n",
      "[0790/1024] tr_loss : 25.65045, val_loss : 72.30028, elapsed : 6.77s, total : 459.57s, remaining : 1583.16s\n",
      "[0800/1024] tr_loss : 25.64666, val_loss : 72.26453, elapsed : 5.50s, total : 465.07s, remaining : 1231.07s\n",
      "[0810/1024] tr_loss : 25.64298, val_loss : 72.22932, elapsed : 5.20s, total : 470.27s, remaining : 1112.57s\n",
      "[0820/1024] tr_loss : 25.63940, val_loss : 72.19465, elapsed : 7.02s, total : 477.29s, remaining : 1432.12s\n",
      "[0830/1024] tr_loss : 25.63591, val_loss : 72.16050, elapsed : 5.13s, total : 482.42s, remaining : 995.06s\n",
      "[0840/1024] tr_loss : 25.63253, val_loss : 72.12686, elapsed : 5.11s, total : 487.53s, remaining : 940.22s\n",
      "[0850/1024] tr_loss : 25.62923, val_loss : 72.09373, elapsed : 5.29s, total : 492.82s, remaining : 920.26s\n",
      "[0860/1024] tr_loss : 25.62602, val_loss : 72.06109, elapsed : 5.17s, total : 497.99s, remaining : 848.59s\n",
      "[0870/1024] tr_loss : 25.62289, val_loss : 72.02891, elapsed : 5.20s, total : 503.20s, remaining : 801.51s\n",
      "[0880/1024] tr_loss : 25.61984, val_loss : 71.99721, elapsed : 5.19s, total : 508.39s, remaining : 747.79s\n",
      "[0890/1024] tr_loss : 25.61686, val_loss : 71.96597, elapsed : 5.10s, total : 513.49s, remaining : 683.57s\n",
      "[0900/1024] tr_loss : 25.61395, val_loss : 71.93517, elapsed : 5.16s, total : 518.65s, remaining : 639.79s\n",
      "[0910/1024] tr_loss : 25.61111, val_loss : 71.90482, elapsed : 4.97s, total : 523.62s, remaining : 566.71s\n",
      "[0920/1024] tr_loss : 25.60833, val_loss : 71.87490, elapsed : 5.24s, total : 528.86s, remaining : 544.97s\n",
      "[0930/1024] tr_loss : 25.60560, val_loss : 71.84543, elapsed : 5.11s, total : 533.98s, remaining : 480.75s\n",
      "[0940/1024] tr_loss : 25.60291, val_loss : 71.81641, elapsed : 5.15s, total : 539.13s, remaining : 432.59s\n",
      "[0950/1024] tr_loss : 25.60026, val_loss : 71.78782, elapsed : 5.16s, total : 544.30s, remaining : 382.18s\n",
      "[0960/1024] tr_loss : 25.59765, val_loss : 71.75966, elapsed : 5.23s, total : 549.52s, remaining : 334.57s\n",
      "[0970/1024] tr_loss : 25.59502, val_loss : 71.73195, elapsed : 5.20s, total : 554.72s, remaining : 280.76s\n",
      "[0980/1024] tr_loss : 25.59238, val_loss : 71.70475, elapsed : 5.17s, total : 559.89s, remaining : 227.40s\n",
      "[0990/1024] tr_loss : 25.58977, val_loss : 71.67796, elapsed : 5.11s, total : 565.00s, remaining : 173.68s\n",
      "[1000/1024] tr_loss : 25.58712, val_loss : 71.65147, elapsed : 5.14s, total : 570.14s, remaining : 123.48s\n",
      "[1010/1024] tr_loss : 25.58428, val_loss : 71.62567, elapsed : 5.19s, total : 575.34s, remaining : 72.69s\n",
      "[1020/1024] tr_loss : 25.58099, val_loss : 71.60098, elapsed : 5.10s, total : 580.44s, remaining : 20.39s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "best_model = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    scheduler,\n",
    "    device,\n",
    "    early_stopping=False,\n",
    "    metric_period=10,\n",
    "    epochs=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca286a4-f888-4c2f-bfe8-b6843770c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ac395-83bb-4aec-92bd-be2e9b7355bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X in iter(test_loader):\n",
    "        X = X.float().to(device)\n",
    "\n",
    "        model_pred = model(X)\n",
    "        model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "        pred_list += model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdd463-d997-42bd-a9c7-15d074ddf8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d7d60-38d7-44d6-82f2-836738b5a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_list = sorted(glob.glob('./test_input/*.csv'))\n",
    "test_target_list = sorted(glob.glob('./out/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1defba-fdc0-4fe4-8c59-36d338851f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_per_case(model, test_loader, test_path, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader):\n",
    "            X = X.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            \n",
    "            model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "            pred_list += model_pred\n",
    "    \n",
    "    submit_df = pd.read_csv(test_path)\n",
    "    submit_df['predicted_weight_g'] = pred_list\n",
    "    submit_df.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e68cb-dec1-439d-9363-03b817230bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_input_path, test_target_path in zip(test_input_list, test_target_list):\n",
    "    test_dataset = CustomDataset([test_input_path], [test_target_path], True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    inference_per_case(best_model, test_loader, test_target_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae208a0-548d-4af6-9798-0e247543b301",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir(\"./data/test_target/\")\n",
    "submission = zipfile.ZipFile(\"../submission.zip\", 'w')\n",
    "for path in test_target_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
