{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfd3826-7ff1-4a26-b431-6f34d34b5125",
   "metadata": {},
   "source": [
    "- kmeans 등으로 y의 max랑 구분지어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4041a5-44ba-4987-a6e7-3c3f40f10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 torch 다운로드\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e5953c-7a86-4c51-a887-423f7beb6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fp = fm.FontProperties(fname='/home/studio-lab-user/Dacon/tools/NanumFont/NanumGothic.ttf', size=10)\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "# # cuda (not Mac)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # mps (Mac)\n",
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('device :',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47cef-a496-4dc7-9a8b-0f363f65cdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':128,#1024,\n",
    "    'PATIENCE':30,\n",
    "    'LEARNING_RATE':0.05,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0fc924-3361-45c3-9f77-a764ef8fbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x,size):\n",
    "\n",
    "    ma = []\n",
    "    for i in range(len(x)):\n",
    "        if i<size:\n",
    "            values = x.values[:(i+1)]\n",
    "        else:\n",
    "            values = x.values[(i-size+1):(i+1)]\n",
    "        ma_value = values.mean()\n",
    "        ma.append(ma_value)\n",
    "        \n",
    "    return ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303d25b8-be32-47f2-bbff-74c7c37eef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, input_paths, label_paths, test_input_paths, test_label_paths):\n",
    "        \n",
    "        self.input, self.label, self.test_input, self.test_label = None, None, None, None\n",
    "        \n",
    "        self.X_train, self.X_valid = None, None\n",
    "        self.y_train, self.y_valid = None, None\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "        input_fn = []\n",
    "        label_fn = []\n",
    "        for input_path, label_path in zip(input_paths, label_paths):\n",
    "            case_num = input_path.replace('./data/train_input/CASE_','').replace('.csv','')\n",
    "            \n",
    "            input_df = pd.read_csv(input_path)\n",
    "            label_df = pd.read_csv(label_path)\n",
    "\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "            input_df['case_num'] = case_num\n",
    "            label_df['case_num'] = case_num\n",
    "            \n",
    "            input_fn.append(input_df)\n",
    "            label_fn.append(label_df)\n",
    "        \n",
    "        test_input_fn = []\n",
    "        test_label_fn = []\n",
    "        for test_input_path, test_label_path in zip(test_input_paths, test_label_paths):\n",
    "            case_num = test_input_path.replace('./data/test_input/TEST_','').replace('.csv','')\n",
    "            \n",
    "            test_input_df = pd.read_csv(test_input_path)\n",
    "            test_label_df = pd.read_csv(test_label_path)\n",
    "            \n",
    "            test_input_df['case_num'] = case_num\n",
    "            test_label_df['case_num'] = case_num\n",
    "            \n",
    "            test_input_fn.append(test_input_df)\n",
    "            test_label_fn.append(test_label_df)\n",
    "            \n",
    "        self.input = pd.concat(input_fn,axis=0).sort_values(['case_num','DAT','obs_time'])\n",
    "        self.label = pd.concat(label_fn,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_fn ,axis=0)\n",
    "        self.test_label  = pd.concat(test_label_fn ,axis=0)\n",
    "        \n",
    "        self.input     .obs_time = list(np.arange(0,24))*int(self.input     .shape[0]/24)\n",
    "        self.test_input.obs_time = list(np.arange(0,24))*int(self.test_input.shape[0]/24)\n",
    "        \n",
    "    def _data_return(self):\n",
    "        return self.input,self.label,self.test_input,self.test_label\n",
    "            \n",
    "    def _target_log(self):\n",
    "        self.label['predicted_weight_g'] = np.log(self.label['predicted_weight_g'])\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        # 1. time 추가 : 1~672 (24시간 x 28일)\n",
    "        self.input     ['time'] = [i+1 for i in range(28*24)]*self.input     .case_num.nunique()\n",
    "        self.test_input['time'] = [i+1 for i in range(28*24)]*self.test_input.case_num.nunique()\n",
    "\n",
    "        features = [\n",
    "            'DAT', 'obs_time', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'case_num', 'time'\n",
    "        ]\n",
    "        # del_features = [\n",
    "        #     '시간당분무량','시간당백색광량','시간당적색광량','시간당청색광량','시간당총광량',\n",
    "        #     # '일간누적분무량','일간누적백색광량','일간누적적색광량','일간누적청색광량','일간누적총광량'\n",
    "        # ]\n",
    "        # self.input     .drop(columns=del_features,inplace=True)\n",
    "        # self.test_input.drop(columns=del_features,inplace=True)\n",
    "        \n",
    "        # 2. 각 컬럼들의 파생변수\n",
    "        input_df       = []\n",
    "        test_input_df  = []\n",
    "        for case_num in self.input.case_num.unique():\n",
    "            i_df = self.input     [self.input     .case_num==case_num]\n",
    "            t_df = self.test_input[self.test_input.case_num==case_num]\n",
    "            \n",
    "            for col in list(set(self.input.columns)-set(['case_num','DAT','obs_time','time'])):\n",
    "                for i in range(4):\n",
    "                    # (1) 이전시간 값\n",
    "                    i_df[f'{col}_bf{i+1}'] = i_df[col].shift(i+1).fillna(0)\n",
    "                    t_df[f'{col}_bf{i+1}'] = t_df[col].shift(i+1).fillna(0)\n",
    "                \n",
    "                    # (2) 전시간대 대비 상승했는지 여부\n",
    "                    i_df[f'{col}_higher_than_{i+1}d'] = np.where(i_df[col]>i_df[col].shift(i+1),1,0)\n",
    "                    t_df[f'{col}_higher_than_{i+1}d'] = np.where(t_df[col]>t_df[col].shift(i+1),1,0)\n",
    "\n",
    "                    # (3) 전시간대 대비 상승률 -> 넣으면 NaN 발생\n",
    "                    if i_df[col].min()<=0:\n",
    "                        offset = i_df[col].min()\n",
    "                        i_df[col] = i_df[col] + offset\n",
    "                        t_df[col] = t_df[col] + offset\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'] = (i_df[col] - i_df[col].shift(i+1)) / i_df[col]\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'] = (t_df[col] - t_df[col].shift(i+1)) / t_df[col]\n",
    "\n",
    "                    # -inf -> min, inf -> max\n",
    "                    tmp = i_df[f'{col}_{i+1}d_rise_rate'].copy()\n",
    "                    tmp = tmp[(tmp!=-np.inf) & (tmp!=np.inf)]\n",
    "                    min_info, max_info = tmp.min(), tmp.max()\n",
    "\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'][i_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']==-np.inf] = min_info\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'][t_df[f'{col}_{i+1}d_rise_rate']== np.inf] = max_info\n",
    "                    \n",
    "                    # fill nan to zero\n",
    "                    i_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    t_df[f'{col}_{i+1}d_rise_rate'].fillna(0,inplace=True)\n",
    "                    \n",
    "                    # (4) moving average\n",
    "                    for size in [2,4,7]:\n",
    "                        i_df[f'{col}_ma{size}'] = moving_average(i_df[col],size=size)\n",
    "                        t_df[f'{col}_ma{size}'] = moving_average(t_df[col],size=size)\n",
    "                \n",
    "                # (5) cumulative sum\n",
    "                i_df[f'{col}_cumsum'] = i_df[col].cumsum()\n",
    "                t_df[f'{col}_cumsum'] = t_df[col].cumsum()\n",
    "\n",
    "            input_df     .append(i_df)\n",
    "            test_input_df.append(t_df)\n",
    "        \n",
    "        # concat\n",
    "        self.input       = pd.concat(input_df     ,axis=0)\n",
    "        self.test_input  = pd.concat(test_input_df,axis=0)\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "        #---------------------------------------------------------------------------\n",
    "        # agg\n",
    "        #---------------------------------------------------------------------------\n",
    "        self.input      = self.input     .drop(['obs_time'],axis=1)\n",
    "        self.test_input = self.test_input.drop(['obs_time'],axis=1)\n",
    "        \n",
    "        self.input      = self.input     .groupby(['case_num','DAT']).mean().reset_index()\n",
    "        self.test_input = self.test_input.groupby(['case_num','DAT']).mean().reset_index()\n",
    "        \n",
    "        self.input.DAT      = self.input     .DAT + 1\n",
    "        self.test_input.DAT = self.test_input.DAT + 1\n",
    "        \n",
    "        # 파생변수 생성 후, 모든 값이 동일하면 삭제\n",
    "        unique_info = self.input.apply(lambda x: x.nunique())\n",
    "        unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "        \n",
    "        # final dataset\n",
    "        self.input      = self.input     .drop(unique_cols,axis=1)\n",
    "        self.test_input = self.test_input.drop(unique_cols,axis=1)\n",
    "        \n",
    "    # https://dacon.io/competitions/official/236033/talkboard/407304?page=1&dtype=recent\n",
    "    def _scale_dataset(self,outlier):\n",
    "        \n",
    "        minmax_info = {\n",
    "            #'None':[0,0],\n",
    "            'DAT':[1,28],\n",
    "            'obs_time':[0,23],\n",
    "            'time':[0,28*24],\n",
    "            '내부온도관측치':[4,40],\n",
    "            '내부습도관측치':[0,100],\n",
    "            'co2관측치':[0,1200],\n",
    "            'ec관측치':[0,8],\n",
    "            '시간당분무량':[0,3000],\n",
    "            '일간누적분무량':[0,72000],\n",
    "            '시간당백색광량':[0,120000],\n",
    "            '일간누적백색광량':[0,2880000],\n",
    "            '시간당적색광량':[0,120000],\n",
    "            '일간누적적색광량':[0,2880000],\n",
    "            '시간당청색광량':[0,120000],\n",
    "            '일간누적청색광량':[0,2880000],\n",
    "            '시간당총광량':[0,120000],\n",
    "            '일간누적총광량':[0,2880000],\n",
    "        }\n",
    "            \n",
    "        scale_feature = [feature for feature,(min_info,max_info) in minmax_info.items() if feature in self.input.columns]\n",
    "        \n",
    "        # for train dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.input[col] = (self.input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.input[col][self.input[col]<0] = 0\n",
    "                self.input[col][self.input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.input[col][(self.input[col]<0) | (self.input[col]>1)] = np.nan\n",
    "            \n",
    "        # for test dataset\n",
    "        for col in scale_feature:\n",
    "            min_info,max_info = minmax_info[col]\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "            \n",
    "            if outlier=='keep':\n",
    "                # 0~1을 벗어나는 값 (minmax_info의 범위를 벗어나는 값)은 0,1로 넣기\n",
    "                # -> 삭제하게되면 24시간의 term이 깨짐\n",
    "                self.test_input[col][self.test_input[col]<0] = 0\n",
    "                self.test_input[col][self.test_input[col]>1] = 1\n",
    "            elif outlier=='drop':\n",
    "                self.test_input[col][(self.test_input[col]<0) | (self.test_input[col]>1)] = np.nan\n",
    "        \n",
    "        # another features\n",
    "        another_features = list(set(self.input.select_dtypes(exclude=[object]).columns)-set(scale_feature))\n",
    "        for col in another_features:\n",
    "            if self.input[col].min()==0:\n",
    "                offset=1e-4\n",
    "                self.input[col]      = self.input     [col]+offset\n",
    "                self.test_input[col] = self.test_input[col]+offset\n",
    "            \n",
    "            min_info,max_info = self.input[col].min(),self.input[col].max()    \n",
    "            self.input[col]      = (self.input[col]     -min_info) / (max_info-min_info)\n",
    "            self.test_input[col] = (self.test_input[col]-min_info) / (max_info-min_info)\n",
    "        \n",
    "    def _interaction_term(self):\n",
    "        # num_features = self.input.select_dtypes(exclude=[object]).columns\n",
    "        num_features = [\n",
    "            'DAT','time',\n",
    "            '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', \n",
    "            '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', \n",
    "            '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량',\n",
    "        ]\n",
    "        for i in range(len(num_features)):\n",
    "            for j in range(len(num_features)):\n",
    "                if i>j:\n",
    "                    self.input     [f'{num_features[i]}*{num_features[j]}'] = self.input     [num_features[i]]*self.input     [num_features[j]]\n",
    "                    self.test_input[f'{num_features[i]}*{num_features[j]}'] = self.test_input[num_features[i]]*self.test_input[num_features[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46afb772-6e30-42b5-b09b-f0cdc584fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, color):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf407c58-fe03-456d-9bad-ec0ff6b18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# val_rate = 0.05\n",
    "\n",
    "# dataset = Preprocess(\n",
    "#     input_paths = all_input_list,\n",
    "#     label_paths = all_target_list,\n",
    "#     test_paths = all_test_list,\n",
    "# )\n",
    "\n",
    "# dataset._preprocess()\n",
    "# dataset._scale_dataset()\n",
    "# input_df, label_df = dataset._data_return()\n",
    "\n",
    "# for case_num in tqdm(sorted(input_df.case_num.unique())):\n",
    "\n",
    "#     input = input_df[input_df.case_num==case_num].drop('case_num',axis=1)\n",
    "#     label = label_df[label_df.case_num==case_num].drop('case_num',axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize=(20,15))\n",
    "#     nrow = 3\n",
    "#     ncol = 5\n",
    "\n",
    "#     iter = 0\n",
    "#     total = len(input.columns)-3\n",
    "#     for col in input.columns:\n",
    "#         if col not in ['time','DAT','obs_time']:\n",
    "#             iter+=1\n",
    "\n",
    "#             y1 = input[col]\n",
    "#             #y1 = (y1-y1.min())/(y1.max()-y1.min())\n",
    "\n",
    "#             y2 = label['predicted_weight_g']\n",
    "#             y2 = (y2-y2.min())/(y2.max()-y2.min())\n",
    "\n",
    "#             y3 = input.groupby('DAT')[col].mean().values\n",
    "\n",
    "#             corr, pvalue = pearsonr(y2,y3)\n",
    "\n",
    "#             fig.add_subplot(ncol,nrow,iter)\n",
    "#             sns.scatterplot(x=input.time  ,y=y1)\n",
    "#             sns.scatterplot(x=label.DAT*24,y=y2,color='red')\n",
    "#             sns.lineplot   (x=label.DAT*24,y=y3,color='blue',linestyle='--',alpha=0.7)\n",
    "#             plt.ylabel('')\n",
    "\n",
    "#             plt.title(f'{col}(corr={corr:.3f}(pvalue={pvalue:.3f}))',fontproperties=fp)\n",
    "\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'./fig/{case_num}.png',dpi=100)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731d783-38cb-43e3-8f4f-1f9c6d187cdc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d57fd0-ea32-4fb4-a626-1006236b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_label_list = sorted(glob.glob('./data/train_target/*.csv'))\n",
    "all_test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "all_test_label_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d65ad91-4f38-4423-99f9-c1e8089c2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.2 s, sys: 138 ms, total: 40.3 s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess Class\n",
    "dataset = Preprocess(\n",
    "    input_paths = all_input_list,\n",
    "    label_paths = all_label_list,\n",
    "    test_input_paths = all_test_input_list,\n",
    "    test_label_paths = all_test_label_list,\n",
    ")\n",
    "\n",
    "# (1) preprocessing + scaling + interaction term\n",
    "dataset._preprocess()\n",
    "# dataset._target_log()\n",
    "dataset._scale_dataset(outlier='keep')\n",
    "# dataset._interaction_term()\n",
    "\n",
    "# (2) Data Return for check\n",
    "input_df, label_df, test_input_df, test_label_df = dataset._data_return()\n",
    "\n",
    "# # (3) Delete Std zero features\n",
    "# std_zero_features = []\n",
    "# for case_num in input_df.case_num.unique():\n",
    "#     tmp = input_df[input_df.case_num==case_num]\n",
    "#     std_zero_feature = tmp.std().index[tmp.std()==0].tolist()\n",
    "#     std_zero_features += std_zero_feature\n",
    "    \n",
    "# std_zero_features = pd.unique(std_zero_features)\n",
    "\n",
    "# input_df = input_df.drop(std_zero_features,axis=1)\n",
    "\n",
    "# # (4) Select Columns\n",
    "# input_df = input_df.drop(columns=['obs_time'])\n",
    "# label_df = label_df['predicted_weight_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec50457-eded-4068-a3ef-8487af93b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_info = input_df.isnull().sum()\n",
    "null_info[null_info!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "521202eb-ce6f-4e3c-a229-ffb2f5bf7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = input_df.copy()\n",
    "\n",
    "# cols = [col for col in d.columns if col.find('_higher_than_')>=0]\n",
    "# std_zero_cols = d[cols].std()==0\n",
    "# del_cols = std_zero_cols[std_zero_cols].index.tolist()\n",
    "# print(del_cols)\n",
    "\n",
    "# # for i in range(len(cols)):\n",
    "# #     print(f'({i}/{len(cols)}) {cols[i]}')\n",
    "# #     plt.figure(figsize=(8,7))\n",
    "# #     sns.lineplot(x=d.time,y=d[cols[i]])\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8979bc0a-1589-44b6-9cc6-6733be3e1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 241)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_df.isnull().sum()\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f93b40f-3e0c-4f7e-b84c-8c0958395769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_num', 'DAT', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'time', 'co2관측치_bf1', 'co2관측치_higher_than_1d', 'co2관측치_1d_rise_rate', 'co2관측치_ma2', 'co2관측치_ma4', 'co2관측치_ma7', 'co2관측치_bf2', 'co2관측치_higher_than_2d', 'co2관측치_2d_rise_rate', 'co2관측치_bf3', 'co2관측치_higher_than_3d', 'co2관측치_3d_rise_rate', 'co2관측치_bf4', 'co2관측치_higher_than_4d', 'co2관측치_4d_rise_rate', 'co2관측치_cumsum', '시간당적색광량_bf1', '시간당적색광량_higher_than_1d', '시간당적색광량_1d_rise_rate', '시간당적색광량_ma2', '시간당적색광량_ma4', '시간당적색광량_ma7', '시간당적색광량_bf2', '시간당적색광량_higher_than_2d', '시간당적색광량_2d_rise_rate', '시간당적색광량_bf3', '시간당적색광량_higher_than_3d', '시간당적색광량_3d_rise_rate', '시간당적색광량_bf4', '시간당적색광량_higher_than_4d', '시간당적색광량_4d_rise_rate', '시간당적색광량_cumsum', '시간당총광량_bf1', '시간당총광량_higher_than_1d', '시간당총광량_1d_rise_rate', '시간당총광량_ma2', '시간당총광량_ma4', '시간당총광량_ma7', '시간당총광량_bf2', '시간당총광량_higher_than_2d', '시간당총광량_2d_rise_rate', '시간당총광량_bf3', '시간당총광량_higher_than_3d', '시간당총광량_3d_rise_rate', '시간당총광량_bf4', '시간당총광량_higher_than_4d', '시간당총광량_4d_rise_rate', '시간당총광량_cumsum', '시간당청색광량_bf1', '시간당청색광량_higher_than_1d', '시간당청색광량_1d_rise_rate', '시간당청색광량_ma2', '시간당청색광량_ma4', '시간당청색광량_ma7', '시간당청색광량_bf2', '시간당청색광량_higher_than_2d', '시간당청색광량_2d_rise_rate', '시간당청색광량_bf3', '시간당청색광량_higher_than_3d', '시간당청색광량_3d_rise_rate', '시간당청색광량_bf4', '시간당청색광량_higher_than_4d', '시간당청색광량_4d_rise_rate', '시간당청색광량_cumsum', '일간누적총광량_bf1', '일간누적총광량_higher_than_1d', '일간누적총광량_1d_rise_rate', '일간누적총광량_ma2', '일간누적총광량_ma4', '일간누적총광량_ma7', '일간누적총광량_bf2', '일간누적총광량_higher_than_2d', '일간누적총광량_2d_rise_rate', '일간누적총광량_bf3', '일간누적총광량_higher_than_3d', '일간누적총광량_3d_rise_rate', '일간누적총광량_bf4', '일간누적총광량_higher_than_4d', '일간누적총광량_4d_rise_rate', '일간누적총광량_cumsum', '내부습도관측치_bf1', '내부습도관측치_higher_than_1d', '내부습도관측치_1d_rise_rate', '내부습도관측치_ma2', '내부습도관측치_ma4', '내부습도관측치_ma7', '내부습도관측치_bf2', '내부습도관측치_higher_than_2d', '내부습도관측치_2d_rise_rate', '내부습도관측치_bf3', '내부습도관측치_higher_than_3d', '내부습도관측치_3d_rise_rate', '내부습도관측치_bf4', '내부습도관측치_higher_than_4d', '내부습도관측치_4d_rise_rate', '내부습도관측치_cumsum', '일간누적청색광량_bf1', '일간누적청색광량_higher_than_1d', '일간누적청색광량_1d_rise_rate', '일간누적청색광량_ma2', '일간누적청색광량_ma4', '일간누적청색광량_ma7', '일간누적청색광량_bf2', '일간누적청색광량_higher_than_2d', '일간누적청색광량_2d_rise_rate', '일간누적청색광량_bf3', '일간누적청색광량_higher_than_3d', '일간누적청색광량_3d_rise_rate', '일간누적청색광량_bf4', '일간누적청색광량_higher_than_4d', '일간누적청색광량_4d_rise_rate', '일간누적청색광량_cumsum', '내부온도관측치_bf1', '내부온도관측치_higher_than_1d', '내부온도관측치_1d_rise_rate', '내부온도관측치_ma2', '내부온도관측치_ma4', '내부온도관측치_ma7', '내부온도관측치_bf2', '내부온도관측치_higher_than_2d', '내부온도관측치_2d_rise_rate', '내부온도관측치_bf3', '내부온도관측치_higher_than_3d', '내부온도관측치_3d_rise_rate', '내부온도관측치_bf4', '내부온도관측치_higher_than_4d', '내부온도관측치_4d_rise_rate', '내부온도관측치_cumsum', '시간당백색광량_bf1', '시간당백색광량_higher_than_1d', '시간당백색광량_1d_rise_rate', '시간당백색광량_ma2', '시간당백색광량_ma4', '시간당백색광량_ma7', '시간당백색광량_bf2', '시간당백색광량_higher_than_2d', '시간당백색광량_2d_rise_rate', '시간당백색광량_bf3', '시간당백색광량_higher_than_3d', '시간당백색광량_3d_rise_rate', '시간당백색광량_bf4', '시간당백색광량_higher_than_4d', '시간당백색광량_4d_rise_rate', '시간당백색광량_cumsum', '일간누적백색광량_bf1', '일간누적백색광량_higher_than_1d', '일간누적백색광량_1d_rise_rate', '일간누적백색광량_ma2', '일간누적백색광량_ma4', '일간누적백색광량_ma7', '일간누적백색광량_bf2', '일간누적백색광량_higher_than_2d', '일간누적백색광량_2d_rise_rate', '일간누적백색광량_bf3', '일간누적백색광량_higher_than_3d', '일간누적백색광량_3d_rise_rate', '일간누적백색광량_bf4', '일간누적백색광량_higher_than_4d', '일간누적백색광량_4d_rise_rate', '일간누적백색광량_cumsum', '시간당분무량_bf1', '시간당분무량_higher_than_1d', '시간당분무량_1d_rise_rate', '시간당분무량_ma2', '시간당분무량_ma4', '시간당분무량_ma7', '시간당분무량_bf2', '시간당분무량_higher_than_2d', '시간당분무량_2d_rise_rate', '시간당분무량_bf3', '시간당분무량_higher_than_3d', '시간당분무량_3d_rise_rate', '시간당분무량_bf4', '시간당분무량_higher_than_4d', '시간당분무량_4d_rise_rate', '시간당분무량_cumsum', '일간누적분무량_bf1', '일간누적분무량_higher_than_1d', '일간누적분무량_1d_rise_rate', '일간누적분무량_ma2', '일간누적분무량_ma4', '일간누적분무량_ma7', '일간누적분무량_bf2', '일간누적분무량_higher_than_2d', '일간누적분무량_2d_rise_rate', '일간누적분무량_bf3', '일간누적분무량_higher_than_3d', '일간누적분무량_3d_rise_rate', '일간누적분무량_bf4', '일간누적분무량_higher_than_4d', '일간누적분무량_4d_rise_rate', '일간누적분무량_cumsum', 'ec관측치_bf1', 'ec관측치_higher_than_1d', 'ec관측치_1d_rise_rate', 'ec관측치_ma2', 'ec관측치_ma4', 'ec관측치_ma7', 'ec관측치_bf2', 'ec관측치_higher_than_2d', 'ec관측치_2d_rise_rate', 'ec관측치_bf3', 'ec관측치_higher_than_3d', 'ec관측치_3d_rise_rate', 'ec관측치_bf4', 'ec관측치_higher_than_4d', 'ec관측치_4d_rise_rate', 'ec관측치_cumsum', '일간누적적색광량_bf1', '일간누적적색광량_higher_than_1d', '일간누적적색광량_1d_rise_rate', '일간누적적색광량_ma2', '일간누적적색광량_ma4', '일간누적적색광량_ma7', '일간누적적색광량_bf2', '일간누적적색광량_higher_than_2d', '일간누적적색광량_2d_rise_rate', '일간누적적색광량_bf3', '일간누적적색광량_higher_than_3d', '일간누적적색광량_3d_rise_rate', '일간누적적색광량_bf4', '일간누적적색광량_higher_than_4d', '일간누적적색광량_4d_rise_rate', '일간누적적색광량_cumsum']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in input_df.columns if col.find('*')<0])\n",
    "# print(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d3f562-5196-4fc2-a3bb-7c45455a14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_df = []\n",
    "for col in input_df.drop(columns=['case_num']).columns:\n",
    "    r_value, p_value = pearsonr(input_df[col],label_df['predicted_weight_g'])\n",
    "    corr_df.append([col,r_value,p_value])\n",
    "    \n",
    "corr_df = pd.DataFrame(corr_df,columns=['feature','r_value','p_value'])\n",
    "# corr_df.sort_values('p_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dccd58fa-b0bf-4263-8494-7b8c94a34a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['co2관측치_higher_than_1d' 'co2관측치_higher_than_2d' 'co2관측치_higher_than_3d'\n",
      " 'co2관측치_higher_than_4d' 'co2관측치_4d_rise_rate' '시간당적색광량_higher_than_1d'\n",
      " '시간당적색광량_higher_than_2d' '시간당적색광량_higher_than_3d'\n",
      " '시간당적색광량_higher_than_4d' '시간당총광량_bf2' '시간당총광량_cumsum'\n",
      " '시간당청색광량_1d_rise_rate' '시간당청색광량_cumsum' '일간누적총광량_1d_rise_rate'\n",
      " '일간누적총광량_2d_rise_rate' '일간누적총광량_3d_rise_rate' '일간누적총광량_4d_rise_rate'\n",
      " '내부습도관측치_1d_rise_rate' '내부습도관측치_higher_than_2d' '내부습도관측치_2d_rise_rate'\n",
      " '내부습도관측치_higher_than_3d' '내부습도관측치_3d_rise_rate' '내부습도관측치_higher_than_4d'\n",
      " '내부습도관측치_4d_rise_rate' '내부온도관측치_higher_than_1d' '내부온도관측치_1d_rise_rate'\n",
      " '내부온도관측치_higher_than_2d' '내부온도관측치_2d_rise_rate' '내부온도관측치_higher_than_3d'\n",
      " '내부온도관측치_3d_rise_rate' '내부온도관측치_higher_than_4d' '내부온도관측치_4d_rise_rate'\n",
      " '시간당백색광량_bf2' '시간당백색광량_bf3' '시간당백색광량_cumsum' '일간누적백색광량_1d_rise_rate'\n",
      " '일간누적백색광량_2d_rise_rate' '일간누적백색광량_3d_rise_rate' '일간누적백색광량_4d_rise_rate'\n",
      " '시간당분무량_1d_rise_rate' '시간당분무량_2d_rise_rate' '시간당분무량_3d_rise_rate'\n",
      " '시간당분무량_4d_rise_rate' '일간누적분무량_1d_rise_rate' '일간누적분무량_2d_rise_rate'\n",
      " '일간누적분무량_3d_rise_rate' '일간누적분무량_4d_rise_rate' 'ec관측치_higher_than_1d'\n",
      " 'ec관측치_1d_rise_rate' 'ec관측치_higher_than_2d' 'ec관측치_2d_rise_rate'\n",
      " 'ec관측치_higher_than_3d' 'ec관측치_3d_rise_rate' 'ec관측치_higher_than_4d'\n",
      " 'ec관측치_4d_rise_rate']\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "del_feature = corr_df.feature[corr_df.p_value>alpha].tolist()\n",
    "len(del_feature)\n",
    "print(np.array(del_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "290fe567-1d52-4079-bb25-9016f212a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df     .drop(columns=del_feature,inplace=True)\n",
    "test_input_df.drop(columns=del_feature,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a2304db-0c27-4e98-b2b0-941e29a8d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asis(241) -> tobe(186)\n"
     ]
    }
   ],
   "source": [
    "print(f'asis({input_df.shape[1]+len(del_feature)}) -> tobe({input_df.shape[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dff77-20cc-4880-9341-8835231ba4b5",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec8035-20fe-4240-8016-9395b0ba39f3",
   "metadata": {},
   "source": [
    "# Pre-Fit Catboost\n",
    "- 아래 모델링과 비슷하게 case_num으로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff29d91e-845a-4840-9bc1-bde8e2f01499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '3'\n",
    "\n",
    "paths = [f'./out/kf_cat_{save_mark}',f'./out/kf_cat_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cec99ac0-2a49-4260-8560-bc574f0eb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 186), (784, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.shape, label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2f85fd8-e663-4efb-a991-831442b86ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1시간 (cpu)\n",
    "\n",
    "# from catboost import CatBoostRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# n_splits = 10\n",
    "\n",
    "# case_num = input_df.case_num.unique()\n",
    "# kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "# kf_iter = 0\n",
    "# for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "#     kf_iter+=1\n",
    "#     print(f'-'*100)\n",
    "#     print(f'({kf_iter}/{n_splits})')\n",
    "#     print(f'-'*100)\n",
    "    \n",
    "#     #------------------------------------------------------------------------------------\n",
    "#     # (1) train validation split\n",
    "#     #------------------------------------------------------------------------------------\n",
    "#     tr_case_num = case_num[tr_idx]\n",
    "#     va_case_num = case_num[va_idx]\n",
    "    \n",
    "#     X_train = input_df[input_df.case_num.isin(tr_case_num)].drop(columns=['case_num'])\n",
    "#     X_valid = input_df[input_df.case_num.isin(va_case_num)].drop(columns=['case_num'])\n",
    "\n",
    "#     y_train = label_df[label_df.case_num.isin(tr_case_num)]['predicted_weight_g']\n",
    "#     y_valid = label_df[label_df.case_num.isin(va_case_num)]['predicted_weight_g']\n",
    "#     # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "#     model = CatBoostRegressor(iterations=5000,metric_period=1000,random_state=42)\n",
    "#     model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "#     tr_pred = model.predict(X_train)\n",
    "#     va_pred = model.predict(X_valid)\n",
    "    \n",
    "#     pred_input_df[f'pred_{kf_iter}'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "#     pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "    \n",
    "#     pred_test_df[f'pred_{kf_iter}'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "#     pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81ac7a-ad5d-4470-9bd8-d58fa38aa5ab",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## kfold-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b44a7-0720-48d3-b51f-9cb6580f9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a66d8ec9-e478-4d9d-bc71-130018b818fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(input_df),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'')\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    X_train = input_df.iloc[tr_idx,:].drop(columns=['case_num'])\n",
    "    X_valid = input_df.iloc[va_idx,:].drop(columns=['case_num'])\n",
    "\n",
    "    y_train = label_df.iloc[tr_idx]['predicted_weight_g']\n",
    "    y_valid = label_df.iloc[va_idx]['predicted_weight_g']\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=40000,metric_period=2000,random_state=42,early_stopping_rounds=4000)\n",
    "    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "    tr_pred = model.predict(X_train)\n",
    "    va_pred = model.predict(X_valid)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    fig.add_subplot(121)\n",
    "    sns.scatterplot(x=tr_pred,y=y_train)\n",
    "    abline(slope=1,intercept=0,color='red')\n",
    "    plt.title(f'Train : RMSE={np.sqrt(mean_squared_error(tr_pred,y_train)):.4f}')\n",
    "    fig.add_subplot(122)\n",
    "    sns.scatterplot(x=va_pred,y=y_valid)\n",
    "    abline(slope=1,intercept=0,color='red')\n",
    "    plt.title(f'Validation : RMSE={np.sqrt(mean_squared_error(va_pred,y_valid)):.4f}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    pred_input_df[f'pred_{kf_iter}'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "    pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "    \n",
    "    pred_test_df[f'pred_{kf_iter}'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "    pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d01be-f200-4a62-a92b-8889fc2a10f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8c3569a7-ba76-466a-b88f-cca7db19428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa24303b-66d3-45b9-9e22-a523cae8dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,X_train,X_valid,y_train,y_valid,verbose):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "\n",
    "        'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "        'od_wait': trial.suggest_int('od_wait', 500, 2300),\n",
    "\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "        'random_strength': trial.suggest_uniform('random_strength',0,100),\n",
    "        'depth': trial.suggest_int('depth',1, 6),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,100),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
    "\n",
    "        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.0, 1.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 10, 300),\n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    }\n",
    "    if verbose:\n",
    "        print('> Hyper-Parameters')\n",
    "        for key,value in params.items():\n",
    "            print(f'  - {key}: {value}')\n",
    "        print('')\n",
    "    \n",
    "    if verbose:\n",
    "        metric_period = 5000\n",
    "    else:\n",
    "        metric_period = None\n",
    "\n",
    "    train_dataset = Pool(data=X_train,label=y_train)\n",
    "    valid_dataset = Pool(data=X_valid,label=y_valid)\n",
    "    \n",
    "    try:\n",
    "        model = CatBoostRegressor(\n",
    "            random_state=0,\n",
    "            loss_function='RMSE',\n",
    "            metric_period=metric_period,\n",
    "            iterations=50000,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            eval_set=valid_dataset,\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=1000,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    \n",
    "        va_pred = model.predict(valid_dataset)\n",
    "        va_true = y_valid\n",
    "\n",
    "        score = mean_squared_error(y_true=va_true,y_pred=va_pred)\n",
    "        score = np.sqrt(score)\n",
    "    except:\n",
    "        score = 99999\n",
    "    \n",
    "    text = f'Cat : score={score:.3f}'\n",
    "    pbar_cat.set_description(text)\n",
    "    pbar_cat.update(1)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ba772629-3e10-4f20-8293-69260ef42cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-14 05:18:02,094]\u001b[0m A new study created in memory with name: CatBoost\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3937fdaebbdd408d857fde3706891c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-14 05:19:55,152]\u001b[0m Trial 0 finished with value: 7.836777683932372 and parameters: {'learning_rate': 0.0012520653814999472, 'subsample': 0.7151893663724195, 'od_wait': 1585, 'reg_lambda': 54.488322850857855, 'random_strength': 42.36547993389047, 'depth': 4, 'min_data_in_leaf': 44, 'leaf_estimation_iterations': 14, 'bagging_temperature': 71.55682161754872, 'colsample_bylevel': 0.3834415188257777, 'max_bin': 240, 'od_type': 'Iter'}. Best is trial 0 with value: 7.836777683932372.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:24:51,879]\u001b[0m Trial 1 finished with value: 8.661178289677688 and parameters: {'learning_rate': 0.007098936257405904, 'subsample': 0.07103605819788694, 'od_wait': 656, 'reg_lambda': 2.0218495418485976, 'random_strength': 83.2619845547938, 'depth': 5, 'min_data_in_leaf': 88, 'leaf_estimation_iterations': 15, 'bagging_temperature': 15.726578854179516, 'colsample_bylevel': 0.46147936225293185, 'max_bin': 237, 'od_type': 'Iter'}. Best is trial 0 with value: 7.836777683932372.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:28:17,206]\u001b[0m Trial 2 finished with value: 11.356436061622391 and parameters: {'learning_rate': 0.00019351140885595286, 'subsample': 0.9446689170495839, 'od_wait': 1439, 'reg_lambda': 41.46619985243296, 'random_strength': 26.455561210462697, 'depth': 5, 'min_data_in_leaf': 46, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.011889379831773014, 'colsample_bylevel': 0.6176354970758771, 'max_bin': 188, 'od_type': 'Iter'}. Best is trial 0 with value: 7.836777683932372.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:30:02,293]\u001b[0m Trial 3 finished with value: 7.626258035958385 and parameters: {'learning_rate': 0.0023101522250182378, 'subsample': 0.359507900573786, 'od_wait': 1287, 'reg_lambda': 69.76312261641452, 'random_strength': 6.022547162926983, 'depth': 5, 'min_data_in_leaf': 68, 'leaf_estimation_iterations': 4, 'bagging_temperature': 0.03278726498335277, 'colsample_bylevel': 0.31542835092418386, 'max_bin': 115, 'od_type': 'IncToDec'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:30:28,906]\u001b[0m Trial 4 finished with value: 8.363236267713969 and parameters: {'learning_rate': 0.009478675948634677, 'subsample': 0.10204481074802807, 'od_wait': 876, 'reg_lambda': 16.130960175404446, 'random_strength': 65.31083254653984, 'depth': 2, 'min_data_in_leaf': 47, 'leaf_estimation_iterations': 4, 'bagging_temperature': 0.04323926814280048, 'colsample_bylevel': 0.11037514116430513, 'max_bin': 200, 'od_type': 'Iter'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:32:17,059]\u001b[0m Trial 5 finished with value: 8.07143771693786 and parameters: {'learning_rate': 0.0005463240777949559, 'subsample': 0.8209932298479351, 'od_wait': 674, 'reg_lambda': 83.79449237043131, 'random_strength': 9.609840789396307, 'depth': 6, 'min_data_in_leaf': 47, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.6265282663264307, 'colsample_bylevel': 0.7392635793983017, 'max_bin': 21, 'od_type': 'IncToDec'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:33:19,921]\u001b[0m Trial 6 finished with value: 9.190189722722906 and parameters: {'learning_rate': 0.0003910933172251061, 'subsample': 0.11872771895424405, 'od_wait': 1072, 'reg_lambda': 41.42630530883705, 'random_strength': 6.4147496348784365, 'depth': 5, 'min_data_in_leaf': 57, 'leaf_estimation_iterations': 4, 'bagging_temperature': 1.2387743772940758, 'colsample_bylevel': 0.09394051075844168, 'max_bin': 177, 'od_type': 'IncToDec'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:34:44,955]\u001b[0m Trial 7 finished with value: 7.636818083430067 and parameters: {'learning_rate': 0.0021618261203953776, 'subsample': 0.13179786240439217, 'od_wait': 1790, 'reg_lambda': 28.94061640065918, 'random_strength': 18.319136200711682, 'depth': 4, 'min_data_in_leaf': 3, 'leaf_estimation_iterations': 13, 'bagging_temperature': 0.010441957103804479, 'colsample_bylevel': 0.6778165367962301, 'max_bin': 88, 'od_type': 'Iter'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:41:24,841]\u001b[0m Trial 8 finished with value: 9.196091662018585 and parameters: {'learning_rate': 0.0003144171936359527, 'subsample': 0.5761573344178369, 'od_wait': 1566, 'reg_lambda': 57.22519485656828, 'random_strength': 22.308163264061832, 'depth': 6, 'min_data_in_leaf': 45, 'leaf_estimation_iterations': 13, 'bagging_temperature': 6.279384856082843, 'colsample_bylevel': 0.29743695085513366, 'max_bin': 246, 'od_type': 'Iter'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:42:09,894]\u001b[0m Trial 9 finished with value: 9.105702354349917 and parameters: {'learning_rate': 0.0014539375242431556, 'subsample': 0.8817353618548528, 'od_wait': 1747, 'reg_lambda': 72.52543072942126, 'random_strength': 50.13243819267023, 'depth': 6, 'min_data_in_leaf': 65, 'leaf_estimation_iterations': 7, 'bagging_temperature': 2.664237007197921, 'colsample_bylevel': 0.019193198309333526, 'max_bin': 97, 'od_type': 'IncToDec'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:42:36,712]\u001b[0m Trial 10 finished with value: 10.406314234120938 and parameters: {'learning_rate': 0.0034093996938792622, 'subsample': 0.34711919200181157, 'od_wait': 2215, 'reg_lambda': 92.96403620637943, 'random_strength': 94.92801439405218, 'depth': 1, 'min_data_in_leaf': 86, 'leaf_estimation_iterations': 2, 'bagging_temperature': 0.10448175709130482, 'colsample_bylevel': 0.9939192176398295, 'max_bin': 113, 'od_type': 'IncToDec'}. Best is trial 3 with value: 7.626258035958385.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:43:44,962]\u001b[0m Trial 11 finished with value: 7.097549876282567 and parameters: {'learning_rate': 0.0027314750570472035, 'subsample': 0.33744973571859677, 'od_wait': 2003, 'reg_lambda': 25.71439768451378, 'random_strength': 2.61783077144006, 'depth': 3, 'min_data_in_leaf': 7, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.20723312332315036, 'colsample_bylevel': 0.773669765475423, 'max_bin': 64, 'od_type': 'IncToDec'}. Best is trial 11 with value: 7.097549876282567.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:44:39,650]\u001b[0m Trial 12 finished with value: 6.82334922894401 and parameters: {'learning_rate': 0.003832622042155371, 'subsample': 0.36292672548257277, 'od_wait': 2267, 'reg_lambda': 69.51018764739234, 'random_strength': 1.028534640055302, 'depth': 3, 'min_data_in_leaf': 2, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.21893053909412485, 'colsample_bylevel': 0.8856926987679451, 'max_bin': 23, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:45:32,994]\u001b[0m Trial 13 finished with value: 7.158938722178178 and parameters: {'learning_rate': 0.004410230238424504, 'subsample': 0.35699383127660195, 'od_wait': 2251, 'reg_lambda': 20.946163998649638, 'random_strength': 0.37042613946616587, 'depth': 3, 'min_data_in_leaf': 1, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.22287359899544593, 'colsample_bylevel': 0.8986767172301237, 'max_bin': 11, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:46:35,614]\u001b[0m Trial 14 finished with value: 7.438726437339197 and parameters: {'learning_rate': 0.00458290825771536, 'subsample': 0.5130348517746661, 'od_wait': 1962, 'reg_lambda': 69.07515409009783, 'random_strength': 37.345461639933944, 'depth': 3, 'min_data_in_leaf': 20, 'leaf_estimation_iterations': 11, 'bagging_temperature': 0.26541331855050904, 'colsample_bylevel': 0.798517941492613, 'max_bin': 49, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:47:20,480]\u001b[0m Trial 15 finished with value: 9.078265632629066 and parameters: {'learning_rate': 0.0007287844924105716, 'subsample': 0.2459375376017481, 'od_wait': 2020, 'reg_lambda': 35.61442295405469, 'random_strength': 59.18860775340747, 'depth': 2, 'min_data_in_leaf': 22, 'leaf_estimation_iterations': 7, 'bagging_temperature': 0.5103071122956205, 'colsample_bylevel': 0.8823676143440017, 'max_bin': 55, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:48:26,426]\u001b[0m Trial 16 finished with value: 8.131658390143453 and parameters: {'learning_rate': 0.0025200161718059743, 'subsample': 0.6314896420956512, 'od_wait': 2066, 'reg_lambda': 0.8898213598230456, 'random_strength': 30.548276485411574, 'depth': 2, 'min_data_in_leaf': 19, 'leaf_estimation_iterations': 11, 'bagging_temperature': 0.08754089012144228, 'colsample_bylevel': 0.5627916098699888, 'max_bin': 298, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:49:28,643]\u001b[0m Trial 17 finished with value: 16.86069830732082 and parameters: {'learning_rate': 0.00012134907084577087, 'subsample': 0.4457575131521882, 'od_wait': 1813, 'reg_lambda': 97.74599744018232, 'random_strength': 15.385133403967133, 'depth': 3, 'min_data_in_leaf': 30, 'leaf_estimation_iterations': 8, 'bagging_temperature': 0.6540851058877009, 'colsample_bylevel': 0.9762081471016015, 'max_bin': 54, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:49:59,286]\u001b[0m Trial 18 finished with value: 10.107562019187375 and parameters: {'learning_rate': 0.006314633409328834, 'subsample': 0.23585856118000653, 'od_wait': 2214, 'reg_lambda': 16.699687519480136, 'random_strength': 75.43740751932292, 'depth': 1, 'min_data_in_leaf': 10, 'leaf_estimation_iterations': 6, 'bagging_temperature': 0.14689631956860252, 'colsample_bylevel': 0.8081771269483196, 'max_bin': 146, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:51:12,239]\u001b[0m Trial 19 finished with value: 7.1970389885078445 and parameters: {'learning_rate': 0.0008853517464211583, 'subsample': 0.23253458569318683, 'od_wait': 2001, 'reg_lambda': 60.12021159126487, 'random_strength': 0.41769876587095733, 'depth': 4, 'min_data_in_leaf': 32, 'leaf_estimation_iterations': 11, 'bagging_temperature': 0.04349360226178667, 'colsample_bylevel': 0.6948969669815148, 'max_bin': 37, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:51:12,262]\u001b[0m Trial 20 finished with value: 99999.0 and parameters: {'learning_rate': 0.0016265805027174747, 'subsample': 0.0006194502144411529, 'od_wait': 2134, 'reg_lambda': 80.24199516117717, 'random_strength': 38.65180393984467, 'depth': 2, 'min_data_in_leaf': 13, 'leaf_estimation_iterations': 9, 'bagging_temperature': 1.3462022342215036, 'colsample_bylevel': 0.8708499316539327, 'max_bin': 77, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:52:06,750]\u001b[0m Trial 21 finished with value: 7.374293118799213 and parameters: {'learning_rate': 0.004098234364820513, 'subsample': 0.4012437827957832, 'od_wait': 2286, 'reg_lambda': 24.249417787177965, 'random_strength': 11.767438088040905, 'depth': 3, 'min_data_in_leaf': 2, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.30354822876846604, 'colsample_bylevel': 0.9206247745763613, 'max_bin': 10, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:53:01,502]\u001b[0m Trial 22 finished with value: 7.176850102904699 and parameters: {'learning_rate': 0.005358163835342766, 'subsample': 0.3224166604943325, 'od_wait': 1850, 'reg_lambda': 12.446481770146967, 'random_strength': 0.33736841615891106, 'depth': 3, 'min_data_in_leaf': 9, 'leaf_estimation_iterations': 12, 'bagging_temperature': 0.22032308073236193, 'colsample_bylevel': 0.8064824911065065, 'max_bin': 12, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:54:05,491]\u001b[0m Trial 23 finished with value: 7.166442016057433 and parameters: {'learning_rate': 0.003218817785110533, 'subsample': 0.4792791164771591, 'od_wait': 2280, 'reg_lambda': 45.96389487622631, 'random_strength': 0.3240487650170144, 'depth': 3, 'min_data_in_leaf': 30, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.4139188392544548, 'colsample_bylevel': 0.9140067322424371, 'max_bin': 72, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:55:08,897]\u001b[0m Trial 24 finished with value: 7.7355239320331926 and parameters: {'learning_rate': 0.008362491918688604, 'subsample': 0.2793303854181376, 'od_wait': 1932, 'reg_lambda': 27.631634572125222, 'random_strength': 19.96555103590541, 'depth': 4, 'min_data_in_leaf': 2, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.09177248187498382, 'colsample_bylevel': 0.7565566632047334, 'max_bin': 33, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:55:49,287]\u001b[0m Trial 25 finished with value: 7.715522400388065 and parameters: {'learning_rate': 0.0031123782920751976, 'subsample': 0.5366712223379697, 'od_wait': 2116, 'reg_lambda': 33.78235264872208, 'random_strength': 11.677657945698858, 'depth': 2, 'min_data_in_leaf': 11, 'leaf_estimation_iterations': 7, 'bagging_temperature': 0.9731462766118145, 'colsample_bylevel': 0.6224380263889648, 'max_bin': 66, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:57:06,284]\u001b[0m Trial 26 finished with value: 7.74268453077179 and parameters: {'learning_rate': 0.0018435726935403384, 'subsample': 0.40526485072062113, 'od_wait': 1684, 'reg_lambda': 9.55892824636661, 'random_strength': 30.742449476242854, 'depth': 3, 'min_data_in_leaf': 36, 'leaf_estimation_iterations': 12, 'bagging_temperature': 0.026100838526820613, 'colsample_bylevel': 0.8433298360201672, 'max_bin': 135, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:58:12,548]\u001b[0m Trial 27 finished with value: 7.076328796792013 and parameters: {'learning_rate': 0.005144145594548222, 'subsample': 0.19417700346338315, 'od_wait': 1251, 'reg_lambda': 22.263200108157484, 'random_strength': 8.021380222693146, 'depth': 4, 'min_data_in_leaf': 17, 'leaf_estimation_iterations': 6, 'bagging_temperature': 0.1379872574521721, 'colsample_bylevel': 0.9418002961973498, 'max_bin': 31, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 05:59:16,463]\u001b[0m Trial 28 finished with value: 7.916112768379571 and parameters: {'learning_rate': 0.001154801839790253, 'subsample': 0.1964719870996013, 'od_wait': 1148, 'reg_lambda': 63.80954019192281, 'random_strength': 25.563343867961265, 'depth': 4, 'min_data_in_leaf': 23, 'leaf_estimation_iterations': 5, 'bagging_temperature': 0.0965792017620909, 'colsample_bylevel': 0.9930363624728771, 'max_bin': 33, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:00:21,673]\u001b[0m Trial 29 finished with value: 7.640328302040323 and parameters: {'learning_rate': 0.0054873581718962, 'subsample': 0.7023242872249209, 'od_wait': 1514, 'reg_lambda': 50.48495879687541, 'random_strength': 46.87469294447734, 'depth': 4, 'min_data_in_leaf': 16, 'leaf_estimation_iterations': 1, 'bagging_temperature': 48.56262159117683, 'colsample_bylevel': 0.5439585588012039, 'max_bin': 118, 'od_type': 'Iter'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:01:19,679]\u001b[0m Trial 30 finished with value: 7.447561206366115 and parameters: {'learning_rate': 0.009839978900418912, 'subsample': 0.18641168649976597, 'od_wait': 1298, 'reg_lambda': 50.581010871831374, 'random_strength': 15.519757457754935, 'depth': 4, 'min_data_in_leaf': 38, 'leaf_estimation_iterations': 6, 'bagging_temperature': 0.01839639256661006, 'colsample_bylevel': 0.4569257983422824, 'max_bin': 92, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:02:14,785]\u001b[0m Trial 31 finished with value: 7.011455352538185 and parameters: {'learning_rate': 0.003974725404163328, 'subsample': 0.3023481445301394, 'od_wait': 1016, 'reg_lambda': 21.183456987537543, 'random_strength': 5.845614915479561, 'depth': 3, 'min_data_in_leaf': 8, 'leaf_estimation_iterations': 8, 'bagging_temperature': 0.1734489077342534, 'colsample_bylevel': 0.9544132159173941, 'max_bin': 32, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:03:12,102]\u001b[0m Trial 32 finished with value: 7.648112739215409 and parameters: {'learning_rate': 0.006924942363325184, 'subsample': 0.2976428839218621, 'od_wait': 922, 'reg_lambda': 9.241786403181877, 'random_strength': 7.8329943766434695, 'depth': 3, 'min_data_in_leaf': 7, 'leaf_estimation_iterations': 8, 'bagging_temperature': 0.05673902104433161, 'colsample_bylevel': 0.9470824313256814, 'max_bin': 41, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:03:53,112]\u001b[0m Trial 33 finished with value: 8.005727968612744 and parameters: {'learning_rate': 0.0028320045136056107, 'subsample': 0.17136049909334966, 'od_wait': 1072, 'reg_lambda': 35.473313633310056, 'random_strength': 6.474023562307413, 'depth': 2, 'min_data_in_leaf': 25, 'leaf_estimation_iterations': 6, 'bagging_temperature': 0.17308953385632295, 'colsample_bylevel': 0.8479531272117047, 'max_bin': 64, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:05:11,367]\u001b[0m Trial 34 finished with value: 8.044917623764636 and parameters: {'learning_rate': 0.00382760902555007, 'subsample': 0.037633396183437084, 'od_wait': 1312, 'reg_lambda': 21.870578015125794, 'random_strength': 12.424347449068671, 'depth': 5, 'min_data_in_leaf': 15, 'leaf_estimation_iterations': 8, 'bagging_temperature': 0.43924201105031035, 'colsample_bylevel': 0.7345095398173472, 'max_bin': 29, 'od_type': 'Iter'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:06:48,453]\u001b[0m Trial 35 finished with value: 7.962144682909855 and parameters: {'learning_rate': 0.002096950915196887, 'subsample': 0.41483470563940245, 'od_wait': 522, 'reg_lambda': 39.595057645694276, 'random_strength': 23.334545020840686, 'depth': 5, 'min_data_in_leaf': 10, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.7795127613043034, 'colsample_bylevel': 0.9364195030009879, 'max_bin': 46, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:07:36,473]\u001b[0m Trial 36 finished with value: 7.69058886107825 and parameters: {'learning_rate': 0.007485201901133369, 'subsample': 0.2809317948212746, 'od_wait': 891, 'reg_lambda': 7.714258410160852, 'random_strength': 30.889770174683566, 'depth': 3, 'min_data_in_leaf': 81, 'leaf_estimation_iterations': 3, 'bagging_temperature': 0.06236387879400053, 'colsample_bylevel': 0.6782721064253034, 'max_bin': 79, 'od_type': 'Iter'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:08:36,915]\u001b[0m Trial 37 finished with value: 7.191724066277415 and parameters: {'learning_rate': 0.00505522871614502, 'subsample': 0.07225815203701721, 'od_wait': 1393, 'reg_lambda': 16.8630302575639, 'random_strength': 5.272640026849468, 'depth': 4, 'min_data_in_leaf': 6, 'leaf_estimation_iterations': 5, 'bagging_temperature': 2.2649882487707043, 'colsample_bylevel': 0.7838525743439096, 'max_bin': 26, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:09:38,491]\u001b[0m Trial 38 finished with value: 7.829871337091812 and parameters: {'learning_rate': 0.002586439834901998, 'subsample': 0.15768029397036148, 'od_wait': 782, 'reg_lambda': 29.469500180041326, 'random_strength': 17.6094904581367, 'depth': 4, 'min_data_in_leaf': 57, 'leaf_estimation_iterations': 8, 'bagging_temperature': 0.15807869190316123, 'colsample_bylevel': 0.2571123968108626, 'max_bin': 174, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:10:22,987]\u001b[0m Trial 39 finished with value: 8.252762139901645 and parameters: {'learning_rate': 0.0013912984022207923, 'subsample': 0.47298012702815284, 'od_wait': 1050, 'reg_lambda': 79.1689238177439, 'random_strength': 4.829005227754123, 'depth': 2, 'min_data_in_leaf': 26, 'leaf_estimation_iterations': 5, 'bagging_temperature': 0.022720584365971817, 'colsample_bylevel': 0.8481428840180947, 'max_bin': 110, 'od_type': 'Iter'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:11:23,332]\u001b[0m Trial 40 finished with value: 7.7204624084399995 and parameters: {'learning_rate': 0.006202479614918019, 'subsample': 0.5907207440928758, 'od_wait': 1403, 'reg_lambda': 4.598799208346001, 'random_strength': 11.097575193924342, 'depth': 3, 'min_data_in_leaf': 16, 'leaf_estimation_iterations': 7, 'bagging_temperature': 5.391938368087728, 'colsample_bylevel': 0.3994639763887762, 'max_bin': 231, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:12:18,406]\u001b[0m Trial 41 finished with value: 7.008409150534818 and parameters: {'learning_rate': 0.0036794343521527263, 'subsample': 0.3557350117507901, 'od_wait': 1197, 'reg_lambda': 21.40565386347636, 'random_strength': 3.3957526642153155, 'depth': 3, 'min_data_in_leaf': 3, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.34111174416675427, 'colsample_bylevel': 0.8985633883910413, 'max_bin': 18, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:13:13,375]\u001b[0m Trial 42 finished with value: 7.164490591436809 and parameters: {'learning_rate': 0.0036368854420388863, 'subsample': 0.37373287053357784, 'od_wait': 1203, 'reg_lambda': 25.467688134247016, 'random_strength': 3.992307811864506, 'depth': 3, 'min_data_in_leaf': 6, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.3633029547774967, 'colsample_bylevel': 0.9521216283091835, 'max_bin': 24, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:14:31,981]\u001b[0m Trial 43 finished with value: 7.528462412533138 and parameters: {'learning_rate': 0.0020913415527508083, 'subsample': 0.32065238842779353, 'od_wait': 969, 'reg_lambda': 14.700858770050946, 'random_strength': 8.804695987326479, 'depth': 4, 'min_data_in_leaf': 100, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.14555791485856112, 'colsample_bylevel': 0.9042646079339123, 'max_bin': 55, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:15:31,625]\u001b[0m Trial 44 finished with value: 7.37306412813215 and parameters: {'learning_rate': 0.004333868791791063, 'subsample': 0.44035699879064993, 'od_wait': 1169, 'reg_lambda': 20.115515407223626, 'random_strength': 15.652272885720857, 'depth': 3, 'min_data_in_leaf': 5, 'leaf_estimation_iterations': 13, 'bagging_temperature': 1.6945638643529235, 'colsample_bylevel': 0.9964710377857207, 'max_bin': 19, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:16:59,372]\u001b[0m Trial 45 finished with value: 7.821824264587772 and parameters: {'learning_rate': 0.002999089119401701, 'subsample': 0.2520596357376278, 'od_wait': 767, 'reg_lambda': 43.5782665788352, 'random_strength': 19.836227466039368, 'depth': 5, 'min_data_in_leaf': 1, 'leaf_estimation_iterations': 12, 'bagging_temperature': 0.583288045227651, 'colsample_bylevel': 0.7249805742996931, 'max_bin': 37, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:17:15,070]\u001b[0m Trial 46 finished with value: 7.8068125646772595 and parameters: {'learning_rate': 0.005616216009663641, 'subsample': 0.971722123892417, 'od_wait': 1226, 'reg_lambda': 32.15836984393919, 'random_strength': 97.58640014653713, 'depth': 2, 'min_data_in_leaf': 18, 'leaf_estimation_iterations': 9, 'bagging_temperature': 0.254383441320903, 'colsample_bylevel': 0.6129779615816999, 'max_bin': 59, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:18:06,322]\u001b[0m Trial 47 finished with value: 7.384043741567703 and parameters: {'learning_rate': 0.007982420135143817, 'subsample': 0.3535781591434286, 'od_wait': 1000, 'reg_lambda': 39.25589934994777, 'random_strength': 60.483275341460235, 'depth': 3, 'min_data_in_leaf': 12, 'leaf_estimation_iterations': 11, 'bagging_temperature': 0.059630846814175034, 'colsample_bylevel': 0.8313332549222059, 'max_bin': 46, 'od_type': 'Iter'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:18:57,343]\u001b[0m Trial 48 finished with value: 10.3701382124481 and parameters: {'learning_rate': 0.0003559882516347161, 'subsample': 0.11187555889777989, 'od_wait': 1494, 'reg_lambda': 20.0129401711297, 'random_strength': 3.0491969720206287, 'depth': 2, 'min_data_in_leaf': 52, 'leaf_estimation_iterations': 10, 'bagging_temperature': 0.110526529730776, 'colsample_bylevel': 0.8757029720244552, 'max_bin': 101, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n",
      "\u001b[32m[I 2022-12-14 06:19:59,853]\u001b[0m Trial 49 finished with value: 11.623890548008127 and parameters: {'learning_rate': 0.0002462567533731637, 'subsample': 0.22571090902593086, 'od_wait': 1669, 'reg_lambda': 55.52514381081561, 'random_strength': 71.17531839997912, 'depth': 4, 'min_data_in_leaf': 8, 'leaf_estimation_iterations': 7, 'bagging_temperature': 0.037261597715856014, 'colsample_bylevel': 0.7672818117485932, 'max_bin': 20, 'od_type': 'IncToDec'}. Best is trial 12 with value: 6.82334922894401.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(input_df,label_df,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train = X_train.drop('case_num',axis=1)\n",
    "X_valid = X_valid.drop('case_num',axis=1)\n",
    "y_train = y_train['predicted_weight_g']\n",
    "y_valid = y_valid['predicted_weight_g']\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize', study_name='CatBoost', sampler=optuna.samplers.TPESampler(seed=0))\n",
    "func = lambda trial: objective(trial,X_train,X_valid,y_train,y_valid,verbose=False)\n",
    "\n",
    "n_trials = 50\n",
    "\n",
    "global pbar_cat\n",
    "with tqdm(total=n_trials) as pbar_cat:\n",
    "    study_cat.optimize(func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e533919f-9d0d-4eb4-930c-653d540afc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.003832622042155371,\n",
       " 'subsample': 0.36292672548257277,\n",
       " 'od_wait': 2267,\n",
       " 'reg_lambda': 69.51018764739234,\n",
       " 'random_strength': 1.028534640055302,\n",
       " 'depth': 3,\n",
       " 'min_data_in_leaf': 2,\n",
       " 'leaf_estimation_iterations': 9,\n",
       " 'bagging_temperature': 0.21893053909412485,\n",
       " 'colsample_bylevel': 0.8856926987679451,\n",
       " 'max_bin': 23,\n",
       " 'od_type': 'IncToDec'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_cat.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ce8a4b16-f260-4742-8144-413cf933feb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 41.2221293\ttest: 42.5178837\tbest: 42.5178837 (0)\ttotal: 1.12ms\tremaining: 9m 18s\n",
      "10000:\tlearn: 2.6505602\ttest: 7.5892120\tbest: 7.5892120 (10000)\ttotal: 10.4s\tremaining: 8m 30s\n",
      "20000:\tlearn: 1.5003878\ttest: 7.0859806\tbest: 7.0859806 (20000)\ttotal: 20.7s\tremaining: 8m 17s\n",
      "30000:\tlearn: 1.0099165\ttest: 6.9365059\tbest: 6.9365011 (29998)\ttotal: 31s\tremaining: 8m 5s\n",
      "40000:\tlearn: 0.7377927\ttest: 6.8643549\tbest: 6.8643549 (40000)\ttotal: 41.2s\tremaining: 7m 54s\n",
      "50000:\tlearn: 0.5630418\ttest: 6.8233631\tbest: 6.8233491 (49993)\ttotal: 51.5s\tremaining: 7m 43s\n",
      "60000:\tlearn: 0.4419458\ttest: 6.8022462\tbest: 6.8022028 (59965)\ttotal: 1m 1s\tremaining: 7m 32s\n",
      "70000:\tlearn: 0.3529503\ttest: 6.7855347\tbest: 6.7854422 (69887)\ttotal: 1m 12s\tremaining: 7m 22s\n",
      "80000:\tlearn: 0.2866395\ttest: 6.7737127\tbest: 6.7737001 (79999)\ttotal: 1m 22s\tremaining: 7m 12s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 6.768572792\n",
      "bestIteration = 87747\n",
      "\n",
      "Shrink model to first 87748 iterations.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Pool(data=X_train,label=y_train)\n",
    "valid_dataset = Pool(data=X_valid,label=y_valid)\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    random_state=0,\n",
    "    loss_function='RMSE',\n",
    "    metric_period=10000,\n",
    "    iterations=500000,\n",
    "    **study_cat.best_params,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    eval_set=valid_dataset,\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=1000,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pred_input_df[f'pred'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "\n",
    "pred_test_df[f'pred'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dc086a47-7672-4d6d-b6c7-db9482204f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = 'pred'\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1adc5c46-43a3-45bb-ae06-dd837a7373cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_cat_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c61badce-1b87-4505-93d1-e6a647ab46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_cat_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_cat_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98c7b7-f67d-4071-857c-aa876af0ea80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## kfold + optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2aca982-2230-45bf-b4c4-c6e224cf3d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71f6491f1484b0e9aeb325d11bcd68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(1/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.2164619\ttest: 43.6867119\tbest: 43.6867119 (0)\ttotal: 3.31ms\tremaining: 27m 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.5875074\ttest: 5.1533543\tbest: 5.1533543 (10000)\ttotal: 31s\tremaining: 25m 18s\n",
      "20000:\tlearn: 1.5036043\ttest: 4.8713857\tbest: 4.8710127 (19992)\ttotal: 46s\tremaining: 18m 23s\n",
      "30000:\tlearn: 1.0439258\ttest: 4.7801514\tbest: 4.7800976 (29998)\ttotal: 56.2s\tremaining: 14m 41s\n",
      "40000:\tlearn: 0.7818381\ttest: 4.7394446\tbest: 4.7394095 (39990)\ttotal: 1m 6s\tremaining: 12m 40s\n",
      "50000:\tlearn: 0.6118600\ttest: 4.7080512\tbest: 4.7077694 (49822)\ttotal: 1m 16s\tremaining: 11m 25s\n",
      "60000:\tlearn: 0.4911956\ttest: 4.6920059\tbest: 4.6920059 (60000)\ttotal: 1m 26s\tremaining: 10m 33s\n",
      "70000:\tlearn: 0.4029936\ttest: 4.6798266\tbest: 4.6797971 (69974)\ttotal: 1m 36s\tremaining: 9m 53s\n",
      "80000:\tlearn: 0.3344967\ttest: 4.6717233\tbest: 4.6717059 (79999)\ttotal: 1m 46s\tremaining: 9m 20s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 4.667409441\n",
      "bestIteration = 85488\n",
      "\n",
      "Shrink model to first 85489 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(2/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.5190124\ttest: 41.1558222\tbest: 41.1558222 (0)\ttotal: 1.51ms\tremaining: 12m 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.5712226\ttest: 7.3453684\tbest: 7.3451275 (9993)\ttotal: 9.88s\tremaining: 8m 4s\n",
      "20000:\tlearn: 1.4966131\ttest: 6.6181518\tbest: 6.6181518 (20000)\ttotal: 19.8s\tremaining: 7m 54s\n",
      "30000:\tlearn: 1.0486684\ttest: 6.3875065\tbest: 6.3873047 (29977)\ttotal: 29.7s\tremaining: 7m 45s\n",
      "40000:\tlearn: 0.7943446\ttest: 6.2883671\tbest: 6.2883257 (39994)\ttotal: 39.6s\tremaining: 7m 35s\n",
      "50000:\tlearn: 0.6257826\ttest: 6.2315926\tbest: 6.2315090 (49987)\ttotal: 49.6s\tremaining: 7m 26s\n",
      "60000:\tlearn: 0.5036846\ttest: 6.1903042\tbest: 6.1902832 (59998)\ttotal: 59.8s\tremaining: 7m 18s\n",
      "70000:\tlearn: 0.4127863\ttest: 6.1619082\tbest: 6.1618112 (69969)\ttotal: 1m 9s\tremaining: 7m 7s\n",
      "80000:\tlearn: 0.3419879\ttest: 6.1429482\tbest: 6.1429482 (80000)\ttotal: 1m 19s\tremaining: 6m 59s\n",
      "90000:\tlearn: 0.2859060\ttest: 6.1290157\tbest: 6.1290157 (90000)\ttotal: 1m 30s\tremaining: 6m 51s\n",
      "100000:\tlearn: 0.2401963\ttest: 6.1163103\tbest: 6.1163024 (99993)\ttotal: 1m 40s\tremaining: 6m 41s\n",
      "110000:\tlearn: 0.2032799\ttest: 6.1059925\tbest: 6.1059925 (110000)\ttotal: 1m 50s\tremaining: 6m 32s\n",
      "120000:\tlearn: 0.1728638\ttest: 6.0990614\tbest: 6.0990443 (119995)\ttotal: 2m\tremaining: 6m 21s\n",
      "130000:\tlearn: 0.1473840\ttest: 6.0929358\tbest: 6.0927819 (129863)\ttotal: 2m 10s\tremaining: 6m 11s\n",
      "140000:\tlearn: 0.1264488\ttest: 6.0881850\tbest: 6.0881800 (139981)\ttotal: 2m 20s\tremaining: 6m\n",
      "150000:\tlearn: 0.1083601\ttest: 6.0845599\tbest: 6.0845578 (149999)\ttotal: 2m 30s\tremaining: 5m 50s\n",
      "160000:\tlearn: 0.0932677\ttest: 6.0815299\tbest: 6.0815299 (160000)\ttotal: 2m 40s\tremaining: 5m 40s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 6.080682354\n",
      "bestIteration = 162871\n",
      "\n",
      "Shrink model to first 162872 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(3/10)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 40.2587683\ttest: 51.2532865\tbest: 51.2532865 (0)\ttotal: 1.69ms\tremaining: 14m 2s\n",
      "10000:\tlearn: 2.4984770\ttest: 6.8837520\tbest: 6.8837520 (10000)\ttotal: 10.8s\tremaining: 8m 49s\n",
      "20000:\tlearn: 1.4631240\ttest: 6.5522907\tbest: 6.5522907 (20000)\ttotal: 20.6s\tremaining: 8m 14s\n",
      "30000:\tlearn: 1.0331246\ttest: 6.4414863\tbest: 6.4413914 (29997)\ttotal: 30.4s\tremaining: 7m 56s\n",
      "40000:\tlearn: 0.7842977\ttest: 6.3950553\tbest: 6.3950301 (39997)\ttotal: 40.6s\tremaining: 7m 46s\n",
      "50000:\tlearn: 0.6173272\ttest: 6.3708319\tbest: 6.3707569 (49991)\ttotal: 50.7s\tremaining: 7m 35s\n",
      "60000:\tlearn: 0.4956285\ttest: 6.3578577\tbest: 6.3577800 (59847)\ttotal: 1m\tremaining: 7m 23s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 6.351364381\n",
      "bestIteration = 67812\n",
      "\n",
      "Shrink model to first 67813 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(4/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.8662526\ttest: 37.9858236\tbest: 37.9858236 (0)\ttotal: 1.05ms\tremaining: 8m 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.6422744\ttest: 5.2948949\tbest: 5.2948949 (10000)\ttotal: 9.9s\tremaining: 8m 4s\n",
      "20000:\tlearn: 1.5493270\ttest: 4.7481073\tbest: 4.7479296 (19993)\ttotal: 19.6s\tremaining: 7m 50s\n",
      "30000:\tlearn: 1.0851399\ttest: 4.5935045\tbest: 4.5934733 (29995)\ttotal: 29.4s\tremaining: 7m 40s\n",
      "40000:\tlearn: 0.8204299\ttest: 4.5259125\tbest: 4.5259125 (40000)\ttotal: 39.2s\tremaining: 7m 30s\n",
      "50000:\tlearn: 0.6458728\ttest: 4.4933778\tbest: 4.4933679 (49994)\ttotal: 49s\tremaining: 7m 21s\n",
      "60000:\tlearn: 0.5190575\ttest: 4.4741378\tbest: 4.4740556 (59997)\ttotal: 58.8s\tremaining: 7m 11s\n",
      "70000:\tlearn: 0.4241660\ttest: 4.4603333\tbest: 4.4601747 (69870)\ttotal: 1m 8s\tremaining: 7m 1s\n",
      "80000:\tlearn: 0.3515361\ttest: 4.4503530\tbest: 4.4502612 (79948)\ttotal: 1m 18s\tremaining: 6m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 4.448356222\n",
      "bestIteration = 81796\n",
      "\n",
      "Shrink model to first 81797 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(5/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.9019151\ttest: 37.4769354\tbest: 37.4769354 (0)\ttotal: 11.4ms\tremaining: 1h 35m 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.6519451\ttest: 4.7898901\tbest: 4.7898901 (10000)\ttotal: 9.79s\tremaining: 7m 59s\n",
      "20000:\tlearn: 1.5739934\ttest: 4.5094809\tbest: 4.5092695 (19996)\ttotal: 19.6s\tremaining: 7m 50s\n",
      "30000:\tlearn: 1.1029676\ttest: 4.4135800\tbest: 4.4131263 (29982)\ttotal: 29.3s\tremaining: 7m 39s\n",
      "40000:\tlearn: 0.8309635\ttest: 4.3644985\tbest: 4.3644554 (39991)\ttotal: 39.1s\tremaining: 7m 29s\n",
      "50000:\tlearn: 0.6523188\ttest: 4.3369571\tbest: 4.3367299 (49937)\ttotal: 48.8s\tremaining: 7m 19s\n",
      "60000:\tlearn: 0.5235455\ttest: 4.3182792\tbest: 4.3181509 (59979)\ttotal: 58.7s\tremaining: 7m 10s\n",
      "70000:\tlearn: 0.4283815\ttest: 4.3057327\tbest: 4.3053446 (69756)\ttotal: 1m 8s\tremaining: 6m 59s\n",
      "80000:\tlearn: 0.3543039\ttest: 4.2984437\tbest: 4.2981804 (79670)\ttotal: 1m 18s\tremaining: 6m 49s\n",
      "90000:\tlearn: 0.2958489\ttest: 4.2943606\tbest: 4.2941740 (89524)\ttotal: 1m 27s\tremaining: 6m 40s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 4.290488236\n",
      "bestIteration = 96110\n",
      "\n",
      "Shrink model to first 96111 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(6/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.1981658\ttest: 44.0086784\tbest: 44.0086784 (0)\ttotal: 1.59ms\tremaining: 13m 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.5834676\ttest: 6.2629392\tbest: 6.2629392 (10000)\ttotal: 9.81s\tremaining: 8m\n",
      "20000:\tlearn: 1.5059532\ttest: 5.7416566\tbest: 5.7412497 (19996)\ttotal: 19.7s\tremaining: 7m 51s\n",
      "30000:\tlearn: 1.0505181\ttest: 5.5540251\tbest: 5.5540251 (30000)\ttotal: 29.5s\tremaining: 7m 41s\n",
      "40000:\tlearn: 0.7929423\ttest: 5.4694465\tbest: 5.4693894 (39990)\ttotal: 39.4s\tremaining: 7m 32s\n",
      "50000:\tlearn: 0.6248293\ttest: 5.4321089\tbest: 5.4318738 (49958)\ttotal: 49s\tremaining: 7m 21s\n",
      "60000:\tlearn: 0.5044771\ttest: 5.4075242\tbest: 5.4074287 (59810)\ttotal: 58.8s\tremaining: 7m 11s\n",
      "70000:\tlearn: 0.4152271\ttest: 5.3902570\tbest: 5.3902570 (70000)\ttotal: 1m 8s\tremaining: 7m 1s\n",
      "80000:\tlearn: 0.3472666\ttest: 5.3775351\tbest: 5.3775184 (79999)\ttotal: 1m 18s\tremaining: 6m 51s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 5.368883416\n",
      "bestIteration = 88912\n",
      "\n",
      "Shrink model to first 88913 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(7/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.8474543\ttest: 38.0825503\tbest: 38.0825503 (0)\ttotal: 1.54ms\tremaining: 12m 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.7353462\ttest: 5.1015922\tbest: 5.1015922 (10000)\ttotal: 9.85s\tremaining: 8m 2s\n",
      "20000:\tlearn: 1.5695779\ttest: 4.4050074\tbest: 4.4050074 (20000)\ttotal: 19.9s\tremaining: 7m 57s\n",
      "30000:\tlearn: 1.0886208\ttest: 4.1865049\tbest: 4.1864491 (29998)\ttotal: 29.8s\tremaining: 7m 46s\n",
      "40000:\tlearn: 0.8088875\ttest: 4.0686618\tbest: 4.0686541 (39999)\ttotal: 39.7s\tremaining: 7m 36s\n",
      "50000:\tlearn: 0.6269449\ttest: 4.0002828\tbest: 4.0001890 (49997)\ttotal: 49.5s\tremaining: 7m 25s\n",
      "60000:\tlearn: 0.4970474\ttest: 3.9577668\tbest: 3.9577668 (60000)\ttotal: 59.2s\tremaining: 7m 14s\n",
      "70000:\tlearn: 0.4004642\ttest: 3.9304600\tbest: 3.9304597 (69992)\ttotal: 1m 9s\tremaining: 7m 4s\n",
      "80000:\tlearn: 0.3285902\ttest: 3.9100031\tbest: 3.9099759 (79995)\ttotal: 1m 18s\tremaining: 6m 54s\n",
      "90000:\tlearn: 0.2713618\ttest: 3.8954742\tbest: 3.8954742 (90000)\ttotal: 1m 29s\tremaining: 6m 46s\n",
      "100000:\tlearn: 0.2264365\ttest: 3.8847538\tbest: 3.8847538 (100000)\ttotal: 1m 40s\tremaining: 6m 42s\n",
      "110000:\tlearn: 0.1901118\ttest: 3.8769521\tbest: 3.8768846 (109950)\ttotal: 1m 50s\tremaining: 6m 33s\n",
      "120000:\tlearn: 0.1607595\ttest: 3.8696586\tbest: 3.8696422 (119988)\ttotal: 2m 1s\tremaining: 6m 24s\n",
      "130000:\tlearn: 0.1364843\ttest: 3.8640853\tbest: 3.8640777 (129957)\ttotal: 2m 11s\tremaining: 6m 15s\n",
      "140000:\tlearn: 0.1161321\ttest: 3.8599662\tbest: 3.8599168 (139790)\ttotal: 2m 22s\tremaining: 6m 6s\n",
      "150000:\tlearn: 0.0993473\ttest: 3.8567975\tbest: 3.8567742 (149945)\ttotal: 2m 32s\tremaining: 5m 55s\n",
      "160000:\tlearn: 0.0853222\ttest: 3.8541441\tbest: 3.8541360 (159994)\ttotal: 2m 43s\tremaining: 5m 47s\n",
      "170000:\tlearn: 0.0733701\ttest: 3.8525367\tbest: 3.8525243 (169950)\ttotal: 2m 53s\tremaining: 5m 37s\n",
      "180000:\tlearn: 0.0631230\ttest: 3.8505259\tbest: 3.8505259 (180000)\ttotal: 3m 3s\tremaining: 5m 26s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 3.85037626\n",
      "bestIteration = 181159\n",
      "\n",
      "Shrink model to first 181160 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(8/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.1944234\ttest: 44.0414142\tbest: 44.0414142 (0)\ttotal: 1.6ms\tremaining: 13m 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.6450791\ttest: 5.9365785\tbest: 5.9365785 (10000)\ttotal: 10.6s\tremaining: 8m 40s\n",
      "20000:\tlearn: 1.5518096\ttest: 5.5180403\tbest: 5.5178600 (19996)\ttotal: 21.2s\tremaining: 8m 27s\n",
      "30000:\tlearn: 1.0862213\ttest: 5.3794928\tbest: 5.3793862 (29976)\ttotal: 32s\tremaining: 8m 21s\n",
      "40000:\tlearn: 0.8189869\ttest: 5.3092600\tbest: 5.3092600 (40000)\ttotal: 42s\tremaining: 8m 2s\n",
      "50000:\tlearn: 0.6415231\ttest: 5.2665275\tbest: 5.2664943 (49991)\ttotal: 52s\tremaining: 7m 48s\n",
      "60000:\tlearn: 0.5151881\ttest: 5.2371557\tbest: 5.2371501 (59998)\ttotal: 1m 2s\tremaining: 7m 37s\n",
      "70000:\tlearn: 0.4205642\ttest: 5.2208431\tbest: 5.2206429 (69961)\ttotal: 1m 12s\tremaining: 7m 27s\n",
      "80000:\tlearn: 0.3479977\ttest: 5.2059625\tbest: 5.2057987 (79856)\ttotal: 1m 23s\tremaining: 7m 17s\n",
      "90000:\tlearn: 0.2903505\ttest: 5.1967975\tbest: 5.1967262 (89841)\ttotal: 1m 33s\tremaining: 7m 5s\n",
      "100000:\tlearn: 0.2446218\ttest: 5.1890495\tbest: 5.1890276 (99871)\ttotal: 1m 43s\tremaining: 6m 54s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 5.184237583\n",
      "bestIteration = 108084\n",
      "\n",
      "Shrink model to first 108085 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(9/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.9494933\ttest: 37.0344986\tbest: 37.0344986 (0)\ttotal: 1.71ms\tremaining: 14m 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.5353707\ttest: 6.6804265\tbest: 6.6803440 (9999)\ttotal: 10s\tremaining: 8m 12s\n",
      "20000:\tlearn: 1.4693797\ttest: 6.1524765\tbest: 6.1524765 (20000)\ttotal: 20.1s\tremaining: 8m 1s\n",
      "30000:\tlearn: 1.0089480\ttest: 5.9639789\tbest: 5.9636613 (29965)\ttotal: 30.7s\tremaining: 8m\n",
      "40000:\tlearn: 0.7452626\ttest: 5.8794339\tbest: 5.8794339 (40000)\ttotal: 40.9s\tremaining: 7m 49s\n",
      "50000:\tlearn: 0.5747216\ttest: 5.8270755\tbest: 5.8270687 (49999)\ttotal: 50.8s\tremaining: 7m 37s\n",
      "60000:\tlearn: 0.4563293\ttest: 5.7949422\tbest: 5.7949273 (59996)\ttotal: 1m 1s\tremaining: 7m 28s\n",
      "70000:\tlearn: 0.3690264\ttest: 5.7775288\tbest: 5.7775284 (69999)\ttotal: 1m 11s\tremaining: 7m 20s\n",
      "80000:\tlearn: 0.3029964\ttest: 5.7649518\tbest: 5.7649134 (79983)\ttotal: 1m 22s\tremaining: 7m 10s\n",
      "90000:\tlearn: 0.2515978\ttest: 5.7569456\tbest: 5.7569438 (89998)\ttotal: 1m 32s\tremaining: 7m\n",
      "100000:\tlearn: 0.2107703\ttest: 5.7512570\tbest: 5.7512392 (99964)\ttotal: 1m 42s\tremaining: 6m 49s\n",
      "110000:\tlearn: 0.1776952\ttest: 5.7473936\tbest: 5.7473824 (109997)\ttotal: 1m 52s\tremaining: 6m 39s\n",
      "120000:\tlearn: 0.1507387\ttest: 5.7441533\tbest: 5.7441449 (119993)\ttotal: 2m 2s\tremaining: 6m 29s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 5.744020074\n",
      "bestIteration = 120205\n",
      "\n",
      "Shrink model to first 120206 iterations.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(10/10)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:\tlearn: 41.8137591\ttest: 38.2512822\tbest: 38.2512822 (0)\ttotal: 1.55ms\tremaining: 12m 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000:\tlearn: 2.6434041\ttest: 6.5356621\tbest: 6.5354243 (9996)\ttotal: 10.2s\tremaining: 8m 18s\n",
      "20000:\tlearn: 1.5294404\ttest: 5.9725871\tbest: 5.9725871 (20000)\ttotal: 20.4s\tremaining: 8m 9s\n",
      "30000:\tlearn: 1.0541063\ttest: 5.8115562\tbest: 5.8114696 (29999)\ttotal: 30.8s\tremaining: 8m 2s\n",
      "40000:\tlearn: 0.7940645\ttest: 5.7445260\tbest: 5.7443340 (39968)\ttotal: 40.6s\tremaining: 7m 46s\n",
      "50000:\tlearn: 0.6243734\ttest: 5.7088396\tbest: 5.7088101 (49907)\ttotal: 50.5s\tremaining: 7m 34s\n",
      "60000:\tlearn: 0.5045552\ttest: 5.6864819\tbest: 5.6863247 (59966)\ttotal: 1m\tremaining: 7m 21s\n",
      "70000:\tlearn: 0.4152344\ttest: 5.6709982\tbest: 5.6708785 (69835)\ttotal: 1m 10s\tremaining: 7m 13s\n",
      "80000:\tlearn: 0.3466206\ttest: 5.6583518\tbest: 5.6583435 (79991)\ttotal: 1m 21s\tremaining: 7m 8s\n",
      "90000:\tlearn: 0.2910936\ttest: 5.6496238\tbest: 5.6495604 (89962)\ttotal: 1m 31s\tremaining: 6m 56s\n",
      "100000:\tlearn: 0.2466563\ttest: 5.6422929\tbest: 5.6422866 (99999)\ttotal: 1m 41s\tremaining: 6m 45s\n",
      "110000:\tlearn: 0.2103727\ttest: 5.6371476\tbest: 5.6371263 (109990)\ttotal: 1m 51s\tremaining: 6m 34s\n",
      "120000:\tlearn: 0.1806291\ttest: 5.6335252\tbest: 5.6334711 (119940)\ttotal: 2m 1s\tremaining: 6m 23s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 5.631110816\n",
      "bestIteration = 126996\n",
      "\n",
      "Shrink model to first 126997 iterations.\n"
     ]
    }
   ],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(input_df),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'')\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    X_train = input_df.iloc[tr_idx,:].drop(columns=['case_num'])\n",
    "    X_valid = input_df.iloc[va_idx,:].drop(columns=['case_num'])\n",
    "\n",
    "    y_train = label_df.iloc[tr_idx]['predicted_weight_g']\n",
    "    y_valid = label_df.iloc[va_idx]['predicted_weight_g']\n",
    "    \n",
    "    train_dataset = Pool(data=X_train,label=y_train)\n",
    "    valid_dataset = Pool(data=X_valid,label=y_valid)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        random_state=0,\n",
    "        loss_function='RMSE',\n",
    "        metric_period=10000,\n",
    "        iterations=500000,\n",
    "        **study_cat.best_params,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        eval_set=valid_dataset,\n",
    "        use_best_model=True,\n",
    "        early_stopping_rounds=1000,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    pred_input_df[f'pred_{kf_iter}'] = model.predict(input_df.drop(columns=['case_num']))\n",
    "    pred_input_df.to_csv(f'./out/kf_cat_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "\n",
    "    pred_test_df[f'pred_{kf_iter}'] = model.predict(test_input_df.drop(columns=['case_num']))\n",
    "    pred_test_df.to_csv(f'./out/kf_cat_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f1f983dc-aaf6-4c44-addc-7db47354d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a6f9cb6e-bc8f-435d-86dc-5fd4d565cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39927131629106166"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "pred_input_df['preds'] = pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1)\n",
    "\n",
    "mean_squared_error(pred_input_df['preds'],pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3faa2158-b6ec-450b-afc3-faadbf1fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols].apply(lambda x: x.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f9b1af6e-29c4-4dbb-9976-dbe525d318ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_cat_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e8f1b7a-a56d-477c-9596-2f7a57168d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_cat_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_cat_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a792e-c83d-4629-a29e-b0fd8b3d50c0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br></br>\n",
    "\n",
    "# Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a08-c07c-42bd-9b38-18eccfeba517",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523935d-af75-45ce-976c-c78f41578e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        # print(x.shape,x_reshape.shape)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446b0f-9dba-4a2b-b25e-5e20da605124",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194faee7-d316-4c85-9cad-8629cffdacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        hidden  = [40]*4      # 40\n",
    "        dropout = [0.5]*4     # 0.5\n",
    "        num_layers = [1]*4    # 1\n",
    "        bidirectional = False # False\n",
    "        if bidirectional:\n",
    "            offset = 2\n",
    "        else:\n",
    "            offset = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden[0],\n",
    "            dropout=dropout[0],\n",
    "            num_layers=num_layers[0],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=offset*hidden[0],\n",
    "            hidden_size=hidden[1],\n",
    "            dropout=dropout[1],\n",
    "            num_layers=num_layers[1],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=offset*hidden[1],\n",
    "            hidden_size=hidden[2],\n",
    "            dropout=dropout[2],\n",
    "            num_layers=num_layers[2],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=offset*hidden[2],\n",
    "            hidden_size=hidden[3],\n",
    "            dropout=dropout[3],\n",
    "            num_layers=num_layers[3],\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.elu  = nn.ELU()\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.fc = nn.Linear(offset * hidden[3], 1)\n",
    "        self.fc = TimeDistributed(self.fc)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st\n",
    "        x, _ = self.lstm1(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 2nd\n",
    "        x, _ = self.lstm2(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 3rd\n",
    "        x, _ = self.lstm3(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # 4th\n",
    "        x, _ = self.lstm4(x)\n",
    "        # x    = self.bn(x)\n",
    "        x    = self.activation(x)\n",
    "        # fully connected layer\n",
    "        x    = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac7bec-be69-4cf4-a663-2c5d0e813db0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb61ab-2360-4805-8407-1005e3ca5b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update)\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                 kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update)\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x)\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd =SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "    \n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen):\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: \n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCINet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, num_decoder_layer = 1, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.num_decoder_layer = num_decoder_layer\n",
    "\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        self.div_projection = nn.ModuleList()\n",
    "        self.overlap_len = self.input_len//4\n",
    "        self.div_len = self.input_len//6\n",
    "\n",
    "        if self.num_decoder_layer > 1:\n",
    "            self.projection1 = nn.Linear(self.input_len, self.output_len)\n",
    "            for layer_idx in range(self.num_decoder_layer-1):\n",
    "                div_projection = nn.ModuleList()\n",
    "                for i in range(6):\n",
    "                    lens = min(i*self.div_len+self.overlap_len,self.input_len) - i*self.div_len\n",
    "                    div_projection.append(nn.Linear(lens, self.div_len))\n",
    "                self.div_projection.append(div_projection)\n",
    "\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x += pe[:, :, :-1]\n",
    "            else:\n",
    "                x += self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        if self.RIN:\n",
    "            print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            #mean\n",
    "            x = x - means\n",
    "            #var\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # affine\n",
    "            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x += res1\n",
    "        if self.num_decoder_layer == 1:\n",
    "            x = self.projection1(x)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    "            for div_projection in self.div_projection:\n",
    "                output = torch.zeros(x.shape,dtype=x.dtype).cuda()\n",
    "                for i, div_layer in enumerate(div_projection):\n",
    "                    div_x = x[:,:,i*self.div_len:min(i*self.div_len+self.overlap_len,self.input_len)]\n",
    "                    output[:,:,i*self.div_len:(i+1)*self.div_len] = div_layer(div_x)\n",
    "                x = output\n",
    "            x = self.projection1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            res2 = x\n",
    "            x = self.blocks2(x)\n",
    "            x += res2\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "            return x, MidOutPut\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b693071-44da-46f4-a007-ccbfa2a9b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class SCINet_decompose(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim = 9, hid_size = 1, num_stacks = 1,\n",
    "                num_levels = 3, concat_len = 0, groups = 1, kernel = 5, dropout = 0.5,\n",
    "                 single_step_output_One = 0, input_len_seg = 0, positionalE = False, modified = True, RIN=False):\n",
    "        super(SCINet_decompose, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.hidden_size = hid_size\n",
    "        self.num_levels = num_levels\n",
    "        self.groups = groups\n",
    "        self.modified = modified\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.single_step_output_One = single_step_output_One\n",
    "        self.concat_len = concat_len\n",
    "        self.pe = positionalE\n",
    "        self.RIN=RIN\n",
    "        self.decomp = series_decomp(25)\n",
    "        self.trend = nn.Linear(input_len,input_len)\n",
    "        self.trend_dec = nn.Linear(input_len,output_len)\n",
    "        self.blocks1 = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        if num_stacks == 2: # we only implement two stacks at most.\n",
    "            self.blocks2 = EncoderTree(\n",
    "                in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN =  modified)\n",
    "\n",
    "        self.stacks = num_stacks\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "        self.projection1 = nn.Conv1d(self.input_len, self.output_len, kernel_size=1, stride=1, bias=False)\n",
    "        if self.single_step_output_One: # only output the N_th timestep.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, 1,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "        else: # output the N timesteps.\n",
    "            if self.stacks == 2:\n",
    "                if self.concat_len:\n",
    "                    self.projection2 = nn.Conv1d(self.concat_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "                else:\n",
    "                    self.projection2 = nn.Conv1d(self.input_len + self.output_len, self.output_len,\n",
    "                                                kernel_size = 1, bias = False)\n",
    "\n",
    "        # For positional encoding\n",
    "        self.pe_hidden_size = input_dim\n",
    "        if self.pe_hidden_size % 2 == 1:\n",
    "            self.pe_hidden_size += 1\n",
    "    \n",
    "        num_timescales = self.pe_hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                max(num_timescales - 1, 1))\n",
    "        temp = torch.arange(num_timescales, dtype=torch.float32)\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "        ### RIN Parameters ###\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "    \n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32, device=x.device)  # tensor([0., 1., 2., 3., 4.], device='cuda:0')\n",
    "        temp1 = position.unsqueeze(1)  # 5 1\n",
    "        temp2 = self.inv_timescales.unsqueeze(0)  # 1 256\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)  # 5 256\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  #[T, C]\n",
    "        signal = F.pad(signal, (0, 0, 0, self.pe_hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.pe_hidden_size)\n",
    "    \n",
    "        return signal\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        x, trend = self.decomp(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            # seq_means = x[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means = x[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # x = x - seq_means\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "            # print('/// RIN ACTIVATED ///\\r',end='')\n",
    "            means2 = trend.mean(1, keepdim=True).detach()\n",
    "            trend = trend - means2\n",
    "            stdev2 = torch.sqrt(torch.var(trend, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            trend /= stdev2\n",
    "            # seq_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.input_len,1).detach()\n",
    "            # pred_means2 = trend[:,-1,:].unsqueeze(1).repeat(1,self.output_len,1).detach()\n",
    "            # trend = trend - seq_means2 \n",
    "            trend = trend * self.affine_weight2 + self.affine_bias2\n",
    "        \n",
    "\n",
    "        if self.pe:\n",
    "            pe = self.get_position_encoding(x)\n",
    "            if pe.shape[2] > x.shape[2]:\n",
    "                x = x + pe[:, :, :-1]\n",
    "            else:\n",
    "                x = x + self.get_position_encoding(x)\n",
    "\n",
    "        ### activated when RIN flag is set ###\n",
    "        \n",
    "\n",
    "        # the first stack\n",
    "        res1 = x\n",
    "        x = self.blocks1(x)\n",
    "        x = self.projection1(x)\n",
    "\n",
    "        trend = trend.permute(0,2,1)\n",
    "        trend = self.trend(trend)  \n",
    "        trend = self.trend_dec(trend).permute(0,2,1)\n",
    "\n",
    "        if self.stacks == 1:\n",
    "            ### reverse RIN ###\n",
    "            if self.RIN:\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                # x = x + pred_means\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend\n",
    "\n",
    "        elif self.stacks == 2:\n",
    "            MidOutPut = x\n",
    "            if self.concat_len:\n",
    "                x = torch.cat((res1[:, -self.concat_len:,:], x), dim=1)\n",
    "            else:\n",
    "                x = torch.cat((res1, x), dim=1)\n",
    "\n",
    "            # the second stack\n",
    "            x = self.blocks2(x)\n",
    "            x = self.projection2(x)\n",
    "            \n",
    "            ### Reverse RIN ###\n",
    "            if self.RIN:\n",
    "                MidOutPut = MidOutPut - self.affine_bias\n",
    "                MidOutPut = MidOutPut / (self.affine_weight + 1e-10)\n",
    "                MidOutPut = MidOutPut * stdev\n",
    "                MidOutPut = MidOutPut + means\n",
    "\n",
    "                x = x - self.affine_bias\n",
    "                x = x / (self.affine_weight + 1e-10)\n",
    "                x = x * stdev\n",
    "                x = x + means\n",
    "\n",
    "                trend = trend - self.affine_bias2\n",
    "                trend = trend / (self.affine_weight2 + 1e-10)\n",
    "                # trend = trend + pred_means2\n",
    "                trend = trend * stdev2\n",
    "                trend = trend + means2\n",
    "\n",
    "            return x + trend, MidOutPut\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    x = Variable(x)\n",
    "    return x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41047c2-5bff-4835-9c26-6f523fbbf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCINet_Model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(SCINet_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # 24,4,1,1,2,0.5,False,1,True,1\n",
    "        window_size = 1 # in (fixed)\n",
    "        horizon = 1      # out\n",
    "        hidden_size = 1\n",
    "        groups = 1\n",
    "        kernel = 1\n",
    "        dropout = 0.5\n",
    "        single_step_output_One = False\n",
    "        num_levels = 1\n",
    "        positionalEcoding = True\n",
    "        num_stacks = 1\n",
    "        self.scinet = SCINet(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        self.scinet_decompose = SCINet_decompose(\n",
    "            output_len = horizon, input_len = window_size, input_dim = input_size, hid_size = hidden_size, \n",
    "            num_stacks = num_stacks, num_levels = num_levels, concat_len = 0, groups = groups, kernel = kernel, \n",
    "            dropout = dropout, single_step_output_One = single_step_output_One, positionalE =  positionalEcoding, \n",
    "            modified = True, RIN = True,\n",
    "        )\n",
    "        \n",
    "        # hidden  = [64, 64, 64]\n",
    "        # dropout = [0.2, 0.5, 0.5]\n",
    "        # num_layers = [1,1,1]\n",
    "        # self.lstm1 = nn.LSTM(\n",
    "        #     input_size=input_size,\n",
    "        #     hidden_size=hidden[0],\n",
    "        #     dropout=dropout[0],\n",
    "        #     num_layers=num_layers[0],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm2 = nn.LSTM(\n",
    "        #     input_size=2*hidden[0],\n",
    "        #     hidden_size=hidden[1],\n",
    "        #     dropout=dropout[1],\n",
    "        #     num_layers=num_layers[1],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        # self.lstm3 = nn.LSTM(\n",
    "        #     input_size=2*hidden[1],\n",
    "        #     hidden_size=hidden[2],\n",
    "        #     dropout=dropout[2],\n",
    "        #     num_layers=num_layers[2],\n",
    "        #     batch_first=True,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bn = nn.BatchNorm1d(24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.fc = nn.Linear(2*hidden[0], 1)\n",
    "        self.fc_1 = nn.Linear(input_size, 16)\n",
    "        self.fc_1 = TimeDistributed(self.fc_1)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        self.fc_2 = TimeDistributed(self.fc_2)\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        \n",
    "        self.nlinear = NLinear(input_size,1)\n",
    "        self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.scinet(x)\n",
    "#         # x = self.bn(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm1(x)\n",
    "#         # x = self.relu(x)\n",
    "#         # x,_ = self.lstm2(x)\n",
    "#         # x = self.selu(x)\n",
    "#         # x,_ = self.lstm3(x)\n",
    "#         # x = self.selu(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leakyrelu(x)\n",
    "#         x = self.fc_2(x[:,-1,:]) # [:,:,-1]\n",
    "        \n",
    "#         # x = self.fc_2(x[:,-1,:])\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.scinet(x)\n",
    "#         x = self.scinet_decompose(x)\n",
    "#         x1,x2 = x[0],x[1]\n",
    "#         x = torch.cat([x1,x2],dim=1)\n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.scinet(x)\n",
    "        x = self.scinet_decompose(x)\n",
    "        # x = self.fc(x[:,-1,:])\n",
    "        x = self.nlinear(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fe908-42f5-43ec-b343-e9d60b7bdce1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341e468-e635-460c-b7f9-ab5475e77b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization-Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len, td=True):\n",
    "        super(NLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
    "        if td:\n",
    "            self.Linear = TimeDistributed(self.Linear)\n",
    "        # Use this line if you want to visualize the weights\n",
    "        self.Linear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "        self._reinitialize()\n",
    "    \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'Linear' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seq_last = x[:,-1:,:].detach()\n",
    "        x = x - seq_last\n",
    "        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = x + seq_last\n",
    "        return x # [Batch, Output length, Channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12749617-0499-43b8-8499-4236f292e471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLinear_Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, input_size):\n",
    "        super(NLinear_Model, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.BatchNorm1d(nodes[1]) , \n",
    "        nodes = [40]*4\n",
    "        dropout = 0.5\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.activation = self.leakyrelu\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            NLinear(seq_len ,nodes[0]), self.dropout, self.activation,\n",
    "            NLinear(nodes[0],nodes[1]), self.dropout, self.activation,\n",
    "            NLinear(nodes[1],nodes[2]), self.dropout, self.activation,\n",
    "            NLinear(nodes[2],nodes[3]), self.dropout, self.activation,\n",
    "            NLinear(nodes[3],pred_len),\n",
    "        )\n",
    "\n",
    "        self.fc   = nn.Linear(input_size,1)\n",
    "        self.fc   = TimeDistributed(self.fc)\n",
    "        # self._reinitialize()\n",
    "\n",
    "        # for name, p in self.named_parameters():\n",
    "        #     print(name, 'scinet' in name)\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.nlinear_1(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.nlinear_2(x)\n",
    "#         # x = self.bn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.gelu(x)\n",
    "        \n",
    "#         x = self.fc(x[:,-1,:])\n",
    "        \n",
    "#         return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train, Validation Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c4697-edc9-4f0c-81d4-3c3aa05eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.EarlyStopping import EarlyStopping\n",
    "\n",
    "inverse_transform_function = np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d419b-7332-4e64-a6e5-68aed8e5b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_fn(output, target):\n",
    "    return torch.sqrt(torch.mean((output-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ee98e-b3ae-4d8e-a18e-8b166686c493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, scheduler, device, \n",
    "    early_stopping, epochs, metric_period=1, best_model_only=True, verbose=True,\n",
    "):\n",
    "    \n",
    "    es = EarlyStopping(patience = CFG['ES_PATIENCE'], verbose = CFG['ES_VERBOSE'], path='./model/checkpoint.pt')\n",
    "    \n",
    "    model.to(device)\n",
    "    # criterion = nn.L1Loss().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            # print(output.shape,Y.shape) # torch.Size([4, 28, 1]) torch.Size([4, 24])\n",
    "            # print(output[:5],Y[:5])\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            # print(output[:5],Y[:5],output.shape,Y.shape)\n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % metric_period == 0:\n",
    "                epoch_str = '0'*(len(str(epochs))-len(str(epoch))) + str(epoch)\n",
    "                progress = '[{}/{}] tr_loss : {:.5f}, val_loss : {:.5f}, elapsed : {:.2f}s, total : {:.2f}s, remaining : {:.2f}s'\\\n",
    "                    .format(\n",
    "                        epoch_str,\n",
    "                        epochs,np.mean(train_loss),\n",
    "                        valid_loss,\n",
    "                        epoch_e-epoch_s,\n",
    "                        epoch_e-start_time,\n",
    "                        (epoch_e-epoch_s)*(epochs-epoch)\n",
    "                    )\n",
    "                epoch_s = time.time()\n",
    "\n",
    "                if best_loss > valid_loss:\n",
    "                    mark = '*'\n",
    "                else:\n",
    "                    mark = ' '\n",
    "            \n",
    "                print(mark+progress)\n",
    "            \n",
    "        if best_model_only:\n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model = model\n",
    "                \n",
    "                path = f'./model/best_model.pt'\n",
    "                torch.save(best_model.state_dict(), path)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if early_stopping:\n",
    "            es(valid_loss, model)\n",
    "\n",
    "            if es.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X).float()\n",
    "            \n",
    "            # # log -> exp\n",
    "            # output = torch.exp(output)\n",
    "            # Y      = torch.exp(Y)\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec6fe-a87d-4fe0-8f91-97136bd96e57",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3e9c6-5ca0-43cf-99d4-4826cedf9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,input,label,infer_mode,seq_length):\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        input = input.sort_values(['case_num','DAT'])\n",
    "        label = label.sort_values(['case_num','DAT'])\n",
    "\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        for i in range(int(label.shape[0]/seq_length)):\n",
    "            i_df = input.iloc[i*seq_length:(i+1)*seq_length,:].drop('case_num',axis=1)\n",
    "            l_df = label.iloc[i*seq_length:(i+1)*seq_length]['predicted_weight_g']\n",
    "            \n",
    "            self.input_list.append(torch.Tensor(i_df.values))\n",
    "            self.label_list.append(torch.Tensor(l_df.values))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data  = self.input_list[index]\n",
    "        label = self.label_list[index]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc703f27-fc08-42ed-b242-4e7764b505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df      = pd.read_csv(f'./out/kf_cat_1/pred_input_df_10.csv').drop('predicted_weight_g',axis=1)\n",
    "test_input_df = pd.read_csv(f'./out/kf_cat_1/pred_test_df_10.csv') .drop('predicted_weight_g',axis=1)\n",
    "\n",
    "input_df.case_num      = ['0'+str(x) if x<10 else str(x) for x in input_df     .case_num]\n",
    "test_input_df.case_num = ['0'+str(x) if x<10 else str(x) for x in test_input_df.case_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adc92f-7d3b-4a78-9898-3ad69b0406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "input_dataset = CustomDataset(input=input_df, label=label_df, infer_mode=False, seq_length=seq_length)\n",
    "input_loader  = DataLoader(input_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "test_dataset = CustomDataset(input=test_input_df, label=test_label_df, infer_mode=True, seq_length=seq_length)\n",
    "test_loader  = DataLoader(test_dataset  , batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132bc04-4a20-43ad-aec5-648acc3fd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_df = pd.concat([input_df,label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)\n",
    "pred_test_df  = pd.concat([test_input_df,test_label_df['predicted_weight_g'].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bd444-65c1-4f0e-9559-75e3d99c0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save_mark = '2'\n",
    "\n",
    "paths = [f'./out/kf_lstm_{save_mark}',f'./out/kf_lstm_{save_mark}_fn']\n",
    "for path in paths:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf5a68-454d-457d-9db5-98056e39363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1시간 (cpu)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "case_num = input_df.case_num.unique()\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "\n",
    "kf_iter = 0\n",
    "for tr_idx,va_idx in tqdm(kf.split(case_num),total=n_splits):\n",
    "    kf_iter+=1\n",
    "    print(f'-'*100)\n",
    "    print(f'({kf_iter}/{n_splits})')\n",
    "    print(f'-'*100)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (1) train validation split\n",
    "    #------------------------------------------------------------------------------------\n",
    "    tr_case_num = case_num[tr_idx]\n",
    "    va_case_num = case_num[va_idx]\n",
    "    \n",
    "    X_train = input_df[input_df.case_num.isin(tr_case_num)]\n",
    "    X_valid = input_df[input_df.case_num.isin(va_case_num)]\n",
    "\n",
    "    y_train = label_df[label_df.case_num.isin(tr_case_num)]\n",
    "    y_valid = label_df[label_df.case_num.isin(va_case_num)]\n",
    "    # print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (2) custom dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    train_dataset = CustomDataset(input=X_train, label=y_train, infer_mode=False, seq_length=seq_length)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "\n",
    "    valid_dataset = CustomDataset(input=X_valid, label=y_valid, infer_mode=False, seq_length=seq_length)\n",
    "    valid_loader  = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers) # CFG['BATCH_SIZE']\n",
    "    \n",
    "    # [(x.size(),y.size()) for x,y in iter(train_loader)]\n",
    "    # [y for x,y in iter(train_loader)]\n",
    "    # sum([y.size(0) for x,y in iter(train_loader)])\n",
    "\n",
    "    # len([x for x,y in iter(train_loader)])\n",
    "\n",
    "    # [(x[0].size(),x[1].size()) for x in train_loader]\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (3) modeling\n",
    "    #------------------------------------------------------------------------------------\n",
    "    seed_everything(CFG['SEED'])\n",
    "\n",
    "    input_size = [np.array(x[0]).shape for x in train_loader][0][2]\n",
    "    model = LSTM_Model(input_size=input_size)\n",
    "    # model = NLinear_Model(seq_len=1,pred_len=1,input_size=input_size)\n",
    "    # model = Model(input_size=input_size)\n",
    "    # model = SCINet_Model(input_size=input_size)\n",
    "    # model = BaseModel(\n",
    "    #     input_size = input_size,\n",
    "    #     hidden_sizes=[400,300],\n",
    "    #     dropout_rates=[0.2,0.2],\n",
    "    #     num_classes=seq_length,\n",
    "    #     num_layers=2,\n",
    "    #     bidirectional=True,\n",
    "    # )\n",
    "\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=100, threshold_mode='abs',min_lr=1e-7, verbose=False)\n",
    "\n",
    "    CFG['ES_PATIENCE'] = 2000\n",
    "    CFG['ES_VERBOSE']  = 0\n",
    "    best_model = train(\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=True,\n",
    "        metric_period=100,\n",
    "        epochs=16000,\n",
    "        best_model_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-1) predict : input dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X,y in iter(input_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_input_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_input_df.to_csv(f'./out/kf_lstm_{save_mark}/pred_input_df_{kf_iter}.csv',index=False)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # (4-2) predict : test dataset\n",
    "    #------------------------------------------------------------------------------------\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    pred_list = []\n",
    "    #true_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader): # train_loader, valid_loader\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            model_pred = best_model(X)\n",
    "            # model_pred = torch.exp(model_pred)\n",
    "\n",
    "            pred_list += model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            #true_list += y         .cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "    pred_test_df[f'pred_{kf_iter}'] = pred_list\n",
    "    pred_test_df.to_csv(f'./out/kf_lstm_{save_mark}/pred_test_df_{kf_iter}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19d5ab-866c-4ce1-b491-b51630e72369",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceade27b-04ab-4d84-ad78-dddf9d5bdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "\n",
    "mse_list = []\n",
    "for case_num in pred_input_df.case_num.unique():\n",
    "    d = pred_input_df[pred_input_df.case_num==case_num]\n",
    "\n",
    "    _mse = mean_squared_error(\n",
    "        d['pred_2'],\n",
    "        #d[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "        d['predicted_weight_g'],\n",
    "    )\n",
    "    mse_list.append([case_num,_mse])\n",
    "\n",
    "    # plt.figure(figsize=(6,4))\n",
    "    # sns.scatterplot(\n",
    "    #     x=d['pred_2'],\n",
    "    #     #x=d[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "    #     y=d['predicted_weight_g'],\n",
    "    # )\n",
    "    # abline(slope=1,intercept=0,color='red')\n",
    "    # plt.title(f'CASE={case_num} : MSE = {_mse:.5f}')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685e738-9cfd-461a-b60c-bedb5dff3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10+1):\n",
    "    _mse = mean_squared_error(\n",
    "        pred_input_df['predicted_weight_g'],\n",
    "        pred_input_df[f'pred_{i}']\n",
    "        # pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1),\n",
    "    )\n",
    "    print(f'pred_{i} :',_mse)\n",
    "    \n",
    "print('all :',mean_squared_error(pred_input_df['predicted_weight_g'], pred_input_df[pred_cols].apply(lambda x: x.mean(),axis=1)))\n",
    "print('2&4 :',mean_squared_error(pred_input_df['predicted_weight_g'], pred_input_df[['pred_2','pred_4']].apply(lambda x: x.mean(),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512188dd-3fa9-469f-9fb2-71ce16393e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(\n",
    "    x=pred_input_df['predicted_weight_g'],\n",
    "    #y=pred_input_df['pred_2'],\n",
    "    y=pred_input_df[['pred_2','pred_4']].apply(lambda x: x.mean(),axis=1),\n",
    ")\n",
    "abline(slope=1,intercept=0,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad6f5f-ef5b-4e5f-b38e-eef2e3143c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mse<10 : y최대가 80정도 이하\n",
    "# # mse>10 : y최대가 80정도인 것도 포함되지만, 높은 것들이 많음\n",
    "# d = pred_input_df[pred_input_df.case_num.isin([case_num for case_num, mse in mse_list if mse>10])]\n",
    "# for case_num in d.case_num.unique():\n",
    "#     dd = d[d.case_num==case_num]\n",
    "#     print(case_num)\n",
    "#     sns.lineplot(x=dd.DAT,y=(dd.pred_2+dd.pred_4)/2)\n",
    "#     sns.lineplot(x=dd.DAT,y=dd.predicted_weight_g)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0964749-d7cd-4210-86c4-eaa690550636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "# pred_cols = ['pred_2','pred_4']\n",
    "pred_input_df['preds'] = pred_input_df[pred_cols].apply(lambda x: x.mean(), axis=1)\n",
    "\n",
    "mean_squared_error(pred_input_df['preds'],pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c11c-ea95-4eeb-bddb-d66fc61c602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "# pred_cols = ['pred_2','pred_4']\n",
    "pred_test_df['predicted_weight_g'] = pred_test_df[pred_cols].apply(lambda x: x.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbc1d9-07b4-45bf-9bb4-1acd0a0ab023",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96f71-2ae7-444e-9437-7bbb251a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_lstm_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_lstm_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_lstm_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545177e-99ce-4047-88c5-f634eba2d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.8433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafea69c-4e00-4499-ab70-f01151caf51c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d317a2-e427-408b-ac40-22903a9232e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_cols = [col for col in pred_input_df.columns if col.find('pred_')>=0]\n",
    "\n",
    "mse_list = []\n",
    "for col in pred_cols:\n",
    "    _mse = mean_squared_error(pred_input_df[col],pred_input_df['predicted_weight_g'])\n",
    "    mse_list.append(_mse)\n",
    "    \n",
    "# weights = [1]*len(mse_list)/sum(mse_list)\n",
    "weights = 1/(mse_list/sum(mse_list))\n",
    "weights = weights / sum(weights)\n",
    "\n",
    "final_pred = weights * pred_input_df[pred_cols]\n",
    "final_pred = final_pred.apply(lambda x: x.sum(),axis=1)\n",
    "\n",
    "mean_squared_error(final_pred,pred_input_df['predicted_weight_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69715bc9-c9af-46fb-8aaf-f3beb082877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = weights * pred_test_df[pred_cols]\n",
    "test_pred = test_pred.apply(lambda x: x.sum(),axis=1)\n",
    "pred_test_df['predicted_weight_g'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49647cd3-313e-46e2-9cbc-10cee9933425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pred_test_df.sort_values(['case_num','DAT'])\n",
    "\n",
    "for case_num in sub.case_num.unique():\n",
    "    s = sub[sub.case_num==case_num][['DAT','predicted_weight_g']]\n",
    "    s.DAT = [i+1 for i in range(28)]\n",
    "    s.to_csv(f'./out/kf_lstm_{save_mark}_fn/TEST_{case_num}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b0cc-fd0c-46b5-9b22-d98b99060717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')\n",
    "os.chdir(f\"./out/kf_lstm_{save_mark}_fn/\")\n",
    "submission = zipfile.ZipFile(f\"../kf_lstm_{save_mark}.zip\", 'w')\n",
    "for path in all_test_label_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()\n",
    "os.chdir('/Users/khj/MyPython/Dacon/6_상추생육환경생성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972264af-354b-4204-a161-12ef5e941910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
