{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f399afe2-31e0-4d6a-bb9f-891a02d1af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khj/anaconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Volumes/KHJ/Github/hyuckjinkim/lib-python')\n",
    "sys.path.append('/Volumes/KHJ/Github/hyuckjinkim/lib-python/torch')\n",
    "from torch_seed import seed_everything\n",
    "from graph import abline, actual_prediction_scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da5c216-2cbf-42d0-82f4-9b253f5ba226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> torch version  : 1.13.1\n",
      "> cuda available : False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "print('> torch version  :',torch.__version__)\n",
    "print('> cuda available :',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab1a6f9-8198-47b1-a514-079b81ee392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jimmy-ai.tistory.com/342\n",
    "# https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path=None, trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            if self.path is not None:\n",
    "                torch.save(model.state_dict(), self.path)\n",
    "                save_message = 'Saving model ...'\n",
    "            else:\n",
    "                save_message = ''\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). {save_message}')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# # https://github.com/pytorch/pytorch/issues/21987\n",
    "# def nanmean(v, *args, inplace=False, **kwargs):\n",
    "#     if not inplace:\n",
    "#         v = v.clone()\n",
    "#     is_nan = torch.isnan(v)\n",
    "#     v[is_nan] = 0\n",
    "#     return v.sum(*args, **kwargs) / (~is_nan).float().sum(*args, **kwargs)\n",
    "\n",
    "# def seq2list_cuda(seq,device):\n",
    "#     nan_value = -99999\n",
    "#     ret_seq = []\n",
    "#     k=0\n",
    "#     N=len(seq)\n",
    "#     for x in seq:\n",
    "#         start_seq = torch.tensor([nan_value]*k).to(device).float()\n",
    "#         end_seq   = torch.tensor([nan_value]*(N-k-1)).to(device).float()\n",
    "#         x = x.to(device)\n",
    "        \n",
    "#         if len(start_seq)==0:\n",
    "#             _seq  = torch.cat([x,end_seq],axis=0)\n",
    "#         elif len(end_seq)==0:\n",
    "#             _seq  = torch.cat([start_seq,x],axis=0)\n",
    "#         else:\n",
    "#             _seq  = torch.cat([start_seq,x,end_seq],axis=0)\n",
    "            \n",
    "#         _seq[_seq==nan_value] = float('nan')\n",
    "#         ret_seq.append(_seq)\n",
    "#         k+=1\n",
    "#     ret_seq = torch.stack(ret_seq,dim=0)\n",
    "#     #print('(1)',ret_seq)\n",
    "#     ret_seq = nanmean(ret_seq,dim=0)\n",
    "#     #print('(2)',ret_seq)\n",
    "#     return ret_seq\n",
    "        \n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, epochs, criterion,\n",
    "    early_stopping=None, device='cpu', scheduler=None, metric_period=1, \n",
    "    verbose=True, print_shape=False, save_model_path = './mc/best_model.pt',\n",
    "    transform_y='identity',\n",
    "):\n",
    "    assert transform_y in ['identity','log','sqrt'], \\\n",
    "        \"transform_y must be one of ['identity','log','sqrt']\"\n",
    "    is_early_stopping = False if early_stopping is None else True\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # great is better\n",
    "    best_loss  = np.inf\n",
    "    best_epoch = 1\n",
    "    best_model = None\n",
    "    is_best    = np.nan\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_s = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).float()\n",
    "            \n",
    "            #Y = seq2list_cuda(Y,device)\n",
    "            #output = seq2list_cuda(output,device)\n",
    "            \n",
    "            if transform_y=='log':\n",
    "                output = torch.exp(output)\n",
    "                Y      = torch.exp(Y)\n",
    "            elif transform_y=='sqrt':\n",
    "                output = output**2\n",
    "                Y      = Y**2\n",
    "                \n",
    "            if print_shape:\n",
    "                    if epoch==1:\n",
    "                        print(output.shape,Y.shape) # torch.Size([16, 1]) torch.Size([16, 1])\n",
    "                        print(output[:2],Y[:2])\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            #loss = torch.sqrt(loss) # MSE -> RMSE\n",
    "            \n",
    "            loss.backward() # Getting gradients\n",
    "            optimizer.step() # Updating parameters\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        valid_loss = validation(model, valid_loader, criterion, device, transform_y)\n",
    "\n",
    "        epoch_e = time.time()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        # update the best epoch & best loss\n",
    "        if (best_loss > valid_loss) | (epoch==1):\n",
    "            best_epoch = epoch\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            is_best = 1\n",
    "            torch.save(best_model.state_dict(), save_model_path)\n",
    "        else:\n",
    "            is_best = 0\n",
    "            \n",
    "        # 결과물 printing\n",
    "        if (verbose) & (epoch % metric_period == 0):\n",
    "            mark = '*' if is_best else ' '\n",
    "            epoch_str = str(epoch).zfill(len(str(epochs)))\n",
    "            progress = '{}[{}/{}] tr_loss: {:.5f}, val_loss: {:.5f}, best_epoch: {}, elapsed: {:.2f}s, total: {:.2f}s, remaining: {:.2f}s'\\\n",
    "                .format(\n",
    "                    mark,\n",
    "                    epoch_str,\n",
    "                    epochs,\n",
    "                    np.mean(train_loss),\n",
    "                    valid_loss,\n",
    "                    best_epoch,\n",
    "                    epoch_e-epoch_s,\n",
    "                    epoch_e-start_time,\n",
    "                    (epoch_e-epoch_s)*(epochs-epoch)/metric_period,\n",
    "                )\n",
    "            epoch_s = time.time()\n",
    "            print(progress)\n",
    "\n",
    "        # early stopping 여부를 체크. 현재 과적합 상황 추적\n",
    "        if is_early_stopping:\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation(model, valid_loader, criterion, device, transform_y):\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "            \n",
    "            output = model(X).float()\n",
    "            \n",
    "            #Y = seq2list_cuda(Y,device)\n",
    "            #output = seq2list_cuda(output,device)\n",
    "            \n",
    "            if transform_y=='log':\n",
    "                output = torch.exp(output)\n",
    "                Y      = torch.exp(Y)\n",
    "            elif transform_y=='sqrt':\n",
    "                output = output**2\n",
    "                Y      = Y**2\n",
    "            \n",
    "            loss = criterion(output, Y)\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(valid_loss)\n",
    "\n",
    "def predict(best_model,loader,device,transform_y):\n",
    "    best_model.to(device)\n",
    "    \n",
    "    true_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for data,label in iter(loader):\n",
    "            data = data.float().to(device)\n",
    "\n",
    "            output = best_model(data).cpu().numpy().tolist()\n",
    "            label  = label.cpu().numpy().tolist()\n",
    "\n",
    "            if transform_y=='log':\n",
    "                output = np.exp(output).tolist()\n",
    "                label  = np.exp(label).tolist()\n",
    "            elif transform_y=='sqrt':\n",
    "                output = np.square(output).tolist()\n",
    "                label  = np.square(label).tolist()\n",
    "\n",
    "            true_list += label\n",
    "            pred_list += output\n",
    "\n",
    "    return true_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a96bb1-06f5-446e-ac7c-637b4bec3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4e4415-714b-48a9-9289-40f3ac2f2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    TARGET = 'ECLO'\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9095df64-e0fd-479e-bc20-d0fbdcd988b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('./out/train_data_identity.parquet')\n",
    "test_df  = pd.read_parquet('./out/test_data_identity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a875af0-6f78-4351-b19e-03e47c710319",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(CFG.TARGET,axis=1)\n",
    "y = train_df[CFG.TARGET]\n",
    "X_test = test_df.copy()\n",
    "\n",
    "unique_info = X.nunique()\n",
    "unique_cols = unique_info[unique_info==1].index.tolist()\n",
    "\n",
    "if len(unique_cols)>0:\n",
    "    X     .drop(unique_cols,axis=1,inplace=True)\n",
    "    X_test.drop(unique_cols,axis=1,inplace=True)\n",
    "    print(f'delete unique columns: {len(unique_cols)}\\ndetail: {unique_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7a19e6-19d6-4448-9db6-4660f8944a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=CFG.SEED,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994d6341-8dff-4c63-a8a0-1af026bdcffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train.values, dtype=torch.float32),\n",
    "    torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val.values, dtype=torch.float32),\n",
    "    torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1),\n",
    ")\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS)\n",
    "val_loader    = DataLoader(val_dataset  , batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f43bd9de-0008-4eaa-ada6-e713a987b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_sizes,dropout_rate):\n",
    "        super(DNN,self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n",
    "        self.hidden_layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)])\n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.bn = nn.ModuleList([nn.BatchNorm1d(hidden_sizes[i]) for i in range(len(hidden_sizes))])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for linear in self.hidden_layers:\n",
    "            x = linear(x)\n",
    "            #x = self.bn[i](x)\n",
    "            x = self.activation(x)\n",
    "            #x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        #x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a89d5679-5458-4d2d-84b5-d541e8c98f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size,dropout_rate):\n",
    "        super(MLP,self).__init__()\n",
    "        #self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        #self.dropout = nn.Dropout(dropout_rate)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size , hidden_size), self.activation, #self.dropout,\n",
    "            nn.Linear(hidden_size, hidden_size), self.activation, #self.dropout,\n",
    "            nn.Linear(hidden_size, hidden_size), self.activation, #self.dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b5eafb2-1b4c-4e60-959c-1ce4ba21859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        #pred, actual = F.relu(pred), F.relu(actual)\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0003b4a9-da30-4dc4-97fb-2d0648711ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "epochs = 256\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b28b755f-f439-4ed1-be95-51f483a1c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "hidden_sizes = [16,32,64,16]\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = DNN(input_size,output_size,hidden_sizes,dropout_rate)\n",
    "# model = MLP(input_size,output_size,hidden_sizes[0],dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8e219b1-b964-413c-9f11-4e0327b6d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = [x for x,y in train_loader][0]\n",
    "# y1 = [y for x,y in train_loader][0]\n",
    "# yhat = model(x1).float()\n",
    "# criterion(yhat,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8092f46f-ef66-4ccf-a860-f34bde7a409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_everything(CFG.SEED)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e28cfc3-b089-4ae1-ae72-1aae533d5a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[001/256] tr_loss: 15.83321, val_loss: 10.13698, best_epoch: 1, elapsed: 0.64s, total: 0.64s, remaining: 161.97s\n",
      " [002/256] tr_loss: 11.82596, val_loss: 10.74293, best_epoch: 1, elapsed: 0.62s, total: 1.26s, remaining: 157.92s\n",
      "*[003/256] tr_loss: 11.08065, val_loss: 9.96707, best_epoch: 3, elapsed: 0.64s, total: 1.90s, remaining: 161.84s\n",
      " [004/256] tr_loss: 10.78447, val_loss: 9.98263, best_epoch: 3, elapsed: 0.66s, total: 2.56s, remaining: 165.23s\n",
      " [005/256] tr_loss: 10.61302, val_loss: 10.20703, best_epoch: 3, elapsed: 0.65s, total: 3.21s, remaining: 163.00s\n",
      " [006/256] tr_loss: 10.54430, val_loss: 10.24987, best_epoch: 3, elapsed: 0.65s, total: 3.86s, remaining: 162.08s\n",
      " [007/256] tr_loss: 10.56029, val_loss: 9.99673, best_epoch: 3, elapsed: 0.65s, total: 4.50s, remaining: 161.11s\n",
      " [008/256] tr_loss: 10.48056, val_loss: 10.18651, best_epoch: 3, elapsed: 0.66s, total: 5.16s, remaining: 163.85s\n",
      " [009/256] tr_loss: 10.50578, val_loss: 10.04334, best_epoch: 3, elapsed: 0.65s, total: 5.81s, remaining: 160.27s\n",
      "*[010/256] tr_loss: 10.51258, val_loss: 9.96083, best_epoch: 10, elapsed: 0.65s, total: 6.46s, remaining: 159.92s\n",
      " [011/256] tr_loss: 10.49378, val_loss: 10.06505, best_epoch: 10, elapsed: 0.65s, total: 7.12s, remaining: 160.39s\n",
      "*[012/256] tr_loss: 10.47687, val_loss: 9.90401, best_epoch: 12, elapsed: 0.66s, total: 7.79s, remaining: 162.19s\n",
      " [013/256] tr_loss: 10.48988, val_loss: 10.01131, best_epoch: 12, elapsed: 0.65s, total: 8.44s, remaining: 157.65s\n",
      " [014/256] tr_loss: 10.44058, val_loss: 9.98035, best_epoch: 12, elapsed: 0.64s, total: 9.08s, remaining: 154.86s\n",
      " [015/256] tr_loss: 10.48302, val_loss: 9.90730, best_epoch: 12, elapsed: 0.65s, total: 9.72s, remaining: 155.49s\n",
      " [016/256] tr_loss: 10.45088, val_loss: 9.99585, best_epoch: 12, elapsed: 0.64s, total: 10.37s, remaining: 154.76s\n",
      " [017/256] tr_loss: 10.43994, val_loss: 9.93481, best_epoch: 12, elapsed: 0.64s, total: 11.01s, remaining: 152.46s\n",
      " [018/256] tr_loss: 10.42669, val_loss: 9.93916, best_epoch: 12, elapsed: 0.64s, total: 11.65s, remaining: 153.04s\n",
      " [019/256] tr_loss: 10.41137, val_loss: 9.93847, best_epoch: 12, elapsed: 0.65s, total: 12.30s, remaining: 153.93s\n",
      " [020/256] tr_loss: 10.44289, val_loss: 9.99866, best_epoch: 12, elapsed: 0.65s, total: 12.95s, remaining: 153.29s\n",
      " [021/256] tr_loss: 10.42213, val_loss: 9.92251, best_epoch: 12, elapsed: 0.65s, total: 13.60s, remaining: 152.62s\n",
      " [022/256] tr_loss: 10.40816, val_loss: 9.95987, best_epoch: 12, elapsed: 0.65s, total: 14.25s, remaining: 151.52s\n",
      " [023/256] tr_loss: 10.42613, val_loss: 9.92262, best_epoch: 12, elapsed: 0.66s, total: 14.91s, remaining: 154.50s\n",
      " [024/256] tr_loss: 10.42194, val_loss: 10.16067, best_epoch: 12, elapsed: 0.65s, total: 15.56s, remaining: 151.41s\n",
      " [025/256] tr_loss: 10.40476, val_loss: 9.95168, best_epoch: 12, elapsed: 0.65s, total: 16.21s, remaining: 150.40s\n",
      " [026/256] tr_loss: 10.41416, val_loss: 9.92930, best_epoch: 12, elapsed: 0.65s, total: 16.87s, remaining: 150.14s\n",
      " [027/256] tr_loss: 10.40013, val_loss: 9.92166, best_epoch: 12, elapsed: 0.66s, total: 17.53s, remaining: 151.23s\n",
      " [028/256] tr_loss: 10.40093, val_loss: 9.97864, best_epoch: 12, elapsed: 0.66s, total: 18.18s, remaining: 149.74s\n",
      "*[029/256] tr_loss: 10.40678, val_loss: 9.88997, best_epoch: 29, elapsed: 0.66s, total: 18.84s, remaining: 149.54s\n",
      "*[030/256] tr_loss: 10.38656, val_loss: 9.88541, best_epoch: 30, elapsed: 0.65s, total: 19.50s, remaining: 147.70s\n",
      " [031/256] tr_loss: 10.37737, val_loss: 9.95601, best_epoch: 30, elapsed: 0.65s, total: 20.16s, remaining: 147.23s\n",
      " [032/256] tr_loss: 10.38219, val_loss: 10.05146, best_epoch: 30, elapsed: 0.66s, total: 20.81s, remaining: 147.16s\n",
      " [033/256] tr_loss: 10.38324, val_loss: 9.93115, best_epoch: 30, elapsed: 0.66s, total: 21.48s, remaining: 147.66s\n",
      " [034/256] tr_loss: 10.39256, val_loss: 9.95998, best_epoch: 30, elapsed: 0.66s, total: 22.13s, remaining: 146.02s\n",
      " [035/256] tr_loss: 10.37692, val_loss: 10.09911, best_epoch: 30, elapsed: 0.66s, total: 22.79s, remaining: 145.41s\n",
      " [036/256] tr_loss: 10.38093, val_loss: 9.94517, best_epoch: 30, elapsed: 0.66s, total: 23.45s, remaining: 145.14s\n",
      " [037/256] tr_loss: 10.39353, val_loss: 9.99802, best_epoch: 30, elapsed: 0.67s, total: 24.13s, remaining: 147.57s\n",
      " [038/256] tr_loss: 10.37944, val_loss: 9.92735, best_epoch: 30, elapsed: 0.67s, total: 24.79s, remaining: 145.18s\n",
      " [039/256] tr_loss: 10.37132, val_loss: 9.90427, best_epoch: 30, elapsed: 0.66s, total: 25.45s, remaining: 142.67s\n",
      " [040/256] tr_loss: 10.43196, val_loss: 9.92197, best_epoch: 30, elapsed: 0.66s, total: 26.11s, remaining: 142.09s\n",
      " [041/256] tr_loss: 10.38451, val_loss: 9.99722, best_epoch: 30, elapsed: 0.65s, total: 26.76s, remaining: 140.45s\n",
      " [042/256] tr_loss: 10.40039, val_loss: 9.92061, best_epoch: 30, elapsed: 0.65s, total: 27.41s, remaining: 139.93s\n",
      " [043/256] tr_loss: 10.37240, val_loss: 9.90761, best_epoch: 30, elapsed: 0.66s, total: 28.07s, remaining: 139.59s\n",
      " [044/256] tr_loss: 10.66698, val_loss: 9.89293, best_epoch: 30, elapsed: 0.65s, total: 28.72s, remaining: 137.73s\n",
      " [045/256] tr_loss: 10.36629, val_loss: 9.98347, best_epoch: 30, elapsed: 0.65s, total: 29.37s, remaining: 138.02s\n",
      " [046/256] tr_loss: 10.37652, val_loss: 9.98921, best_epoch: 30, elapsed: 0.65s, total: 30.03s, remaining: 137.24s\n",
      " [047/256] tr_loss: 10.38981, val_loss: 10.03549, best_epoch: 30, elapsed: 0.69s, total: 30.71s, remaining: 143.65s\n",
      " [048/256] tr_loss: 10.36383, val_loss: 9.99174, best_epoch: 30, elapsed: 0.66s, total: 31.38s, remaining: 138.05s\n",
      " [049/256] tr_loss: 10.35431, val_loss: 9.91967, best_epoch: 30, elapsed: 0.65s, total: 32.03s, remaining: 134.29s\n",
      " [050/256] tr_loss: 10.37252, val_loss: 9.91682, best_epoch: 30, elapsed: 0.66s, total: 32.68s, remaining: 135.36s\n",
      " [051/256] tr_loss: 10.39097, val_loss: 9.95110, best_epoch: 30, elapsed: 0.67s, total: 33.36s, remaining: 137.75s\n",
      " [052/256] tr_loss: 10.36963, val_loss: 9.93869, best_epoch: 30, elapsed: 0.66s, total: 34.02s, remaining: 134.79s\n",
      " [053/256] tr_loss: 10.36940, val_loss: 9.92413, best_epoch: 30, elapsed: 0.66s, total: 34.68s, remaining: 133.70s\n",
      " [054/256] tr_loss: 10.39017, val_loss: 10.07247, best_epoch: 30, elapsed: 0.68s, total: 35.36s, remaining: 137.44s\n",
      " [055/256] tr_loss: 10.36306, val_loss: 9.93962, best_epoch: 30, elapsed: 0.69s, total: 36.04s, remaining: 138.12s\n"
     ]
    }
   ],
   "source": [
    "# criterion = RMSLELoss().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-2, momentum=0.9)\n",
    "scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-7, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=epochs//10,verbose=False,path=None)\n",
    "# early_stopping = None\n",
    "\n",
    "best_model = train(\n",
    "    model, optimizer, train_loader, val_loader, epochs, criterion,\n",
    "    early_stopping, device, scheduler,\n",
    "    metric_period=1,\n",
    "    verbose=True,\n",
    "    print_shape=False,\n",
    "    save_model_path = './mc/best_model.pt',\n",
    "    transform_y='identity',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee79b9-0663-49d6-be40-769a0cfc9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = MLP(input_size,output_size,hidden_sizes[0],dropout_rate)\n",
    "# best_model.load_state_dict(torch.load('./mc/best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c8fb5-1c12-4872-baef-3bef034c225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true,pred = predict(best_model,val_loader,device,'identity')\n",
    "RMSLELoss()(torch.tensor(true),torch.tensor(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83520665-015a-449a-9d22-52ed9ba5e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_prediction_scatterplot(np.array(true).flatten(),np.array(pred).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412e2f9-cb50-400f-ade1-e7a4e56b23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_3.10.13",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
